Paper URL,Title,Author,Email,Confidence
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Lei Li,LI-LEI@USTC.EDU,95%
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Zhongfeng Kang,KANGZHF@GMAIL.COM,78%
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Wenhan Zhang,WENHANZHANG430@GMAIL.COM,95%
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Tianfang Zhang,,0%
https://arxiv.org/pdf/2302.00117.pdf,Real Estate Property Valuation using Self-Supervised Vision Transformers,Mahdieh Yazdani,,0%
https://arxiv.org/pdf/2302.00117.pdf,Real Estate Property Valuation using Self-Supervised Vision Transformers,Maziar Raissi,,0%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Ching-yao Chuang,cychuang@mit.edu,82%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Antonio Torralba,torralba@mit.edu,78%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Yuanzhen Li,yzli@google.com,82%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Varun Jampani,varunjampani@google.com,95%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Stefanie Jegelka,,0%
https://arxiv.org/pdf/2302.00059.pdf,NASiam: Efficient Representation Learning using Neural Architecture Search for Siamese Networks,Hedi Tabia,hedi.tabia@univ-evry.fr,95%
https://arxiv.org/pdf/2302.00059.pdf,NASiam: Efficient Representation Learning using Neural Architecture Search for Siamese Networks,Hichem Arioui,hichem.arioui@univ-evry.fr,95%
https://arxiv.org/pdf/2302.00059.pdf,NASiam: Efficient Representation Learning using Neural Architecture Search for Siamese Networks,Alexandre Heuillet,alexandre.heuillet@univ-evry.fr,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Hongbin Zha,zha@cis.pku.edu.cn,78%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Hao Zhao,hao.zhao@intel.com,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Guyue Zhou,zhouguyue@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Pengfei Li,li-pf22@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Yurong Chen,yurong.chen@intel.com,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Xiaoxue Chen,chenxx21@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Huan-ang Gao,,0%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Beiwen Tian,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,N. Joseph Tatro,joseph.tatro@str.us,78%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Brandon B. May,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Dylan Walker,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Piyush Kumar,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Nathan Shnidman,,0%
https://arxiv.org/pdf/2301.13823.pdf,Grounding Language Models to Images for Multimodal Inputs and Outputs,Jing Yu Koh,jingyuk@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.13823.pdf,Grounding Language Models to Images for Multimodal Inputs and Outputs,Ruslan Salakhutdinov,,0%
https://arxiv.org/pdf/2301.13823.pdf,Grounding Language Models to Images for Multimodal Inputs and Outputs,Daniel Fried,,0%
https://arxiv.org/pdf/2301.13838.pdf,Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression,Martha Larson,m.larson@cs.ru.nl,82%
https://arxiv.org/pdf/2301.13838.pdf,Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression,Zhuoran Liu,z.liu@cs.ru.nl,82%
https://arxiv.org/pdf/2301.13838.pdf,Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression,Zhengyu Zhao,zhengyu.zhao@cispa.de,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Yael Vinker,yael.vinker@mail.huji.ac.il,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Hila Chefer,hilach70@gmail.com,85%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Lior Wolf,liorwolf@gmail.com,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Yuval Alaluf,yuvalalaluf@gmail.com,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Deepak K. Gupta,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Gowreesh Mago,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Arnav Chavan,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Dilip K. Prasad,,0%
https://arxiv.org/pdf/2301.13809.pdf,A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control,Mahesh Raveendranatha Panicker,mahesh@iitpkd.ac.in,85%
https://arxiv.org/pdf/2301.13809.pdf,A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control,Ayush Singh,,0%
https://arxiv.org/pdf/2301.13809.pdf,A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control,Pisharody Harikrishnan Gopalkrishnan,,0%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Dongxiao Zhu,dzhu@wayne.edu,82%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Chengyin Li,cyli@wayne.edu,82%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Prashant Khanduri,khanduri.prashant@wayne.edu,95%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Yao Qiang,yao@wayne.edu,85%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Daniel Capellán-martín,daniel.capellan@upm.es,85%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Elisa López-varela,mj.ledesma@upm.es,60%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Juan J. Gómez-valverde,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Ramon Sanchez-jacob,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,David Bermejo-peláez,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Lara García-delgado,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Maria J. Ledesma-carbayo,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Zihao Wang,zihao.wang@ieee.org,95%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Yingyu Yang,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Maxime Sermesant,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Hervé Delingette,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Ona Wu,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Chun Yuan,yuanc@sz.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Dachuan Shi,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Chaofan Tao,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Ying Jin,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Zhendong Yang,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Jiaqi Wang,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Samuel Hurault,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Antonin Chambolle,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Arthur Leclaire,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Nicolas Papadakis,,0%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Tao Yang,wang-yuwang@mail.tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Nanning Zheng,nnzheng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Yuwang Wang,,0%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Yan Lv,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Eva Schnider,eva.schnider@unibas.ch,95%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Julia Wolleb,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Antal Huck,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Mireille Toranelli,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Georg Rauter,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Magdalena Müller-gerbl,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Philippe C. Cattin,,0%
https://arxiv.org/pdf/2301.13670.pdf,What Makes Good Examples for Visual In-Context Learning?,Ziwei Liu,ziwei.liu@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.13670.pdf,What Makes Good Examples for Visual In-Context Learning?,Yuanhan Zhang,,0%
https://arxiv.org/pdf/2301.13670.pdf,What Makes Good Examples for Visual In-Context Learning?,Kaiyang Zhou,,0%
https://arxiv.org/pdf/2301.13659.pdf,Spyker: High-performance Library for Spiking Deep Neural Networks,Shahriar Rezghi Shirsavar,shahriar.rezghi@ut.ac.ir,85%
https://arxiv.org/pdf/2301.13659.pdf,Spyker: High-performance Library for Spiking Deep Neural Networks,Mohammad-reza A. Dehaqani,dehaqani@ut.ac.ir,78%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Raphael Sulzer,raphaelsulzer@gmx.de,95%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Renaud Marlet,,0%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Bruno Vallet,,0%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Loic Landrieu,,0%
https://arxiv.org/pdf/2301.13622.pdf,Learning Data Representations with Joint Diffusion Models,Kamil Deja,kamil.deja@pw.edu.pl,95%
https://arxiv.org/pdf/2301.13622.pdf,Learning Data Representations with Joint Diffusion Models,Tomasz Trzcinski,tomasz.trzcinski@pw.edu.pl,95%
https://arxiv.org/pdf/2301.13622.pdf,Learning Data Representations with Joint Diffusion Models,Jakub M. Tomczak,j.m.tomczak@tue.nl,82%
https://arxiv.org/pdf/2301.13592.pdf,Priors are Powerful: Improving a Transformer for Multi-camera 3D Detection with 2D Priors,Di Feng,fengdi1015@gmail.com,95%
https://arxiv.org/pdf/2301.13592.pdf,Priors are Powerful: Improving a Transformer for Multi-camera 3D Detection with 2D Priors,Francesco Ferroni,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Boris Mansencal,mediaeval.sport.task@diff.u-bordeaux.fr,70%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Pierre-etienne Martin,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Jordan Calandre,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Jenny Benois-pineau,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Renaud Péteri,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Laurent Mascarilla,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Julien Morlier,,0%
https://arxiv.org/pdf/2301.13569.pdf,NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning,Thomas Lukasiewicz,thomas.lukasiewicz@cs.ox.ac.uk,95%
https://arxiv.org/pdf/2301.13569.pdf,NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning,Xiaolin Hu,xlhu@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.13569.pdf,NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning,Jianfeng Wang,jianfeng.wang@cs.ox.ac.uk,95%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Hang Su,suhangss@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Liyuan Wang,,0%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Xingxing Zhang,,0%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Jun Zhu,,0%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Federico Tombar,tombari@in.tum.de,78%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Artem Savkin,artem.savkin@tum.de,95%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Yida Wang,,0%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Sebastian Wirkert,,0%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Nassir Navab,,0%
https://arxiv.org/pdf/2301.13554.pdf,NoiseTransfer: Image Noise Generation with Contrastive Embeddings,Seunghwan Lee,seunghwanlee@hanyang.ac.kr,95%
https://arxiv.org/pdf/2301.13554.pdf,NoiseTransfer: Image Noise Generation with Contrastive Embeddings,Tae Hyun Kim,taehyunkim@hanyang.ac.kr,95%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Michał Grochowski,michal.grochowski@pg.edu.pl,82%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Zuzanna Klawikowska,zuzanna.klawikowska@pg.edu.pl,95%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Maria Ferlin,maria.ferlin@pg.edu.pl,95%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Edyta Szurowska,eszurowska@gumed.edu.pl,82%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Małgorzata Grzywińska,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Jun Li,lijuncst@njnu.edu.cn,95%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Guang Yang,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Yin Tang,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Jianhua Xu,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Xili Wan,,0%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Ofir Lindenbaum,ofirlin@gmail.com,85%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Amit Rozner,,0%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Barak Battash,,0%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Lior Wolf,,0%
https://arxiv.org/pdf/2301.13514.pdf,Fourier Sensitivity and Regularization of Computer Vision Models,Chuan-sheng Foo,foo_chuan_sheng@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.13514.pdf,Fourier Sensitivity and Regularization of Computer Vision Models,Kiran Krishnamachari,kirank@u.nus.edu,85%
https://arxiv.org/pdf/2301.13514.pdf,Fourier Sensitivity and Regularization of Computer Vision Models,See-kiong Ng,seekiong@nus.edu.sg,95%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Siyu Zhu,siting.zsy@alibaba-inc.com,60%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Weihao Yuan,,0%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Xiaodong Gu,,0%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Heng Li,,0%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Zilong Dong,,0%
https://arxiv.org/pdf/2301.13504.pdf,Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease,Maha M. Alwuthaynani,maha.alwuthaynani@bristol.ac.uk,95%
https://arxiv.org/pdf/2301.13504.pdf,Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease,Zahraa S. Abdallah,zahraa.abdallah@bristol.ac.uk,95%
https://arxiv.org/pdf/2301.13504.pdf,Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease,Raul Santos-rodriguez,,0%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Zhiyuan Cheng,cheng443@purdue.edu,78%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Xiangyu Zhang,xyzhang@cs.purdue.edu,82%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Guanhong Tao,taog@purdue.edu,78%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Dongfang Liu,dongfang.liu@rit.edu,95%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,James Liang,,0%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Swagat Kumar,kumars@edgehill.ac.uk,78%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Darshita Jain,,0%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Anima Majumder,,0%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Samrat Dutta,,0%
https://arxiv.org/pdf/2301.13459.pdf,Learning Generalized Hybrid Proximity Representation for Image Recognition,Anca Ralescu,ralescal@ucmail.uc.edu,65%
https://arxiv.org/pdf/2301.13459.pdf,Learning Generalized Hybrid Proximity Representation for Image Recognition,Zhiyuan Li,li3z3@mail.uc.edu,78%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Asef Islam,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Anthony Wu,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Jay Mandavilli,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Wojtek Zbijewski,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Jeff Siewerdsen,,0%
https://arxiv.org/pdf/2301.13445.pdf,A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics,Naveed Akhtar,naveed.akhtar@uwa.edu.au,95%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Min-kook Choi,mkchoi@hutom.io,82%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Seungbum Hong,,0%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Jihun Yoon,,0%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Bogyu Park,,0%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Jinzheng He,jinzhenghe@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Zhenhui Ye,zhenhuiye@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Jinglin Liu,jinglinliu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Ziyue Jiang,jiangziyue@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Zhou Zhao,zhaozhou@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Yi Ren,ren.yi@bytedance.com,95%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Yonggang Li,liyonggang@zjxu.edu.cn,95%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Xiangbin Zhu,zhuxb@zjnu.cn,78%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Yingjian Li,liyingjian@zjnu.edu.cn,95%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Yuqi Chen,,0%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Haojie Fang,,0%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Jingtao Li,JingtaoLi@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Hengwei Zhao,whu_zhaohw@whu.edu.cn,78%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Yanfei Zhong,zhongyanfei@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Xinyu Wang,wangxinyu@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Shaoyu Wang,wangshaoyu@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jiayi Yuan,jiayiyuan@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jianjun Qian,csjqian@njust.edu.cn,78%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jun Li,junli@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Xiang Li,xiang.li.implus@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jian Yang,jiang.hao.bo@njust.edu.cn,85%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Haobo Jiang,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Chong Wang,chong.wang@adelaide.edu.au,95%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Yuanhong Chen,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Yuyuan Liu,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Michael Elliott,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Chun Fung Kwok,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Carlos Pena-solorzano,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Yu Tian,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Fengbei Liu,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Helen Frazer,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Davis J. Mccarthy,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Gustavo Carneiro,,0%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jiayi Yuan,jiayiyuan@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jianjun Qian,csjqian@njust.edu.cn,78%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jun Li,junli@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Xiang Li,xiang.li.implus@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jian Yang,jiang.hao.bo@njust.edu.cn,85%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Haobo Jiang,,0%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Ke Yan,kerwinyan@tencent.com,95%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Jiaming Han,hanjiaming@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Yuqiang Ren,condiren@tencent.com,78%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Gui-song Xia,guisong.xia@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Jian Ding,jian.ding@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Mohsen Dorodchi,mdorodch@uncc.edu,90%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Ayman Ali,,0%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Ekkasit Pinyoanuntapong,,0%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Pu Wang,,0%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Bingchuan Li,libingchuan@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Qian He,heqian@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Peng Zhang,zhangpeng.ucas@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Wei Liu,liuwei.jikun@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Tianxiang Ma,matianxiang.724@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Miao Hua,huamiao@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Zili Yi,yizili@bytedance.com,95%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Chung-i Huang,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Wei-yu Chen,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Wei Jan Ko,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Jih-sheng Chang,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Chen-kai Sun,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Hui Hung Yu,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Fang-pang Lin,,0%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Minwoo Lee,minwoo.lee@uncc.edu,95%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Zhi Sun,qzhisun@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Ekkasit Pinyoanuntapong,epinyoan@uncc.edu,90%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Chen Chen,chen.chen@crcv.ucf.edu,95%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Pu Wang,pwang13@uncc.edu,82%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Ayman Ali,aali26@uncc.edu,82%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Kalvik Jakkala,kjakkala@uncc.edu,82%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Qucheng Peng,qucheng.peng@knights.ucf.edu,95%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Li Yi,jli3779@uwo.ca,85%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Boyu Wang,bwang@csd.uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Pengcheng Xu,pxu67@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Ruizhi Pu,rpu2@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,A. Ian Mcleod,aimcleod@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Charles Ling,charles.ling@uwo.ca,95%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Gezheng Xu,gxu86@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Jiaqi Li,,0%
https://arxiv.org/pdf/2301.13376.pdf,Quantized Neural Networks for Low-Precision Accumulation with Guaranteed Overflow Avoidance,Ian Colbert,ian.colbert@amd.com,95%
https://arxiv.org/pdf/2301.13376.pdf,Quantized Neural Networks for Low-Precision Accumulation with Guaranteed Overflow Avoidance,Alessandro Pappalardo,,0%
https://arxiv.org/pdf/2301.13376.pdf,Quantized Neural Networks for Low-Precision Accumulation with Guaranteed Overflow Avoidance,Jakoba Petri-koenig,,0%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Hamed Hassani,hassani@seas.upenn.edu,82%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Xinmeng Huang,xinmengh@sas.upenn.edu,85%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Behrad Moniri,bemoniri@seas.upenn.edu,82%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Edgar Dobriban,dobriban@wharton.upenn.edu,78%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Donghwan Lee,,0%
https://arxiv.org/pdf/2301.13366.pdf,CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects,Murray Loew,loew@gwu.edu,78%
https://arxiv.org/pdf/2301.13366.pdf,CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects,Shuyue Guan,frankshuyueguan@gwu.edu,95%
https://arxiv.org/pdf/2301.13366.pdf,CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects,Ange Lou,ange.lou@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Yi Zhu,zhuyi36@huawei.com,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Jianzhuang Liu,liu.jianzhuang@huawei.com,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Pengzhen Ren,pzhren@foxmail.com,82%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Guangrun Wang,wanggrun@gmail.com,78%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Changlin Li,changlinli.ai@gmail.com,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Xiaodan Liang,xdliang328@gmail.com,82%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Xiaojun Chang,xiaojun.chang@uts.edu.au,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Hang Xu,xu.hang@huawei.com,95%
https://arxiv.org/pdf/2301.13361.pdf,Iterative Loop Method Combining Active and Semi-Supervised Learning for Domain Adaptive Semantic Segmentation,Xue Yuan,xyuan@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.13361.pdf,Iterative Loop Method Combining Active and Semi-Supervised Learning for Domain Adaptive Semantic Segmentation,Licong Guan,,0%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Mohsen Dorodchi,mdorodch@uncc.edu,90%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Ayman Ali,,0%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Ekkasit Pinyoanuntapong,,0%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Pu Wang,,0%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Jinbao Wang,jasoncjwang@tencent.com,82%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Yaochu Jin,jinyaochu@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Jiaqi Liu,chaosliu@tencent.com,78%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Jiayi Lyu,lyujiayi21@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Feng Zheng,f.zheng@ieee.org,82%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Guoyang Xie,guoyang.xie@ieee.org,95%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Yong Liu,,0%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,Hu Chen,huchen@scu.edu.cn,95%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,Yi Zhang,yzhang@scu.edu.cn,82%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,Wenchao Du,wenchaodu.scu@gmail.com,95%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,H. Yang,yanghongyu@scu.edu.cn,78%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Hugo Lemarchant,hugo@is.ids.osaka-u.ac.jp,85%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Yuta Nakashima,n-yuta@ids.osaka-u.ac.jp,85%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Hajime Nagahara,nagahara@ids.osaka-u.ac.jp,78%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Yiming Qian,yimingqian@ids.osaka-u.ac.jp,95%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Liangzi Li,li@ids.osaka-u.ac.jp,82%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Jun Sakuma,jun@cs.tsukuba.ac.jp,85%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Rei Sato,reisato@bbo.cs.tsukuba.ac.jp,95%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Kazuto Fukuchi,fukuchi@cs.tsukuba.ac.jp,78%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Youhei Akimoto,akimoto@cs.tsukuba.ac.jp,78%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,David Van Dijk,david.vandijk@yale.edu,95%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,Antonio H. De O. Fonseca,,0%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,Emanuele Zappala,,0%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,Josue Ortega Caro,,0%
https://arxiv.org/pdf/2301.13335.pdf,Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Jian Zhu,jianzhu@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.13335.pdf,Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Miaojing Shi,mshi@tongji.edu.cn,82%
https://arxiv.org/pdf/2301.13335.pdf,Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Hanli Wang,hanliwang@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Deepika Bablani,deepika.bablani@ibm.com,95%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Jeffrey L. Mckinstry,,0%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Steven K. Esser,,0%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Rathinakumar Appuswamy,,0%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Dharmendra S. Modha,,0%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Klaus H. Maier-hein,klaus.maier-hein@dkfz-heidelberg.de,95%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Karol Gotkowski,karol.gotkowski@dkfz.de,95%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Shuvam Gupta,c.guimaraes-da-silva-tochtrop@hzdr.de,85%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Jose R. A. Godinho,j.godinho@hzdr.de,82%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Fabian Isensee,f.isensee@dkfz-heidelberg.de,82%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Camila G. S. Tochtrop,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Erik Wijmans,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Manolis Savva,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Irfan Essa,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Stefan Lee,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Ari S. Morcos,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Dhruv Batra,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Travis Driver,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Kento Tomita,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Koki Ho,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Panagiotis Tsiotras,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Haonan Chang,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Dhruv Metha Ramesh,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Shijie Geng,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Yuqiu Gan,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Abdeslam Boularias,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Yan Zhang,yan@cyan.zone,85%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,David W. Zhang,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Simon Lacoste-julien,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Gertjan J. Burghouts,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Cees G. M. Snoek,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Fuzhao Xue,xuefuzhao24@gmail.com,95%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Mostafa Dehghani,dehghani@google.com,78%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Valerii Likhosherstov,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Anurag Arnab,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Neil Houlsby,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Yang You,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Yiran Zhong,zhongyiran@gmail.com,95%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Meng Wang,eric.mengwang@gmail.com,95%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jinxing Zhou,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Xuyang Shen,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jianyuan Wang,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jiayi Zhang,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Weixuan Sun,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jing Zhang,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Stan Birchfield,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Dan Guo,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Lingpeng Kong,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Nicholas Carlini,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Jamie Hayes,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Milad Nasr,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Matthew Jagielski,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Vikash Sehwag,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Florian Tramèr,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Borja Balle,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Daphne Ippolito,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Eric Wallace,,0%
https://arxiv.org/pdf/2301.13186.pdf,Accurate Gaze Estimation using an Active-gaze Morphable Model,Hao Sun,,0%
https://arxiv.org/pdf/2301.13186.pdf,Accurate Gaze Estimation using an Active-gaze Morphable Model,Nick Pears,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Yao-chih Lee,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Ji-ze Genevieve Jang,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Yi-ting Chen,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Elizabeth Qiu,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Jia-bin Huang,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Xin Eric Wang,xwang366@ucsc.edu,82%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Kaiwen Zhou,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Kaizhi Zheng,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Connor Pryor,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Yilin Shen,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Hongxia Jin,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Lise Getoor,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Li Zhang,lizhangfd@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Qiang Wan,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Zilong Huang,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Jiachen Lu,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Gang Yu,,0%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Hong-yu Zhou,whuzhouhongyu@gmail.com,95%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Chenyu Lian,cylian@stu.xmu.edu.cn,82%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Liansheng Wang,lswang@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Duaa Alsaeed,dalsaeed@ksu.edu.sa,82%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Remy Peyret,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Fouad Khelifi,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Nadia Al-ghreimil,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Heyam Al-baity,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Ahmed Bouridane,,0%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Talha Qaiser,talha.qaiser@warwick.ac.uk,95%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Shan E Ahmed Raza,shan.raza@warwick.ac.uk,95%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Raja Muhammad Saad Bashir,saad.bashir@warwick.ac.uk,78%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Nasir M. Rajpoot,n.m.rajpoot@warwick.ac.uk,82%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Nicolas Nerrienet,nicolas.n@primaalab.com,85%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Rémy Peyret,,0%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Marie Sockeel,,0%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Stéphane Sockeel,,0%
https://arxiv.org/pdf/2301.13104.pdf,Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models,Florian A. Hölzl,florian.hoelzl@tum.de,85%
https://arxiv.org/pdf/2301.13104.pdf,Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models,Georgios Kaissis,g.kaissis@tum.de,82%
https://arxiv.org/pdf/2301.13104.pdf,Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models,Daniel Rueckert,daniel.rueckert@tum.de,95%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Mengyun Qiao,m.qiao21@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Shuo Wang,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Huaqi Qiu,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Antonio De Marvao,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Declan P. O'regan,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Daniel Rueckert,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Wenjia Bai,,0%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Zhanhao Hu,huzhanha17@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Xiaolin Hu,xlhu@mail.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Wei Zhang,zhang-w19@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Xiao Li,lixiao20@mails.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Yining Liu,,0%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Bo Zhang,,0%
https://arxiv.org/pdf/2301.13090.pdf,Action Capsules: Human Skeleton Action Recognition,Hamid D. Taghirad,taghirad@kntu.ac.ir,78%
https://arxiv.org/pdf/2301.13090.pdf,Action Capsules: Human Skeleton Action Recognition,Ali Farajzadeh Bavil,,0%
https://arxiv.org/pdf/2301.13090.pdf,Action Capsules: Human Skeleton Action Recognition,Hamed Damirchi,,0%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yisheng Yuan,y.yuan@hw.ac.uk,82%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Zhang Luo,luozhang@seafogai.com,95%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yang Xu,yang.xu@sdsu.edu,95%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yue Wang,wang.yue.f07@kyoto-u.jp,95%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Wei Pang,w.pang@hw.ac.uk,82%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Zuhao Yang,,0%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Huajun Bai,,0%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yingfang Yuan,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Yinfei Yang,feiy@apple.com,90%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Bowen Zhang,zhang4@apple.com,78%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Chen Chen,chen999@apple.com,95%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Liangliang Cao,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Jiguang Shen,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Tom Gunter,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Albin Madappally Jose,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Alexander Toshev,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Jonathon Shlens,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Ruoming Pang,,0%
https://arxiv.org/pdf/2301.13018.pdf,DELTA: degradation-free fully test-time adaptation,Shu-tao Xia,xiast@sz.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13018.pdf,DELTA: degradation-free fully test-time adaptation,Chen Chen,chen1634chen@gmail.com,95%
https://arxiv.org/pdf/2301.13018.pdf,DELTA: degradation-free fully test-time adaptation,Bowen Zhao,,0%
https://arxiv.org/pdf/2301.12995.pdf,FedFA: Federated Feature Augmentation,Ender Konukoglu,kender@vision.ee.ethz.ch,85%
https://arxiv.org/pdf/2301.12995.pdf,FedFA: Federated Feature Augmentation,Tianfei Zhou,tiazhou@vision.ee.ethz.ch,82%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Florian Stimberg,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Ayan Chakrabarti,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Chun-ta Lu,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Hussein Hazimeh,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Otilia Stretcu,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Wei Qiao,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Yintao Liu,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Merve Kaya,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Cyrus Rashtchian,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Ariel Fuxman,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Mehmet Tek,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Sven Gowal,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Sunghwan Yoo,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Yeongjeong Jeong,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Maryam Jameela,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Gunho Sohn,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Ming Tao,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Bing-kun Bao,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Hao Tang,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Changsheng Xu,,0%
https://arxiv.org/pdf/2301.13648.pdf,CSDN: Combing Shallow and Deep Networks for Accurate Real-time Segmentation of High-definition Intravascular Ultrasound Images,Feng Yang,yangf@smu.edu.cn,78%
https://arxiv.org/pdf/2301.13648.pdf,CSDN: Combing Shallow and Deep Networks for Accurate Real-time Segmentation of High-definition Intravascular Ultrasound Images,Shaofeng Yuan,shaofeng.yuan.smu@gmail.com,95%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Nikhil S. Narayan,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Shashanka B. R.,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Rohit Damodaran,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Chandrashekhar Jayaram,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,M. A. Kareem,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Mamta P.,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Saravanan K. R.,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Monu Krishnan,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Raja Indana,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Alexandros Kalimeris,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Ioannis Psarros,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Giorgos Giannopoulos,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Manolis Terrovitis,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,George Papastefanatos,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Gregory Kotsis,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Shengming Li,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Luping Liu,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Runnan Li,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Xu Tan,,0%
https://arxiv.org/pdf/2301.12914.pdf,PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks,Alexandros Iosifidis,ai@ece.au.dk,90%
https://arxiv.org/pdf/2301.12914.pdf,PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks,Arian Bakhtiarnia,arianbakh@ece.au.dk,85%
https://arxiv.org/pdf/2301.12914.pdf,PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks,Qi Zhang,qz@ece.au.dk,90%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Xinyin Ma,maxinyin@u.nus.edu,95%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Xinchao Wang,xinchao@nus.edu.sg,85%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Gongfan Fang,gongfan@u.nus.edu,85%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Mingli Song,,0%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Michael Bi Mi,,0%
https://arxiv.org/pdf/2301.12891.pdf,Half of an image is enough for quality assessment,Junyong You,,0%
https://arxiv.org/pdf/2301.12891.pdf,Half of an image is enough for quality assessment,Yuan Lin,,0%
https://arxiv.org/pdf/2301.12891.pdf,Half of an image is enough for quality assessment,Jari Korhonen,,0%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Shiqi Wang,shiqwang@cityu.edu.hk,82%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Anderson Rocha,anderson.rocha@ic.unicamp.br,95%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Haoliang Li,haoliang.li@cityu.edu.hk,95%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Chenqi Kong,cqkong2-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Yibing Liu,lyibing112@gmail.com,85%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Kexin Zheng,kzhengaj@connect.ust.hk,82%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Marc Gebauer,gebaumar@b-tu.de,75%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Toni Schneidereit,schneton@b-tu.de,60%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Slavomira Schneidereit,,0%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Ashkan Mansouri Yarahmadi,,0%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Michael Breuß,,0%
https://arxiv.org/pdf/2301.12799.pdf,Eye Image-based Algorithms to Estimate Percentage Closure of Eye and Saccadic Ratio for Alertness Detection,Supratim Gupta,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Huazhu Fu,hzfu@ieee.org,82%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Yong Liu,liuyong@ihpc.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Meng Wang,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Kai Yu,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Chun-mei Feng,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Yiming Qian,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Ke Zou,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Lianyu Wang,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Rick Siow Mong Goh,,0%
https://arxiv.org/pdf/2301.12796.pdf,Rendering the Directional TSDF for Tracking and Multi-Sensor Registration with Point-To-Plane Scale ICP,Malte Splietker,,0%
https://arxiv.org/pdf/2301.12796.pdf,Rendering the Directional TSDF for Tracking and Multi-Sensor Registration with Point-To-Plane Scale ICP,Sven Behnke,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Muhammad Al-barham,muhammadal-barham@ieee.org,95%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Adham Alsharkawi,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Musa Al-yaman,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Mohammad Al-fetyani,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Ashraf Elnagar,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Ahmad Abu Saaleek,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Mohammad Al-odat,,0%
https://arxiv.org/pdf/2301.12744.pdf,PointSmile: Point Self-supervised Learning via Curriculum Mutual Information,Songcan Chen,s.chen@nuaa.edu.cn,82%
https://arxiv.org/pdf/2301.12744.pdf,PointSmile: Point Self-supervised Learning via Curriculum Mutual Information,Mingqiang Wei,mingqiang.wei@gmail.com,95%
https://arxiv.org/pdf/2301.12744.pdf,PointSmile: Point Self-supervised Learning via Curriculum Mutual Information,Xin Li,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Nan Li,chuanqil@sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Weijie Lv,lvweijie@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Xuan Xia,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Xing He,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Chuanqi Liu,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Ning Ding,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Penghao Jiang,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Xin Ke,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Zifeng Wang,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Chunxi Li,,0%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Younggun Cho,yg.cho@inha.ac.kr,82%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Myung-hwan Jeon,myunghwan.jeon@snu.ac.kr,95%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Dong-guw Lee,,0%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Ayoung Kim,,0%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Linning Xu,linningxu@ie.cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Yuwei Guo,guoyuwei@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Libiao Jin,libiao@cuc.edu.cn,85%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Anyi Rao,anyirao@stanford.edu,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Bo Dai,daibo@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Lei Yang,yanglei@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Xuekun Jiang,jiangxuekun@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Naoki Murata,naoki.murata@sony.com,95%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Koichi Saito,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Chieh-hsin Lai,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Yuhta Takida,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Toshimitsu Uesaka,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Yuki Mitsufuji,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Stefano Ermon,,0%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,M. Sohel Rahman,msrahman@cse.buet.ac.bd,82%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,Mohimenul Kabir,,0%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,Jaiaid Mobin,,0%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,Ahmad Hassanat,,0%
https://arxiv.org/pdf/2301.12667.pdf,NeSyFOLD: Neurosymbolic Framework for Interpretable Image Classification,Parth Padalkar,parth.padalkar@utdallas.edu,95%
https://arxiv.org/pdf/2301.12667.pdf,NeSyFOLD: Neurosymbolic Framework for Interpretable Image Classification,Huaduo Wang,,0%
https://arxiv.org/pdf/2301.12667.pdf,NeSyFOLD: Neurosymbolic Framework for Interpretable Image Classification,Gopal Gupta,,0%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Lijian Lin,ljlin@stu.xmu.edu.cn,82%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Jin Ma,majin01@mail.ustc.edu.cn,95%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Yizhen Chen,grayyzchen@tencent.com,78%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Zhongang Qi,zhongangqi@tencent.com,95%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Ying Shan,yingsshan@tencent.com,95%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Jie Wang,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Yabin Zhang,csybzhang@comp.polyu.edu.hk,78%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Bin Deng,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Ruihuang Li,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Kui Jia,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Lei Zhang,,0%
https://arxiv.org/pdf/2301.12637.pdf,Lateralized Learning for Multi-Class Visual Classification Tasks,Abubakar Siddique,,0%
https://arxiv.org/pdf/2301.12637.pdf,Lateralized Learning for Multi-Class Visual Classification Tasks,Will N. Browne,,0%
https://arxiv.org/pdf/2301.12637.pdf,Lateralized Learning for Multi-Class Visual Classification Tasks,Gina M. Grimshaw,,0%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Curtis Langlotz,langlotz@stanford.edu,78%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Akshay Chaudhari,akshaysc@stanford.edu,85%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Rogier Van Der Sluijs,sluijs@stanford.edu,78%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Daniel Rubin,dlrubin@stanford.edu,82%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Nandita Bhaskhar,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Gunnar A. Sigurdsson,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Jesse Thomason,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Gaurav S. Sukhatme,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Robinson Piramuthu,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Bingbing Ni,nibingbing@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Xiaoyang Huang,huangxiaoyang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Yanjun Wang,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Yang Liu,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Wenjun Zhang,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Jinxian Liu,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Teng Li,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Junnan Li,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Dongxu Li,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Silvio Savarese,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Steven Hoi,,0%
https://arxiv.org/pdf/2301.12592.pdf,Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition,Ross Greer,regreer@ucsd.edu,82%
https://arxiv.org/pdf/2301.12592.pdf,Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.12589.pdf,Confidence-Aware Calibration and Scoring Functions for Curriculum Learning,Shuang Ao,,0%
https://arxiv.org/pdf/2301.12589.pdf,Confidence-Aware Calibration and Scoring Functions for Curriculum Learning,Stefan Rueger,,0%
https://arxiv.org/pdf/2301.12589.pdf,Confidence-Aware Calibration and Scoring Functions for Curriculum Learning,Advaith Siddharthan,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Bahare Samadi,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Maxime Raison,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Philippe Mahaudens,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Christine Detrembleur,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Sofiane Achiche,,0%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Yatong Bai,yatong_bai@berkeley.edu,95%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Somayeh Sojoudi,sojoudi@berkeley.edu,82%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Aerin Kim,aerinykim@gmail.com,95%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Brendon G. Anderson,bganderson@berkeley.edu,82%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Matt Fredrikson,mfredrik@cs.cmu.edu,90%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Klas Leino,kleino@cs.cmu.edu,82%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Zifan Wang,zifan@safe.ai,85%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Kai Hu,kaihu@andrew.cmu.edu,95%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Andy Zou,andyzou@cmu.edu,95%
https://arxiv.org/pdf/2301.12541.pdf,Supervised and Contrastive Self-Supervised In-Domain Representation Learning for Dense Prediction Problems in Remote Sensing,Ali Ghanbarzade,,0%
https://arxiv.org/pdf/2301.12541.pdf,Supervised and Contrastive Self-Supervised In-Domain Representation Learning for Dense Prediction Problems in Remote Sensing,Hossein Soleimani,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Yiming Zhou,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Callen Macphee,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Madhuri Suthar,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Bahram Jalali,,0%
https://arxiv.org/pdf/2301.12527.pdf,"Diverse, Difficult, and Odd Instances (D2O): A New Test Set for Object Classification",Ali Borji,aliborji@gmail.com,95%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Jin Fang,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Dingfu Zhou,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Jingjing Zhao,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Chenming Wu,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Chulin Tang,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Cheng-zhong Xu,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Liangjun Zhang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Yangguang Li,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Bin Huang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Zeren Chen,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Yufeng Cui,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Feng Liang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Mingzhu Shen,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Fenggang Liu,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Enze Xie,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Lu Sheng,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Wanli Ouyang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Jing Shao,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Danyang Hou,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Liang Pang,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Yanyan Lan,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Huawei Shen,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Xueqi Cheng,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Isaac Osei Agyemang,ioagyemang@std.uestc.edu.cn,82%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Isaac Adjei Mensah,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Sophyani Banaamwini Yussif,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Fiasam Linda Delali,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Bernard Cobinnah Mawuli,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Bless Lord Y. Agbley,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Collins Sey,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Joshua Berkohd,,0%
https://arxiv.org/pdf/2301.12459.pdf,The Influences of Color and Shape Features in Visual Contrastive Learning,Xiaoqi Zhuang,xiaoqizhuang@outlook.com,95%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Wenjie Ruan,w.ruan@exeter.ac.uk,82%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Peipei Xu,peipei.xu@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Xiaowei Huang,xiaowei.huang@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Fu Wang,,0%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Qixiang Ye,qxye@ucas.ac.cn,82%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Jiahan Li,han.li@cumt.edu.cn,78%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Qiong Wu,qiong@stu.xmu.edu.cn,85%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Pingyang Dai,pydai@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Liujuan Cao,caoliujuan@xmu.edu.cn,95%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Rongrong Ji,rrji@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Yongjian Wu,,0%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Yi Cheng,cheng yi@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Ying Sun,suny@i2r.a-star.edu.sg,78%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Qianli Xu,qxu@i2r.a-star.edu.sg,82%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Hao Xuan Woon,haoxuan.woon@u.nus.edu,95%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Dongyun Lin,lin dongyun@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Fen Fang,fang fen@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Beier Zhu,beier002@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Hanwang Zhang,hanwangzhang@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Saeil Lee,saeil.lee@hmgics.com,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Yulei Niu,yn.yuleiniu@gmail.com,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Minhoe Hur,minhoe.hur@hyundai.com,95%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Ming Tang,tangm@nlpr.ia.ac.cn,78%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Xiaomei Zhang,xiaomei.zhang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Xiangyu Zhu,xiangyu.zhu.chen@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Zhen Lei,zlei@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Kan Ren,kan.ren@microsoft.com,95%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Ziyue Li,litzy0619owned@gmail.com,78%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Yifan Yang,,0%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Xinyang Jiang,,0%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Yuqing Yang,,0%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Dongsheng Li,,0%
https://arxiv.org/pdf/2301.12356.pdf,Exploiting High Performance Spiking Neural Networks with Efficient Spiking Patterns,Yi Zeng,yi.zeng@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12356.pdf,Exploiting High Performance Spiking Neural Networks with Efficient Spiking Patterns,Guobin Shen,shenguobin2021@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12356.pdf,Exploiting High Performance Spiking Neural Networks with Efficient Spiking Patterns,Dongcheng Zhao,zhaodongcheng2016@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Jialin Yuan,yuanjial@oregonstate.edu,78%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Hung Nguyen,nguyehu5@oregonstate.edu,85%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Jay Patravali,patravaj@oregonstate.edu,65%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Li Fuxin,lif@oregonstate.edu,85%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Chanho Kim,kimchanh@oregonstate.edu,78%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Weihua Zhou,whzhou@mtu.edu,82%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Ni Yao,niceday1987@hotmail.com,85%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Yanhui Tian,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Daniel Gama Das Neves,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Chen Zhao,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Claudio Tinoco Mesquita,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Wolney De Andrade Martins,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Alair Augusto Sarmet Moreira Damas Dos Santos,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Yanting Li,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Chuang Han,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Fubao Zhu,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Neng Dai,,0%
https://arxiv.org/pdf/2301.12334.pdf,Don't Play Favorites: Minority Guidance for Diffusion Models,Suhyeon Lee,suhyeon.lee@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12334.pdf,Don't Play Favorites: Minority Guidance for Diffusion Models,Soobin Um,sum@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.12334.pdf,Don't Play Favorites: Minority Guidance for Diffusion Models,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Peng Qiao,pengqiao@nudt.edu.cn,95%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Sidun Liu,,0%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Tao Sun,,0%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Ke Yang,,0%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Yong Dou,,0%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Zeba Karishma,zebakarishma@gmail.com,95%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Jian Wu,jwu@cs.odu.edu,82%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Shaurya Rohatgi,,0%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Kavya Shrinivas Puranik,,0%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,C. Lee Giles,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jieneng Chen,jienengchen01@gmail.com,95%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Zaiyi Liu,zyliu@163.com,82%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yingda Xia,yingda.xia@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jiawen Yao,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Ke Yan,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jianpeng Zhang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Le Lu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Fakai Wang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Bo Zhou,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Mingyan Qiu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Qihang Yu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Mingze Yuan,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Wei Fang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yuxing Tang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Minfeng Xu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jian Zhou,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yuqian Zhao,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Qifeng Wang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Xianghua Ye,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Xiaoli Yin,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yu Shi,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Xin Chen,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jingren Zhou,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Alan Yuille,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Ling Zhang,,0%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Bartosz Zieliński,bartosz.zielinski@uj.edu.pl,95%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Jacek Tabor,lukasz.struski;jacek.tabor;bartosz.zielinski@uj.edu.pl,95%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Dawid Rymarczyk,dawid.rymarczyk@doctoral.uj.edu.pl,95%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Mikołaj Sacha,,0%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Łukasz Struski,,0%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Sonia Moshfeghi,smoshfeghi@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Jiannan Zhai,jzhai@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,David Newman,dnewma@fau.edu,90%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Kwangsoo Yang,yangk@fau.edu,78%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Monica Rosselli,mrossell@fau.edu,90%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Borko Furht,bfurht@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Jinwoo Jang,jangj@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Ruth Tappen,rtappen@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Muhammad Tanveer Jan,mjan2021@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Joshua William Conniff,fau.jconniff@health.fau.edu,78%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Xiaoyu Wang,fanghuaxue@gmail.com,65%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Nannan Wang,nnwang@xidian.edu.cn,82%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Ruoyu Zhao,royzhao@stu.xidian.edu.cn,82%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Mingrui Zhu,mrzhu@xidian.edu.cn,82%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Manuel Brack,brack@cs.tu-darmstadt.de,78%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Felix Friedrich,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Dominik Hintersdorf,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Lukas Struppek,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Patrick Schramowski,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Kristian Kersting,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Jingkuan Song,jingkuan.song@gmail.com,95%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Xu Luo,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Hao Wu,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Ji Zhang,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Lianli Gao,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Jing Xu,,0%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Junyou Wang,wangjunyou@stu.scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Jianwei Zhang,zhangjianwei@stu.scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Lei Zhang,leizhang@scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Xin Wei,weixin@stu.scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Jiaqi Li,lijiaqicd@gmail.com,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Wenjie Liu,liuwj@stu.scu.edu.cn,78%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Xian Jiang,,0%
https://arxiv.org/pdf/2301.12176.pdf,Neural Gas Network Image Features and Segmentation for Brain Tumor Detection Using Magnetic Resonance Imaging Data,S. Muhammad Hossein Mousavi,mosavi.a.i.buali@gmail.com,85%
https://arxiv.org/pdf/2301.12171.pdf,ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12171.pdf,ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts,Yujin Oh,yujin.oh@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12171.pdf,ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts,Kwanyoung Kim,,0%
https://arxiv.org/pdf/2301.12168.pdf,"Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits",Matteo Matteucci,matteo.matteucci@polimi.it,95%
https://arxiv.org/pdf/2301.12168.pdf,"Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits",Simone Sarti,simone.sarti@mail.polimi.it,95%
https://arxiv.org/pdf/2301.12168.pdf,"Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits",Eugenio Lomurno,eugenio.lomurno@polimi.it,95%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Jianqiang Wang,,0%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Dandan Ding,,0%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Hao Chen,,0%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Zhan Ma,,0%
https://arxiv.org/pdf/2301.12159.pdf,ClusterFuG: Clustering Fully connected Graphs by Multicut,Ahmed Abbas,ahmed.abbas@mpi-inf.mpg.de,95%
https://arxiv.org/pdf/2301.12159.pdf,ClusterFuG: Clustering Fully connected Graphs by Multicut,Paul Swoboda,,0%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Aibin Huang,huangaibin@hdu.edu.cn,95%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Yuanqi Chang,yuanqichang@hdu.edu.cn,95%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Jiawei Mao,jiaweima0@hdu.edu.cn,85%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Binling Nie,binlingnie@hdu.edu.cn,95%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Xuesong Yin,yinxs@hdu.edu.cn,78%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Rui Xu,,0%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Xiaoya Yang,yangxiaoya@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Dongxv Liu,liudongxv@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Pu Cao,caopu@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Tianrui Huang,huangtianrui@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Lu Yang,,0%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Qing Song,,0%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Gim Hee Lee,gimhee.lee@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Jianming Li,jianming.li@ninebot.com,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Shu Song,songshu0905@gmail.com,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Yu Chen,chenyu@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Tianning Yu,tianning.yu@rlm.segway.com,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Zihao Yu,yuzihao@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.12093.pdf,Local Contrast and Global Contextual Information Make Infrared Small Object Salient Again,Chenyi Wang,Nanjing@qq.com,55%
https://arxiv.org/pdf/2301.12093.pdf,Local Contrast and Global Contextual Information Make Infrared Small Object Salient Again,Huan Wang,,0%
https://arxiv.org/pdf/2301.12093.pdf,Local Contrast and Global Contextual Information Make Infrared Small Object Salient Again,Peiwen Pan,,0%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Feng Zheng,zhengf@sustech.edu.cn,78%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Guoyang Xie,guoyang.xie@surrey.ac.uk,95%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Yaochu Jin,yaochu.jin@uni-bielefeld.de,95%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Jiaqi Liu,liujq32021@mail.sustech.edu.cn,78%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Jinbao Wang,,0%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Mingyu Xu,xumingyu2021@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Zheng Lian,lianzheng2016@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Lei Feng,,0%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Bin Liu,,0%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Jianhua Tao,,0%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Lia Coleman,liac@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Peter Schaldenbrand,pschalde@andrew.cmu.edu,90%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Youeun Shin,youeuns@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Jihie Kim,jihie.kim@dgu.edu,95%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Jean Oh,jeanoh@cmu.edu,95%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Zhixuan Liu,zhixuan2@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Youngsik Yun,youngsiy@andrew.cmu.edu,75%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Beverley-claire Okogwu,bokogwu@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Lav R. Varshney,varshney@illinois.edu,78%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Moulik Choraria,moulikc2@illinois.edu,85%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Ibtihal Ferwana,iferwna2@illinois.edu,65%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Ankur Mani,amani@umn.edu,82%
https://arxiv.org/pdf/2301.12058.pdf,Aerial Image Object Detection With Vision Transformer Detector (ViTDet),Liya Wang,,0%
https://arxiv.org/pdf/2301.12058.pdf,Aerial Image Object Detection With Vision Transformer Detector (ViTDet),Alex Tien,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Kaijie Zhao,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Haitao Zhao,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Zhongze Wang,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Jingchao Peng,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Zhengwei Hu,,0%
https://arxiv.org/pdf/2301.12053.pdf,Weakly Supervised Image Segmentation Beyond Tight Bounding Box Annotations,Bin Xia,b.xia@sibionics.com,82%
https://arxiv.org/pdf/2301.12053.pdf,Weakly Supervised Image Segmentation Beyond Tight Bounding Box Annotations,Juan Wang,wangjuan313@gmail.com,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yue Bai,bai.yue@northeastern.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yun Fu,yunfu@ece.neu.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yi Xu,xu.yi@northeastern.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Xu Ma,ma.xu1@northeastern.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Can Qin,qin.ca@northeastern.edu,78%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yizhou Wang,,0%
https://arxiv.org/pdf/2301.12046.pdf,Semantic Adversarial Attacks on Face Recognition through Significant Attributes,Yasmeen M. Khedr,yasmeenkhedr@hust.edu.cn,95%
https://arxiv.org/pdf/2301.12046.pdf,Semantic Adversarial Attacks on Face Recognition through Significant Attributes,Yifeng Xiong,xiongyf@hust.edu.cn,78%
https://arxiv.org/pdf/2301.12046.pdf,Semantic Adversarial Attacks on Face Recognition through Significant Attributes,Kun He,,0%
https://arxiv.org/pdf/2301.12032.pdf,BinaryVQA: A Versatile Test Set to Evaluate the Out-of-Distribution Generalization of VQA Models,Ali Borji,aliborji@gmail.com,95%
https://arxiv.org/pdf/2301.12025.pdf,Cross-Architectural Positive Pairs improve the effectiveness of Self-Supervised Learning,Pranav Singh,,0%
https://arxiv.org/pdf/2301.12025.pdf,Cross-Architectural Positive Pairs improve the effectiveness of Self-Supervised Learning,Jacopo Cirrone,,0%
https://arxiv.org/pdf/2301.12006.pdf,Improved knowledge distillation by utilizing backward pass knowledge in neural networks,Aref Jafari,aref.jafari@uwaterloo.ca,95%
https://arxiv.org/pdf/2301.12006.pdf,Improved knowledge distillation by utilizing backward pass knowledge in neural networks,Mehdi Rezagholizadeh,mehdi.rezagholizadeh@huawei.com,95%
https://arxiv.org/pdf/2301.12006.pdf,Improved knowledge distillation by utilizing backward pass knowledge in neural networks,Ali Ghodsi,ali.ghodsi@uwaterloo.ca,95%
https://arxiv.org/pdf/2301.12003.pdf,Minimizing Trajectory Curvature of ODE-based Generative Models,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12003.pdf,Minimizing Trajectory Curvature of ODE-based Generative Models,Sangyun Lee,,0%
https://arxiv.org/pdf/2301.12003.pdf,Minimizing Trajectory Curvature of ODE-based Generative Models,Beomsu Kim,,0%
https://arxiv.org/pdf/2301.11990.pdf,Alignment with human representations supports robust few-shot learning,Ilia Sucholutsky,,0%
https://arxiv.org/pdf/2301.11990.pdf,Alignment with human representations supports robust few-shot learning,Thomas L. Griffiths,,0%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Hamid Hassanpour,h.hassanpour@shahroodut.ac.ir,82%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Soroush Hashemifar,hashemifar_soroush@cmps2.iust.ac.ir,95%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Javad Hassannataj Joloudari,javad.hassannataj@birjand.ac.ir,85%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Abdolreza Marefat,rzamarefat@gmail.com,78%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Jie Zhu,zhujie@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Jiyang Qi,jiyangqi@hust.edu.cn,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Mingyu Ding,myding@berkeley.edu,82%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Ping Luo,pluo@cs.hku.hk,82%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Jingdong Wang,wangjingdong@outlook.com,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Leye Wang,leyewang@pku.edu.cn,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Wenyu Liu,liuwy@hust.edu.cn,78%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Xiaokang Chen,,0%
https://arxiv.org/pdf/2301.11752.pdf,Inter-View Depth Consistency Testing in Depth Difference Subspace,Markus Flierl,markus.ﬂierl@kth.se,95%
https://arxiv.org/pdf/2301.11752.pdf,Inter-View Depth Consistency Testing in Depth Difference Subspace,Pravin Kumar Rana,pravin.rana@tobii.com,95%
https://arxiv.org/pdf/2301.11892.pdf,Streaming LifeLong Learning With Any-Time Inference,Vinay Kumar Verma,vinaykumar.verma@duke.edu,95%
https://arxiv.org/pdf/2301.11892.pdf,Streaming LifeLong Learning With Any-Time Inference,Soumya Banerjee,soumyab@cse.iitk.ac.in,85%
https://arxiv.org/pdf/2301.11892.pdf,Streaming LifeLong Learning With Any-Time Inference,Vinay P. Namboodiri,,0%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Yan Yan,yyan34@iit.edu,95%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Bin Duan,bduan2@hawk.iit.edu,82%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Keshav Bhandari,,0%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Gaowen Liu,,0%
https://arxiv.org/pdf/2301.11843.pdf,Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking,Oana Cocarascu,oana.cocarascu@kcl.ac.uk,95%
https://arxiv.org/pdf/2301.11843.pdf,Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking,Elena Simperl,elena.simperl@kcl.ac.uk,95%
https://arxiv.org/pdf/2301.11843.pdf,Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking,Mubashara Akhtar,mubashara.akhtar@kcl.ac.uk,95%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Erdi Kara,erdikara@spelman.edu,95%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,George Zhang,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Joseph J. Williams,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Gonzalo Ferrandez-quinto,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Leviticus J. Rhoden,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Maximilian Kim,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,J. Nathan Kutz,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Aminur Rahman,,0%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Zahra Arjmandi,zahraarj@yorku.com,85%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Yujia Zhang,zhang89@yorku.ca,78%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Mostafa Ahmadi,ahmadism@yorku.ca,78%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Amin Alizadeh Naeini,naeini@yorku.ca,78%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Gunho Sohn,gsohn@yorku.ca,82%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Mohammad Moein Sheikholeslami,,0%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,Floran De Putter,f.a.m.d.putter@tue.nl,82%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,David Van Son,d.v.son@tue.nl,82%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,Sebastian Vogel,sebastian.vogel@nxp.com,95%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,Henk Corporaal,h.corporaal@tue.nl,82%
https://arxiv.org/pdf/2301.11806.pdf,PCV: A Point Cloud-Based Network Verifier,Matthew B. Dwyer,matthewbdwyer@virginia.edu,95%
https://arxiv.org/pdf/2301.11806.pdf,PCV: A Point Cloud-Based Network Verifier,Arup Kumar Sarker,,0%
https://arxiv.org/pdf/2301.11806.pdf,PCV: A Point Cloud-Based Network Verifier,Farzana Yasmin Ahmad,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Sumukh Aithal,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Anirudh Goyal,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Alex Lamb,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Yoshua Bengio,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Michael Mozer,,0%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Jiajing Zhang,zhangjj83@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Yifeng Guo,guoyf25@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Zhifan Gao,gaozhifan@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Tieyong Zeng,zeng@math.cuhk.edu.hk,78%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.11753.pdf,Détection d'Objets dans les documents numérisés par réseaux de neurones profonds,Mélodie Boillet,,0%
https://arxiv.org/pdf/2301.11745.pdf,Side Auth: Synthesizing Virtual Sensors for Authentication,Kevin Fu,k.fu@northeastern.edu,82%
https://arxiv.org/pdf/2301.11745.pdf,Side Auth: Synthesizing Virtual Sensors for Authentication,Yan Long,yanlong@umich.edu,95%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Mang Ning,m.ning@uu.nl,82%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Enver Sangineto,,0%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Angelo Porrello,,0%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Simone Calderara,,0%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Rita Cucchiara,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Ziwei Luo,ziwei.luo@it.uu.se,95%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Fredrik K. Gustafsson,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Zheng Zhao,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Jens Sjölund,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Thomas B. Schön,,0%
https://arxiv.org/pdf/2301.11663.pdf,Deep Residual Compensation Convolutional Network without Backpropagation,Richard Wilson,richard.wilson@york.ac.uk,95%
https://arxiv.org/pdf/2301.11663.pdf,Deep Residual Compensation Convolutional Network without Backpropagation,Mubarakah Alotaibi,,0%
https://arxiv.org/pdf/2301.11650.pdf,Fast Region of Interest Proposals on Maritime UAVs,Andreas Zell,prename.surname@uni-tuebingen.de,60%
https://arxiv.org/pdf/2301.11650.pdf,Fast Region of Interest Proposals on Maritime UAVs,Benjamin Kiefer,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Przemysław Spurek,myslaw.spurek@uj.edu.pl,78%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Adam Kania,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Artur Kasymov,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Jakub Kościukiewicz,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Artur Górak,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Marcin Mazur,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Maciej Zięba,,0%
https://arxiv.org/pdf/2301.11630.pdf,Joint Geometry and Attribute Upsampling of Point Clouds Using Frequency-Selective Models with Overlapped Support,Viktoria Heimann,viktoria.heimann@fau.de,95%
https://arxiv.org/pdf/2301.11630.pdf,Joint Geometry and Attribute Upsampling of Point Clouds Using Frequency-Selective Models with Overlapped Support,André Kaup,andre.kaup@fau.de,95%
https://arxiv.org/pdf/2301.11630.pdf,Joint Geometry and Attribute Upsampling of Point Clouds Using Frequency-Selective Models with Overlapped Support,Andreas Spruck,,0%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Jason Gu,jason.gu@dal.ca,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Tiefeng Li,litiefeng@zju.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yi Ren,even.renyi@huawei.com,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Xiaowen Chu,xwchu@hkust-gz.edu.cn,82%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Piaopiao Jin,piaopiaojin@zju.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Penglei Sun,psun012@connect.hkust-gz.edu.cn,82%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yaoxian Song,songyaoxian@zju.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yue Zhang,zhangyue@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Zhixu Li,zhixuli@ruc.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yu Zheng,yu.zheng3@ubtrobot.com,95%
https://arxiv.org/pdf/2301.11558.pdf,Accelerating Guided Diffusion Sampling with Splitting Numerical Methods,Suttisak Wizadwongsa,suttisak.w s19@vistec.ac.th,85%
https://arxiv.org/pdf/2301.11558.pdf,Accelerating Guided Diffusion Sampling with Splitting Numerical Methods,Supasorn Suwajanakorn,supasorn.s@vistec.ac.th,85%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Hossein Kashiani,h_asgariandehkordi@elec.iust.ac.ir,85%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Shahriar Baradaran Shokouhi,bshokouhi@iust.ac.ir,78%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Omid Nejati Manzari,omid_nejaty@alumni.iust.ac.ir,85%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Hojat Asgarian Dehkordi,,0%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Farzad Beizaee,farzad.beizaee.1@ens.etsmtl.ca,95%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Christian Desrosiers,,0%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Gregory A. Lodygensky,,0%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Jose Dolz,,0%
https://arxiv.org/pdf/2301.11525.pdf,Mixed Attention Network for Hyperspectral Image Denoising,Ying Fu,fuying@bit.edu.cn,95%
https://arxiv.org/pdf/2301.11525.pdf,Mixed Attention Network for Hyperspectral Image Denoising,Zeqiang Lai,laizeqiang@bit.edu.cn,95%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Saulo Abraham Gante,sganted1500@ipn.mx,82%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Juan Irving Vasquez,,0%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Marco Antonio Valencia,,0%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Mauricio Olguín Carbajal,,0%
https://arxiv.org/pdf/2301.11520.pdf,SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning,H. Jin Kim,hjinkim@snu.ac.kr,82%
https://arxiv.org/pdf/2301.11520.pdf,SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning,Dongseok Shim,,0%
https://arxiv.org/pdf/2301.11520.pdf,SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning,Seungjae Lee,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Jiaqi Liu,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Guoyang Xie,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Jinbao Wang,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Shangnian Li,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Feng Zheng,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Yaochu Jin,,0%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Chunhui Li,lich@smail.nju.edu.cn,78%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Zhiling Yan,zhilingyan724@outlook.com,95%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Guanglei Zhang,guangleizhang@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Yunlu Feng,yunluf@icloud.com,85%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Nan Ying,rain986532@126.com,60%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Tianyi Zhang,,0%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Yanli Lei,,0%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Yu Zhao,,0%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Minjoon Seo,minjoon@kaist.ac.kr,85%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Sungdong Kim,,0%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Jin-hwa Kim,,0%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Jiyoung Lee,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Dong Sun,medsun@cityu.edu.hk,78%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Fei Pan,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Yutong Wu,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Kangning Cui,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Shuxun Chen,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Yanfang Li,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Yaofang Liu,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Adnan Shakoor,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Han Zhao,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Beijia Lu,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Shaohua Zhi,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Raymond Chan,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Fenggen Yu,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Qimin Chen,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Maham Tanveer,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Ali Mahdavi Amiri,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Hao Zhang,,0%
https://arxiv.org/pdf/2301.11495.pdf,Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks,Lei Lyu,lvlei@sdnu.edu.cn,85%
https://arxiv.org/pdf/2301.11495.pdf,Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks,Chen Pang,,0%
https://arxiv.org/pdf/2301.11495.pdf,Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks,Xuequan Lu,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Yitong Deng,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Hong-xing Yu,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Jiajun Wu,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Bo Zhu,,0%
https://arxiv.org/pdf/2302.08901.pdf,Exploring External Knowledge for Accurate modeling of Visual and Language Problems,Xuewen Yang,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Jacob Mallet,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Laura Pryor,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Rushit Dave,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Mounika Vanamala,,0%
https://arxiv.org/pdf/2301.11468.pdf,Multi-limb Split Learning for Tumor Classification on Vertically Distributed Data,Mayar M. Alfares,mayar.mohamed@guc.edu.eg,85%
https://arxiv.org/pdf/2301.11468.pdf,Multi-limb Split Learning for Tumor Classification on Vertically Distributed Data,Mohammed A. -m. Salem,mohammed.salem@guc.edu.eg,95%
https://arxiv.org/pdf/2301.11468.pdf,Multi-limb Split Learning for Tumor Classification on Vertically Distributed Data,Omar S. Ads,omar.ads@student.guc.edu.eg,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Shu Hu,shuhu@cmu.edu,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Xi Wu,xi.wu@imde.ac.cn,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Siwei Lyu,siweilyu@buffalo.edu,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Yunxu Xie,xieyunxu@imde.ac.cn,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Xin Wang,xwang264@buffalo.edu,82%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Bin Zhu,binzhu@microsoft.com,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Quanyu Liao,,0%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Bibek Aryal,baryal@miners.utep.edu,82%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Katie E. Miles,,0%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Sergio A. Vargas Zesati,,0%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Olac Fuentes,,0%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Matthias Niessner,niessner@tum.de,78%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Jiapeng Tang,jiapeng.tang@tum.de,95%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Biao Zhang,biao.zhang@kaust.edu.sa,95%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Peter Wonka,,0%
https://arxiv.org/pdf/2301.11431.pdf,Semidefinite Relaxations for Robust Multiview Triangulation,Daniel Cremers,cremers@tum.de,78%
https://arxiv.org/pdf/2301.11431.pdf,Semidefinite Relaxations for Robust Multiview Triangulation,Linus Härenstam-nielsen,linus.nielsen@tum.de,85%
https://arxiv.org/pdf/2301.11431.pdf,Semidefinite Relaxations for Robust Multiview Triangulation,Niclas Zeller,niclas.zeller@h-ka.de,95%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Masoud Zarepisheh,zarepism@mskcc.org,65%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Donghoon Lee,leed10@mskcc.org,78%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Yu-chi Hu,huj@mskcc.org,78%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Saad Nadeem,nadeems@mskcc.org,78%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Ellen Yorke,yorkee@mskcc.org,78%
https://arxiv.org/pdf/2301.11418.pdf,Parkinson gait modelling from an anomaly deep representation,Edgar Rangel,edgar.rangel@correo.uis.edu.co,95%
https://arxiv.org/pdf/2301.11418.pdf,Parkinson gait modelling from an anomaly deep representation,Fabio Martinez,,0%
https://arxiv.org/pdf/2301.11417.pdf,Are Labels Needed for Incremental Instance Learning?,Joaquin Vanschoren,j.vanschoren@tue.nl,82%
https://arxiv.org/pdf/2301.11417.pdf,Are Labels Needed for Incremental Instance Learning?,Mert Kilickaya,kilickayamert@gmail.com,95%
https://arxiv.org/pdf/2301.11405.pdf,Discriminative Entropy Clustering and its Relation to K-means and SVM,Zhongwen Zhang,z889zhan@uwaterloo.ca,65%
https://arxiv.org/pdf/2301.11405.pdf,Discriminative Entropy Clustering and its Relation to K-means and SVM,Yuri Boykov,yboykov@uwaterloo.ca,82%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Qingsong Xu,qingsong.xu@tum.de,95%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Xin Yuan,xyuan@westlake.edu.cn,82%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Xiao Xiang Zhu,xiaoxiang.zhu@tum.de,95%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Yilei Shi,yilei.shi@tum.de,95%
https://arxiv.org/pdf/2301.11367.pdf,Style-Aware Contrastive Learning for Multi-Style Image Captioning,Guodong Long,guodong.long@uts.edu.au,95%
https://arxiv.org/pdf/2301.11367.pdf,Style-Aware Contrastive Learning for Multi-Style Image Captioning,Yucheng Zhou,yucheng.zhou-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.11362.pdf,Improving Cross-modal Alignment for Text-Guided Image Inpainting,Guodong Long,guodong.long@uts.edu.au,95%
https://arxiv.org/pdf/2301.11362.pdf,Improving Cross-modal Alignment for Text-Guided Image Inpainting,Yucheng Zhou,yucheng.zhou-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.11360.pdf,The Power of Linear Combinations: Learning with Random Convolutions,Paul Gavrikov,paul.gavrikov@hs-offenburg.de,95%
https://arxiv.org/pdf/2301.11360.pdf,The Power of Linear Combinations: Learning with Random Convolutions,Janis Keuper,janis.keuper@hs-offenburg.de,95%
https://arxiv.org/pdf/2301.11357.pdf,Multimodal Event Transformer for Image-guided Story Ending Generation,Guodong Long,guodong.long@uts.edu.au,95%
https://arxiv.org/pdf/2301.11357.pdf,Multimodal Event Transformer for Image-guided Story Ending Generation,Yucheng Zhou,yucheng.zhou-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Malte Hoffmann,mhoffmann@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Andrew Hoopes,,0%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Douglas N. Greve,,0%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Bruce Fischl,,0%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Adrian V. Dalca,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Aliaksandr Siarohin,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Willi Menapace,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Ivan Skorokhodov,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Kyle Olszewski,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Jian Ren,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Hsin-ying Lee,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Menglei Chai,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Sergey Tulyakov,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Xudong Wang,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Rohit Girdhar,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Stella X. Yu,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Ishan Misra,,0%
https://arxiv.org/pdf/2301.11316.pdf,Open Problems in Applied Deep Learning,Maziar Raissi,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Mingquan Lin,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Yuyun Xiao,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Bojian Hou,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Tingyi Wanyan,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Mohit Manoj Sharma,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Zhangyang Wang,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Fei Wang,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Sarah Van Tassel,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Yifan Peng,,0%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Luca De Luigi,luca.deluigi4@unibo.it,95%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Pierluigi Zama Ramirez,pierluigi.zama@unibo.it,85%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Samuele Salti,samuele.salti@unibo.it,95%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Alessio Tonioni,alessiot@google.com,85%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Luigi Di Stefano,luigi.distefano@unibo.it,95%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Adriano Cardace,adriano.cardace2@unibo.it,95%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Uriel Singer,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Shelly Sheynin,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Adam Polyak,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Oron Ashual,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Iurii Makarov,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Filippos Kokkinos,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Naman Goyal,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Andrea Vedaldi,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Devi Parikh,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Justin Johnson,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Yaniv Taigman,,0%
https://arxiv.org/pdf/2301.11274.pdf,Self-Supervised RGB-T Tracking with Cross-Input Consistency,Xingchen Zhang,xingchen.zhang@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.11274.pdf,Self-Supervised RGB-T Tracking with Cross-Input Consistency,Yiannis Demiris,y.demiris@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Xianglong Liu,xlliu@buaa.edu.cn,82%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Haotong Qin,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Mingyuan Zhang,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Yifu Ding,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Aoyu Li,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Zhongang Cai,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Ziwei Liu,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Fisher Yu,,0%
https://arxiv.org/pdf/2301.11201.pdf,Relative-Interior Solution for the (Incomplete) Linear Assignment Problem with Applications to the Quadratic Assignment Problem,Bogdan Savchynskyy,bogdan.savchynskyy@iwr.uni-heidelberg.de,95%
https://arxiv.org/pdf/2301.11201.pdf,Relative-Interior Solution for the (Incomplete) Linear Assignment Problem with Applications to the Quadratic Assignment Problem,Tomáš Dlask,dlaskto2@fel.cvut.cz,78%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Derek Gloudemans,derek.gloudemans@vanderbilt.edu,95%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Yanbing Wang,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Junyi Ji,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Gergely Zachar,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Will Barbour,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Daniel B. Work,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Matthew J. Muckley,mmuckley@meta.com,82%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Alaaeldin El-nouby,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Karen Ullrich,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Hervé Jégou,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Jakob Verbeek,,0%
https://arxiv.org/pdf/2301.11180.pdf,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,Mingbao Lin,linmb001@outlook.com,78%
https://arxiv.org/pdf/2301.11180.pdf,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,Ziran Qin,qinziran@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.11180.pdf,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,Weiyao Lin,wylin@sjtu.edu.cn,82%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,Tae-hyun Oh,taehyun@postech.ac.kr,85%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,Dong-jin Kim,jinsc37@kaist.ac.kr,85%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,In So Kweon,iskweon77@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,Jinsoo Choi,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Michal Kawulok,michal.kawulok@polsl.pl,95%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Jakub Nalepa,jakub.nalepa@polsl.pl,95%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Tomasz Tarasiewicz,tomasz.tarasiewicz@polsl.pl,95%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Reuben A. Farrugia,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Gianluca Valentino,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Mang Chen,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Johann A. Briffa,,0%
https://arxiv.org/pdf/2301.11145.pdf,Learning from Mistakes: Self-Regularizing Hierarchical Representations in Point Cloud Semantic Segmentation,Umberto Michieli,umberto.michieli@dei.unipd.it,95%
https://arxiv.org/pdf/2301.11145.pdf,Learning from Mistakes: Self-Regularizing Hierarchical Representations in Point Cloud Semantic Segmentation,Elena Camuffo,elena.camuffo@dei.unipd.it,95%
https://arxiv.org/pdf/2301.11145.pdf,Learning from Mistakes: Self-Regularizing Hierarchical Representations in Point Cloud Semantic Segmentation,Simone Milani,simone.milani@dei.unipd.it,95%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Jingjia Huang,huangjingjia@bytedance.com,95%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Jiashi Feng,jshfeng@bytedance.com,82%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Xinglong Wu,wuxinglong@bytedance.com,95%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Thomas H. Li,thomas@.pku.edu.cn,85%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Ruyang Liu,,0%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Ge Li,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Sangwoo Mo,swmo@umich.edu,82%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Younghyun Kim,younghyun.kim@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Minkyu Kim,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Kyungmin Lee,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Jaeho Lee,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Jinwoo Shin,,0%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Melissa Hall,melissahall@meta.com,95%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Candace Ross,ccross@meta.com,82%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Laura Gustafson,,0%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Aaron Adcock,,0%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Ishan Misra,,0%
https://arxiv.org/pdf/2301.11093.pdf,Simple diffusion: End-to-end diffusion for high resolution images,Emiel Hoogeboom,emielh@google.com,85%
https://arxiv.org/pdf/2301.11093.pdf,Simple diffusion: End-to-end diffusion for high resolution images,Jonathan Heek,,0%
https://arxiv.org/pdf/2301.11093.pdf,Simple diffusion: End-to-end diffusion for high resolution images,Tim Salimans,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Penghao Jiang,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Ke Xin,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Zifeng Wang,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Chunxi Li,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Hyeongji Kim,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Pekka Parviainen,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Terje Berge,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Ketil Malde,,0%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Abhishek Kumar,abhishek.ai@knu.ac.kr,85%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Athul Shibu,athulshibu@knu.ac.kr,95%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Dong-gyu Lee,dglee@knu.ac.kr,82%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Heechul Jung,heechul@knu.ac.kr,85%
https://arxiv.org/pdf/2302.01394.pdf,Understanding and contextualising diffusion models,Stefano Scotta,,0%
https://arxiv.org/pdf/2302.01394.pdf,Understanding and contextualising diffusion models,Alberto Messina,,0%
https://arxiv.org/pdf/2301.11022.pdf,Semantic Segmentation Enhanced Transformer Model for Human Attention Prediction,Shuo Zhang,,0%
https://arxiv.org/pdf/2301.11015.pdf,Explore the Power of Dropout on Few-shot Learning,Shaobo Lin,,0%
https://arxiv.org/pdf/2301.11015.pdf,Explore the Power of Dropout on Few-shot Learning,Xingyu Zeng,,0%
https://arxiv.org/pdf/2301.11015.pdf,Explore the Power of Dropout on Few-shot Learning,Rui Zhao,,0%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Yao Zhao,yzhao@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Shangrong Yang,sr yang@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Kang Liao,kang liao@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Chunyu Lin,cylin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.10972.pdf,On the Importance of Noise Scheduling for Diffusion Models,Ting Chen,iamtingchen@google.com,95%
https://arxiv.org/pdf/2301.10958.pdf,Learning Large Scale Sparse Models,Atul Dhingra,atul.dhingra@rutgers.edu,95%
https://arxiv.org/pdf/2301.10958.pdf,Learning Large Scale Sparse Models,Jie Shen,,0%
https://arxiv.org/pdf/2301.10958.pdf,Learning Large Scale Sparse Models,Nicholas Kleene,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Atul Dhingra,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Adeboye A. Adejare,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Adam Fendler,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Roopeswar Kommalapati,,0%
https://arxiv.org/pdf/2301.10951.pdf,Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images,Ali Vosoughi,mvosough@ece.rochester.edu,55%
https://arxiv.org/pdf/2301.10951.pdf,Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images,Nathan Hadjiyski,,0%
https://arxiv.org/pdf/2301.10951.pdf,Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images,Axel Wismueller,,0%
https://arxiv.org/pdf/2301.10941.pdf,GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,Jiuhn Song,song@korea.ac.kr,78%
https://arxiv.org/pdf/2301.10941.pdf,GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,Seungryong Kim,kim@korea.ac.kr,78%
https://arxiv.org/pdf/2301.10941.pdf,GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,Min-seop Kwak,mskwak01@korea.ac.kr,82%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Sachit Menon,sachit.menon@columbia.edu,95%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Scott Geng,scott.geng@columbia.edu,95%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Revant Teotia,,0%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Purva Tendulkar,,0%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Carl Vondrick,,0%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Junqing Yu,yjqing@hust.edu.cn,75%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Zikai Song,skyesong@hust.edu.cn,78%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Yi-ping Phoebe Chen,phoebe.chen@latrobe.edu.au,78%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Wei Yang,weiyangcs@hust.edu.cn,95%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Run Luo,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Fanman Meng,fmmeng@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Qingbo Wu,qbwu@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Linfeng Xu,lfxu@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Hongliang Li,hlli@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Lili Pan,lilipan@uestc.edu.cn,95%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Chiyuan He,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Hanxin Wang,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Shaoxu Cheng,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Yu Dai,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Gui-song Xia,guisong.xia@whu.edu.cn,95%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Chao Pang,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Jiang Wu,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Jian Ding,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Can Song,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Ran Tao,taoran1@cmu.edu,95%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Marios Savvides,marioss@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Hao Chen,haoc3@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Jindong Wang,jindong.wang@microsoft.com,95%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Yue Fan,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Yidong Wang,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Xing Xie,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Bhiksha Raj,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Yunpeng Bai,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Jiayue Liu,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Chao Dong,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Chun Yuan,,0%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,James Bailey,baileyj@unimelb.edu.au,78%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,Xingjun Ma,xingjunma@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,Sarah Erfani,sarah.erfani@unimelb.edu.au,95%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,Hanxun Huang,hanxunh@student.unimelb.edu.au,85%
https://arxiv.org/pdf/2301.10906.pdf,Facial Expression Recognition using Squeeze and Excitation-powered Swin Transformers,Arpita Vats,avats@scu.edu,82%
https://arxiv.org/pdf/2301.10906.pdf,Facial Expression Recognition using Squeeze and Excitation-powered Swin Transformers,Aman Chadha,,0%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Errui Ding,dingerrui@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Jian Wang,wangjian33@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Jingdong Wang,wangjingdong@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Haocheng Feng,fenghaocheng@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Xiaohu Huang,huangxiaohu@hust.edu.cn,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Junyu Han,hanjunyu@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Hao Zhou,zhouhao14@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Bin Feng,fengbin@hust.edu.cn,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Wenyu Liu,liuwy@hust.edu.cn,78%
https://arxiv.org/pdf/2301.10877.pdf,The Projection-Enhancement Network (PEN),Bo Sun,sunb@onid.orst.edu,78%
https://arxiv.org/pdf/2301.10877.pdf,The Projection-Enhancement Network (PEN),Christopher Z. Eddy,,0%
https://arxiv.org/pdf/2301.10877.pdf,The Projection-Enhancement Network (PEN),Austin Naylor,,0%
https://arxiv.org/pdf/2301.10876.pdf,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,Rohitash Chandra,rohitash.chandra@unsw.edu.au,95%
https://arxiv.org/pdf/2301.10876.pdf,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,Saharsh Barve,saharshbarve3@gmail.com,95%
https://arxiv.org/pdf/2301.10876.pdf,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,Jody M. Webster,jody.webster@sydney.edu.au,95%
https://arxiv.org/pdf/2301.10863.pdf,Shape Reconstruction from Thoracoscopic Images using Self-supervised Virtual Learning,Tomoki Oya,t-oya@sys.i.kyoto-u.ac.jp,82%
https://arxiv.org/pdf/2301.10863.pdf,Shape Reconstruction from Thoracoscopic Images using Self-supervised Virtual Learning,Megumi Nakao,,0%
https://arxiv.org/pdf/2301.10863.pdf,Shape Reconstruction from Thoracoscopic Images using Self-supervised Virtual Learning,Tetsuya Matsuda,,0%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Yiwei Jia,yiwei.jia@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Reza Azad,reza.azad; yiwei.jia@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Julien Cohen-adad,jcohen@polymtl.ca,90%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Dorit Merhof,dorit.merhof@ur.de,95%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Ehsan Khodapanah Aghdam,ehsan.khpaghdam@gmail.com,95%
https://arxiv.org/pdf/2301.10829.pdf,TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction,Zeynel A. Samak,,0%
https://arxiv.org/pdf/2301.10829.pdf,TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction,Philip Clatworthy,,0%
https://arxiv.org/pdf/2301.10829.pdf,TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction,Majid Mirmehdi,,0%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Zichao Li,zli489@ucsc.edu,82%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Shaoyuan Xie,shaoyux@uci.edu,84%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Cihang Xie,cixie@ucsc.edu,82%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Zeyu Wang,zwang615@ucsc.edu,82%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Stavros Tsogkas,stavros.t@samsung.com,85%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Alex Levinshtein,alex.lev@samsung.com,85%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Fengjia Zhang,f.zhang2@samsung.com,82%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Allan Jepson,allan.jepson@samsung.com,95%
https://arxiv.org/pdf/2301.10750.pdf,Out of Distribution Performance of State of Art Vision Model,Salman Rahman,salman@nyu.edu,85%
https://arxiv.org/pdf/2301.10750.pdf,Out of Distribution Performance of State of Art Vision Model,Wonkwon Lee,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Aotian Wu,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Pan He,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Xiao Li,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Ke Chen,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Sanjay Ranka,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Anand Rangarajan,,0%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Iván De Andrés Tamé,ivan.andrest@estudiante.uam.es,85%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Marcos Escudero-viñolo,marcos.escudero@uam.es,85%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Pablo Carballeira,pablo.carballeira@uam.es,95%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Kirill Sirotkin,kirill.sirotkin@uam.es,95%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Yunpeng Bai,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Zihan Zhong,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Chao Dong,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Weichen Zhang,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Guowei Xu,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Chun Yuan,,0%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Carsten T. Lüth,carsten.lueth@dkfz-heidelberg.de,85%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Till J. Bungert,,0%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Lukas Klein,,0%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Paul F. Jaeger,,0%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Enjie Ghorbel,enjie.ghorbel@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Djamila Aouada,djamila.aouada@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Arunkumar Rathinam,arunkumar.rathinam@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Anis Kacem,anis.kacem@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Indel Pal Singh,inder.singh@uni.lu,82%
https://arxiv.org/pdf/2301.10608.pdf,Connecting metrics for shape-texture knowledge in computer vision,Tiago Oliveira,,0%
https://arxiv.org/pdf/2301.10608.pdf,Connecting metrics for shape-texture knowledge in computer vision,Tiago Marques,,0%
https://arxiv.org/pdf/2301.10608.pdf,Connecting metrics for shape-texture knowledge in computer vision,Arlindo L. Oliveira,,0%
https://arxiv.org/pdf/2302.10801.pdf,Deep Generative Neural Embeddings for High Dimensional Data Visualization,Gerardo Hermosillo Valadez,gerardo.hermosillovaladez@siemens-healthineers.com,95%
https://arxiv.org/pdf/2302.10801.pdf,Deep Generative Neural Embeddings for High Dimensional Data Visualization,Halid Ziya Yerebakan,halid.yerebakan@siemens-healthineers.com,95%
https://arxiv.org/pdf/2301.10593.pdf,Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition,Denis Coquenet,denis.coquenet@lecnam.net,95%
https://arxiv.org/pdf/2301.10593.pdf,Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition,Thierry Paquet,thierry.paquet@litislab.eu,95%
https://arxiv.org/pdf/2301.10593.pdf,Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition,Clément Chatelain,clement.chatelain@litislab.eu,95%
https://arxiv.org/pdf/2301.10584.pdf,A Method For Eliminating Contour Errors In Self-Encoder Reconstructed Images,Hao Zhang,haozhang@stu.sicnu.edu.cn,95%
https://arxiv.org/pdf/2301.10584.pdf,A Method For Eliminating Contour Errors In Self-Encoder Reconstructed Images,Yonggang Li,,0%
https://arxiv.org/pdf/2301.10583.pdf,An Efficient Approximate Method for Online Convolutional Dictionary Learning,Farshad G. Veshki,,0%
https://arxiv.org/pdf/2301.10583.pdf,An Efficient Approximate Method for Online Convolutional Dictionary Learning,Sergiy A. Vorobyov,,0%
https://arxiv.org/pdf/2301.10575.pdf,Trainable Loss Weights in Super-Resolution,Arash Chaichi Mellatshahi,,0%
https://arxiv.org/pdf/2301.10575.pdf,Trainable Loss Weights in Super-Resolution,Shohreh Kasaei,,0%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Chamath Abeysinghe,chamath.abeysinghe@monash.edu,95%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Bernd Meyer,bernd.meyer@monash.edu,95%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Chris Reid,chris.reid@mq.edu.au,95%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Hamid Rezatofighi,hamid.rezatoﬁghi@monash.edu,95%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Mingle Xu,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Jaehwan Lee,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Sook Yoon,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Hyongsuk Kim,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Dong Sun Park,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,David W. Romero,d.w.romeroguzman@vu.nl,82%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,David M. Knigge,d.m.knigge@uva.nl,82%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Albert Gu,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Efstratios Gavves,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Erik J. Bekkers,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Jakub M. Tomczak,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Mark Hoogendoorn,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Jan-jakob Sonke,,0%
https://arxiv.org/pdf/2301.10531.pdf,3D Tooth Mesh Segmentation with Simplified Mesh Cell Representation,Ananya Jana,,0%
https://arxiv.org/pdf/2301.10531.pdf,3D Tooth Mesh Segmentation with Simplified Mesh Cell Representation,Hrebesh Molly Subhash,,0%
https://arxiv.org/pdf/2301.10531.pdf,3D Tooth Mesh Segmentation with Simplified Mesh Cell Representation,Dimitris N. Metaxas,,0%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Nassir Navab,nassir.navab@tum.de,95%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Mehrdad Salehi,mehrdad.salehi@tum.de,95%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Benjamin Busam,b.busam@tum.de,82%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Christine Eilers,christine.eilers@tum.de,95%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Mohammad Farid Azampour,mf.azampour@tum.de,82%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Magdalena Wysocki,magdalena.wysocki@tum.de,95%
https://arxiv.org/pdf/2302.10306.pdf,Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet Integration,Masoud Shahraki Mohammadi,mahdavi@mshdiau.ac.ir,65%
https://arxiv.org/pdf/2302.10306.pdf,Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet Integration,Seyed Javad Seyed Mahdavi Chabok,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Yushan Zhang,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Andreas Robinson,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Maria Magnusson,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Michael Felsberg,,0%
https://arxiv.org/pdf/2301.10473.pdf,Aircraft Skin Inspections: Towards a New Model for Dent Evaluation,Pasquale Lafiosca,pasquale.lafiosca@cranfield.ac.uk,95%
https://arxiv.org/pdf/2301.10473.pdf,Aircraft Skin Inspections: Towards a New Model for Dent Evaluation,Ip-shing Fan,,0%
https://arxiv.org/pdf/2301.10473.pdf,Aircraft Skin Inspections: Towards a New Model for Dent Evaluation,Nicolas P. Avdelidis,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Fenggen Yu,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Yiming Qian,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Francisca Gil-ureta,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Brian Jackson,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Eric Bennett,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Hao Zhang,,0%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Pengwei Zhang,zhangpengwei@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Chengqian Ma,machengqian01@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Long Zheng,zhenglong@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Chunlei Cai,caichunlei@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Yi Wang,wangyi@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Chao Chen,chenchao02@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Zhiqiang Wu,wuzhiqiang01@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Quan Zhou,zhouquan@bilibili.com,95%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Arman Zarei,arman.zarei@sharif.edu,95%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Mohammad Azizmalayeri,m.azizmalayeri@sharif.edu,82%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Mohammad Hossein Rohban,rohban@sharif.edu,78%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Alireza Isavand,alireza.isavand@sharif.edu,95%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Mohammad Taghi Manzuri,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Tongzhi Niu,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Bin Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Kai Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Yufeng Lin,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Yuwei Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Weifeng Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Zhenrong Wang,,0%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Angela Yao,ayao@comp.nus.edu.sg,82%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Kerui Gu,keruigu@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Linlin Yang,yangll@comp.nus.edu.sg,78%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Michael Bi Mi,,0%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Chen Sun,chen.sun@sony.com,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Lixu Wang,lixuwang2025@u.northwestern.edu,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Lingjuan Lyu,Lingjuan.Lv@sony.com,85%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Xiao Wang,wangxiao@northwestern.edu,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Chenxi Liu,chenxiliu2020@u.northwestern.edu,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Qi Zhu,qzhu@northwestern.edu,82%
https://arxiv.org/pdf/2301.10413.pdf,Local Feature Extraction from Salient Regions by Feature Map Transformation,Nur Suriza Syazwany Binti Ahmad Nizam,surizasyazwany@inha.edu,90%
https://arxiv.org/pdf/2301.10413.pdf,Local Feature Extraction from Salient Regions by Feature Map Transformation,Sang-chul Lee,sclee@inha.ac.kr,82%
https://arxiv.org/pdf/2301.10413.pdf,Local Feature Extraction from Salient Regions by Feature Map Transformation,Yerim Jung,,0%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Neel Dey,dey@mit.edu,78%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Nalini M. Singh,nmsingh@mit.edu,82%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Bruce Fischl,bfischl@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Adrian V. Dalca,adalca@mit.edu,82%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Polina Golland,polina@csail.mit.edu,85%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Elfar Adalsteinsson,elfar@mit.edu,85%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Robert Frost,srfrost@mgh.harvard.edu,78%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Malte Hoffmann,mhoffmann@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,John Lagergren,lagergrenjh@ornl.gov,78%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Hari B. Chhetri,streichjc@ornl.gov,75%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Daniel Jacobson,jacobsonda@ornl.gov,78%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Mirko Pavicic,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Larry M. York,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,P. Doug Hyatt,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,David Kainer,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Erica M. Rutter,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Kevin Flores,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Jack Bailey-bale,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Marie Klein,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Gail Taylor,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Jared Streich,,0%
https://arxiv.org/pdf/2301.10327.pdf,Generating Multidimensional Clusters With Support Lines,Nuno Fachada,nuno.fachada@ulusofona.pt,95%
https://arxiv.org/pdf/2301.10327.pdf,Generating Multidimensional Clusters With Support Lines,Diogo De Andrade,diogo.andrade@ulusofona.pt,95%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Benjamin Recht,brecht@berkeley.edu,82%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Angjoo Kanazawa,kanazawa@berkeley.edu,78%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Giacomo Meanti,giacomo.meanti@iit.it,95%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Sara Fridovich-keil,sfk@berkeley.edu,98%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Frederik Warburg,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Angelika Ando,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Spyros Gidaris,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Andrei Bursuc,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Gilles Puy,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Alexandre Boulch,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Renaud Marlet,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,Xueshen Li,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,Yu Gan,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,David Duan,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,Xiao Yang,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Junyu Wang,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Shijie Wang,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Wenyu Liu,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Zengqiang Zheng,,0%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Min Xian,mxian@uidaho.edu,82%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Sujata Butte,,0%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Haotian Wang,,0%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Aleksandar Vakanski,,0%
https://arxiv.org/pdf/2301.10134.pdf,Bipartite Graph Diffusion Model for Human Interaction Generation,Baptiste Chopin,,0%
https://arxiv.org/pdf/2301.10134.pdf,Bipartite Graph Diffusion Model for Human Interaction Generation,Hao Tang,,0%
https://arxiv.org/pdf/2301.10134.pdf,Bipartite Graph Diffusion Model for Human Interaction Generation,Mohamed Daoudi,,0%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Lennart Svensson,lennart.svensson@chalmers.se,95%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Lars Hammarstrand,lars.hammarstrand@chalmers.se,95%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Erik Wallin,walline@chalmers.se,78%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Fredrik Kahl,fredrik.kahl@chalmers.se,95%
https://arxiv.org/pdf/2301.10100.pdf,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,Gilles Puy,,0%
https://arxiv.org/pdf/2301.10100.pdf,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,Alexandre Boulch,,0%
https://arxiv.org/pdf/2301.10100.pdf,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,Renaud Marlet,,0%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Charles Dansereau,charles.dansereau@polymtl.ca,95%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Mehdi Zalai,mehdi.zalai@polymtl.ca,95%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Milo Sobral,milo.sobral@polymtl.ca,95%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Maninder Bhogal,maninder.bhogal@polymtl.ca,95%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Romain Xu-darme,romain.xu-darme@cea.fr,95%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Zakaria Chihani,zakaria.chihani@cea.fr,95%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Julien Girard-satabin,julien.girard2@cea.fr,85%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Darryl Hond,,0%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Gabriele Incorvaia,,0%
https://arxiv.org/pdf/2301.10057.pdf,Planar Object Tracking via Weighted Optical Flow,Jiri Matas,matas@fel.cvut.cz,78%
https://arxiv.org/pdf/2301.10057.pdf,Planar Object Tracking via Weighted Optical Flow,Jonas Serych,serycjon@fel.cvut.cz,75%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Blas Kojusner,bkojusner@ufl.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Pirouz Naghavi,pnaghavi@ufl.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Kevin Butler,butler@ufl.edu,78%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Sara Rampazzi,srampazzi@ufl.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Kevin Fu,k.fu@northeastern.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Yan Long,yanlong@umich.edu,95%
https://arxiv.org/pdf/2301.10052.pdf,Event Detection in Football using Graph Convolutional Networks,Aditya Sangram Singh Rana,adityasangramsingh.rana@e-campus.uab.cat,95%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Zanjia Tong,,0%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Yuhang Chen,,0%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Zewei Xu,,0%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Rong Yu,,0%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Jingjing Fu,jifu@microsoft.com,82%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Dong Liu,dongeliu@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Kaidong Zhang,,0%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Jialun Peng,,0%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Naye Ji,jinaye@cuz.edu.cn,95%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Fan Zhang,fanzhang@cuz.edu.cn,95%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Yongping Li,liyongping@nbufe.edu.cn,95%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Fuxing Gao,fuxing@cuz.edu.cn,85%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Peijie Dong,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Xin Niu,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Zhiliang Tian,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Lujun Li,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Xiaodong Wang,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Zimian Wei,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Hengyue Pan,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Dongsheng Li,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Xiao He,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Mingrui Zhu,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Nannan Wang,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Xinbo Gao,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Heng Yang,,0%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Li Liu,dreamliu2010@gmail.com,95%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Wanxia Deng,dengwanxia14@nudt.edu.cn,95%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Yawen Cui,,0%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Haoyu Chen,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Verena Jasmin Hallitschke,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Tobias Schlumberger,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Philipp Kataliakos,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Zdravko Marinov,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Moon Kim,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Lars Heiliger,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Constantin Seibold,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Jens Kleesiek,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Rainer Stiefelhagen,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Mathias Zinnen,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Prathmesh Madhu,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Peter Bell,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Andreas Maier,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Vincent Christlein,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Azadeh Fakhrzadeh,fakhrzadeh@irandoc.ac.ir,78%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Pouya Karimian,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Mahsa Meyari,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Cris L. Luengo Hendriks,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Lena Holm,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Christian Sonne,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Rune Dietz,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Ellinor Spörndly-nees,,0%
https://arxiv.org/pdf/2301.09879.pdf,Data Augmentation Alone Can Improve Adversarial Training,Lin Li,michael.spratling@kcl.ac.uk,95%
https://arxiv.org/pdf/2301.09879.pdf,Data Augmentation Alone Can Improve Adversarial Training,Michael Spratling,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Mathias Zinnen,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Prathmesh Madhu,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Ronak Kosti,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Peter Bell,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Andreas Maier,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Vincent Christlein,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Jinpeng Shi,jinpeeeng.s@gmail.com,75%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Hui Li,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Tianle Liu,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Yulong Liu,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Mingjian Zhang,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Jinchen Zhu,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Ling Zheng,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Shizhuang Weng,,0%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Edouard Yvinec,ey@datakalab.com,90%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Arnaud Dapogny,,0%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Matthieu Cord,,0%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Kevin Bailly,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Peijie Dong,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Xin Niu,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Lujun Li,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Zhiliang Tian,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Xiaodong Wang,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Zimian Wei,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Hengyue Pan,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Dongsheng Li,,0%
https://arxiv.org/pdf/2301.09799.pdf,LDMIC: Learning-based Distributed Multi-view Image Coding,Xinjie Zhang,xinjie.zhang@connect.ust.hk,95%
https://arxiv.org/pdf/2301.09799.pdf,LDMIC: Learning-based Distributed Multi-view Image Coding,Jiawei Shao,jiawei.shao@connect.ust.hk,95%
https://arxiv.org/pdf/2301.09799.pdf,LDMIC: Learning-based Distributed Multi-view Image Coding,Jun Zhang,,0%
https://arxiv.org/pdf/2301.11726.pdf,GAN-Based Object Removal in High-Resolution Satellite Images,Hadi Mansourifar,,0%
https://arxiv.org/pdf/2301.11726.pdf,GAN-Based Object Removal in High-Resolution Satellite Images,Steven J. Simske,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Joseph Bentsman,jbentsma@illinois.edu,90%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Martin Ostoja-starzewski,martinos@illinois.edu,85%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Leonardo P. Chamorro,lpchamo@illinois.edu,65%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Richard Berlin,pubs-permissions@ieee.org,65%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Junren Ran,jran2@illinois.edu,82%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Hamza El-kebir,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Yongseok Lee,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Gabriela M. Aguiluz Cornejo,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Enrico Benedetti,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Pier C. Giulianotti,,0%
https://arxiv.org/pdf/2301.09724.pdf,Long-tail Detection with Effective Class-Margins,Jang Hyun Cho,janghyuncho7@utexas.edu,95%
https://arxiv.org/pdf/2301.09724.pdf,Long-tail Detection with Effective Class-Margins,Philipp Krähenbühl,philkr@cs.utexas.edu,60%
https://arxiv.org/pdf/2301.09702.pdf,Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification,Jiaqi Guo,guo498@purdue.edu,78%
https://arxiv.org/pdf/2301.09702.pdf,Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification,Amy R. Reibman,reibman@purdue.edu,78%
https://arxiv.org/pdf/2301.09702.pdf,Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification,Edward J. Delp,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Amir Ghasemi,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Nasrin Bayat,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Fatemeh Mottaghian,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Akram Bayat,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Chieh Hubert Lin,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Hsin-ying Lee,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Willi Menapace,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Menglei Chai,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Aliaksandr Siarohin,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Sergey Tulyakov,,0%
https://arxiv.org/pdf/2301.09632.pdf,HexPlane: A Fast Representation for Dynamic Scenes,Ang Cao,ancao@umich.edu,82%
https://arxiv.org/pdf/2301.09632.pdf,HexPlane: A Fast Representation for Dynamic Scenes,Justin Johnson,justincj@umich.edu,85%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Qiuhong Anna Wei,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Sijie Ding,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Jeong Joon Park,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Rahul Sajnani,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Adrien Poulenard,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Srinath Sridhar,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Leonidas Guibas,,0%
https://arxiv.org/pdf/2301.09624.pdf,Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images,Piotr Keller,,0%
https://arxiv.org/pdf/2301.09624.pdf,Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images,Muhammad Dawood,,0%
https://arxiv.org/pdf/2301.09624.pdf,Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images,Fayyaz Ul Amir Afsar Minhas,,0%
https://arxiv.org/pdf/2301.09620.pdf,Tracking the industrial growth of modern China with high-resolution panchromatic imagery: A sequential convolutional approach,Ethan Brewer,ethan.brewer@nyu.edu,95%
https://arxiv.org/pdf/2301.09620.pdf,Tracking the industrial growth of modern China with high-resolution panchromatic imagery: A sequential convolutional approach,Dan Runfola,danr@wm.edu,85%
https://arxiv.org/pdf/2301.09620.pdf,Tracking the industrial growth of modern China with high-resolution panchromatic imagery: A sequential convolutional approach,Zhonghui Lv,zlv@wm.edu,82%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Sophia J. Wagner,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Daniel Reisenbüchler,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Nicholas P. West,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Jan Moritz Niehues,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Gregory Patrick Veldhuizen,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Philip Quirke,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Heike I. Grabsch,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Piet A. Van Den Brandt,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Gordon G. A. Hutchins,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Susan D. Richman,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Tanwei Yuan,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Rupert Langer,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Josien Christina Anna Jenniskens,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Kelly Offermans,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Wolfram Mueller,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Richard Gray,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Stephen B. Gruber,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Joel K. Greenson,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Gad Rennert,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Joseph D. Bonner,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Daniel Schmolze,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Jacqueline A. James,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Maurice B. Loughrey,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Manuel Salto-tellez,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Hermann Brenner,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Michael Hoffmeister,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Daniel Truhn,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Julia A. Schnabel,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Melanie Boxberg,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Tingying Peng,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Jakob Nikolas Kather,,0%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Jesus Angulo,jesus.angulo@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Etienne Decencière,etienne.decenciere@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Santiago Velasco-forero,santiago.velasco@minesparis.psl.eu,85%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Joao P. C. Bertoldo,jpcbertoldo@minesparis.psl.eu,82%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Adrià Recasens,arecasens@google.com,82%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Jason Lin,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Joāo Carreira,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Drew Jaegle,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Luyu Wang,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Jean-baptiste Alayrac,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Pauline Luc,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Antoine Miech,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Lucas Smaira,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Ross Hemsley,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Andrew Zisserman,,0%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Nathalie Majcherczyk,majcherc@amazon.com,90%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Xuewei Qi,qixuewei@amazon.com,95%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Rajasimman Madhivanan,rajasimm@amazon.com,90%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Wenhao Ding,wenhaod@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Arnie Sen,senarnie@amazon.com,95%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Ding Zhao,dingzhao@andrew.cmu.edu,95%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Mohit Deshpande,deshmohi@amazon.com,60%
https://arxiv.org/pdf/2301.09542.pdf,Improving Presentation Attack Detection for ID Cards on Remote Verification Systems,Sebastian Gonzalez,sebastian.gonzalez@tocbiometrics.com,95%
https://arxiv.org/pdf/2301.09542.pdf,Improving Presentation Attack Detection for ID Cards on Remote Verification Systems,Juan Tapia,juan.tapia-farias@h-da.de,95%
https://arxiv.org/pdf/2301.09525.pdf,DeepFEL: Deep Fastfood Ensemble Learning for Histopathology Image Analysis,Nima Hatami,hatami@creatis.insa-lyon.fr,78%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Dengyu Wu,dengyu.wu@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Xiaowei Huang,xiaowei.huang@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Gaojie Jin,,0%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Han Yu,,0%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Xinping Yi,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Axel Sauer,a.sauer@uni-tuebingen.de,82%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Tero Karras,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Samuli Laine,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Andreas Geiger,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Timo Aila,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Keyan Chen,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Xiaolong Jiang,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Yao Hu,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Xu Tang,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Yan Gao,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Jianqi Chen,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Weidi Xie,,0%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Liyan Zhang,zhangliyan@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Xiangbo Shu,shuxb@njust.edu.cn,78%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Fei Shen,feishen@njust.edu.cn,95%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Xiaoyu Du,duxy@njust.edu.cn,78%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Jinhui Tang,jinhuitang@njust.edu.cn,95%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Alessandro Flaborea,flaborea@di.uniroma1.it,78%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Guido D'amely,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Stefano D'arrigo,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Marco Aurelio Sterpa,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Alessio Sampieri,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Fabio Galasso,,0%
https://arxiv.org/pdf/2301.09461.pdf,Study on the identification limits of craniofacial superimposition,Óscar Ibáñez,oscar.ibanez@udc.es,95%
https://arxiv.org/pdf/2301.09461.pdf,Study on the identification limits of craniofacial superimposition,Enrique Bermejo,,0%
https://arxiv.org/pdf/2301.09461.pdf,Study on the identification limits of craniofacial superimposition,Andrea Valsecchi,,0%
https://arxiv.org/pdf/2301.09460.pdf,HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images,Kun Li,,0%
https://arxiv.org/pdf/2301.09460.pdf,HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images,George Vosselman,,0%
https://arxiv.org/pdf/2301.09460.pdf,HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images,Michael Ying Yang,,0%
https://arxiv.org/pdf/2301.09451.pdf,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,Ishan Misra,imisra@meta.com,82%
https://arxiv.org/pdf/2301.09451.pdf,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,Nicolas Ballas,ballasn@meta.com,78%
https://arxiv.org/pdf/2301.09451.pdf,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,Quentin Duval,qduval@meta.com,82%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Thibaut Eloy,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Etienne Baudrier,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Marine Laporte,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Virginie Hamel,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Paul Guichard,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Denis Fortun,,0%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Shuaicheng Liu,liushuaicheng@uestc.edu.cn,95%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Haipeng Li,,0%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Kunming Luo,,0%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Bing Zeng,,0%
https://arxiv.org/pdf/2301.09431.pdf,Multi-domain stain normalization for digital pathology: A cycle-consistent adversarial network for whole slide images,Tabea-clara Bucher,tabea.bucher@dkfz.de,95%
https://arxiv.org/pdf/2301.09431.pdf,Multi-domain stain normalization for digital pathology: A cycle-consistent adversarial network for whole slide images,Martin J. Hetz,martinjoachim.hetz@dkfz.de,95%
https://arxiv.org/pdf/2301.09431.pdf,Multi-domain stain normalization for digital pathology: A cycle-consistent adversarial network for whole slide images,Titus J. Brinker,titus.brinker@dkfz.de,95%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Yiyang Shen,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Mingqiang Wei,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Yongzhen Wang,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Xueyang Fu,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Jing Qin,,0%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Jiahao Huang,j.huang21@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Angelica Aviles-rivero,,0%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Carola-bibiane Schonlieb,,0%
https://arxiv.org/pdf/2302.10272.pdf,Is Autoencoder Truly Applicable for 3D CT Super-Resolution?,Weixun Luo,,0%
https://arxiv.org/pdf/2302.10272.pdf,Is Autoencoder Truly Applicable for 3D CT Super-Resolution?,Xiaodan Xing,,0%
https://arxiv.org/pdf/2302.10272.pdf,Is Autoencoder Truly Applicable for 3D CT Super-Resolution?,Guang Yang,,0%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Hao Wen,wenhao@tju.edu.cn,95%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Haozhe Lin,linhz@tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Huili Cui,huilicui 1@tju.edu.cn,95%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Kun Li,lik@tju.edu.cn,78%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Yukun Lai,LaiY4@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Lu Fang,fanglu@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Jing Huang,,0%
https://arxiv.org/pdf/2301.09339.pdf,Computer Vision for a Camel-Vehicle Collision Mitigation System,Khalid Alnujaidi,,0%
https://arxiv.org/pdf/2301.09339.pdf,Computer Vision for a Camel-Vehicle Collision Mitigation System,Ghadah Alhabib,,0%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Eva Vandersmissen,eva.vandersmissen@agfa.com,95%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Annemiek Snoeckx,Annemiek.Snoeckx@uza.be,95%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Dimitrios Lenis,lenis@vrvis.at,78%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Jeroen Cant,jeroen.cant@agfa.com,95%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Maria Wimmer,mwimmer@vrvis.at,82%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Astrid Berg,berg@vrvis.at,78%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Theresa Neubauer,tneubauer@vrvis.at,82%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,David Major,major@vrvis.at,78%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Katja Bühler,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Neus Rodeja Ferrer,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Malini Vendela Sagar,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Kiril Vadimovic Klein,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Christina Kruuse,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Mads Nielsen,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Mostafa Mehdipour Ghazi,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Johannes Jakubik,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Michal Muszynski,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Michael Vössing,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Niklas Kühl,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Thomas Brunschwiler,,0%
https://arxiv.org/pdf/2301.09315.pdf,AI-Based Framework for Understanding Car Following Behaviors of Drivers in A Naturalistic Driving Environment,Yaw Adu-gyamfi,adugyamfiym@missouri.edu,75%
https://arxiv.org/pdf/2301.09315.pdf,AI-Based Framework for Understanding Car Following Behaviors of Drivers in A Naturalistic Driving Environment,Armstrong Aboah,,0%
https://arxiv.org/pdf/2301.09315.pdf,AI-Based Framework for Understanding Car Following Behaviors of Drivers in A Naturalistic Driving Environment,Abdul Rashid Mussah,,0%
https://arxiv.org/pdf/2301.09299.pdf,Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay,Yinheng Li,,0%
https://arxiv.org/pdf/2301.09299.pdf,Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay,Han Ding,,0%
https://arxiv.org/pdf/2301.09299.pdf,Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay,Shaofei Wang,,0%
https://arxiv.org/pdf/2301.09282.pdf,Classification of Luminal Subtypes in Full Mammogram Images Using Transfer Learning,Adarsh Bhandary Panambur,,0%
https://arxiv.org/pdf/2301.09282.pdf,Classification of Luminal Subtypes in Full Mammogram Images Using Transfer Learning,Prathmesh Madhu,,0%
https://arxiv.org/pdf/2301.09282.pdf,Classification of Luminal Subtypes in Full Mammogram Images Using Transfer Learning,Andreas Maier,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Brian Li,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Steven Palayew,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Francis Li,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Saad Abbasi,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Saeejith Nair,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Alexander Wong,,0%
https://arxiv.org/pdf/2301.09266.pdf,FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing Flows,Aditya Kallappa,aditya.kallappa@research.iiit.ac.in,95%
https://arxiv.org/pdf/2301.09266.pdf,FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing Flows,Sandeep Nagar,sandeep.nagar@research.iiit.ac.in,95%
https://arxiv.org/pdf/2301.09266.pdf,FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing Flows,Girish Varma,girish.varma@iiit.ac.in,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Mahdi Zolnouri,mahdi.zolnouri@huawei.com,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Sébastien Le Digabel,sebastien.le.digabel@gerad.ca,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Eyyüb Sari,eyyub.sari@huawei.com,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Christophe Tribes,christophe.tribes@polymtl.ca,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Dounia Lakhmiri,dounia.lakhmiri@polymtl.ca,95%
https://arxiv.org/pdf/2301.09257.pdf,Real-Time Simultaneous Localization and Mapping with LiDAR intensity,Giovanni Beltrame,giovanni.beltrame@polymtl.ca,95%
https://arxiv.org/pdf/2301.09257.pdf,Real-Time Simultaneous Localization and Mapping with LiDAR intensity,Wenqiang Du,wenqiang.du@polymtl.ca,95%
https://arxiv.org/pdf/2301.09255.pdf,Combined Use of Federated Learning and Image Encryption for Privacy-Preserving Image Classification with Vision Transformer,Hitoshi Kiya,kiya@tmu.ac.jp,78%
https://arxiv.org/pdf/2301.09255.pdf,Combined Use of Federated Learning and Image Encryption for Privacy-Preserving Image Classification with Vision Transformer,Teru Nagamori,nagamori-teru@ed.tmu.ac.jp,95%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Peter A. Beerel,pabeerel@usc.edu,82%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Jacqueline Liu,jtliu@usc.edu,82%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Souvik Kundu,souvikk.kundu@intel.com,95%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Shunlin Lu,shunlinlu@usc.edu,95%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Yuke Zhang,yukezhan@usc.edu,85%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Huan Lei,,0%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Ruitao Leng,,0%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Liang Zheng,,0%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Hongdong Li,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Yadan Luo,y.luo@uq.edu.au,82%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Zhuoxiao Chen,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Zijian Wang,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Xin Yu,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Zi Huang,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Mahsa Baktashmotlagh,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Chandana Raju,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Sumedh Vilas Datar,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Kushala Hari,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Kavin Vijay,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Suma Ningappa,,0%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,Nikolaos Stathoulopoulos,niksta@ltu.se,60%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,Anton Koval,,0%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,Ali-akbar Agha-mohammadi,,0%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,George Nikolakopoulos,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Razvan-george Pasca,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Alexey Gavryushin,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Muhammad Hamza,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Yen-ling Kuo,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Kaichun Mo,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Luc Van Gool,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Otmar Hilliges,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Xi Wang,,0%
https://arxiv.org/pdf/2301.09190.pdf,Apples and Oranges? Assessing Image Quality over Content Recognition,Junyong You,,0%
https://arxiv.org/pdf/2301.09190.pdf,Apples and Oranges? Assessing Image Quality over Content Recognition,Zheng Zhang,,0%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Luis F. Gomez,luisf.gomez@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Aythami Morales,aythami.morales@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Julian Fierrez,julian.ﬁerrez@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Ruben Tolosana,ruben.tolosana@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Ruth Cobos,ruth.cobos@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Roberto Daza,roberto.daza@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Javier Ortega-garcia,javier.ortega@uam.es,85%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Kevin Mcguinness,kevin.mcguinness@dcu.ie,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Eric Arazo,eric.arazo@insight-centre.org,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Alexandru Drimbarean,Alexandru.Drimbarean@xperi.com,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Paul Albert,paul.albert@insight-centre.org,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Tarun Krishna,tarun.krishna2@mail.dcu.ie,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Noel E O'connor,noel.oconnor@dcu.ie,85%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Alan F Smeaton,alan.smeaton@dcu.ie,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Ayush K Rai,ayush.rai3@mail.dcu.ie,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Mihir Tale,4mihir.tale18@vit.edu,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Sandeep Shinde,1sandeep.shinde@vit.edu,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Aniket Ghorpade,3aniket.ghorpade18@vit.edu,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Tejas Pradhan,2tejas.pradhan18@vit.edu,95%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Jilan Xu,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Junlin Hou,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Yuejie Zhang,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Rui Feng,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Yi Wang,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Yu Qiao,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Weidi Xie,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Minjung Shin,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Yunji Seo,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Jeongmin Bae,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Young Sun Choi,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Hyunsu Kim,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Hyeran Byun,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Youngjung Uh,,0%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Yixuan Yuan,yxyuan@ee.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Junhui Hou,jh.hou@cityu.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Qijian Zhang,qijizhang3-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Yifan Zhang,yzhang3362-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Guoliang Xing,glxing@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Wenqiao Zhang,wenqiao@nus.edu.sg,85%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Linchao Zhu,zhulinchao@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Fei Wu,wufei@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Siliang Tang,siliang@zju.edu.cn,85%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Yi Yang,yangyics@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Juncheng Li,junchengli@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Tat-seng Chua,,0%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Yueting Zhuang,,0%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Jihong Zhu,jhzhu@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Eksan Firkat,eksan@stu.xju.edu.cn.com,85%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Askar Hamdulla,askar@xju.edu.cn,85%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Yucheng Huang,,0%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Ziwang Xiao,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Basilio Caruso,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Trupti Mahendrakar,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Van Minh Nguyen,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Ryan T. White,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Todd Steffen,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Trupti Mahendrakar,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Steven Holmberg,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Andrew Ekblad,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Emma Conti,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Ryan T. White,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Markus Wilde,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Isaac Silver,,0%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Nathan Fischer,nfischer2018@my.fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Isaac Silver,isaac@energymanagementaero.com,85%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Markus Wilde,mwilde@fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Ryan T. White,rwhite@my.fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Brian Kish,bkish@fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Andrew Ekblad,aekblad2019@my.fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Trupti Mahendrakar,tmahendrakar2020@my.fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Ryan T. White,rwhite@fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Brooke Wheeler,bwheeler@fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Isaac Silver,isaac@energymanagementaero.com,85%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Markus Wilde,mwilde@fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Andrew Ekblad,aekblad2019@my.fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Trupti Mahendrakar,Tmahendrakar2020@my.fit.edu,82%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Shengyi Gao,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Zhe Chen,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Guo Chen,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Wenhai Wang,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Tong Lu,,0%
https://arxiv.org/pdf/2301.09015.pdf,E$^3$Pose: Energy-Efficient Edge-assisted Multi-camera System for Multi-human 3D Pose Estimation,Jie Xu,jiexu@miami.edu,95%
https://arxiv.org/pdf/2301.09015.pdf,E$^3$Pose: Energy-Efficient Edge-assisted Multi-camera System for Multi-human 3D Pose Estimation,Letian Zhang,,0%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Yash Patel,yspatel@uwm.edu,82%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Zeyun Yu,yuz@uwm.edu,78%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Hosein Barzekar,barzekar@uwm.edu,78%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Ling Tong,ltong@uwm.edu,82%
https://arxiv.org/pdf/2302.08503.pdf,Unpaired Image-to-Image Translation with Limited Data to Reveal Subtle Phenotypes,Anis Bourou,anis.bourou@ens.fr,95%
https://arxiv.org/pdf/2302.08503.pdf,Unpaired Image-to-Image Translation with Limited Data to Reveal Subtle Phenotypes,Auguste Genovesio,auguste.genovesio@ens.psl.eu,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,William Ljungbergh,william.ljungbergh@liu.se,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,Christoffer Petersson,christoffer.petersson@zenseact.com,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,Joakim Johnander,joakim.johnander@zenseact.com,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,Michael Felsberg,michael.felsberg@liu.se,95%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Xiaofeng Liu,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Fangxu Xing,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Hanna K. Gaggin,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,C. -c. Jay Kuo,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Georges El Fakhri,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Jonghye Woo,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Saeed Anwar,saeed.anwar@kfupm.edu.sa,95%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Muhammad Ibrahim,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Naveed Akhtar,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Michael Wise,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Ajmal Mian,,0%
https://arxiv.org/pdf/2301.08951.pdf,Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction,Bin Li,libin@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.08951.pdf,Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction,Chengmin Gao,,0%
https://arxiv.org/pdf/2301.08939.pdf,Counterfactual Explanation and Instance-Generation using Cycle-Consistent Generative Adversarial Networks,Tehseen Zia,tehseen.zia@comsats.edu.pk,95%
https://arxiv.org/pdf/2301.08939.pdf,Counterfactual Explanation and Instance-Generation using Cycle-Consistent Generative Adversarial Networks,Zeeshan Nisar,,0%
https://arxiv.org/pdf/2301.08939.pdf,Counterfactual Explanation and Instance-Generation using Cycle-Consistent Generative Adversarial Networks,Shakeeb Murtaza,,0%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Ping Tan,pingtan@ust.hk,95%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Luwei Yang,luweiy@sfu.ca,85%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Weihao Yuan,qianmu.ywh@alibaba-inc.com,65%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Zilong Dong,dadong.gxd@alibaba-inc.com,78%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Heng Li,lh.heng.li@connect.ust.hk,95%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Xiaodong Gu,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Shihao Zhang,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Linlin Yang,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Michael Bi Mi,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Xiaoxu Zheng,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Angela Yao,,0%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Houqiang Li,lihq@ustc.edu.cn,78%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Keyi Zhou,kyzhou2000@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Jiajun Deng,jiajun.deng@adelaide.edu.au,95%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Hao Feng,haof@mail.ustc.edu.cn,85%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Yufei Yin,yinyufei@mail.ustc.edu.cn,95%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Wengang Zhou,,0%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Qi Sun,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Fouzia Altaf,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Syed M. S. Islam,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Naeem K. Janjua,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Naveed Akhtar,,0%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Chi-man Pun,cmpun@umac.mo,82%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Zinuo Li,,0%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Xuhang Chen,,0%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Shuqiang Wang,,0%
https://arxiv.org/pdf/2301.08874.pdf,Improving Zero-Shot Action Recognition using Human Instruction with Text Description,Hiroshi Kera,kera@chiba-u.jp,78%
https://arxiv.org/pdf/2301.08874.pdf,Improving Zero-Shot Action Recognition using Human Instruction with Text Description,Kazuhiko Kawamoto,kawa@faculty.chiba-u.jp,90%
https://arxiv.org/pdf/2301.08874.pdf,Improving Zero-Shot Action Recognition using Human Instruction with Text Description,Nan Wu,gonan@chiba-u.jp,85%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Eric Z. Chen,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Chi Zhang,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Xiao Chen,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Yikang Liu,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Terrence Chen,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Shanhui Sun,,0%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Weixuan Liang,weixuanliang@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Siwei Wang,wangsiwei13@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Jiyuan Liu,liujiyuan13@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Xinhang Wan,wanxinhang@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Lu Zhou,lu.zhou@nuaa.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Yi Wen,wenyi21@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,En Zhu,enzhu@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Xinwang Liu,xinwangliu@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Zhe Liu,zhe.liu@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.08849.pdf,CADA-GAN: Context-Aware GAN with Data Augmentation,Sofie Daniels,,0%
https://arxiv.org/pdf/2301.08849.pdf,CADA-GAN: Context-Aware GAN with Data Augmentation,Jiugeng Sun,,0%
https://arxiv.org/pdf/2301.08849.pdf,CADA-GAN: Context-Aware GAN with Data Augmentation,Jiaqing Xie,,0%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Tao Qin,taoqin@microsoft.com,95%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Jiang Bian,jiabia@microsoft.com,65%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Yoshua Bengio,2yoshua.bengio@mila.quebec,95%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Tie-yan Liu,tyliu@microsoft.com,82%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Xu Tan,xuta@microsoft.com,85%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Md Selim,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Jie Zhang,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Michael A. Brooks,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Ge Wang,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Jin Chen,,0%
https://arxiv.org/pdf/2301.08802.pdf,Impact of PCA-based preprocessing and different CNN structures on deformable registration of sonograms,Christian Schmidt,christian.schmidt@w-hs.de,95%
https://arxiv.org/pdf/2301.08802.pdf,Impact of PCA-based preprocessing and different CNN structures on deformable registration of sonograms,Heinrich Martin Overhoff,heinrich-martin.overhoff@w-hs.de,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Rui Kou,kourui.mun@gmail.com,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Satish Kumar,satishkumar@ucsb.edu,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Vikram Jayaram,Vikram.Jayaram@pxd.com,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Henry Hill,,0%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Jake Lempges,,0%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Eric Qian,,0%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Yunan Wu,yunanwu2020@u.northwestern.edu,95%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Amil Dravid,,0%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Ramsey Michael Wehbe,,0%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Aggelos K. Katsaggelos,,0%
https://arxiv.org/pdf/2301.08794.pdf,"Robot Skill Learning Via Classical Robotics-Based Generated Datasets: Advantages, Disadvantages, and Future Improvement",Batu Kaan Oezen,ozenbatukaan@gmail.com,85%
https://arxiv.org/pdf/2301.08784.pdf,Visual Semantic Relatedness Dataset for Image Captioning,Ahmed Sabir,,0%
https://arxiv.org/pdf/2301.08784.pdf,Visual Semantic Relatedness Dataset for Image Captioning,Francesc Moreno-noguer,,0%
https://arxiv.org/pdf/2301.08784.pdf,Visual Semantic Relatedness Dataset for Image Captioning,Lluís Padró,,0%
https://arxiv.org/pdf/2301.08783.pdf,An Asynchronous Intensity Representation for Framed and Event Video Sources,Andrew C. Freeman,acfreeman@cs.unc.edu,82%
https://arxiv.org/pdf/2301.08783.pdf,An Asynchronous Intensity Representation for Framed and Event Video Sources,Montek Singh,montek@cs.unc.edu,85%
https://arxiv.org/pdf/2301.08783.pdf,An Asynchronous Intensity Representation for Framed and Event Video Sources,Ketan Mayer-patel,kmp@cs.unc.edu,98%
https://arxiv.org/pdf/2301.08782.pdf,Estimation of mitral valve hinge point coordinates -- deep neural net for echocardiogram segmentation,Christian Schmidt,christian.schmidt@w-hs.de,95%
https://arxiv.org/pdf/2301.08782.pdf,Estimation of mitral valve hinge point coordinates -- deep neural net for echocardiogram segmentation,Heinrich Martin Overhoff,heinrich-martin.overhoff@w-hs.de,95%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Zhijian Liu,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Xinyu Yang,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Haotian Tang,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Shang Yang,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Song Han,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Changan Chen,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Alexander Richard,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Roman Shapovalov,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Vamsi Krishna Ithapu,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Natalia Neverova,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Andrea Vedaldi,,0%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Sourav Saha,souravsaha0152@gmail.com,95%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Trina Chakraborty,trinasustcse41@gmail.com,85%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Tithi Paul,tithi.cse3.bu@gmail.com,85%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Rejwan Bin Sulaiman,rejwan.binsulaiman@gmail.com,95%
https://arxiv.org/pdf/2302.06389.pdf,Deep-Learning Quantitative Structural Characterization in Additive Manufacturing,Amra Peles,pelesa@ornl.gov,78%
https://arxiv.org/pdf/2302.06389.pdf,Deep-Learning Quantitative Structural Characterization in Additive Manufacturing,Vincent C. Paquit,,0%
https://arxiv.org/pdf/2302.06389.pdf,Deep-Learning Quantitative Structural Characterization in Additive Manufacturing,Ryan R. Dehoff,,0%
https://arxiv.org/pdf/2301.08669.pdf,Holistically Explainable Vision Transformers,Moritz Böhle,,0%
https://arxiv.org/pdf/2301.08669.pdf,Holistically Explainable Vision Transformers,Mario Fritz,,0%
https://arxiv.org/pdf/2301.08669.pdf,Holistically Explainable Vision Transformers,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Liang Mi,liangmi@smail.nju.edu.cn,95%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Haipeng Dai,haipengdai@nju.edu.cn,95%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Xiaoming Fu,fu@cs.uni-goettingen.de,78%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Tingting Yuan,tingting.yuan@cs.uni-goettingen.de,95%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Weijun Wang,weijun.wang@cs.uni-goettingen.de,95%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Justyna P. Zwolak,jpzwolak@nist.gov,82%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Joshua Ziegler,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Florian Luthi,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Mick Ramsey,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Felix Borjans,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Guoji Zheng,,0%
https://arxiv.org/pdf/2301.08647.pdf,Image Memorability Prediction with Vision Transformers,Thomas Hagen,,0%
https://arxiv.org/pdf/2301.08647.pdf,Image Memorability Prediction with Vision Transformers,Thomas Espeseth,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Romain Xu-darme,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Georges Quénot,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Zakaria Chihani,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Marie-christine Rousset,,0%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Laurent Ferro-famil,laurent.ferro-famil@isae-supaero.fr,95%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Yue Huang,yhuang228@gmail.com,82%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Loïc Denis,loic.denis@univ-st-etienne.fr,95%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Zoé Berenger,,0%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Florence Tupin,,0%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Pinar Duygulu,pinar@cs.hacettepe.edu.tr,85%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Emre Akbas,emre@ceng.metu.edu.tr,85%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Nermin Samet,nermin.samet@enpc.fr,95%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Samet Hicsonmez,samethicsonmez@hacettepe.edu.tr,95%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Bernt Schiele,schiele@mpi-inf.mpg.de,78%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Vera Demberg,vera@coli.uni-saarland.de,85%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Xudong Hong,xhong@coli.uni-saarland.de,82%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Asad Sayeed,asad.sayeed@gu.se,95%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Khushboo Mehra,kmehra@coli.uni-saarland.de,82%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Zoltan Galaz,xgalaz00@gmail.com,78%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Jiri Mekyska,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Jan Mucha,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Vojtech Zvoncak,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Zdenek Smekal,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Marcos Faundez-zanuy,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Lubos Brabenec,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Ivona Moravkova,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Irena Rektorova,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Irena Rektorova,irena.rektorova@fnusa.cz,95%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Jan Mucha,mucha@vut.cz,78%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Zoltan Galaz,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Jiri Mekyska,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Marcos Faundez-zanuy,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Vojtech Zvoncak,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Zdenek Smekal,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Lubos Brabenec,,0%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Rizwan Ahmed Khan,Rizwan.Khan@shu.edu.pk,95%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Wardah Ali,,0%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Eesha Qureshi,,0%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Omama Ahmed Farooqi,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Jianyuan Wang,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Lalit Bhagat,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Ceyuan Yang,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Yinghao Xu,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Yujun Shen,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Hongdong Li,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Bolei Zhou,,0%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Seogkyu Jeon,jone9312@yonsei.ac.kr,55%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Hyeran Byun,hrbyun@yonsei.ac.kr,82%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Sunhee Hwang,sunheehwang@lguplus.co.kr,95%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Pilhyeon Lee,,0%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Minjung Shin,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Dongsik Yoon,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Jeong-gi Kwak,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Yuanming Li,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,David Han,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Hanseok Ko,,0%
https://arxiv.org/pdf/2301.08433.pdf,Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction,Nan Meng,nanmeng@hku.hk,95%
https://arxiv.org/pdf/2301.08433.pdf,Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction,Shansi Zhang,sszhang@eee.hku.hk,82%
https://arxiv.org/pdf/2301.08433.pdf,Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction,Edmund Y. Lam,elam@eee.hku.hk,82%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Zhenghao Zhang,zhangzhenghao.zzh@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Siyu Zhu,siting.zsy@alibaba-inc.com,60%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Zuozhuo Dai,zuozhuo.dzz@alibaba-inc.com,85%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Fangtao Shao,shaofangtao.sft@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Yong Liu,yongliu@iipc.zju.edu.cn,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Junyu Zhu,junyuzhu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Hongbo Zhang,zhanghongbo888@huawei.com,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Wanlong Li,liwanlong@huawei.com,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Lina Liu,linaliu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Feng Wen,wenfeng3@huawei.com,95%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Yan Li,yanli@shnu.edu.cn,95%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Wenming Cao,wmcao@szu.edu.cn,82%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Hong Wang,wanghong1@cetc.com.cn,95%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Guitao Cao,gtcao@sei.ecnu.edu.cn,82%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Chunwei Wu,,0%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Xidong Xi,,0%
https://arxiv.org/pdf/2301.09420.pdf,On Multi-Agent Deep Deterministic Policy Gradients and their Explainability for SMARTS Environment,Aditya Malte,malte@usc.edu,78%
https://arxiv.org/pdf/2301.09420.pdf,On Multi-Agent Deep Deterministic Policy Gradients and their Explainability for SMARTS Environment,Ansh Mittal,anshm@usc.edu,85%
https://arxiv.org/pdf/2301.08408.pdf,Identity masking effectiveness and gesture recognition: Effects of eye enhancement in seeing through the mask,Madeline Rachow,mrachow@uark.edu,82%
https://arxiv.org/pdf/2301.08408.pdf,Identity masking effectiveness and gesture recognition: Effects of eye enhancement in seeing through the mask,Thomas Karnowski,karnowskitp@ornl.gov,78%
https://arxiv.org/pdf/2301.08408.pdf,Identity masking effectiveness and gesture recognition: Effects of eye enhancement in seeing through the mask,Alice J. O'toole,otoole@utdallas.edu,55%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Miao Yin,miao.yin@rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Yang Sui,yang.sui@rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Dingwen Tao,ditao@iu.edu,82%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Lizhi Xiang,lizhi.xiang@wsu.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Jinqi Xiao,jinqi.xiao@rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Bo Yuan,bo.yuan@soe.rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Chengming Zhang,,0%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Yu Gong,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Malik Boudiaf,lik.boudiaf.1@etsmtl.net,78%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Etienne Bennequin,etienneb@sicara.com,85%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Myriam Tami,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Antoine Toubhans,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Pablo Piantanida,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Céline Hudelot,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Ismail Ben Ayed,,0%
https://arxiv.org/pdf/2301.08387.pdf,Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree Canopies,George Kantor,kantor@andrew.cmu.edu,78%
https://arxiv.org/pdf/2301.08387.pdf,Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree Canopies,Chung Hee Kim,chunghek@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,George Yiasemis,,0%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,Clara I. Sánchez,,0%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,Jan-jakob Sonke,,0%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,Jonas Teuwen,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Antanas Kascenas,antanas.kascenas@mre.medical.canon,95%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Pedro Sanchez,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Patrick Schrempf,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Chaoyang Wang,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,William Clackett,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Shadia S. Mikhael,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Jeremy P. Voisey,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Keith Goatman,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Alexander Weir,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Nicolas Pugeault,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Sotirios A. Tsaftaris,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Alison Q. O'neil,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Chiara Di Vece,chiara.divece.20@ucl.ac.uk,95%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Maela Le Lous,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Brian Dromey,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Francisco Vasconcelos,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Anna L David,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Donald Peebles,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Danail Stoyanov,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Chao-yuan Wu,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Justin Johnson,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Jitendra Malik,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Christoph Feichtenhofer,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Georgia Gkioxari,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Pierluigi Zama Ramirez,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Alex Costanzino,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Fabio Tosi,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Matteo Poggi,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Samuele Salti,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Stefano Mattoccia,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Luigi Di Stefano,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Mahmoud Assran,massran@meta.com,82%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Quentin Duval,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Ishan Misra,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Piotr Bojanowski,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Pascal Vincent,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Michael Rabbat,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Yann Lecun,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Nicolas Ballas,,0%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,Feng Cheng,fengchan@cs.unc.edu,85%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,Xizi Wang,xiziwang@iu.edu,95%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,Gedas Bertasius,gedas@cs.unc.edu,85%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,David Crandall,,0%
https://arxiv.org/pdf/2301.08229.pdf,Estimating Remaining Lifespan from the Face,Amir Fekrazad,afekrazad@tamusa.edu,82%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Mihir Durve,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Sibilla Orsini,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Adriano Tiribocchi,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Andrea Montessori,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Jean-michel Tucny,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Marco Lauricella,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Andrea Camposeo,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Dario Pisignano,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Sauro Succi,,0%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,George Deligiannidis,deligian@stats.ox.ac.uk,90%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Fabian Falck,fabian.falck@stats.ox.ac.uk,95%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Dominic Danks,ddanks@turing.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Matthew Willetts,mwilletts@turing.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Christopher Williams,williams@stats.ox.ac.uk,78%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Christopher Yau,cyau@turing.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Chris Holmes,cholmes@stats.ox.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Arnaud Doucet,doucet@stats.ox.ac.uk,78%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Alistair Weld,a.weld20@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Michael Dyck,michael.dyck@dlr.de,95%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Julian Klodmann,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Giulio Anichini,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Luke Dixon,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Sophie Camp,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Alin Albu-schäffer,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Stamatia Giannarou,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Huafeng Liu,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Pai Peng,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Tao Chen,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Qiong Wang,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Yazhou Yao,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Xian-sheng Hua,,0%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Luigi Manfredi,l.manfredi@dundee.ac.uk,82%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Alwyn Mathew,,0%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Ludovic Magerand,,0%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Emanuele Trucco,,0%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Yunzhao Zeng,zengyunzhao@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Li Chen,chenli.phd@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Ming Wu,wuming@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Hao Yang,yang.hao@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Min Zheng,zhengmin.666@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Shizun Wang,wangshizun@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Xu Wang,wangxu.ailab@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Chuang Zhang,zhangchuang@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Yi Yuan,yuanyi.cv@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Weihong Zeng,zengweihong@bytedance.com,95%
https://arxiv.org/pdf/2301.08147.pdf,"RGB-D-Based Categorical Object Pose and Shape Estimation: Methods, Datasets, and Evaluation",Leonard Bruns,leonardb@kth.se,85%
https://arxiv.org/pdf/2301.08147.pdf,"RGB-D-Based Categorical Object Pose and Shape Estimation: Methods, Datasets, and Evaluation",Patric Jensfelt,patric@kth.se,85%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Alistair Weld,a.weld20@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Joao Cartucho,,0%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Chi Xu,,0%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Joseph Davids,,0%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Stamatia Giannarou,,0%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Conghao Xiong,chxiong21@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Irwin King,king@cse.cuhk.edu.hk,78%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Joseph J. Y. Sung,josephsung@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Hao Chen,,0%
https://arxiv.org/pdf/2301.08113.pdf,Soft Thresholding for Visual Image Enhancement,Christoph Dalitz,christoph.dalitz@hsnr.de,95%
https://arxiv.org/pdf/2301.08092.pdf,RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation,Yingzhen Yang,yyang409@asu.edu,82%
https://arxiv.org/pdf/2301.08092.pdf,RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation,Utkarsh Nath,unath@asu.edu,82%
https://arxiv.org/pdf/2301.08092.pdf,RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation,Yancheng Wang,ywan1053@asu.edu,65%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Leyuan Fang,fangleyuan@gmail.com,95%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Yue Deng,yuedeng.thu@gmail.com,95%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Shaobo Xia,shaobo.xia@csust.edu.cn,95%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Jiayi Ma,jyma2010@gmail.com,82%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Jun Yue,,0%
https://arxiv.org/pdf/2301.08067.pdf,Interpreting CNN Predictions using Conditional Generative Adversarial Networks,R T Akash Guna,sikhakrishnanunni@gmail.com,85%
https://arxiv.org/pdf/2301.08067.pdf,Interpreting CNN Predictions using Conditional Generative Adversarial Networks,Raul Benitez,raul.benitez@upc.edu,95%
https://arxiv.org/pdf/2301.08067.pdf,Interpreting CNN Predictions using Conditional Generative Adversarial Networks,O K Sikha,,0%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Julia Wolleb,julia.wolleb@unibas.ch,95%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Florentin Bieder,florentin.bieder@unibas.ch,95%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Philippe C. Cattin,philippe.cattin@unibas.ch,95%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Robin Sandkühler,robin.sandkuehler@unibas.ch,85%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Dongsik Yoon,kevinds1106@korea.ac.kr,60%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Youngsaeng Jin,youngsjin@korea.ac.kr,82%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Hanseok Ko,hsko@korea.ac.kr,82%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Jeonggi Kwak,,0%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Yuanming Li,,0%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,David Han,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Rosalba Calvini,rosalba.calvini@unimore.it,95%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Veronica Ferrari,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Bas Boom,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Camilla Menozzi,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Aravind Krishnaswamy Rangarajan,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Lara Maistrello,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Peter Offermans,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Alessandro Ulrici,,0%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Sylwia Majchrowska,sylwia.majchrowska@ai.se,95%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Jakub Nalepa,jnalepa@ieee.org,82%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Maria Ferlin,maria.ferlin@pg.edu.pl,95%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Alicja Kwaśniwska,alicja@sima.ai,85%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Marta Plantykow,m.plantykow@gmail.com,82%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Agnieszka Mikołajczyk-bareła,agnieszka.mikolajczyk@voicelab.ai,85%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Milena Olech,milena.w.olech@gmail.com,95%
https://arxiv.org/pdf/2301.08555.pdf,Hybrid Open-set Segmentation with Synthetic Negative Data,Siniša Šegvić,sinisa.segvic@fer.hr,95%
https://arxiv.org/pdf/2301.08555.pdf,Hybrid Open-set Segmentation with Synthetic Negative Data,Matej Grcić,matej.grcic@fer.hr,95%
https://arxiv.org/pdf/2301.07969.pdf,Fast Inference in Denoising Diffusion Models via MMD Finetuning,Emanuele Aiello,name.surname@polito.it,60%
https://arxiv.org/pdf/2301.07969.pdf,Fast Inference in Denoising Diffusion Models via MMD Finetuning,Diego Valsesia,,0%
https://arxiv.org/pdf/2301.07969.pdf,Fast Inference in Denoising Diffusion Models via MMD Finetuning,Enrico Magli,,0%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Xiaoguang Han,hanxiaoguang@cuhk.edu.cn,95%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Bingchen Gong,gongbingchen@gmail.com,95%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Qi Dou,qidou@cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Yuehao Wang,yhwang@link.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Boris Mocialov,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Eirik Eythorsson,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Reza Parseh,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Hoang Tran,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Vegard Flovik,,0%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Mengmeng Wang,mengmengwang@zju.edu.cn,95%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Boyu Mu,muboyu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Jiazheng Xing,jiazhengxing@zju.edu.cn,95%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Yong Liu,yongliu@iipc.zju.edu.cn,95%
https://arxiv.org/pdf/2301.07927.pdf,Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning,Jun Huang,huangj@sari.ac.cn,78%
https://arxiv.org/pdf/2301.07927.pdf,Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning,Shuzhen Rao,,0%
https://arxiv.org/pdf/2301.07927.pdf,Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning,Zengming Tang,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Snehashis Majhi,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Rui Dai,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Quan Kong,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Lorenzo Garattoni,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Gianpiero Francesca,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Francois Bremond,,0%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Xiuen Wu,xiuen_wu@163.com,95%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Tao Wang,twang@mju.edu.cn,82%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Fum Yew Ching,fyching@student.usm.my,82%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Lingyu Liang,eelyliang@scut.edu.cn,78%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Zuoyong Li,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Barak Or,barak@almatechnologies.com,85%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Areej Eweida,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Nimord Segol,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Maxim Freydin,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Niv Sfaradi,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Dongdong Liu,ddliu@nyu.edu,82%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Hang Zhang,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Rongguang Wang,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Jinwei Zhang,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Chao Li,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Jiahao Li,,0%
https://arxiv.org/pdf/2301.07879.pdf,Unposed: Unsupervised Pose Estimation based Product Image Recommendations,Faizan Ahemad,ahemf@amazon.com,73%
https://arxiv.org/pdf/2301.07879.pdf,Unposed: Unsupervised Pose Estimation based Product Image Recommendations,Saurabh Sharma,sharsar@amazon.com,65%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Bin Huang,huangbin1@senseauto.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Yangguang Li,liyangguang@sensetime.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Fenggang Liu,liufenggang@senseauto.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Luya Wang,wangluya@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Enze Xie,xieenze@connect.hku.hk,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Ping Luo,pluo@cs.hku.hk,82%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Mingzhu Shen,shenmingzhu@sensetime.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Jing Shao,shaojing@senseauto.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Feng Liang,,0%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Tianqi Wang,,0%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Bowen Zhang,zhangbowen.17@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Xiaojie Jin,jinxiaojie@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Kai Xu,xukai.1993@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Peng Wang,peng.wang@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Jiashi Feng,jshfeng@bytedance.com,82%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Weibo Gong,gongweibo@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Xiaohui Shen,shenxiaohui.kevin@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Xueqing Deng,xueqingdeng@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Zhao Zhang,,0%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Junde Wu,jundewu@ieee.org,95%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Yanwu Xu,ywxu@ieee.org,82%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Min Xu,xumin100@gmail.com,95%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Wei Ji,,0%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Huazhu Fu,,0%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Yueming Jin,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Yue Han,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Sri Kalyan Yarlagadda,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Tonmoy Ghosh,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Fengqing Zhu,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Edward Sazonov,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Edward J. Delp,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Qiuhao Zeng,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Wei Wang,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Fan Zhou,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Charles Ling,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Boyu Wang,,0%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Yinfei Yang,yinfei yang@apple.com,95%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Angelos Katharopoulos,a katharopoulos@apple.com,82%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Vaishaal Shankar,vaishaal shankar@apple.com,95%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Tom Gunter,tom gunter@apple.com,95%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Floris Weers,fweers@apple.com,82%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Allan Zhou,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Moo Jin Kim,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Lirui Wang,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Pete Florence,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Chelsea Finn,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Renjie Li,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Chun Yu Lao,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Rebecca St. George,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Katherine Lawler,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Saurabh Garg,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Son N. Tran,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Quan Bai,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Jane Alty,,0%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Ruben Coen-cagli,ruben.coen-cagli@einsteinmed.edu,95%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Jonathan Vacher,jonathan.vacher@u-paris.fr,95%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Claire Launay,,0%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Pascal Mamassian,,0%
https://arxiv.org/pdf/2301.07805.pdf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,Hsiang-wei Huang,hwhuang@uw.edu,82%
https://arxiv.org/pdf/2301.07805.pdf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,Cheng-yen Yang,cycyang@uw.edu,82%
https://arxiv.org/pdf/2301.07805.pdf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,Jenq-neng Hwang,hwang@uw.edu,78%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Megan M. Baker,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Alexander New,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Mario Aguilar-simon,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ziad Al-halah,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Sébastien M. R. Arnold,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ese Ben-iwhiwhu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Andrew P. Brna,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ethan Brooks,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ryan C. Brown,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Zachary Daniels,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Anurag Daram,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Fabien Delattre,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ryan Dellana,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Eric Eaton,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Haotian Fu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Jesse Hostetler,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Shariq Iqbal,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Cassandra Kent,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Nicholas Ketz,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Soheil Kolouri,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,George Konidaris,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Dhireesha Kudithipudi,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Erik Learned-miller,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Seungwon Lee,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Michael L. Littman,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Sandeep Madireddy,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Jorge A. Mendez,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Eric Q. Nguyen,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Christine D. Piatko,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Praveen K. Pilly,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Aswin Raghavan,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Abrar Rahman,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Santhosh Kumar Ramakrishnan,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Neale Ratzlaff,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Andrea Soltoggio,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Peter Stone,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Indranil Sur,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Zhipeng Tang,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Saket Tiwari,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Kyle Vedder,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Felix Wang,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Zifan Xu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Angel Yanguas-gil,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Harel Yedidsion,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Shangqun Yu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Gautam K. Vallabha,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Zifan Shi,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Yujun Shen,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Yinghao Xu,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Sida Peng,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Yiyi Liao,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Sheng Guo,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Qifeng Chen,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Dit-yan Yeung,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Ziyu Su,zsu@wakehealth.edu,82%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Mostafa Rezapour,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Usama Sajjad,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Metin Nafi Gurcan,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Muhammad Khalid Khan Niazi,,0%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Xiao Fu,fuxiao@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Ziwei Liu,ziwei.liu@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Jiawei Ren,jiawei011@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Yuxin Wang,ywangom@connect.ust.hk,82%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Chen Qian,qianchen@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Lei Yang,yanglei@sensetime.com,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Wayne Wu,wuwenyan0503@gmail.com,82%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Jiaqi Wang,wangjiaqi@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Liang Pan,liang.pan@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Tong Wu,,0%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Jiarui Zhang,,0%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Guanghui Yue,yueguanghui@szu.edu.cn,95%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Hantao Liu,liuh35@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Wei Zhou,wei.zhou@uwaterloo.ca,95%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Yipeng Qin,qiny16@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Ruizeng Zhang,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Xingyi He,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Jiaming Sun,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Yuang Wang,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Di Huang,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Hujun Bao,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Xiaowei Zhou,,0%
https://arxiv.org/pdf/2301.07670.pdf,Active learning for medical image segmentation with stochastic batches,Mélanie Gaillochet,,0%
https://arxiv.org/pdf/2301.07670.pdf,Active learning for medical image segmentation with stochastic batches,Christian Desrosiers,,0%
https://arxiv.org/pdf/2301.07670.pdf,Active learning for medical image segmentation with stochastic batches,Hervé Lombaert,,0%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Daniel Cremers,cremers@tum.de,78%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Nan Yang,nan.yang@tum.de,95%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Felix Wimbauer,felix.wimbauer@tum.de,95%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Christian Rupprecht,chrisr@robots.ox.ac.uk,81%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,Suya You,suya.you.civ@army.mil,95%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,A S M Iftekhar,,0%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,Raphael Ruschel,,0%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,Satish Kumar,,0%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,B. S. Manjunath,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Yangang Wang,yangangwang@seu.edu.cn,95%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Wei Xie,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Zhipeng Yu,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Zimeng Zhao,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Binghui Zuo,,0%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Diana Carolina Lopez-medina,diana.lopezme@campusucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Renato Zambrano-cruz,renato.zambrano@ucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Victor H. Aristizabal-tique,victor.aristizabalt@campusucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Marcela Henao-perez,marcela.henaop@campusucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Gloria Diaz-londoñod,,0%
https://arxiv.org/pdf/2301.07634.pdf,Training Semantic Segmentation on Heterogeneous Datasets,Panagiotis Meletis,p.c.meletis@tue.nl,82%
https://arxiv.org/pdf/2301.07634.pdf,Training Semantic Segmentation on Heterogeneous Datasets,Gijs Dubbelman,g.dubbelman@tue.nl,82%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Huadeng Wang,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Hao Xu,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Bingbing Li,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Xipeng Pan,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Lingqi Zeng,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Rushi Lan,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Xiaonan Luo,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Muhammad Ali Farooq,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Waseem Shariff,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Faisal Khan,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Peter Corcoran,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Henry Zheng,jh-zheng22@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Gao Huang,gaohuang@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Shiji Song,shijis@tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Rui Huang,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Xuran Pan,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Haojun Jiang,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Zhifeng Xie,,0%
https://arxiv.org/pdf/2301.07583.pdf,A Survey of Advanced Computer Vision Techniques for Sports,Tiago Mendes-neves,,0%
https://arxiv.org/pdf/2301.07583.pdf,A Survey of Advanced Computer Vision Techniques for Sports,Luís Meireles,,0%
https://arxiv.org/pdf/2301.07583.pdf,A Survey of Advanced Computer Vision Techniques for Sports,João Mendes-moreira,,0%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Jitka Kostkova,kostkova@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Jan Flusser,ﬂusser@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Matej Lebl,lebl@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Filip Sroubek,sroubekf@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Matteo Pedone,,0%
https://arxiv.org/pdf/2301.07565.pdf,Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using a New Frame Selection Policy and Gating Mechanism,Vasileios Mezaris,bmezaris@iti.gr,78%
https://arxiv.org/pdf/2301.07565.pdf,Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using a New Frame Selection Policy and Gating Mechanism,Dimitrios Daskalakis,dimidask@iti.gr,65%
https://arxiv.org/pdf/2301.07565.pdf,Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using a New Frame Selection Policy and Gating Mechanism,Nikolaos Gkalelis,gkalelis@iti.gr,78%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Michele Buzzicotti,michele.buzzicotti@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Luca Biferale,luca.biferale@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Fabio Bonaccorso,fabio.bonaccorso@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Tianyi Li,tianyi.li@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Tao Wang,twang@mju.edu.cn,82%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Yuanzheng Cai,yuanzheng_cai@mju.edu.cn,95%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Lingyu Liang,eelyliang@scut.edu.cn,78%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Zhongzheng Huang,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Yuanyuan Peng,pengmi467347713@126.com,78%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Lin Pan,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Pengpeng Luan,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Hongbin Tu,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Xiong Li,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Jérémy Chopin,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Jean-baptiste Fasquel,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Harold Mouchère,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Rozenn Dahyot,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Isabelle Bloch,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Aviad Aberdam,aaberdam@amazon.com,82%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,David Bensaïd,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Alona Golts,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Roy Ganz,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Oren Nuriel,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Royee Tichauer,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Shai Mazor,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Ron Litman,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Jiashi Feng,jshfeng@bytedance.com,82%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Fan Ma,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Xiaojie Jin,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Heng Wang,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Jingjia Huang,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Linchao Zhu,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Yi Yang,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Carlo Metta,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Riccardo Guidotti,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Yuan Yin,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Patrick Gallinari,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Salvatore Rinzivillo,,0%
https://arxiv.org/pdf/2301.07431.pdf,Sharp Eyes: A Salient Object Detector Working The Same Way as Human Visual Characteristics,Yahong Guo,guoyh@qlu.edu.cn,78%
https://arxiv.org/pdf/2301.07431.pdf,Sharp Eyes: A Salient Object Detector Working The Same Way as Human Visual Characteristics,Jinbao Li,jinb@sdas.org,90%
https://arxiv.org/pdf/2301.07431.pdf,Sharp Eyes: A Salient Object Detector Working The Same Way as Human Visual Characteristics,Ge Zhu,zhuge@hlju.edu.cn,95%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Yushu Zhang,yushu@nuaa.edu.cn,85%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Xiaochun Cao,caoxiaochun@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Chao Wang,c.wang@nuaa.edu.cn,82%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Yong Xiang,yong.xiang@deakin.edu.au,95%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Tao Xiang,txiang@cqu.edu.cn,82%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Shuren Qi,,0%
https://arxiv.org/pdf/2301.07407.pdf,TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks,Vasileios Mezaris,bmezaris@iti.gr,78%
https://arxiv.org/pdf/2301.07407.pdf,TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks,Mariano Ntrougkas,ntrougkas@iti.gr,78%
https://arxiv.org/pdf/2301.07407.pdf,TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks,Nikolaos Gkalelis,gkalelis@iti.gr,78%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Cédric Demonceaux,cedric.demonceaux@u-bourgogne.fr,95%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Guillaume Allibert,allibert@i3s.unice.fr,78%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Chao Ma,chaoma@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Zongwei Wu,,0%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Fabrice Meriaudeau,,0%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Aviad Aberdam,aaberdam@amazon.com,82%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Roy Ganz,ganz@cs.technion.ac.il,78%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Yair Kittenplon,yairk@amazon.com,85%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Shai Mazor,smazor@amazon.com,82%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Ron Litman,litmanr@amazon.com,78%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Oren Nuriel,,0%
https://arxiv.org/pdf/2301.07385.pdf,Three-dimensional reconstruction and characterization of bladder deformations,Stanislas Rapacchi,stanislas.rapacchi@univ-amu.fr,95%
https://arxiv.org/pdf/2301.07385.pdf,Three-dimensional reconstruction and characterization of bladder deformations,Augustin C. Ogier,augustin.ogier@gmail.com,95%
https://arxiv.org/pdf/2301.07385.pdf,Three-dimensional reconstruction and characterization of bladder deformations,Marc-emmanuel Bellemare,marc-emmanuel.bellemare@univ-amu.fr,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Benedikt Wiestler,b.wiestler@tum.de,82%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Jiancheng Yang,jiancheng.yang@epfl.ch,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Hongwei Bran Li,hongwei.li@tum.de,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Suprosana Shit,suprosanna.shit@tum.de,82%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Bjoern Menze,bjoern.menze@uzh.ch,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Chinmay Prabhakar,chinmay.prabhakar@uzh.ch,95%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Munan Ning,munanning@pku.edu.cn,95%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Shuicheng Yan,yansc@sea.com,78%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Li Yuan,yuanli-ece@pku.edu.cn,95%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Yonghong Tian,yhtian@pku.edu.cn,82%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Donghuan Lu,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Yujia Xie,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Dongdong Chen,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Dong Wei,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Yefeng Zheng,,0%
https://arxiv.org/pdf/2301.07340.pdf,Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant,Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.07340.pdf,Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant,Ying Jin,,0%
https://arxiv.org/pdf/2301.07340.pdf,Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant,Jiaqi Wang,,0%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Dinh Phung,dinh.phung@monash.edu,95%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Son Duy Dao,duy.dao@monash.edu,78%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Jianfei Cai,jianfei.cai@monash.edu,95%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Hengcan Shi,hengcan.shi@monash.edu,95%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Jinkyu Kim,jinkyukim@korea.ac.kr,95%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Gyeongrok Oh,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Sungjune Kim,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Heon Gu,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Sang Ho Yoon,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Sangpil Kim,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jimmy Ren,pubs-permissions@ieee.org,55%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jiawei Zhang,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jinshan Pan,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Daoye Wang,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Shangchen Zhou,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Xing Wei,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Furong Zhao,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jianbo Liu,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Xiaoye Qian,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Youbao Tang,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Ning Zhang,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Mei Han,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Jing Xiao,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Ming-chun Huang,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Ruei-sung Lin,,0%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Jianfeng Weng,jwen0609@uni.sydney.edu.au,65%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Jingya Wang,wangjingya@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Zhiyong Wang,zhiyong.wang@sydney.edu.au,95%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Kun Hu,,0%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Tingting Yao,,0%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Kanghao Chen,kanec9707@gmail.com,65%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Ruixuan Wang,wangruix5@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Wei-shi Zheng,wszheng@ieee.org,82%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Sijia Liu,liusj56@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.07315.pdf,Face Recognition in the age of CLIP & Billion image datasets,Aaditya Bhat,aadityaubhat@gmail.com,95%
https://arxiv.org/pdf/2301.07315.pdf,Face Recognition in the age of CLIP & Billion image datasets,Shrey Jain,,0%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Jun Shu,xjtushujun@gmail.com,95%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Kehui Ding,,0%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Deyu Meng,,0%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Zongben Xu,,0%
https://arxiv.org/pdf/2301.07301.pdf,PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection,Tianyun Zhao,zhaoty@nwpu.edu.cn,78%
https://arxiv.org/pdf/2301.07301.pdf,PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection,Rui Wan,,0%
https://arxiv.org/pdf/2301.07301.pdf,PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection,Wei Zhao,,0%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Aswathnarayan Radhakrishnan,radhakrishnan.39@osu.edu,78%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Jim Davis,davis.1719@osu.edu,78%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Matthew Scherreik,matthew.scherreik.1@us.af.mil,95%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Roman Ilin,roman.ilin.1@us.af.mil,95%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Benjamin Lewis,benjamin.lewis.13@us.af.mil,95%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Zachary Rabin,rabin.30@osu.edu,78%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Cecilia Morales,cgmorale@andrew.cmu.edu,65%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Howie Choset,choset@andrew.cmu.edu,78%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Tejas Rane,tejasr@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Robert Edman,redman@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Jason Yao,jlyao@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Artur Dubrawski,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Andrej Janda,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Brandon Wagstaff,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Edwin G. Ng,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Jonathan Kelly,,0%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Guohang Yan,yanguohang@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Zhuochun Liu,liuzhuochun@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Yikang Li,liyikang@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Zhaotong Luo,luozhaotong@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Gang Chen,chengang08@semi.ac.cn,95%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Jixing Li,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Xiaozhou Guo,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Benzhe Dai,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Guoliang Gong,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Min Jin,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Wenyu Mao,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Huaxiang Lu,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Olivia Weng,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Gabriel Marcano,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Vladimir Loncar,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Alireza Khodamoradi,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Nojan Sheybani,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Andres Meza,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Farinaz Koushanfar,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Kristof Denolf,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Javier Mauricio Duarte,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Ryan Kastner,,0%
https://arxiv.org/pdf/2301.07236.pdf,Effective End-to-End Vision Language Pretraining with Semantic Visual Loss,Fayao Liu,fayaoliu@gmail.com,95%
https://arxiv.org/pdf/2301.07236.pdf,Effective End-to-End Vision Language Pretraining with Semantic Visual Loss,Guosheng Lin,gslin@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.07236.pdf,Effective End-to-End Vision Language Pretraining with Semantic Visual Loss,Xiaofeng Yang,xiaofeng001@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Aaron Carass,carass@jhu.edu,78%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Fangxu Xing,fxing1@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Muhan Shao,muhan@jhu.edu,85%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Zhangxing Bian,zbian4@jhu.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Yihao Liu,yliu236@jhu.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jiachen Zhuo,jzhuo@umm.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jinglun Yu,jyu146@jhu.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jerry L. Prince,prince@jhu.edu,78%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jonghye Woo,jwoo@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Bipasha Sen,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Aditya Agarwal,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Gaurav Singh,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Brojeshwar B.,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Srinath Sridhar,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Madhava Krishna,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Shervin Dehghani,shervin.dehghani@tum.de,95%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Michael Sommersperger,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Peiyao Zhang,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Alejandro Martin-gomez,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Benjamin Busam,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Peter Gehlbach,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Nassir Navab,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,M. Ali Nasseri,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Iulian Iordachita,,0%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Renell Castro,renell@yukoai.com,85%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Shubhra Aich,saich@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Jean Marie Uwabeza Vianney,jean@yukoai.com,85%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Sajith Rajapaksa,sajith@yukoai.com,85%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Farzad Khalvati,farzad.khalvati@utoronto.ca,95%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Marcella Cornia,firstname.lastname@unimore.it,70%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Roberto Bigazzi,,0%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Silvia Cascianelli,,0%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Lorenzo Baraldi,,0%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Rita Cucchiara,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Manthan Patel,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Marco Karrer,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Philipp Bänninger,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Margarita Chli,,0%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Ce Liu,ce.liu@microsoft.com,95%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Kilho Son,kilhoson@microsoft.com,95%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Jianfeng Gao,jfgao@microsoft.com,82%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Chunyuan Li,chunyl@microsoft.com,81%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Jianwei Yang,jianwyan@microsoft.com,65%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Yong Jae Lee,yongjaelee@cs.wisc.edu,95%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Haotian Liu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Yuheng Li,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Haotian Liu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Qingyang Wu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Fangzhou Mu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Jianwei Yang,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Jianfeng Gao,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Chunyuan Li,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Yong Jae Lee,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Bingchen Zhao,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Quan Cui,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Hao Wu,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Osamu Yoshie,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Cheng Yang,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Oisin Mac Aodha,,0%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Vishwa S. Parekh,vparekh@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Adway U. Kanhere,akanhere@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Pranav Kulkarni,pkulkarni@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Paul H. Yi,pyi@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Joël L. Lavanchy,joel.lavanchy@ihu-strasbourg.eu,95%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Armine Vardazaryan,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Pietro Mascagni,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Ai4safechole Consortium,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Didier Mutter,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Nicolas Padoy,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,H. Ayoobi,h.ayoobi@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,H. Kasaei,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,M. Cao,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,R. Verbrugge,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,B. Verheij,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Hanwei Zhang,zhanghanwei0912@gmail.com,95%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Felipe Torres,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Ronan Sicre,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Yannis Avrithis,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Stephane Ayache,,0%
https://arxiv.org/pdf/2301.06975.pdf,Vision Based Machine Learning Algorithms for Out-of-Distribution Generalisation,Hamza Riaz,hamza.riaz2@mail.dcu.ie,95%
https://arxiv.org/pdf/2301.06975.pdf,Vision Based Machine Learning Algorithms for Out-of-Distribution Generalisation,Alan F. Smeaton,,0%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Xiang-li Li,lixl19@mails.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Ralph R. Martin,martinrr@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Tai-jiang Mu,taijiang@tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Shi-min Hu,shimin@tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Meng-hao Guo,,0%
https://arxiv.org/pdf/2301.06961.pdf,Composite Deep Network with Feature Weighting for Improved Delineation of COVID Infection in Lung CT,Pallabi Dutta,duttapallabi2907@gmail.com,95%
https://arxiv.org/pdf/2301.06961.pdf,Composite Deep Network with Feature Weighting for Improved Delineation of COVID Infection in Lung CT,Sushmita Mitra,sushmita@isical.ac.in,85%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Shusheng Yang,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Yixiao Ge,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Kun Yi,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Dian Li,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Ying Shan,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Xiaohu Qie,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Yi Yang,yi@bit.edu.cn,85%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Yu Gao,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Xi Xu,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Tianji Jiang,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Siyuan Chen,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Yufeng Yue,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Mengyin Fu,,0%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Peng Cao,caopeng@mail.neu.edu.cn,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Osmar R. Zaiane,zaiane@cs.ualberta.ca,78%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Xiaoli Liu,liuxiaoli.lxl@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Qingshan Hou,houqingshancv@gmail.com,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Jinzhu Yang,yangjinzhu@cse.neu.edu.cn,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Jiaqi Wang,,0%
https://arxiv.org/pdf/2301.06910.pdf,BSNet: Lane Detection via Draw B-spline Curves Nearby,Haoxin Chen,,0%
https://arxiv.org/pdf/2301.06910.pdf,BSNet: Lane Detection via Draw B-spline Curves Nearby,Mengmeng Wang,,0%
https://arxiv.org/pdf/2301.06910.pdf,BSNet: Lane Detection via Draw B-spline Curves Nearby,Yong Liu,,0%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Zhaohong Deng,dengzhaohong@jiangnan.edu.cn,95%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Yuanyuan Wang,wxwangst@aliyun.com,78%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Kup-sze Choi,thomasks.choi@polyu.edu.hk,78%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Qiongdan Lou,,0%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Shudong Hu,,0%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Shitong Wang,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Benjamin Tams,benjamin.tams@secunet.com,95%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Christian Rathgeb,christian.rathgeb@secunet.com,95%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Johannes Merkle,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Vanessa Nesterowicz,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Ulrike Korte,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Matthias Neu,,0%
https://arxiv.org/pdf/2301.06874.pdf,Training Methods of Multi-label Prediction Classifiers for Hyperspectral Remote Sensing Images,Salma Haidar,salma.haidar@uantwerpen.be,95%
https://arxiv.org/pdf/2301.06874.pdf,Training Methods of Multi-label Prediction Classifiers for Hyperspectral Remote Sensing Images,José Oramas,,0%
https://arxiv.org/pdf/2301.06871.pdf,Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks,Anna Midgley,amidgley@g.harvard.edu,82%
https://arxiv.org/pdf/2301.06871.pdf,Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks,Lars Lien Ankile,larsankile@g.harvard.edu,95%
https://arxiv.org/pdf/2301.06871.pdf,Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks,Sebastian Weisshaar,sweisshaar@g.harvard.edu,82%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Junjie Zhou,zhoujunjie@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Chinwai Chiu,chiuchinwai@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Yongping Xiong,ypxiong@bupt.edu.cn,82%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Xiangyang Gong,xygong@bupt.edu.cn,82%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Fangyu Liu,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Aniket Agarwal,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Alex Zhang,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Karthik Narasimhan,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Igor Gilitschenski,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Vishvak Murahari,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Yash Kant,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Manasi Muglikar,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Leonard Bauersfeld,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Diederik Paul Moeys,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Davide Scaramuzza,,0%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Zhong Ji,jizhong@tju.edu.cn,95%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Xuelong Li,li@nwpu.edu.cn,78%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Di Wang,wangdi2015@tju.edu.cn,95%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Yan Zhang,yzhang1995@tju.edu.cn,82%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Yanwei Pang,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,A. V. Dobshik,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,S. K. Verbitskiy,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,I. A. Pestunov,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,K. M. Sherman,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,Yu. N. Sinyavskiy,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,A. A. Tulupov,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,V. B. Berikov,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Chongshan Lu,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Fukun Yin,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Xin Chen,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Tao Chen,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Gang Yu,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Jiayuan Fan,,0%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Jiawan Zhang,jwzhang@tju.edu.cn,82%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Xiaojie Guo,xj.max.guo@gmail.com,82%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Meng Wang,,0%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Wenjing Dai,,0%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Shehroz S. Khan,shehroz.khan@uhn.ca,95%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Chinchu Thomas,chinchu.thomas@iiitb.ac.in,95%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Dinesh Babu Jayagopi,jdinesh@iiitb.ac.in,85%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Ali Abedi,ali.abedi@uhn.ca,95%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Peng Tu,yh.peng.tu@gmail.com,95%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Xu Xie,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Guo Ai,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Yuexiang Li,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Yawen Huang,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Yefeng Zheng,,0%
https://arxiv.org/pdf/2301.06715.pdf,SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network,H. Jin Kim,hjinkim@snu.ac.kr,82%
https://arxiv.org/pdf/2301.06715.pdf,SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network,Dongseok Shim,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Jing Li,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Di Kang,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Wenjie Pei,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Xuefei Zhe,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Ying Zhang,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Linchao Bao,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Zhenyu He,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Yuchen Wu,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Kun Song,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Fangzheng Zhao,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Jiansheng Chen,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Huimin Ma,,0%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Paul H. Yi,paul.yi@stjude.org,95%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Pranav Kulkarni,pkulkarni@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Adway Kanhere,akanhere@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Vishwa S. Parekh,vparekh@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Hengrong Lan,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Lijie Huang,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Zhiqiang Li,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Jing Lv,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Jianwen Luo,,0%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Prateek Chhikara,prateek.chhikara@housing.com,95%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Harshul Kuhar,harshul.kuhar@housing.com,95%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Anil Goyal,anil.goyal@housing.com,95%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Chirag Sharma,chirag.sharma@housing.com,95%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Changqun Xia,xiachq@pcl.ac.cn,78%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Jia Li,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Shengye Qiao,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Zhirui Zhao,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Chenxi Xie,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Xiaowu Chen,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Fintan O'sullivan,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Kirita-rose Escott,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Rachael C. Shaw,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Andrew Lensen,,0%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Parsa Delavari,parsadlr@student.ubc.ca,85%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Gulcenur Ozturan,,0%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Ozgur Yilmaz,,0%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Ipek Oruc,,0%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Hai-dang Nguyen,nhdang@selab.hcmus.edu.vn,85%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Minh-triet Tran,tmtriet@fit.hcmus.edu.vn,85%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Trong-hieu Nguyen Mau,,0%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Quoc-huy Trinh,,0%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Nhat-tan Bui,,0%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Ramzi Majaj,majaj@umass.edu,78%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Kaidong Chai,kchai@umass.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Haowen Yu,hyu@cs.umass.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Edward Wang,ejaywang@eng.ucsd.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Donghyun Kim,donghyunkim@cs.umass.edu,95%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Upal Mahbub,upalmahbub@yahoo.com,95%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Hava Siegelmann,hava@umass.edu,85%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Tauhidur Rahman,trahman@ucsd.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Francesca Walsh,fnwalsh@umass.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Zhongyang Zhang,,0%
https://arxiv.org/pdf/2301.06629.pdf,Diverse Multimedia Layout Generation with Multi Choice Learning,David D. Nguyen,d.d.nguyen@unsw.edu.au,82%
https://arxiv.org/pdf/2301.06629.pdf,Diverse Multimedia Layout Generation with Multi Choice Learning,Salil S. Kanhere,salil.kanhere@unsw.edu.au,95%
https://arxiv.org/pdf/2301.06629.pdf,Diverse Multimedia Layout Generation with Multi Choice Learning,Surya Nepal,surya.nepal@data61.csiro.au,95%
https://arxiv.org/pdf/2301.06624.pdf,TAAL: Test-time Augmentation for Active Learning in Medical Image Segmentation,Mélanie Gaillochet,melanie.gaillochet.1@ens.etsmtl.ca,95%
https://arxiv.org/pdf/2301.06624.pdf,TAAL: Test-time Augmentation for Active Learning in Medical Image Segmentation,Christian Desrosiers,,0%
https://arxiv.org/pdf/2301.06624.pdf,TAAL: Test-time Augmentation for Active Learning in Medical Image Segmentation,Hervé Lombaert,,0%
https://arxiv.org/pdf/2301.06567.pdf,Scalable Surface Water Mapping up to Fine-scale using Geometric Features of Water from Topographic Airborne LiDAR Data,Hunsoo Song,,0%
https://arxiv.org/pdf/2301.06567.pdf,Scalable Surface Water Mapping up to Fine-scale using Geometric Features of Water from Topographic Airborne LiDAR Data,Jinha Jung,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Jannes Gladrow,jannes.gladrow@microsoft.com,95%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Joowon Lim,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Douglas Kelly,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Greg O'shea,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Govert Verkes,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Ioan Stefanovici,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Sebastian Nowozin,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Benn Thomsen,,0%
https://arxiv.org/pdf/2301.06489.pdf,Simplex Autoencoders,David Naccache,david.naccache@ens.fr,95%
https://arxiv.org/pdf/2301.06489.pdf,Simplex Autoencoders,Aymene Mohammed Bouayed,Aymene.Bouayed@ens.fr,95%
https://arxiv.org/pdf/2301.06443.pdf,Sparse resultant based minimal solvers in computer vision and their connection with the action matrix,Zuzana Kukelova,kukelova@cmp.felk.cvut.cz,78%
https://arxiv.org/pdf/2301.06443.pdf,Sparse resultant based minimal solvers in computer vision and their connection with the action matrix,Snehal Bhayani,,0%
https://arxiv.org/pdf/2301.06443.pdf,Sparse resultant based minimal solvers in computer vision and their connection with the action matrix,Janne Heikkilä,,0%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Xiaotong Li,lixiaotong@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Ling-yu Duan,lingyu@pku.edu.cn,85%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Zixuan Hu,hzxuan@pku.edu.cn,75%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Yixiao Ge,aoge@tencent.com,78%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Yongxing Dai,dai@pku.edu.cn,78%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Jun Liu,,0%
https://arxiv.org/pdf/2303.11223.pdf,Monocular Cyclist Detection with Convolutional Neural Networks,Charles Tang,ctang5@wpi.edu,82%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Zhichao Wei,,0%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Xiaohao Chen,,0%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Mingqiang Chen,,0%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Siyu Zhu,,0%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Junmo Kim,junmo.kim@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Jiwan Hur,jiwan.hur@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Jaehyun Choi,chlwogus@kaist.ac.kr,55%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Jae Young Lee,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Hyung-min Park,hpark@sogang.ac.kr,82%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Jeongkyun Park,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Jung-wook Hwang,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Kwanghee Choi,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Seung-hyun Lee,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Jun Hwan Ahn,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Rae-hong Park,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Yassir Bendou,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Lucas Drumetz,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Vincent Gripon,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Giulia Lioi,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Bastien Pasdeloup,,0%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Anuja Vats,anuja.vats@ntnu.no,95%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Marius Pedersen,,0%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Ahmed Mohammed,,0%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Øistein Hovde,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Andrea Coletta,coletta@di.uniroma1.it,78%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Flavio Giorgi,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Gaia Maselli,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Matteo Prata,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Domenicomichele Silvestri,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Jonathan Ashdown,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Francesco Restuccia,,0%
https://arxiv.org/pdf/2301.07502.pdf,Multimodal Side-Tuning for Document Classification,Stefano Pio Zingaro,,0%
https://arxiv.org/pdf/2301.07502.pdf,Multimodal Side-Tuning for Document Classification,Giuseppe Lisanti,,0%
https://arxiv.org/pdf/2301.07502.pdf,Multimodal Side-Tuning for Document Classification,Maurizio Gabbrielli,,0%
https://arxiv.org/pdf/2301.06358.pdf,Post-Train Adaptive U-Net for Image Segmentation,Kostiantyn Khabarlak,habarlack@gmail.com,65%
https://arxiv.org/pdf/2301.06324.pdf,Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification,Youngjae Song,yuong13@skku.edu,65%
https://arxiv.org/pdf/2301.06324.pdf,Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification,Kwang-su Kim,kim.kwangsu@skku.edu,95%
https://arxiv.org/pdf/2301.06324.pdf,Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification,Sung Kuk Shyn,davidshyn@skku.edu,78%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Jingdong Wang,wangjingdong@baidu.com,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Wenhao Wu,wenhao.wu@sydney.edu.au,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Chang Liu,liuchang2022@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Yuxin Song,songyuxin02@baidu.com,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Xiangyang Ji,xyji@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Xiangbo Shu,shuxb@njust.edu.cn,78%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Weiping Wang,wangweiping@iie.ac.cn,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Yu Zhou,zhouyu@iie.ac.cn,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Bo Fang,fangbo@iie.ac.cn,95%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Jeroen Van Der Laak,Jeroen.vanderLaak@radboudumc.nl,95%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Yiping Jiao,ping@nuist.edu.cn,90%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Francesco Ciompi,francesco.ciompi@radboudumc.nl,95%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Shadi Albarqouni,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Zhang Li,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Tao Tan,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Abhir Bhalerao,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Jiabo Ma,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Jiamei Sun,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Johnathan Pocock,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Josien P. W. Pluim,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Navid Alemi Koohbanani,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Raja Muhammad Saad Bashir,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Shan E Ahmed Raza,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Sibo Liu,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Simon Graham,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Suzanne Wetstein,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Syed Ali Khurram,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Thomas Watson,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Nasir Rajpoot,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Mitko Veta,,0%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Lucas Heublein,heublels@iis.fraunhofer.de,65%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Christopher Mutschler,christopher.mutschler@iis.fraunhofer.de,95%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Felix Ott,felix.ott@iis.fraunhofer.de,95%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Bernd Bischl,bernd.bischl@stat.uni-muenchen.de,95%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,David Rügamer,david.ruegamer@stat.uni-muenchen.de,85%
https://arxiv.org/pdf/2301.06286.pdf,Meta Generative Attack on Person Reidentification,A V Subramanyam,subramanyam@iiitd.ac.in,95%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Youxin Pang,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Yong Zhang,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Weize Quan,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Yanbo Fan,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Xiaodong Cun,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Ying Shan,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Dong-ming Yan,,0%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Jinli Suo,jlsuo@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Zhihong Zhang,zhangzh19@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Yuchen Guo,yuchen.w.guo@gmail.com,95%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Qionghai Dai,qhdai@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Bo Zhang,b-zhang18@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Jiayi Xie,xiejiayi97@163.com,95%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Runzhao Yang,yangrz20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Zhiyi Kuang,zkuang@cs.cmu.edu,82%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Deepak Pathak,dpathak@cs.cmu.edu,82%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Zhiqiu Lin,zhiqiul@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Deva Ramanan,deva@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Samuel Yu,samuelyu@cs.cmu.edu,95%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Yushan Han,yushanhan@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Congyan Lang,cylang@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Yi Jin,yjin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Hui Zhang,huizhang1@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Yidong Li,ydli@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Huifang Li,,0%
https://arxiv.org/pdf/2301.06230.pdf,Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems,Giovanni Beltrame,giovanni.beltrame@polymtl.ca,95%
https://arxiv.org/pdf/2301.06230.pdf,Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems,Pierre-yves Lajoie,pierre-yves.lajoie@polymtl.ca,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Ujjwal Baid,baidujjwal@sggs.ac.in,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Sanjay Talbar,sntalbar@sggs.ac.in,82%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Prasad Dutande,prasad.dutande@sggs.ac.in,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Bhakti Baheti,bahetibhakti@sggs.ac.in,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Shubham Innani,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,André Santos,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,João Dinis Ferreira,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,Onur Mutlu,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,Gabriel Falcao,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Lei Li,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Tianfang Zhang,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Stefan Oehmcke,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Fabian Gieseke,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Christian Igel,,0%
https://arxiv.org/pdf/2301.06187.pdf,CNN-Based Action Recognition and Pose Estimation for Classifying Animal Behavior from Videos: A Survey,Michael Perez,,0%
https://arxiv.org/pdf/2301.06187.pdf,CNN-Based Action Recognition and Pose Estimation for Classifying Animal Behavior from Videos: A Survey,Corey Toler-franklin,,0%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Tian Guo,tian@wpi.edu,85%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Yiqin Zhao,yzhao11@wpi.edu,82%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Chongyang Ma,chongyangm@gmail.com,85%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Haibin Huang,jackiehuanghaibin@gmail.com,95%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Pedro Machado,pedro.machado@ntu.ac.uk,95%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Isibor Kennedy Ihianle,isibor.ihianle@ntu.ac.uk,95%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Nicholas Murray-hill,,0%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Laura Fontes,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Shu Zhong,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Miriam Ribul,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Youngjun Cho,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Marianna Obrist,,0%
https://arxiv.org/pdf/2301.06152.pdf,Inpainting borehole images using Generative Adversarial Networks,Rachid Belmeskine,rachid@convergaince.com,85%
https://arxiv.org/pdf/2301.06152.pdf,Inpainting borehole images using Generative Adversarial Networks,Abed Benaichouche,abed@convergaince.com,85%
https://arxiv.org/pdf/2301.06143.pdf,Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality,Yiqin Zhao,yzhao11@wpi.edu,82%
https://arxiv.org/pdf/2301.06143.pdf,Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality,Tian Guo,tian@wpi.edu,85%
https://arxiv.org/pdf/2301.06143.pdf,Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality,Sean Fanello,seanfa@google.com,85%
https://arxiv.org/pdf/2301.06133.pdf,Improving Reliability of Fine-tuning with Block-wise Optimisation,Basel Barakat,,0%
https://arxiv.org/pdf/2301.06133.pdf,Improving Reliability of Fine-tuning with Block-wise Optimisation,Qiang Huang,,0%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Deyu Meng,dymeng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Huanqiang Zeng,zeng0043@hqu.edu.cn,78%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Hui Liu,h2liu@sfu.edu.hk,82%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Jinhui Hou,jh.hou@cityu.edu.hk,82%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Zhiyu Zhu,zhiyuzhu2@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Junhui Hou,,0%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Yiming Lei,ymlei@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Zilong Li,zilongli21@m.fudan.edu.cn,95%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Hongming Shan,hmshan@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Junping Zhang,jpzhang@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Yangyang Li,yyli@amss.ac.cn,82%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Federico Pernici,federico.pernici@unifi.it,95%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Alberto Del Bimbo,alberto.delbimbo@unifi.it,95%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Claudio Baecchi,claudio.baecchi@unifi.it,95%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Matteo Bruni,matteo.bruni@unifi.it,95%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Chuanmin Jia,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Feng Ye,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Huifang Sun,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Siwei Ma,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Wen Gao,,0%
https://arxiv.org/pdf/2301.06103.pdf,Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,Ajmal Mian,ajmal.mian@uwa.edu.au,95%
https://arxiv.org/pdf/2301.06103.pdf,Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,Ghulam Mubashar Hassan,ghulam.hassan@uwa.edu.au,95%
https://arxiv.org/pdf/2301.06103.pdf,Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,Sania Zahan,sania.zahan@research.uwa.edu.au,95%
https://arxiv.org/pdf/2301.06077.pdf,MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation,Junichiro Fujii,jn-fujii@yachiyo-eng.co.jp,82%
https://arxiv.org/pdf/2301.06077.pdf,MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation,Takato Yasuno,tk-yasuno@yachiyo-eng.co.jp,82%
https://arxiv.org/pdf/2301.06077.pdf,MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation,Masahiro Okano,ms-okano@yachiyo-eng.co.jp,82%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Linchao Bao,linchaobao@gmail.com,95%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Haoxian Zhang,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Yue Qian,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Tangli Xue,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Changhai Chen,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Xuefei Zhe,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Di Kang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Jianrong Zhang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Yangsong Zhang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Xiaodong Cun,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Shaoli Huang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Yong Zhang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Hongwei Zhao,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Hongtao Lu,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Xi Shen,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Sen Wang,wangsen31@huawei.com,95%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Chen Shi,schiele@mpi-inf.mpg.de,75%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Shaoshuai Shi,sshi@mpi-inf.mpg.de,82%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Haiyang Wang,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Meng Lei,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Di He,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Liwei Wang,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Sihan Wang,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Fuping Wu,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Lei Li,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Zheyao Gao,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Byung-woo Hong,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Xiahai Zhuang,,0%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Yebin Liu,liuyebin@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Hongwen Zhang,zhanghongwen@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Kai Jia,kajia@umich.edu,82%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Liang An,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Xiaojie Jin,jinxiaojie@bytedance.com,95%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Cheng-ze Lu,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Zhicheng Huang,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Qibin Hou,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Ming-ming Cheng,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Jiashi Feng,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Wei Liang,liangwei@bit.edu.cn,95%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Siyuan Huang,syhuang@bigai.ai,82%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Zan Wang,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Puhao Li,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Baoxiong Jia,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Tengyu Liu,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Yixin Zhu,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Song-chun Zhu,,0%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Weiyang Ding,dingwy@fudan.edu.cn,78%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Jianfeng Feng,jffeng@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Jiayi Han,,0%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Longbin Zeng,,0%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Liang Du,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Chen Li,lichen@bmie.neu.edu.cn,95%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Feng-lei Fan,fengleifan@cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Ao Chen,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Jinghua Zhang,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Md Mamunur Rahaman,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Hongzan Sun,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,M. D.,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Tieyong Zeng,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Marcin Grzegorzek,,0%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Yuexian Zou,zouyx@pku.edu.cn,78%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Hongxiang Li,lihongxiang@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Zhihong Zhu,zhihongzhu@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Xuxin Cheng,chengxx@stu.pku.edu.cn,78%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Meng Cao,mengcao@pku.edu.cn,95%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Yaowei Li,,0%
https://arxiv.org/pdf/2301.05994.pdf,Min-Max-Jump distance and its applications,Gangli Liu,gl-liu13@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Iván Vallés-pérez,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Emilio Soria-olivas,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Marcelino Martínez-sober,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Antonio J. Serrano-lópez,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Joan Vila-francés,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Juan Gómez-sanchís,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Zhaohui Zheng,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Yuming Chen,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Qibin Hou,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Xiang Li,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Ming-ming Cheng,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Andy N. D. Nguyen,Nghia.D.Nguyen@uth.tmc.edu,78%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Kareem Allam,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Xiaohong Iris Wang,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Songlin Zhang,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Jianmin Ding,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Kevin Chiu,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Karan Saluja,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Amer Wahed,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Hongxia Sun,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Enrique Vidal,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Alejandro H. Toselli,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Antonio Ríos-vila,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Jorge Calvo-zaragoza,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Xin Jin,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Wu Zhou,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Jinyu Wang,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Duo Xu,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Yiqing Rong,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Shuai Cui,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xinghui Li,li.xinghui@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Erik Isai Valle Salgado,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Haoxin Yan,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Yue Hong,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Peiyuan Zhu,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Shidong Zhu,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Chengwei Liao,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Yanxiang Wen,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xiu Li,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xiang Qian,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xiaohao Wang,,0%
https://arxiv.org/pdf/2301.05892.pdf,Object Detection performance variation on compressed satellite image datasets with iquaflow,Katalin Takats,katalin.takats@satellogic.com,95%
https://arxiv.org/pdf/2301.05892.pdf,Object Detection performance variation on compressed satellite image datasets with iquaflow,Pau Gallés,pau.galles@satellogic.com,95%
https://arxiv.org/pdf/2301.05892.pdf,Object Detection performance variation on compressed satellite image datasets with iquaflow,Javier Marin,jmarin@satellogic.com,82%
https://arxiv.org/pdf/2301.05871.pdf,Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes,Songchun Zhang,,0%
https://arxiv.org/pdf/2301.05871.pdf,Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes,Chunhui Zhao,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Erland Hilman Fuadi,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Aristo Renaldo Ruslim,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Putu Wahyu Kusuma Wardhana,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Novanto Yudistira,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,Jinyang Wang,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,Tao Wang,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,Min Gan,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,George Hadjichristofi,,0%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Jian Guan,j.guan@hrbeu.edu.cn,82%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Youtian Lin,linyoutian.loyot@gmail.com,95%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Pengming Feng,p.feng.cn@outlook.com,82%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Mingjie Xie,xiemingjie@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Guangjun He,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Zhe Sheng,shengzhe.sz@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Meng Li,list.dzl@alibaba-inc.com,78%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Senbo Wang,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Weihao Yuan,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Weichao Shen,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Zilong Dong,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Ross Greer,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Lulua Rakla,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Samveed Desai,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Afnan Alofi,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Akshay Gopalkrishnan,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05839.pdf,NCP: Neural Correspondence Prior for Effective Unsupervised Shape Matching,Maks Ovsjanikov,maks@lix.polytechnique.fr,85%
https://arxiv.org/pdf/2301.05839.pdf,NCP: Neural Correspondence Prior for Effective Unsupervised Shape Matching,Souhaib Attaiki,attaiki@lix.polytechnique.fr,78%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Ross Greer,,0%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Lulua Rakla,,0%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Anish Gopalan,,0%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Kundan Patil,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Shrushti Kale,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Jaivanti Dhokey,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Abhishek Gulhane,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Ross Greer,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Nachiket Deo,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Akshay Rangesh,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Pujitha Gunaratne,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Ross Greer,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Akshay Gopalkrishnan,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Nachiket Deo,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Akshay Rangesh,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Chen Zhou,chen.zhou@gatech.edu,95%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Ghassan Alregib,alregib@gatech.edu,78%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Armin Parchami,,0%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Kunjan Singh,,0%
https://arxiv.org/pdf/2301.05792.pdf,RMM: Reinforced Memory Management for Class-Incremental Learning,Yaoyao Liu,yaoyao.liu@mpi-inf.mpg.de,95%
https://arxiv.org/pdf/2301.05792.pdf,RMM: Reinforced Memory Management for Class-Incremental Learning,Bernt Schiele,schiele@mpi-inf.mpg.de,78%
https://arxiv.org/pdf/2301.05792.pdf,RMM: Reinforced Memory Management for Class-Incremental Learning,Qianru Sun,qianrusun@smu.edu.sg,95%
https://arxiv.org/pdf/2301.05776.pdf,Young Labeled Faces in the Wild (YLFW): A Dataset for Children Faces Recognition,Nuno Gonçalves,nunogon@deec.uc.pt,85%
https://arxiv.org/pdf/2301.05776.pdf,Young Labeled Faces in the Wild (YLFW): A Dataset for Children Faces Recognition,Iurii Medvedev,iurii.medvedev@isr.uc.pt,95%
https://arxiv.org/pdf/2301.05776.pdf,Young Labeled Faces in the Wild (YLFW): A Dataset for Children Faces Recognition,Farhad Shadmand,farhad.shadmand@isr.uc.pt,95%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Berton Earnshaw,berton.earnshaw@recursion.com,95%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Maciej Sypetkowski,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Morteza Rezanejad,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Saber Saberian,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Oren Kraus,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,John Urbanik,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,James Taylor,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Ben Mabey,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Mason Victors,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Jason Yosinski,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Alborz Rezazadeh Sereshkeh,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Imran Haque,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Pol Moreno,polc@deepmind.com,85%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Adam R. Kosiorek,adamrk@deepmind.com,85%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Heiko Strathmann,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Daniel Zoran,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Rosalia G. Schneider,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Björn Winckler,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Larisa Markeeva,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Théophane Weber,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Danilo J. Rezende,,0%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Matthew Hagen,mathage@amazon.com,65%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Unaiza Ahsan,unaiza_ahsan@homedepot.com,95%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Yuanbo Wang,,0%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Hanyan Li,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Dalí F. D. Dos Santos,dalifreire@gmail.com,85%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Paulo R. De Faria,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Adriano M. Loyola,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Sérgio V. Cardoso,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Bruno A. N. Travençolo,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Marcelo Z. Do Nascimento,,0%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Hung-kuo Chu,hkchu@cs.nthu.edu.tw,82%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Chao-chen Gao,,0%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Cheng-hsiu Chen,,0%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Jheng-wei Su,,0%
https://arxiv.org/pdf/2301.05623.pdf,Reworking geometric morphometrics into a methodology of transformation grids,Fred L. Bookstein,fred.bookstein@univie.ac.at,95%
https://arxiv.org/pdf/2301.05609.pdf,Co-manipulation of soft-materials estimating deformation from depth images,Giorgio Nicola,giorgio.nicola@stiima.cnr.it,95%
https://arxiv.org/pdf/2301.05609.pdf,Co-manipulation of soft-materials estimating deformation from depth images,Enrico Villagrossi,,0%
https://arxiv.org/pdf/2301.05609.pdf,Co-manipulation of soft-materials estimating deformation from depth images,Nicola Pedrocchi,,0%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Meng Cheng,chengmeng05@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Zaidan Ke,kezaidan@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Xiangxiang Chu,chuxiangxiang@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Xiaoming Xu,xuxiaoming04@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Hongliang Jiang,jianghongliang02@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Chuyi Li,lichuyi@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Lulu Li,lilulu05@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Bo Zhang,zhangbo97@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Yifei Geng,gengyifei@meituan.com,95%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Cristina P. Santos,cristina@dei.uminho.pt,85%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Carolina Gonçalves,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,João M. Lopes,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Sara Moccia,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Daniele Berardini,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Lucia Migliorelli,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,Li Xiang,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,He Miao,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,Luo Haibo,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,Xiao Jiajie,,0%
https://arxiv.org/pdf/2301.07030.pdf,Computational Pathology for Brain Disorders,Daniel Racoceanu,daniel.racoceanu@sorbonne-universite.fr,95%
https://arxiv.org/pdf/2301.07030.pdf,Computational Pathology for Brain Disorders,Gabriel Jimenez,,0%
https://arxiv.org/pdf/2301.05528.pdf,Development of a Prototype Application for Rice Disease Detection Using Convolutional Neural Networks,Arpee Callejo-arruejo,arpee.callejo@unp.edu.ph,85%
https://arxiv.org/pdf/2301.05528.pdf,Development of a Prototype Application for Rice Disease Detection Using Convolutional Neural Networks,Harold Costales,hlcostales@unp.edu.ph,82%
https://arxiv.org/pdf/2301.05528.pdf,Development of a Prototype Application for Rice Disease Detection Using Convolutional Neural Networks,Noel Rafanan,noel.rafanan@unp.edu.ph,95%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Qi Zhao,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Shuchang Lyu,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Binghao Liu,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Lijiang Chen,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Hongbo Zhao,,0%
https://arxiv.org/pdf/2301.05506.pdf,On the feasibility of attacking Thai LPR systems with adversarial examples,Norrathep Rattanavipanon,norrathep.r@phuket.psu.ac.th,85%
https://arxiv.org/pdf/2301.05506.pdf,On the feasibility of attacking Thai LPR systems with adversarial examples,Jakapan Suaboot,jakapan.su@phuket.psu.ac.th,85%
https://arxiv.org/pdf/2301.05506.pdf,On the feasibility of attacking Thai LPR systems with adversarial examples,Chissanupong Jiamsuchon,,0%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Zengxin Qi,qizengxin@huashan.org.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Ying Mao,maoying@huashan.org.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Xiangyu Zhao,xiangyu.zhao@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Qian Wang,qianwang@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Lichi Zhang,lichizhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Sheng Wang,wsheng@sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Xuehai Wu,wuxuehai2013@163.com,95%
https://arxiv.org/pdf/2301.05499.pdf,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,Martin Engilberge,firstname.lastname@epfl.ch,70%
https://arxiv.org/pdf/2301.05499.pdf,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,Vidit Vidit,,0%
https://arxiv.org/pdf/2301.05499.pdf,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,Mathieu Salzmann,,0%
https://arxiv.org/pdf/2301.05496.pdf,Learning Transformations To Reduce the Geometric Shift in Object Detection,Martin Engilberge,firstname.lastname@epfl.ch,70%
https://arxiv.org/pdf/2301.05496.pdf,Learning Transformations To Reduce the Geometric Shift in Object Detection,Vidit Vidit,,0%
https://arxiv.org/pdf/2301.05496.pdf,Learning Transformations To Reduce the Geometric Shift in Object Detection,Mathieu Salzmann,,0%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Guillaume Sautière,gsautie@qti.qualcomm.com,90%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Tianlin Xu,tianlin.xu1@gmail.com,95%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Auke Wiggers,auke@qti.qualcomm.com,85%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Jens Petersen,jpeterse@qti.qualcomm.com,90%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Noor Fathima Ghouse,noor@qti.qualcomm.com,85%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Julian Schön,julian.e.s@di.ku.dk,85%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Raghavendra Selvan,,0%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Lotte Nygård,,0%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Ivan Richter Vogelius,,0%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Jens Petersen,,0%
https://arxiv.org/pdf/2301.05440.pdf,Learnable Heterogeneous Convolution: Learning both topology and strength,Zhenzhi Wu,zhenzhi.wu@lynxi.com,95%
https://arxiv.org/pdf/2301.05440.pdf,Learnable Heterogeneous Convolution: Learning both topology and strength,Qikun Zhang,qikun.zhang@lynxi.com,95%
https://arxiv.org/pdf/2301.05440.pdf,Learnable Heterogeneous Convolution: Learning both topology and strength,Rongzhen Zhao,rongzhen.zhao@lynxi.com,95%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Marian Bittner,mbittner.work@gmail.com,82%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Wei-tse Yang,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Xucong Zhang,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Ajay Seth,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Jan Van Gemert,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Frans C. T. Van Der Helm,,0%
https://arxiv.org/pdf/2301.05434.pdf,LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility,Pratik Narang,pratik.narang@pilani.bits-pilani.ac.in,95%
https://arxiv.org/pdf/2301.05434.pdf,LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility,Esha Pahwa,achleshl@andrew.cmu.edu,60%
https://arxiv.org/pdf/2301.05434.pdf,LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility,Achleshwar Luthra,,0%
https://arxiv.org/pdf/2301.05421.pdf,Anti-aliasing Predictive Coding Network for Future Video Frame Prediction,Chaofan Ling,wichaofan@mail.scut.edu.cn,85%
https://arxiv.org/pdf/2301.05421.pdf,Anti-aliasing Predictive Coding Network for Future Video Frame Prediction,Junpei Zhong,joni.zhong@polyu.edu.hk,82%
https://arxiv.org/pdf/2301.05421.pdf,Anti-aliasing Predictive Coding Network for Future Video Frame Prediction,Weihua Li,,0%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Jiajun Deng,djiajun1206@gmail.com,85%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Houqiang Li,lihq@ustc.edu.cn,78%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Jianmin Ji,jianmin@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Yu Zhang,yuzhang@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Yanyong Zhang,yanyongz@ustc.edu.cn,85%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Xiaomeng Chu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Kaiwen Wan,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Lei Li,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Dengqiang Jia,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Shangqi Gao,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Wei Qian,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Yingzhi Wu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Huandong Lin,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Xiongzheng Mu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Xin Gao,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Sijia Wang,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Fuping Wu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Xiahai Zhuang,,0%
https://arxiv.org/pdf/2301.05372.pdf,Text to Point Cloud Localization with Relation-Enhanced Transformer,Hehe Fan,hehe.fan@nus.edu.sg,95%
https://arxiv.org/pdf/2301.05372.pdf,Text to Point Cloud Localization with Relation-Enhanced Transformer,Guangzhi Wang,guangzhi.wang@u.nus.edu,95%
https://arxiv.org/pdf/2301.05372.pdf,Text to Point Cloud Localization with Relation-Enhanced Transformer,Mohan Kankanhalli,mohan@comp.nus.edu.sg,85%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Scott Howland,scott.howland@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Keerti Kappagantula,keertisahithi.kappagantula@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Tegan Emerson,tegan.emerson@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Lara Kassab,lara.kassab@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Henry Kvinge,henry.kvinge@pnnl.gov,95%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Miao Yin,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Burak Uzkent,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Yilin Shen,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Hongxia Jin,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Bo Yuan,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Simbarashe Nyatsanga,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Taras Kucherenko,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Chaitanya Ahuja,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Gustav Eje Henter,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Michael Neff,,0%
https://arxiv.org/pdf/2301.05323.pdf,Salient Object Detection for Images Taken by People With Vision Impairments,Jarek Reynolds,,0%
https://arxiv.org/pdf/2301.05323.pdf,Salient Object Detection for Images Taken by People With Vision Impairments,Chandra Kanth Nagesh,,0%
https://arxiv.org/pdf/2301.05323.pdf,Salient Object Detection for Images Taken by People With Vision Impairments,Danna Gurari,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Yinghao Xu,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Yujun Shen,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Jiapeng Zhu,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Ceyuan Yang,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Bolei Zhou,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Anas Mahmoud,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Jordan S. K. Hu,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Tianshu Kuai,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Ali Harakeh,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Liam Paull,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Steven L. Waslander,,0%
https://arxiv.org/pdf/2301.05246.pdf,Online Class-Incremental Learning For Real-World Food Image Classification,Jiangpeng He,he416@purdue.edu,78%
https://arxiv.org/pdf/2301.05246.pdf,Online Class-Incremental Learning For Real-World Food Image Classification,Fengqing Zhu,zhu0@purdue.edu,78%
https://arxiv.org/pdf/2301.05246.pdf,Online Class-Incremental Learning For Real-World Food Image Classification,Siddeshwar Raghavan,raghav12@purdue.edu,55%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Zhenfang Chen,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Qinhong Zhou,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Yikang Shen,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Yining Hong,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Hao Zhang,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Chuang Gan,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Yotam Nitzan,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Michaël Gharbi,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Richard Zhang,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Taesung Park,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Jun-yan Zhu,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Eli Shechtman,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Ziyi Li,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Qinye Zhou,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Xiaoyun Zhang,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Ya Zhang,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Yanfeng Wang,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Weidi Xie,,0%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Huan Wang,wang.huan@northeastern.edu,95%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Can Qin,,0%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Yue Bai,,0%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Yun Fu,,0%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Artem Moskalev,a.moskalev@uva.nl,82%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Ivan Sosnovik,i.sosnovik@uva.nl,82%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Arnold Smeulders,a.w.m.smeulders@uva.nl,82%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Cees Kaandorp,cees.kaandorp@gmail.com,95%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Hong-xing Yu,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Samir Agarwala,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Charles Herrmann,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Richard Szeliski,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Noah Snavely,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Jiajun Wu,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Deqing Sun,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Xiaoping Hong,thongxpu@sustech.edu.cn,78%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Fu Zhang,fuzhangu@connect.hku.hk,95%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Jiarong Lin,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Chongjiang Yuan,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Yixi Cai,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Haotian Li,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Yunfan Ren,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Yuying Zou,,0%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Daniel Gehrig,dgehrig@ifi.uzh.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Jingyun Liang,jinliang@vision.ee.ethz.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Christos Sakaridis,csakaridis@vision.ee.ethz.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Lei Sun,sun@zju.edu.cn,78%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Luc Van Gool,vangool@vision.ee.ethz.ch,78%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Mathias Gehrig,mgehrig@ifi.uzh.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Peng Sun,pengsunr@zju.edu.cn,95%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Davide Scaramuzza,sdavide@ifi.uzh.ch,85%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Zhijie Xu,z.xu@hud.ac.uk,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Kaiwei Wang,wangkaiwei@zju.edu.cn,95%
https://arxiv.org/pdf/2301.05180.pdf,Effective Decision Boundary Learning for Class Incremental Learning,Kunchi Li,likunchi2020@ia.ac.cn,95%
https://arxiv.org/pdf/2301.05180.pdf,Effective Decision Boundary Learning for Class Incremental Learning,Jun Wan,,0%
https://arxiv.org/pdf/2301.05180.pdf,Effective Decision Boundary Learning for Class Incremental Learning,Shan Yu,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Diogo Luvizon,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Marc Habermann,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Vladislav Golyanik,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Adam Kortylewski,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Christian Theobalt,,0%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Maarten De Rijke,m.derijke@uva.nl,82%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Mariya Hendriksen,m.hendriksen@uva.nl,82%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Ernst Kuiper,ekuiper@bol.com,82%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Svitlana Vakulenko,,0%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Dominik Zietlow,ZIETLD@AMAZON.DE,76%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Francesco Locatello,LOCATELF@AMAZON.DE,65%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Bernhard Schölkopf,BS@TUEBINGEN.MPG.DE,90%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Yuejiang Liu,YUEJIANG.LIU@EPFL.CH,95%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Max Horn,HORNMAX@AMAZON.DE,95%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Alexandre Alahi,ALEXANDRE.ALAHI@EPFL.CH,95%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Chris Russell,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Jovana Mitrovic,mitrovic@deepmind.com,78%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Pierre H. Richemond,richemond@deepmind.com,78%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Matko Bošnjak,matko@deepmind.com,85%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Nenad Tomasev,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Florian Strub,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Jacob C. Walker,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Felix Hill,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Lars Holger Buesing,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Razvan Pascanu,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Charles Blundell,,0%
https://arxiv.org/pdf/2301.05124.pdf,Poses of People in Art: A Data Set for Human Pose Estimation in Digital Art History,Ricarda Vollmer,ricarda.vollmer@campus.lmu.de,95%
https://arxiv.org/pdf/2301.05124.pdf,Poses of People in Art: A Data Set for Human Pose Estimation in Digital Art History,Stefanie Schneider,stefanie.schneider@itg.uni-muenchen.de,95%
https://arxiv.org/pdf/2301.05106.pdf,Forgetful Active Learning with Switch Events: Efficient Sampling for Out-of-Distribution Data,Ghassan Alregib,alregib@gatech.edu,78%
https://arxiv.org/pdf/2301.05106.pdf,Forgetful Active Learning with Switch Events: Efficient Sampling for Out-of-Distribution Data,Ryan Benkert,rbenkert3@gatech.edu,82%
https://arxiv.org/pdf/2301.05106.pdf,Forgetful Active Learning with Switch Events: Efficient Sampling for Out-of-Distribution Data,Mohit Prabhushankar,,0%
https://arxiv.org/pdf/2301.05070.pdf,Wildfire Smoke Detection with Computer Vision,Eldan R. Daniel,deldanr@gmail.com,85%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Hang Li,xszhang0320@gmail.com,85%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Xinsong Zhang,,0%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Yan Zeng,,0%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Jipeng Zhang,,0%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Simon Mangold,simon.mangold@kit.edu,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Alexander Cebulla,alexander.cebulla@kit.edu,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Xuelei Bi,xuelei.bi@student.kit.edu,95%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Siddharth Ravi,siddharth.ravi@gcloud.ua.es,95%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Martin Kampel,martin.kampel@tuwien.ac.at,95%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Sophie Noiret,snoiret@cvl.tuwien.ac.at.com,82%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Francisco Florez-revuelta,,0%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Lianghua He,helianghua@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Longzhen Yang,yanglongzhen@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Yitao Peng,,0%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Yihang Liu,,0%
https://arxiv.org/pdf/2301.04956.pdf,Graph Laplacian for Semi-Supervised Learning,Or Streicher,orr.shtr@gmail.com,85%
https://arxiv.org/pdf/2301.04956.pdf,Graph Laplacian for Semi-Supervised Learning,Guy Gilboa,guy.gilboa@ee.technion.ac.il,95%
https://arxiv.org/pdf/2301.04954.pdf,Reaching the Edge of the Edge: Image Analysis in Space,Robert Bayer,,0%
https://arxiv.org/pdf/2301.04954.pdf,Reaching the Edge of the Edge: Image Analysis in Space,Julian Priest,,0%
https://arxiv.org/pdf/2301.04954.pdf,Reaching the Edge of the Edge: Image Analysis in Space,Pınar Tözün,,0%
https://arxiv.org/pdf/2301.04944.pdf,ViTs for SITS: Vision Transformers for Satellite Image Time Series,Erik Chavez,erik.chavez@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.04944.pdf,ViTs for SITS: Vision Transformers for Satellite Image Time Series,Stefanos Zafeiriou,s.zafeiriou@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.04944.pdf,ViTs for SITS: Vision Transformers for Satellite Image Time Series,Michail Tarasiou,michail.tarasiou10@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Giovanna Castellano,,0%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Eugenio Cotardo,,0%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Corrado Mencar,,0%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Gennaro Vessio,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Runnan Chen,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Youquan Liu,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Lingdong Kong,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Xinge Zhu,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yuexin Ma,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yikang Li,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yuenan Hou,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yu Qiao,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Wenping Wang,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Guanbin Li,liguanbin@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Ruifei Zhang,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Peiwen Lai,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Xiang Wan,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,De-jun Fan,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Feng Gao,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Xiao-jian Wu,,0%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Itsumi Saito,itsumi.saito.df@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Kyosuke Nishida,kyosuke.nishida.rx@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Ryota Tanaka,ryouta.tanaka.rg@hco.ntt.co.jp,82%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Taku Hasegawa,taku.hasegawa.ps@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Kuniko Saito,kuniko.saito.ku@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Kosuke Nishida,kosuke.nishida.ap@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04882.pdf,ZScribbleSeg: Zen and the Art of Scribble Supervised Medical Image Segmentation,Ke Zhang,,0%
https://arxiv.org/pdf/2301.04882.pdf,ZScribbleSeg: Zen and the Art of Scribble Supervised Medical Image Segmentation,Xiahai Zhuang,,0%
https://arxiv.org/pdf/2301.04875.pdf,Color-NeuraCrypt: Privacy-Preserving Color-Image Classification Using Extended Random Neural Networks,Hitoshi Kiya,kiya@tmu.ac.jp,78%
https://arxiv.org/pdf/2301.04875.pdf,Color-NeuraCrypt: Privacy-Preserving Color-Image Classification Using Extended Random Neural Networks,Zheng Qi,,0%
https://arxiv.org/pdf/2301.04875.pdf,Color-NeuraCrypt: Privacy-Preserving Color-Image Classification Using Extended Random Neural Networks,Aprilpyone Maungmaung,,0%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Changxin Gao,cgao@hust.edu.cn,82%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Nong Sang,nsang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Aoyan Li,aoyanli@hust.edu.cn,95%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Changqian Yu,changqianyu@meituan.com,95%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Zilin Guo,zilin guo@hust.edu.cn,95%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Dongyue Wu,dongyue wu@hust.edu.cn,95%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Guanbin Li,liguanbin@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Ruifei Zhang,,0%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Sishuo Liu,,0%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Yizhou Yu,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Xiaogang Wang,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Yuhang Cheng,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Liang Wang,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Jiangbo Lu,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Kai Xu,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Guoqiang Xiao,,0%
https://arxiv.org/pdf/2301.04847.pdf,Real-time FPGA implementation of the Semi-Global Matching stereo vision algorithm for a 4K/UHD video stream,Mariusz Grabowski,grabowski@student.agh.edu.pl,78%
https://arxiv.org/pdf/2301.04847.pdf,Real-time FPGA implementation of the Semi-Global Matching stereo vision algorithm for a 4K/UHD video stream,Tomasz Kryjak,tomasz.kryjak@agh.edu.pl,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Ling Li,lingl@njust.edu.cn,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Lin Zhao,linzhao@njust.edu.cn,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Jie Xu,jiexu@njust.edu.cn,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Linhao Xu,,0%
https://arxiv.org/pdf/2301.04811.pdf,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,Lei Fan,Lei.Fan@xjtlu.edu.cn,95%
https://arxiv.org/pdf/2301.04811.pdf,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,Yang Zhao,,0%
https://arxiv.org/pdf/2301.04811.pdf,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,Hyungjoon Seo,,0%
https://arxiv.org/pdf/2301.04805.pdf,DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention,Zewei He,zeweihe@zju.edu.cn,95%
https://arxiv.org/pdf/2301.04805.pdf,DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention,Zixuan Chen,,0%
https://arxiv.org/pdf/2301.04805.pdf,DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention,Zhe-ming Lu,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Mohamed Akrout,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Bálint Gyepesi,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Péter Holló,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Adrienn Poór,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Blága Kincső,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Stephen Solis,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Katrina Cirone,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Jeremy Kawahara,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Dekker Slade,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Latif Abid,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Máté Kovács,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,István Fazekas,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Guanbin Li,liguanbin@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Ruifei Zhang,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Zhen Li,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Shuguang Cui,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Dahong Qian,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Yizhou Yu,,0%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Binbin Chen,chenbinbin8@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Weijie Chen,chenweijie5@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Di Xie,xiedi@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Shiliang Pu,pushiliang.hri@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Yueting Zhuang,yzhuang@zju.edu.cn,82%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Wei Zhao,zhaowei29@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Shicai Yang,yangshicai@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Yueting Zhuang,yzhuang@zju.edu.cn,82%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Weijie Chen,chenweijie5@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Di Xie,xiedi@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Shiliang Pu,pushiliang.hri@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Yilu Guo,guoyilu5@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Xingyue Shi,shixy@stu.pku.edu.cn,78%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Shicai Yang,yangshicai@hikvision.com,95%
https://arxiv.org/pdf/2301.04791.pdf,Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction,Khai Nguyen,khainb@utexas.edu,85%
https://arxiv.org/pdf/2301.04791.pdf,Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction,Dang Nguyen,,0%
https://arxiv.org/pdf/2301.04791.pdf,Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction,Nhat Ho,,0%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Alexander Carballo,alex@gifu-u.ac.jp,90%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Robin Karlsson,karlsson.robin@g.sp.m.is.nagoya-u.ac.jp,95%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Kazuya Takeda,kazuya.takeda@nagoya-u.jp,95%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Keisuke Fujii,fujii@i.nagoya-u.ac.jp,78%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Kento Ohtani,ohtani.kento@g.sp.m.is.nagoya-u.ac.jp,95%
https://arxiv.org/pdf/2301.04751.pdf,Artificial Intelligence Generated Coins for Size Comparison,Gerald Artner,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Zhihua Liu,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Bin Yang,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Yan Shen,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Xuejun Ni,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Huiyu Zhou,,0%
https://arxiv.org/pdf/2301.04746.pdf,Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning,Eduardo Alonso,e.alonso@city.ac.uk,82%
https://arxiv.org/pdf/2301.04746.pdf,Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning,Chi-hang Suen,chi.suen@city.ac.uk,95%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Han Lin,hanhlin@gene.com,95%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Hai Ngu,hain@gene.com,85%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Mohsen Hejrati,hejratis@gene.com,78%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Sarah Chu,chus18@gene.com,78%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Baris Bingol,barisb@gene.com,85%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Fatemeh Haghighi,fhaghigh@asu.edu,90%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Soumitra Ghosh,ghoshs29@gene.com,78%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Somaye Hashemifar,,0%
https://arxiv.org/pdf/2301.04742.pdf,HADA: A Graph-based Amalgamation Framework in Image-text Retrieval,Manh-duy Nguyen,,0%
https://arxiv.org/pdf/2301.04742.pdf,HADA: A Graph-based Amalgamation Framework in Image-text Retrieval,Binh T. Nguyen,,0%
https://arxiv.org/pdf/2301.04742.pdf,HADA: A Graph-based Amalgamation Framework in Image-text Retrieval,Cathal Gurrin,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Weihua Zhou,whzhou@mtu.edu,82%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Chen Zhao,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Zhihui Xu,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Jingfeng Jiang,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Michele Esposito,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Drew Pienta,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Guang-uei Hung,,0%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Ruta Desai,rutadesai@meta.com,95%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Nitin Kamra,nitinkamra@meta.com,95%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Weichao Mao,weichao2@illinois.edu,85%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Michael Louis Iuzzolino,mliuzzolino@meta.com,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Lijun Qian,liqian@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Pamela Obiomon,phobiomon@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Taoreed Akinola,takinola2@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Richard Wilkins,rtwilkins@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Xiangfang Li,xili@pvamu.edu,82%
https://arxiv.org/pdf/2301.04695.pdf,Learning Continuous Mesh Representation with Spherical Implicit Surface,Zhongpai Gao,,0%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Euntai Kim,etkim@yonsei.ac.kr,82%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Suhyeon Lee,hyeon93@yonsei.ac.kr,60%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Kyoungwon Min,minkw@keti.re.kr,78%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Hongje Seong,hjseong@yonsei.ac.kr,82%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Seokbeom Song,,0%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Mayank Agarwal,mayankag@cmu.edu,85%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Naveen Venkat,nvenkat@cmu.edu,82%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Maneesh Singh,dr.maneesh.singh@ieee.org,95%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Shubham Tulsiani,,0%
https://arxiv.org/pdf/2301.04648.pdf,Head-Free Lightweight Semantic Segmentation with Linear Transformer,Fan Wang,fan.w@alibaba-inc.com,85%
https://arxiv.org/pdf/2301.04648.pdf,Head-Free Lightweight Semantic Segmentation with Linear Transformer,Pichao Wang,pichaowang@gmail.com,95%
https://arxiv.org/pdf/2301.04648.pdf,Head-Free Lightweight Semantic Segmentation with Linear Transformer,Bo Dong,bo.dong.cst@gmail.com,95%
https://arxiv.org/pdf/2301.04647.pdf,EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata,Chenhao Zheng,,0%
https://arxiv.org/pdf/2301.04647.pdf,EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata,Ayush Shrivastava,,0%
https://arxiv.org/pdf/2301.04647.pdf,EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata,Andrew Owens,,0%
https://arxiv.org/pdf/2301.04644.pdf,Does progress on ImageNet transfer to real-world datasets?,Simon Kornblith,skornblith@google.com,82%
https://arxiv.org/pdf/2301.04644.pdf,Does progress on ImageNet transfer to real-world datasets?,Ludwig Schmidt,schmidt@cs.washington.edu,78%
https://arxiv.org/pdf/2301.04644.pdf,Does progress on ImageNet transfer to real-world datasets?,Alex Fang,,0%
https://arxiv.org/pdf/2301.04634.pdf,Street-View Image Generation from a Bird's-Eye View Layout,Bolei Zhou,bolei@cs.ucla.edu,85%
https://arxiv.org/pdf/2301.04634.pdf,Street-View Image Generation from a Bird's-Eye View Layout,Alexander Swerdlow,aswerdlow@ucla.edu,82%
https://arxiv.org/pdf/2301.04634.pdf,Street-View Image Generation from a Bird's-Eye View Layout,Runsheng Xu,,0%
https://arxiv.org/pdf/2301.04631.pdf,Deep Residual Axial Networks,Nazmul Shahadat,nazmul.ruet@gmail.com,85%
https://arxiv.org/pdf/2301.04631.pdf,Deep Residual Axial Networks,Anthony S. Maida,maida@louisiana.edu,78%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,John Elliott,john.o.elliott@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Masahiro Ono,masahiro.ono@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Abhishek Cauligi,abhishek.s.cauligi@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,R. Michael Swan,robert.m.swan@jpl.nasa.gov,82%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Deegan Atha,deegan.j.atha@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Shreyansh Daftry,shreyansh.daftry@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Larry Matthies,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Ayşegül Dündar,adundar@cs.bilkent.edu.tr,82%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Yusuf Dalva,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Hamza Pehlivan,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Cansu Moran,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Öykü Irmak Hatipoğlu,,0%
https://arxiv.org/pdf/2301.04626.pdf,Deep Axial Hypercomplex Networks,Nazmul Shahadat,nazmul.ruet@gmail.com,85%
https://arxiv.org/pdf/2301.04626.pdf,Deep Axial Hypercomplex Networks,Anthony S. Maida,maida@louisiana.edu,78%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Kanran Zhou,kanran.zhou@student.kit.edu,95%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Linxi Qiu,linxi.qiu@student.kit.edu,95%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2301.04623.pdf,Enhancing ResNet Image Classification Performance by using Parameterized Hypercomplex Multiplication,Nazmul Shahadat,nazmul.ruet@gmail.com,85%
https://arxiv.org/pdf/2301.04623.pdf,Enhancing ResNet Image Classification Performance by using Parameterized Hypercomplex Multiplication,Anthony S. Maida,maida@louisiana.edu,78%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Feiyan Hu,feiyan.hu@dcu.ie,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Kevin Mcguinness,kevin.mcguinness@dcu.ie,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Simone Palazzo,simone.palazzo@unict.it,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Concetto Spampinato,concetto.spampinato@unict.it,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Federica Proietto Salanitri,,0%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Giovanni Bellitto,,0%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Morteza Moradi,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Chengzhi Wu,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Julius Pfrommer,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Jürgen Beyerer,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Kangning Li,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Boris Neubert,,0%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Mingyuan Zhou,mingyuan.zhou@student.kit.edu,95%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.04608.pdf,Padding Module: Learning the Padding in Deep Neural Networks,Pei-chi Huang,phuang@unomaha.edu,82%
https://arxiv.org/pdf/2301.04608.pdf,Padding Module: Learning the Padding in Deep Neural Networks,Xin Zhong,xzhong@unomaha.edu,82%
https://arxiv.org/pdf/2301.04608.pdf,Padding Module: Learning the Padding in Deep Neural Networks,Fahad Alrasheedi,falrasheedi@unomaha.edu,82%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Kanran Zhou,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Jan-philipp Kaiser,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Norbert Mitschke,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Jan-felix Klein,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Julius Pfrommer,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Jürgen Beyerer,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Gisela Lanza,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Michael Heizmann,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Kai Furmans,,0%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Yaoyao Liu,yaoyao.liu@mpi-inf.mpg.de,95%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Yingying Li,yingli2@caltech.edu,82%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Bernt Schiele,schiele@mpi-inf.mpg.de,78%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Qianru Sun,qianrusun@smu.edu.sg,95%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Jiapeng Zhu,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Ceyuan Yang,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Yujun Shen,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Zifan Shi,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Bo Dai,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Deli Zhao,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Qifeng Chen,,0%
https://arxiv.org/pdf/2301.04584.pdf,Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning,Andrey Zhmoginov,azhmogin@google.com,90%
https://arxiv.org/pdf/2301.04584.pdf,Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning,Mark Sandler,sandler@google.com,78%
https://arxiv.org/pdf/2301.04584.pdf,Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning,Max Vladymyrov,,0%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Kaiqiang Chen,chenkaiqiang14@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Kun Fu,kunfuiecas@gmail.com,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Deke Tang,tangdk@geovis.com.cn,78%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Wenjie Liu,liuwenjie18@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Yongqiang Mao,maoyongqiang19@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Wei Chen,chenwei@geovis.com.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Xian Sun,sunxian@aircas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Liangjin Zhao,,0%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Zhirui Wang,,0%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Wenhui Diao,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Ozan Oktay,ozan.oktay@microsoft.com,95%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Shruthi Bannur,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Stephanie Hyland,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Qianchu Liu,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Fernando Pérez-garcía,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Maximilian Ilse,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Daniel C. Castro,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Benedikt Boecking,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Harshita Sharma,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Kenza Bouzid,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Anja Thieme,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Anton Schwaighofer,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Maria Wetscherek,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Matthew P. Lungren,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Aditya Nori,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Javier Alvarez-valle,,0%
https://arxiv.org/pdf/2301.04554.pdf,Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,Wei Guo,wei.guo.cn@outlook.com,95%
https://arxiv.org/pdf/2301.04554.pdf,Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,Benedetta Tondi,,0%
https://arxiv.org/pdf/2301.04554.pdf,Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,Mauro Barni,,0%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Jiwen Lu,wen@tsinghua.edu.cn,90%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Jie Zhou,jzhou@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Xumin Yu,yuxm20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Ziyi Wang,wziyi22@mails.tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Yongming Rao,ongmimg95@gmail.com,60%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Robbie Holland,robert.holland15@ic.ac.uk,82%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Oliver Leingang,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Christopher Holmes,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Philipp Anders,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Rebecca Kaye,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Sophie Riedl,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Johannes C. Paetzold,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Ivan Ezhov,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Hrvoje Bogunović,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Ursula Schmidt-erfurth,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Lars Fritsche,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Hendrik P. N. Scholl,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Sobha Sivaprasad,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Andrew J. Lotery,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Daniel Rueckert,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Martin J. Menten,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Cesar Henrique Comin,comin@ufscar.br,82%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Matheus Viana Da Silva,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Natália De Carvalho Santos,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Julie Ouellette,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Baptiste Lacoste,,0%
https://arxiv.org/pdf/2304.00001.pdf,Determination of cutting positions of honeycomb blocks using computer vision,Alexander Razumovsky,,0%
https://arxiv.org/pdf/2304.00001.pdf,Determination of cutting positions of honeycomb blocks using computer vision,Yakov Pikalov,,0%
https://arxiv.org/pdf/2304.00001.pdf,Determination of cutting positions of honeycomb blocks using computer vision,Mikhail Saramud,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Seyyed Amirhossein Ameli Kalkhoran,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Mohammadamin Banayeeanzade,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Mahdi Samiei,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Mahdieh Soleymani Baghshah,,0%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Xiaoliang Dai,xiaoliangdai@meta.com,95%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Graham Cormode,gcormode@meta.com,82%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Peter Vajda,vajdap@meta.com,78%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Sayan Ghosh,sayanghosh@meta.com,95%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Karthik Prasad,,0%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Peizhao Zhang,,0%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Bichen Wu,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Jingchao Peng,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Haitao Zhao,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Kaijie Zhao,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Zhongze Wang,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Lujian Yao,,0%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Indel Pal Singh,inder.singh@uni.lu,82%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Enjie Ghorbel,,0%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Oyebade Oyedotun,,0%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Djamila Aouada,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Peng Cao,caopengneu@gmail.com,95%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Zhiqiang Shen,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Hua Yang,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Xiaoli Liu,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Jinzhu Yang,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Osmar R. Zaiane,,0%
https://arxiv.org/pdf/2301.04460.pdf,Fast spline detection in high density microscopy data,Albert Alonso,,0%
https://arxiv.org/pdf/2301.04460.pdf,Fast spline detection in high density microscopy data,Julius B. Kirkegaard,,0%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Anne Spalanzani,Name.LastName@inria.fr,60%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Rabbia Asghar,,0%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Lukas Rummelhard,,0%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Christian Laugier,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Ahmet Karagoz,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Mustafa Ege Seker,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Mert Yergin,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Tarkan Atak Kan,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Mustafa Said Kartal,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Ercan Karaarslan,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Deniz Alis,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Ilkay Oksuz,,0%
https://arxiv.org/pdf/2301.04451.pdf,Heterogeneous Tri-stream Clustering Network,Chang-dong Wang,changdongwang@hotmail.com,95%
https://arxiv.org/pdf/2301.04451.pdf,Heterogeneous Tri-stream Clustering Network,Xiaozhi Deng,dengxiaozhi45@gmail.com,95%
https://arxiv.org/pdf/2301.04451.pdf,Heterogeneous Tri-stream Clustering Network,Dong Huang,huangdonghere@gmail.com,95%
https://arxiv.org/pdf/2301.04447.pdf,VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection,Mridula Verma,vmridula@idrbt.ac.in,85%
https://arxiv.org/pdf/2301.04447.pdf,VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection,Ramalingaswamy Cheruku,rmlswamy@nitw.ac.in,60%
https://arxiv.org/pdf/2301.04447.pdf,VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection,Hemraj Singh,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Frauke Wilm,frauke.wilm@fau.de,95%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Marco Fragoso,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Christof A. Bertram,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Nikolas Stathonikos,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Mathias Öttl,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Jingna Qiu,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Robert Klopfleisch,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Andreas Maier,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Katharina Breininger,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Marc Aubreville,,0%
https://arxiv.org/pdf/2301.04422.pdf,"Optical Flow for Autonomous Driving: Applications, Challenges and Improvements",Shihao Shen,,0%
https://arxiv.org/pdf/2301.04422.pdf,"Optical Flow for Autonomous Driving: Applications, Challenges and Improvements",Louis Kerofsky,,0%
https://arxiv.org/pdf/2301.04422.pdf,"Optical Flow for Autonomous Driving: Applications, Challenges and Improvements",Senthil Yogamani,,0%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Hong Wang,hong_wang@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Liang Peng,peng-l20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Wenbo Shao,,0%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Yanchao Xu,,0%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Jun Li,,0%
https://arxiv.org/pdf/2301.04416.pdf,pyssam -- a Python library for statistical modelling of biomedical shape and appearance,Josh Williams,,0%
https://arxiv.org/pdf/2301.04416.pdf,pyssam -- a Python library for statistical modelling of biomedical shape and appearance,Ali Ozel,,0%
https://arxiv.org/pdf/2301.04416.pdf,pyssam -- a Python library for statistical modelling of biomedical shape and appearance,Uwe Wolfram,,0%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Hong Wang,hong_wang@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Chen Lv,lyuchen@ntu.edu.sg,85%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Jun Li,lijun1958@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Weida Wang,wangwd0430@163.com,82%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Wenbo Shao,,0%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Yanchao Xu,,0%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Hong-yu Zhou,whuzhouhongyu@gmail.com,95%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Liansheng Wang,lswang@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Chixiang Lu,luchixiang@gmail.com,95%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Fernando Alonso-fernandez,Fernando.alonso@uam.es,85%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Julian Fierrez-aguilar,,0%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Javier Ortega-garcia,,0%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Joaquin Gonzalez-rodriguez,,0%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Changzhen Qiu,qiuchzh@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Yuhan Xie,,0%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Zhiyong Zhang,,0%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Shaolong Chen,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Chiara Romanengo,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Andrea Raffo,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Silvia Biasotti,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Bianca Falcidieno,,0%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Zongkai Wu,wuzongkai@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Ting Wang,wangting@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Feiyu Yao,feiyu.yao@columbia.edu,95%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Donglin Wang,wangdonglin@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Soo-hyung Kim,shkim@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Guee-sang Lee,gslee@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Van Thong Huynh,vthuynh@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Hyung-jeong Yang,hjyang@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04275.pdf,LENet: Lightweight And Efficient LiDAR Semantic Segmentation Using Multi-Scale Convolution Attention,Ben Ding,,0%
https://arxiv.org/pdf/2301.04272.pdf,Data Distillation: A Survey,Noveen Sachdeva,nosachde@ucsd.edu,75%
https://arxiv.org/pdf/2301.04272.pdf,Data Distillation: A Survey,Julian Mcauley,jmcauley@ucsd.edu,82%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Xiu Li,li.xiu@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Guangyi Chen,guangyichen1994@gmail.com,95%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Kai Li,li.gml.kai@gmail.com,95%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Qiaosong Chu,,0%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Shuyan Li,,0%
https://arxiv.org/pdf/2301.04261.pdf,Towards Microstructural State Variables in Materials Systems,Megna N. Shah,megna.shah.1@us.af.mil,95%
https://arxiv.org/pdf/2301.04261.pdf,Towards Microstructural State Variables in Materials Systems,Veera Sundararaghavan,veeras@umich.edu,85%
https://arxiv.org/pdf/2301.04261.pdf,Towards Microstructural State Variables in Materials Systems,Jeff P. Simmons,jeff.simmons.3@afrl.af.mil,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Ye Huang,edward.ye.huang@qq.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Xiangjian He,xiangjian.he@gmail.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Xuefei Zhe,zhexuefei@outlook.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Lixin Duan,lxduan@gmail.com,82%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Di Kang,di.kang@outlook.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Linchao Bao,linchaobao@gmail.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Wenjing Jia,Wenjing.Jia@uts.edu.au,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Liang Chen,liang.chen@outlook.com,95%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,Hengyuan Zhang,,0%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,Jing-yan Liao,,0%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,David Paz,,0%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,Henrik I. Christensen,,0%
https://arxiv.org/pdf/2301.04233.pdf,Adapting to Skew: Imputing Spatiotemporal Urban Data with 3D Partial Convolutions and Biased Masking,Bill Howe,billhowe@uw.edu,95%
https://arxiv.org/pdf/2301.04233.pdf,Adapting to Skew: Imputing Spatiotemporal Urban Data with 3D Partial Convolutions and Biased Masking,Bin Han,,0%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Deva Ramanan,deva@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Xindi Wu,xindiw@princeton.edu,85%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Kwunfung Lau,kwun.fung.lau@intel.com,82%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Francesco Ferroni,fferroni@nvidia.com,82%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Aljoša Ošep,aosep@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.04221.pdf,Explaining Deep Models through Forgettable Learning Dynamics,Ghassan Alregib,alregib@gatech.edu,78%
https://arxiv.org/pdf/2301.04221.pdf,Explaining Deep Models through Forgettable Learning Dynamics,Ryan Benkert,rbenkert3@gatech.edu,82%
https://arxiv.org/pdf/2301.04221.pdf,Explaining Deep Models through Forgettable Learning Dynamics,Oluwaseun Joseph Aribido,,0%
https://arxiv.org/pdf/2301.04218.pdf,Leveraging Diffusion For Strong and High Quality Face Morphing Attacks,Chen Liu,cliu@clarkson.edu,82%
https://arxiv.org/pdf/2301.04218.pdf,Leveraging Diffusion For Strong and High Quality Face Morphing Attacks,Zander W. Blasingame,,0%
https://arxiv.org/pdf/2301.04212.pdf,Deep Learning based Multi-Label Image Classification of Protest Activities,Kosaku Sato,Ksato@vt.edu,82%
https://arxiv.org/pdf/2301.04212.pdf,Deep Learning based Multi-Label Image Classification of Protest Activities,Jialu Wang,jialu@gwu.edu,85%
https://arxiv.org/pdf/2301.04212.pdf,Deep Learning based Multi-Label Image Classification of Protest Activities,Yingzhou Lu,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Alexandre Adam,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Laurence Perreault-levasseur,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Yashar Hezaveh,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Max Welling,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Shiyi Lan,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Xitong Yang,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Zhiding Yu,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Zuxuan Wu,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Jose M. Alvarez,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Anima Anandkumar,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Matthew Wallingford,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Aditya Kusupati,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Alex Fang,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Vivek Ramanujan,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Aniruddha Kembhavi,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Roozbeh Mottaghi,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Ali Farhadi,,0%
https://arxiv.org/pdf/2301.04467.pdf,FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection,Zhaoxiang Zhang,zhaoxiang.zhang@ia.ac.cn,95%
https://arxiv.org/pdf/2301.04467.pdf,FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection,Yuqi Wang,wangyuqi2020@ia.ac.cn,95%
https://arxiv.org/pdf/2301.04467.pdf,FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection,Yuntao Chen,chenyuntao08@gmail.com,95%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Chen Wang,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Angtian Wang,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Junbo Li,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Alan Yuille,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Cihang Xie,,0%
https://arxiv.org/pdf/2301.04058.pdf,Rethinking Voxelization and Classification for 3D Object Detection,Dmitry Yudin,yudin.da@mipt.ru,78%
https://arxiv.org/pdf/2301.04058.pdf,Rethinking Voxelization and Classification for 3D Object Detection,Alexander Golodkov,golodkov.ao@phystech.edu,78%
https://arxiv.org/pdf/2301.04058.pdf,Rethinking Voxelization and Classification for 3D Object Detection,Youshaa Murhij,yosha.morheg@phystech.edu,60%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Youcef Mezouar,youcef.mezouar@sigma-clermont.fr,95%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Adrien Bartoli,adrien.bartoli@gmail.com,95%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Mohammadreza Shetab-bushehri,m.r.shetab@gmail.com,65%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Miguel Aranda,miguel.aranda@unizar.es,95%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Erol Ozgur,erolozgur@gmail.com,95%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Sivaramakrishnan Rajaraman,sivaramakrishnan.rajaraman@nih.gov,95%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Feng Yang,,0%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Ghada Zamzmi,,0%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Zhiyun Xue,,0%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Sameer Antani,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Jian Yan,tianqj@nju.edu.cn,75%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Xuming Zhang,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Jia Tian,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Wei Li,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Xingfa Gu,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Qingjiu Tian,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Chengbin Peng,pengchengbin@nbu.edu.cn,95%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Hao Xu,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Hui Xiao,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Huazheng Hao,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Li Dong,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Xiaojie Qiu,,0%
https://arxiv.org/pdf/2301.03966.pdf,AdvBiom: Adversarial Attacks on Biometric Matchers,Vishesh Mistry,vishesh.mistry@tech5-sa.com,95%
https://arxiv.org/pdf/2301.03966.pdf,AdvBiom: Adversarial Attacks on Biometric Matchers,Debayan Deb,debayan.deb@tech5-sa.com,95%
https://arxiv.org/pdf/2301.03966.pdf,AdvBiom: Adversarial Attacks on Biometric Matchers,Rahul Parthe,rahul.parthe@tech5-sa.com,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Nicu Sebe,niculae.sebe@unitn.it,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Mengyi Zhao,zhaomengyi@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Shuling Dai,sldai@buaa.edu.cn,82%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Bin Ren,bin.ren@unitn.it,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Mengyuan Liu,liumy85@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Muhammad Arshad Khan,MuKhan@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Amir Ghalamzan E.,aghalamzanesfahani@lincoln.ac.uk,75%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Bappaditya Debnath,b.debnath2017@gmail.com,82%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Soran Parsa,soran.parsa@gmail.com,95%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Ryosuke Hyodo,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Susumu Saito,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Teppei Nakano,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Makoto Akabane,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Ryoichi Kasuga,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Tetsuji Ogawa,,0%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Dan Bigioi,jordanhu@tcd.ie,85%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Maciej Zięba,maciej.zieba@pwr.edu.pl,95%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Shubhajit Basak,s.basak1@nuigalway.ie,82%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Michał Stypułkowski,michal.stypulkowski@cs.uni.wroc.pl,85%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Peter Corcoran,peter.corcoran@universityofgalway.ie,95%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Rachel Mcdonnell,ramcdonn@tcd.ie,65%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Hugh Jordan,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Thomas Walter,Thomas.Walter@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Thomas Bonte,Thomas.Bonte@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Maxence Philbert,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Emeline Coleno,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Edouard Bertrand,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Arthur Imbert,,0%
https://arxiv.org/pdf/2301.03867.pdf,Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction,Laslo Dinges,laslo.dinges@ovgu.de,95%
https://arxiv.org/pdf/2301.03867.pdf,Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction,Ayoub Al-hamadi,ayoub.al-hamadi@ovgu.de,95%
https://arxiv.org/pdf/2301.03867.pdf,Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction,Thorsten Hempel,thorsten.hempel@ovgu.de,95%
https://arxiv.org/pdf/2302.08493.pdf,Deep Multi-stream Network for Video-based Calving Sign Detection,Teppei Nakano,teppei@pcl.cs.waseda.ac.jp,85%
https://arxiv.org/pdf/2302.08493.pdf,Deep Multi-stream Network for Video-based Calving Sign Detection,Ryosuke Hyodo,hyodo@pcl.cs.waseda.ac.jp,78%
https://arxiv.org/pdf/2302.08493.pdf,Deep Multi-stream Network for Video-based Calving Sign Detection,Tetsuji Ogawa,ogawa@pcl.cs.waseda.ac.jp,78%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Liming Chen,liming.chen@ec-lyon.fr,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Thomas Duboudin,thomas.duboudin@ec-lyon.fr,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Corentin Abgrall,corentin.abgrall@fr.thalesgroup.com,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Emmanuel Dellandréa,emmanuel.dellandrea@ec-lyon.fr,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Gilles Hénaff,gilles.henaff@fr.thalesgroup.com,95%
https://arxiv.org/pdf/2301.03843.pdf,A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models,Rei Aso,,0%
https://arxiv.org/pdf/2301.03843.pdf,A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models,Tatsuya Chuman,,0%
https://arxiv.org/pdf/2301.03843.pdf,A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models,Hitoshi Kiya,,0%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Juyeb Shin,juyebshin@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Dongsuk Kum,dskum@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Hyeonjun Jeong,hyeonjun.jeong@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Francois Rameau,francois.rameau@sunykorea.ac.kr,95%
https://arxiv.org/pdf/2301.03832.pdf,Video Semantic Segmentation with Inter-Frame Feature Fusion and Inner-Frame Feature Refinement,Zilei Wang,zlwang@ustc.edu.cn,82%
https://arxiv.org/pdf/2301.03832.pdf,Video Semantic Segmentation with Inter-Frame Feature Fusion and Inner-Frame Feature Refinement,Jiafan Zhuang,jfzhuang@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2301.03832.pdf,Video Semantic Segmentation with Inter-Frame Feature Fusion and Inner-Frame Feature Refinement,Junjie Li,,0%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Songyang Zhang,sy.zhangbuaa@gmail.com,82%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Hongbin Sun,hsun@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Songtao Liu,liusongtao@megvii.com,95%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Zeming Li,lizeming@megvii.com,95%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Jian Sun,sunjian@megvii.com,95%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Xuming He,hexm@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Nanning Zheng,nnzheng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Lin Song,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Kaiping Zheng,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Thao Nguyen,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Jesslyn Hwei Sing Chong,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Charlene Enhui Goh,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Melanie Herschel,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Hee Hoon Lee,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Changshuo Liu,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Beng Chin Ooi,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Wei Wang,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,James Yip,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Nishant Yadav,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Mahbubul Alam,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Ahmed Farahat,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Dipanjan Ghosh,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Chetan Gupta,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Auroop R. Ganguly,,0%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Payman Moallem,moallem@eng.ui.ac.ir,78%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Saed Moradi,,0%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Alireza Memarmoghadam,,0%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Mohamad Farzan Sabahi,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Shuai Shen,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Wenliang Zhao,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Zibin Meng,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Wanhua Li,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Zheng Zhu,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Jie Zhou,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Jiwen Lu,,0%
https://arxiv.org/pdf/2301.03769.pdf,Learning from What is Already Out There: Few-shot Sign Language Recognition with Online Dictionaries,Matyáš Boháček,,0%
https://arxiv.org/pdf/2301.03769.pdf,Learning from What is Already Out There: Few-shot Sign Language Recognition with Online Dictionaries,Marek Hrúz,,0%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Seonguk Seo,seonguk@meta.com,95%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Sara Cao,xuefeicao01@meta.com,78%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Ser-nam Lim,sernam@ucf.edu,85%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Bohyung Han,bhhan@snu.ac.kr,82%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Mustafa Gokhan Uzunbas,,0%
https://arxiv.org/pdf/2301.03730.pdf,Learning to Perceive in Deep Model-Free Reinforcement Learning,Gonçalo Querido,goncalo.querido@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2301.03730.pdf,Learning to Perceive in Deep Model-Free Reinforcement Learning,Alberto Sardinha,jose.alberto.sardinha@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2301.03730.pdf,Learning to Perceive in Deep Model-Free Reinforcement Learning,Francisco S. Melo,fmelo@inesc-id.pt,82%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Chris Matthews,chriscrick@cs.okstate.edu,85%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Md Zobaer Islam,zobaer.islam@okstate.edu,78%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Sabit Ekin,sabitekin@tamu.edu,95%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Dane C. Johnson,dane.johnson@okstate.edu,95%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Russ Messenger,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Matthew Whitlock,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Erik Spong,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Nate Morton,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Layne Claggett,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Jordan Fox,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Leland Palmer,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,John F. O'hara,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Christopher J. Crick,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Jamey D. Jacob,,0%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Max H. Siegel,maxs@mit.edu,85%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Ilker Yildirim,ilker.yildirim@yale.edu,95%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Amir A. Soltani,,0%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Shraman Ray Chaudhari,,0%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Joshua B. Tenenbaum,,0%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Alberto Díaz-álvarez,alberto.diaz@upm.es,85%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Guillermo Iglesias,guillermo.iglesias@upm.es,95%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Edgar Talavera,e.talavera@upm.es,82%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Jesús Troya Garcìa,,0%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Miguel Gracía-remesal,,0%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Keyu Tian,keyutian@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Qishuai Diao,diaoqishuai@bytedance.com,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Chen Lin,chen.lin@eng.ox.ac.uk,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Liwei Wang,wanglw@pku.edu.cn,78%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Zehuan Yuan,yuanzehuan@bytedance.com,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Yi Jiang,jiangyi.enjoy@bytedance.com,95%
https://arxiv.org/pdf/2301.02667.pdf,Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,Hanbyul Joo,hbjoo@snu.ac.kr,82%
https://arxiv.org/pdf/2301.02667.pdf,Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,Jiye Lee,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Bowen Lei,bowenlei@stat.tamu.edu,95%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Dongkuan Xu,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Ruqi Zhang,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Shuren He,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Bani K. Mallick,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Nikolaos Tsakas,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Maria Lymperaiou,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Giorgos Filandrianos,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Giorgos Stamou,,0%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Christopher Neff,cneff1@uncc.edu,82%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Hamed Tabkhi,htabkhiv@uncc.edu,82%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Ghazal Alinezhad Noghre,galinezh@uncc.edu,60%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Mohammadreza Baharani,mbaharan@uncc.edu,90%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Shanle Yao,adaneshp@uncc.edu,60%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Armin Danesh Pazho,,0%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Babak Rahimi Ardabili,,0%
https://arxiv.org/pdf/2301.03553.pdf,FedDebug: Systematic Debugging for Federated Learning Applications,Waris Gill,waris@vt.edu,85%
https://arxiv.org/pdf/2301.03553.pdf,FedDebug: Systematic Debugging for Federated Learning Applications,Muhammad Ali Gulzar,gulzar@cs.vt.edu,78%
https://arxiv.org/pdf/2301.03553.pdf,FedDebug: Systematic Debugging for Federated Learning Applications,Ali Anwar,aanwar@umn.edu,82%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Julian Jordan,julian.jordan@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Steffen Staab,steffen.staab@ipvs.uni-stuttgart.de,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Julian Schmidt,julian.sj.schmidt@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Jan Rupprecht,jan.rupprecht@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Daniel Frank,daniel.frank@ipvs.uni-stuttgart.de,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Thomas Monninger,thomas.monninger@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,David Raba,david.raba@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Klaus Dietmayer,klaus.dietmayer@uni-ulm.de,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Fenggang Liu,liufenggang@senseauto.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Yangguang Li,liyangguang@sensetime.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Changxin Gao,cgao@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Nong Sang,nsang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Huan Peng,penghuan@senseauto.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Jing Shao,shaojing@senseauto.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Bin Huang,huangbin1@senseauto.com,95%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Dorit Merhof,dorit.merhof@ur.de,95%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Reza Azad,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Amirhossein Kazerouni,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Moein Heidari,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Ehsan Khodapanah Aghdam,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Amirali Molaei,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Yiwei Jia,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Abin Jose,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Rijo Roy,,0%
https://arxiv.org/pdf/2301.03495.pdf,On the challenges to learn from Natural Data Streams,Davide Maltoni,davide.maltoni@unibo.it,95%
https://arxiv.org/pdf/2301.03495.pdf,On the challenges to learn from Natural Data Streams,Guido Borghi,guido.borghi@unibo.it,95%
https://arxiv.org/pdf/2301.03495.pdf,On the challenges to learn from Natural Data Streams,Gabriele Graffieti,,0%
https://arxiv.org/pdf/2301.03461.pdf,DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction,Yibo Yang,ibo@pku.edu.cn,90%
https://arxiv.org/pdf/2301.03461.pdf,DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction,Lefei Zhang,zhanglefei@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03461.pdf,DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction,Yangyang Xu,yangyangxu@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03439.pdf,Generalized adaptive smoothing based neural network architecture for traffic state estimation,Chuhan Yang,,0%
https://arxiv.org/pdf/2301.03439.pdf,Generalized adaptive smoothing based neural network architecture for traffic state estimation,Sai Venkata Ramana Ambadipudi,,0%
https://arxiv.org/pdf/2301.03439.pdf,Generalized adaptive smoothing based neural network architecture for traffic state estimation,Saif Eddin Jabari,,0%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Yilei Shi,yilei.shi@tum.de,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Xiao Xiang Zhu,xiaoxiang.zhu@tum.de,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Wen Yang,yangwen@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Fang Xu,xufang@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Patrick Ebel,patrick.ebel@tum.de,95%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Marc Hanheide,mhanheide@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Sergi Molina,smolinamellado@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Riccardo Polvara,rpolvara@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Ibrahim Hroob,ihroob@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Grzegorz Cielniak,gcielniak@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Robert Jewsbury,rob.jewsbury@warwick.ac.uk,82%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Nasir Rajpoot,n.m.rajpoot@warwick.ac.uk,82%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Quoc Dang Vu,quoc-dang.vu@warwick.ac.uk,95%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Simon Graham,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Mostafa Jahanifar,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Shan E Ahmed Raza,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Fayyaz Minhas,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Abhir Bhalerao,,0%
https://arxiv.org/pdf/2301.03407.pdf,On Advantages of Mask-level Recognition for Outlier-aware Segmentation,Matej Grcić,,0%
https://arxiv.org/pdf/2301.03407.pdf,On Advantages of Mask-level Recognition for Outlier-aware Segmentation,Josip Šarić,,0%
https://arxiv.org/pdf/2301.03407.pdf,On Advantages of Mask-level Recognition for Outlier-aware Segmentation,Siniša Šegvić,,0%
https://arxiv.org/pdf/2301.03362.pdf,Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --,Michael Elad,elad@cs.technion.ac.il,78%
https://arxiv.org/pdf/2301.03362.pdf,Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --,Bahjat Kawar,bahjat.kawar@cs.technion.ac.il,95%
https://arxiv.org/pdf/2301.03362.pdf,Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --,Gregory Vaksman,,0%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Hai Zhao,hai@cs.sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Kehai Chen,chenkehai@hit.edu.cn,95%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Rui Wang,wangrui.nlp@gmail.com,95%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Zhuosheng Zhang,zhangzs@sjtu.edu.cn,82%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Masao Utiyama,mutiyama@nict.go.jp,82%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Eiichiro Sumita,eiichiro.sumita@nict.go.jp,95%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Zuchao Li,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Meng Wang,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Feng Gao,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Junyu Dong,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Heng-chao Li,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Qian Du,,0%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Chen Dong,dongchen@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Haotai Liang,tai@bupt.edu.cn,90%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Geng Liu,liugeng@sgchip.sgcc.com.cn,95%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Senran Fan,,0%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Xiaodong Xu,,0%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Xiang Wang,wxiang@hust.edu.cn,85%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Changxin Gao,cgao@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Nong Sang,nsang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Zhengrong Zuo,zhrzuo@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Rong Jin,rongjinemail@gmail.com,95%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Shiwei Zhang,,0%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Zhiwu Qing,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Giacomo Zara,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Victor Guilherme Turrisi Da Costa,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Subhankar Roy,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Paolo Rota,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Elisa Ricci,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xiangyu Li,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Gongning Luo,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Kuanquan Wang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Hongyu Wang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jun Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xinjie Liang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jie Jiang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Zhenghao Song,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Chunyue Zheng,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Haokai Chi,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Mingwang Xu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Yingte He,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xinghua Ma,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jingwen Guo,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Yifan Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Chuanpu Li,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Zeli Chen,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Md Mahfuzur Rahman Siddiquee,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Andriy Myronenko,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Antoine P. Sanner,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Anirban Mukhopadhyay,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Ahmed E. Othman,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xingyu Zhao,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Weiping Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jinhuang Zhang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xiangyuan Ma,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Qinghui Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Bradley J. Macintosh,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Wei Liang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Moona Mazher,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Abdul Qayyum,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Valeriia Abramova,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xavier Lladó,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Shuo Li,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Zhongling Huang,huangzhongling@nwpu.edu.cn,95%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Mihai Datcu,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Andrei Anghel,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Juanping Zhao,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Remus Cacoveanu,,0%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Hao Tang,haotang@meta.com,95%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Weiyao Wang,weiyaowang@meta.com,95%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Kevin Liang,kevinjliang@meta.com,95%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Matt Feiszli,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Ling Zhu,zhul1757@2m9h.net,78%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Ya Zhang,ya_zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Chaoyi Wu,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Feng Chang,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Xiao Su,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Zhihan Wu,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Yanfeng Wang,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,A. T. Gifford,alessandro.gifford@gmail.com,82%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,R. M. Cichy,rmcichy@zedat.fu-berlin.de,82%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,B. Lahner,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,S. Saba-sadiya,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,M. G. Vilas,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,A. Lascelles,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,A. Oliva,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,K. Kay,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,G. Roig,,0%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Yang Gao,gaoy@nju.edu.cn,78%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Wenzhe Yin,w.yin@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Haochen Wang,h.wang3@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Jie Liu,j.liu5@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Yanqi Bao,yanqibao1997@gmail.com,95%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Efstratios Gavves,egavves@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Jan-jakob Sonke,j.sonke@nki.nl,82%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Qing Guo,ingqguo@ieee.org,78%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Ivor W. Tsang,tsang@cfar.a-star.edu.sg,78%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Lan Fu,lan.fu@innopeaktech.com,95%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Wei Feng,wfeng@ieee.org,82%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Rynson W. H. Lau,Rynson.Lau@cityu.edu.hk,95%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Yuhao Liu,yuhaoliu7456@outlook.com,95%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Ke Xu,zhanghake2-c@my.cityu.edu.hk,85%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Zhanghan Ke,,0%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Haoqian Liang,lianghq@bupt.edu.cn,78%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Ya Yang,yangya@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Zhichao Li,,0%
https://arxiv.org/pdf/2301.03169.pdf,A Study on the Generality of Neural Network Structures for Monocular Depth Estimation,Sunghoon Im,sunghoonim@dgist.ac.kr,95%
https://arxiv.org/pdf/2301.03169.pdf,A Study on the Generality of Neural Network Structures for Monocular Depth Estimation,Jinwoo Bae,woobae@hyundai.com,78%
https://arxiv.org/pdf/2301.03169.pdf,A Study on the Generality of Neural Network Structures for Monocular Depth Estimation,Kyumin Hwang,kyumin@dgist.ac.kr,85%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Jinwon Lee,djwlee@gwnu.ac.kr,78%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Seungeun Lim,aseungeunlim@korea.ac.kr,95%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Duhwan Mun,edhmun@korea.ac.kr,78%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Fazhi He,cfzhe@whu.edu.cn,78%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Changmo Yeo,,0%
https://arxiv.org/pdf/2301.03164.pdf,Cursive Caption Text Detection in Videos,Imran Siddiqi,imran.siddiqi@bahria.edu.pk,95%
https://arxiv.org/pdf/2301.03164.pdf,Cursive Caption Text Detection in Videos,Ali Mirza,ali.mirza@mantalus.com,95%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Hanlong Chen,hanlong@g.ucla.edu,85%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Luzhe Huang,lzhuang0324@g.ucla.edu,82%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Tairan Liu,liutr@g.ucla.edu,78%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Aydogan Ozcan,ozcan@ucla.edu,78%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Haowei Wang,wanghaowei@stu.xmu.edu.cn,95%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Yongjian Wu,littlekenwu@tencent.com,78%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Xiaoshuai Sun,xssun@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Yiyi Zhou,zhouyiyi@xmu.edu.cn,95%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Jiayi Ji,,0%
https://arxiv.org/pdf/2301.03155.pdf,Instance Segmentation Based Graph Extraction for Handwritten Circuit Diagram Images,Johannes Bayer,johannes.bayer@dfki.de,95%
https://arxiv.org/pdf/2301.03155.pdf,Instance Segmentation Based Graph Extraction for Handwritten Circuit Diagram Images,Amit Kumar Roy,amit.roy@dfki.de,95%
https://arxiv.org/pdf/2301.03155.pdf,Instance Segmentation Based Graph Extraction for Handwritten Circuit Diagram Images,Andreas Dengel,andreas.dengel@dfki.de,95%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Mohammadreza Naderi,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Mohammadhossein Givkashi,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Nader Karimi,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Shahram Shirani,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Shadrokh Samavi,,0%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Stylianos Oikonomou,stylianos@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Anil Bandhakavi,anil@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Adelize Van Eeden,adelize.ve@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Pim Jordi Verschuuren,Pim.jv@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Jie Gao,jie@logically.co.uk,85%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Cory Cornelius,cory.cornelius@intel.com,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Shengyun Peng,speng65@gatech.edu,82%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Jason Martin,jason.martin@intel.com,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Weilin Xu,weilin.xu@intel.com,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Kevin Li,kevin.li@gatech.edu,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Rahul Duggal,rahulduggal@gatech.edu,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Duen Horng Chau,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Rui Zheng,zhengrui@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Man Chen,maggiech1221@126.com,75%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Jiawen Li,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Yunqian Huang,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Sheng Song,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Hongbo Chen,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Junni Shi,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Duo Xu,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Haibin Zhang,,0%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Yisroel Mirsky,yisroel@bgu.ac.il,85%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Lior Yasur,lioryasu@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Guy Frankovits,guyfrank@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Fred M. Grabovski,freddie@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Liheng Bian,bian@bit.edu.cn,78%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Daoyu Li,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Hanwen Xu,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Miao Cao,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Xin Yuan,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,David J. Brady,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Ming Li,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Xiangyu Xu,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Hehe Fan,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Pan Zhou,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jun Liu,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jia-wei Liu,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jiahe Li,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jussi Keppo,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Mike Zheng Shou,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Shuicheng Yan,,0%
https://arxiv.org/pdf/2301.03045.pdf,Seamless Multimodal Biometrics for Continuous Personalised Wellbeing Monitoring,João Ribeiro Pinto,,0%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,James Tin-yau Kwok,jamesk@cse.ust.hk,85%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Ming Lin,minglamz@amazon.com,85%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Jie Gui,guijie@seu.edu.cn,95%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Lanting Fang,lanting@seu.edu.cn,85%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Liguo Huang,lghuang@smu.edu,82%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Bin Luo,luobin@nju.edu.cn,95%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Jidong Ge,,0%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Yuxiang Liu,,0%
https://arxiv.org/pdf/2301.03039.pdf,Equivalence of Two Expressions of Principal Line,Cheng-yen Hsu,,0%
https://arxiv.org/pdf/2301.03039.pdf,Equivalence of Two Expressions of Principal Line,Hsin-yi Chen,,0%
https://arxiv.org/pdf/2301.03039.pdf,Equivalence of Two Expressions of Principal Line,Jen-hui Chuang,,0%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Zhengyi Liu,liuzywen@ahu.edu.cn,78%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Bin Tang,pubs-permissions@ieee.org,60%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Yacheng Tan,,0%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Qian He,,0%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Wei Wu,liuzywen@ahu.edu.cn,85%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Zhengyi Liu,,0%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Yacheng Tan,,0%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Guanghui Zhang,,0%
https://arxiv.org/pdf/2301.03027.pdf,Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.03027.pdf,Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction,Jeong Eun Lee,leeje290@gmail.com,78%
https://arxiv.org/pdf/2301.03027.pdf,Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction,Gyutaek Oh,,0%
https://arxiv.org/pdf/2302.05294.pdf,MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope,Jingwei Zhang,jwzhang22@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2302.05294.pdf,MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope,Farzan Farnia,farnia@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Chong Wang,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Yuyuan Liu,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Yuanhong Chen,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Fengbei Liu,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Yu Tian,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Davis J. Mccarthy,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Helen Frazer,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Gustavo Carneiro,,0%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Lijun Zhao,zhaolj@hit.edu.cn,78%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Ke Wang,wangke@hit.edu.cn,95%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Tao Xie,xietao1997@hit.edu.cn,95%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Kun Dai,,0%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Ruifeng Li,,0%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Yiyu Shi,yshi4@nd.edu,82%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Tsung-yi Ho,tyho@cs.nthu.edu.tw,82%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Yu-jen Chen,yujenchen@gapp.nthu.edu.tw,95%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Ching-hao Chiu,,0%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Hao-wei Chung,,0%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Yuyin Sun,yuyinsun@amazon.com,95%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Jiajia Luo,lujiajia@amazon.com,85%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Lu Xia,luxial@amazon.com,95%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Zhongyu Jiang,zyjiang@uw.edu,82%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Nan Qiao,qiaonan@amazon.com,95%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Ke Zhang,kezha@amazon.com,85%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Jenq-neng Hwang,hwang@uw.edu,78%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Cheng-yen Yang,cycyang@uw.edu,82%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Ying Wei,weiying@ise.neu.edu.cn,95%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Yuefeng Wang,wangyuefeng0203@gmail.com,95%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Shuailei Ma,,0%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Shanze Wang,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Jie Li,jielixjtu@xjtu.edu.cn,95%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Fengping Wang,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Chun Qi,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Lin Wang,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Pan Wang,,0%
https://arxiv.org/pdf/2301.02934.pdf,Advancing 3D finger knuckle recognition via deep feature learning,Xu Cheng,xcheng@nuist.edu.cn,82%
https://arxiv.org/pdf/2301.02934.pdf,Advancing 3D finger knuckle recognition via deep feature learning,Guoying Zhao,guoying.zhao@oulu.fi,95%
https://arxiv.org/pdf/2301.02934.pdf,Advancing 3D finger knuckle recognition via deep feature learning,Kevin H. M. Cheng,homan.cheng@oulu.fi,78%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Pushpak Pati,pus@zurich.ibm.com,90%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Guillaume Jaume,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Zeineb Ayadi,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Kevin Thandiackal,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Behzad Bozorgtabar,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Maria Gabrani,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Orcun Goksel,,0%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Han Hui Lin,hanhlin@gene.com,95%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Hai Ngu,hain@gene.com,85%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Mohsen Hejrati,hejratis@gene.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Sarah Chu,chus18@gene.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Baris Bingol,barisb@gene.com,85%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Hosein Barzekar,barzekar.h@gmail.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Soumitra Ghosh,ghoshs29@gene.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Steven Ray Valdespino,,0%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Somaye Hashemifar,,0%
https://arxiv.org/pdf/2301.02911.pdf,Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants,Marwa Mahmoud,marwa.mahmoud@glasgow.ac.uk,95%
https://arxiv.org/pdf/2301.02911.pdf,Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants,Bruno Tafur,,0%
https://arxiv.org/pdf/2301.02911.pdf,Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants,Staci Weiss,,0%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Byoungjip Kim,bjkim@lgresearch.ai,82%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Moontae Lee,moontae.lee@lgresearch.ai,95%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Dasol Hwang,dasol.hwang@lgresearch.ai,95%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Honglak Lee,honglak@lgresearch.ai,85%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Sungik Choi,sungik.choi@lgresearch.ai,95%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Jiangying Qin,jy_qin@whu.edu.cn,82%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Hanqi Zhang,hqzhang@whu.edu.cn,82%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Ming Li,lisouming@whu.edu.cn,95%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Jiageng Zhong,zhongjiageng@whu.edu.cn,95%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Hui Yuan,huiyuan@sdu.edu.cn,95%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Pan Gao,Pan.Gao@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Zihao Li,,0%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Ran Wei,,0%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Rob Brennan,rob.brennan@adaptcentre.ie,95%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Alessandra Mileo,alessandra.mileo@dcu.ie,95%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Teerath Kumar,teerath.menghwar2@mail.dcu.ie,85%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Malika Bendechache,malika.bendechache@universityofgalway.ie,95%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Zongmin Liu,liu_zm@ctbu.edu.cn,78%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Jirui Wang,,0%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Jie Li,,0%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Pengda Liu,,0%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Kai Ren,,0%
https://arxiv.org/pdf/2301.02789.pdf,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,Gangwei Xu,gwxu@hust.edu.cn,82%
https://arxiv.org/pdf/2301.02789.pdf,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,Xin Yang,xinyang2014@hust.edu.cn,95%
https://arxiv.org/pdf/2301.02789.pdf,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,Huan Zhou,huanzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Xinpeng Zhang,xzhang@shu.edu.cn,82%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Weisi Lin,wslin@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Zhi Liu,liuzhisjtu@163.com,95%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Gongyang Li,ligongyang@shu.edu.cn,95%
https://arxiv.org/pdf/2301.02761.pdf,Active Learning Guided by Efficient Surrogate Learners,Kwang In Kim,2kimkin@postech.ac.kr,78%
https://arxiv.org/pdf/2301.02761.pdf,Active Learning Guided by Efficient Surrogate Learners,Yunpyo An,anyunpyo@unist.ac.kr,95%
https://arxiv.org/pdf/2301.02761.pdf,Active Learning Guided by Efficient Surrogate Learners,Suyeong Park,suyeong@unist.ac.kr,85%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,John S. Schreck,schreck@ucar.edu,78%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,Matthew Hayman,mhayman@ucar.edu,82%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,Gabrielle Gantos,,0%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,Aaron Bansemer,,0%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,David John Gagne,,0%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Amirreza Babaahmadi,babaahmadi.amir@ut.ac.ir,78%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Sahar Khalafi,,0%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Masoud Shariatpanahi,,0%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Moosa Ayati,,0%
https://arxiv.org/pdf/2301.02726.pdf,Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation,Hilmil Pradana,hilmi@nict.go.jp,90%
https://arxiv.org/pdf/2301.02726.pdf,Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation,Koji Zettsu,zettsu@nict.go.jp,78%
https://arxiv.org/pdf/2301.02726.pdf,Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation,Minh-son Dao,dao@nict.go.jp,78%
https://arxiv.org/pdf/2301.02703.pdf,RUPNet: Residual upsampling network for real-time polyp segmentation,Ulas Bagci,ulas.bagci@northwestern.edu,95%
https://arxiv.org/pdf/2301.02703.pdf,RUPNet: Residual upsampling network for real-time polyp segmentation,Debesh Jha,debesh.jha@northwestern.edu,95%
https://arxiv.org/pdf/2301.02703.pdf,RUPNet: Residual upsampling network for real-time polyp segmentation,Nikhil Kumar Tomar,nikhilroxtomar@gmail.com,95%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Rameen Abdal,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Hsin-ying Lee,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Peihao Zhu,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Menglei Chai,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Aliaksandr Siarohin,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Peter Wonka,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Sergey Tulyakov,,0%
https://arxiv.org/pdf/2301.02693.pdf,Design of Arabic Sign Language Recognition Model,Muhammad Al-barham,,0%
https://arxiv.org/pdf/2301.02693.pdf,Design of Arabic Sign Language Recognition Model,Ahmad Jamal,,0%
https://arxiv.org/pdf/2301.02693.pdf,Design of Arabic Sign Language Recognition Model,Musa Al-yaman,,0%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Bastian Leibe,leibe@vision.rwth-aachen.de,78%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Jonathon Luiten,luiten@vision.rwth-aachen.de,78%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Alexander Hermans,hermans@vision.rwth-aachen.de,78%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Deva Ramanan,deva@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Ali Athar,athar@vision.rwth-aachen.de,82%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Roberto Martín-martín,robertomm@cs.utexas.edu,85%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Le Xue,jniebles@cs.stanford.edu,85%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Manli Shu,manli.shu@salesforce.com,95%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Caiming Xiong,cxiong@salesforce.com,82%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Tom Goldstein,tomg@umd.edu,85%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Ran Xu,ran.xu@salesforce.com,95%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Ning Yu,ning.yu@salesforce.com,95%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Juan Carlos Niebles,,0%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Hjalmar Kühl,hjalmar.kuehl@idiv.de,85%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Tilo Burghardt,tilo@cs.bris.ac.uk,85%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Otto Brookes,otto.brookes@bristol.ac.uk,95%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Majid Mirmehdi,majid@cs.bris.ac.uk,85%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Pedro C. Neto,2pedro.d.carneiro@inesctec.pt,85%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Sara P. Oliveira,4s.oliveira@nki.nl,78%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Diana Montezuma,3diana.felizardo@impdiagnostics.com,85%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Domingos Oliveira,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,João Fraga,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Ana Monteiro,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,João Monteiro,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Liliana Ribeiro,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Sofia Gonçalves,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Stefan Reinhard,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Inti Zlobec,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Isabel M. Pinto,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Jaime S. Cardoso,,0%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Xudong Lin,xudong.lin@columbia.edu,95%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Andrew Lu,,0%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Yulei Niu,,0%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Shih-fu Chang,,0%
https://arxiv.org/pdf/2301.02524.pdf,Tackling Data Bias in Painting Classification with Style Transfer,Frederick W. B. Li,frederick.li@durham.ac.uk,95%
https://arxiv.org/pdf/2301.02524.pdf,Tackling Data Bias in Painting Classification with Style Transfer,Hubert P. H. Shum,hubert.shum@durham.ac.uk,95%
https://arxiv.org/pdf/2301.02524.pdf,Tackling Data Bias in Painting Classification with Style Transfer,Mridula Vijendran,mridula.vijendran@durham.ac.uk,95%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Konstantinos Vougioukas,k.vougioukas@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Maciej Zięba,maciej.zieba@pwr.edu.pl,95%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Michał Stypułkowski,michal.stypulkowski@cs.uni.wroc.pl,85%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Sen He,senhe752@gmail.com,95%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Maja Pantic,m.pantic@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Stavros Petridis,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Sijin Chen,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Hongyuan Zhu,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Xin Chen,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Yinjie Lei,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Tao Chen,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Gang Yu,,0%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Xiaopeng Yang,xiaopengyang@bit.edu.cn,95%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Xiaodong Qu,xdqu@bit.edu.cn,82%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Tian Lan,tlan@bit.edu.cn,82%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Weicheng Gao,,0%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Guangqi Jiang,jiang@cczu.edu.cn,78%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Huibing Wang,huibing.wang@dlmu.edu.cn,95%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Zetian Mi,tian@dlmu.edu.cn,90%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Mingze Yao,,0%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Xianping Fu,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Benbakreti Samir,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Said Mwanahija,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Benbakreti Soumia,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Umut Özkaya,,0%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Gabriele Graffieti,gabriele.graffieti@unibo.it,95%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Vincenzo Lomonaco,vincenzo.lomonaco@unipi.it,95%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Lorenzo Pellegrini,l.pellegrini@unibo.it,82%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Davide Maltoni,davide.maltoni@unibo.it,95%
https://arxiv.org/pdf/2301.02440.pdf,An Image captioning algorithm based on the Hybrid Deep Learning Technique (CNN+GRU),Hina Sattar,hinasattar987@gmail.com,95%
https://arxiv.org/pdf/2301.02440.pdf,An Image captioning algorithm based on the Hybrid Deep Learning Technique (CNN+GRU),Muhammad Azhar,Muhammad.azhar@chosun.ac.kr,95%
https://arxiv.org/pdf/2301.02440.pdf,An Image captioning algorithm based on the Hybrid Deep Learning Technique (CNN+GRU),Rana Adnan Ahmad,rana22293@gmail.com,85%
https://arxiv.org/pdf/2301.02437.pdf,Valid P-Value for Deep Learning-Driven Salient Region,Daiki Miwa,miwa.daiki.mllab.nit@gmail.com,95%
https://arxiv.org/pdf/2301.02437.pdf,Valid P-Value for Deep Learning-Driven Salient Region,Ichiro Takeuchi,ichiro.takeuchi@mae.nagoya-u.ac.jp,95%
https://arxiv.org/pdf/2301.02437.pdf,Valid P-Value for Deep Learning-Driven Salient Region,Vo Nguyen Le Duy,duy.mllab.nit@gmail.com,78%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Yabiao Wang,caseywang@tencent.com,78%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Xiangyang Xue,xiangyangxue@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Siqian Yang,seasonsyang@tencent.com,82%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Chengming Xu,cmxu18@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Zhanxiong Wang,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Liu Liu,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Yukai Lin,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Xiao Liang,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Qichao Xu,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Miao Jia,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Yangdong Liu,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Yuxiang Wen,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Wei Luo,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Jiangwei Li,,0%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Gangming Zhao,ming@bupt.edu.cn,90%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Xianpeng Wu,wxpzju123@163.com,65%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Chengwei Pan,pancw@buaa.edu.cn,78%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Fandong Zhang,zhangfandong@deepwise.com,95%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Kongming Liang,,0%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Xinyang Hu,,0%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Tomoyuki Hiroyasu,tomo@is.doshisha.ac.jp,90%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Kyoka Yoshiok,,0%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Kensuke Tanioka,,0%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Satoru Hiwa,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Tomoyuki Hiroyasu,tomo@is.doshisha.ac.jp,90%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Yusuke Nagira,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Yuzuha Hara,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Satoru Hiwa,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Naoki Okumura,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Noriko Koizumi,,0%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Menghan Xia,menghanxyz@gmail.com,85%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Yuechen Zhang,yczhang21@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Tien-tsin Wong,ttwong@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Jinbo Xing,jbxing@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Xiaodong Cun,,0%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Jue Wang,,0%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Zehao Huang,zehaohuang18@gmail.com,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Si Liu,liusi@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Zi-han Ding,zihanding819@gmail.com,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Zhenwei Shen,shenzhenwei@outlook.com,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Jiao Dai,daijiao@iie.ac.cn,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Jizhong Han,hanjizhong@iie.ac.cn,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Shaofei Huang,nowherespyfly@gmail.com,60%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Zehao Huang,zehaohuang18@gmail.com,95%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Si Liu,liusi@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Jiahui Fu,jiahuifu@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Zitian Wang,wangzt.kghl@gmail.com,78%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Hongteng Xu,hongtengxu@ruc.edu.cn,95%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Chuhao Jin,jinchuhao@ruc.edu.cn,95%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Ruihua Song,rsong@ruc.edu.cn,82%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Zhiwu Lu,,0%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Yifei Lou,yifei.lou@utdallas.edu,95%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Jack Xin,jxin@math.uci.edu,82%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Kevin Bui,kevinb3@uci.edu,85%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Fredrick Park,fpark@whittier.edu,82%
https://arxiv.org/pdf/2301.02341.pdf,A survey on Organoid Image Analysis Platforms,Alireza Ranjbaran,Ranjbarana@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.02341.pdf,A survey on Organoid Image Analysis Platforms,Azadeh Nazemi,azadeh1972@gmail.com,85%
https://arxiv.org/pdf/2301.02317.pdf,Convolutional XGBoost (C-XGBOOST) Model for Brain Tumor Detection,Muyiwa Babayomi,,0%
https://arxiv.org/pdf/2301.02317.pdf,Convolutional XGBoost (C-XGBOOST) Model for Brain Tumor Detection,Oluwatosin Atinuke Olagbaju,,0%
https://arxiv.org/pdf/2301.02317.pdf,Convolutional XGBoost (C-XGBOOST) Model for Brain Tumor Detection,Abdulrasheed Adedolapo Kadiri,,0%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Sabine Süsstrunk,sabine.susstrunk@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Tong Zhang,tong.zhang@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Mathieu Salzmann,mathieu.salzmann@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Bahar Aydemir,bahar.aydemir@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Ludo Hoffstetter,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Kumar Ashutosh,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Rohit Girdhar,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Lorenzo Torresani,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Patrick Grady,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Jeremy A. Collins,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Chengcheng Tang,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Christopher D. Twigg,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Kunal Aneja,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,James Hays,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Charles C. Kemp,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Kumar Ashutosh,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Rohit Girdhar,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Lorenzo Torresani,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Vishwanath Saragadam,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Daniel Lejeune,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Jasper Tan,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Guha Balakrishnan,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Ashok Veeraraghavan,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Richard G. Baraniuk,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Filip Radenovic,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Abhimanyu Dubey,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Abhishek Kadian,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Todor Mihaylov,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Simon Vandenhende,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Yash Patel,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Yi Wen,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Vignesh Ramanathan,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Dhruv Mahajan,,0%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Ivan Fung,work.ivanfung@gmail.com,95%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Meihua Zhou,mhzhou@wnmc.edu.cn,82%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Nan Wan,wannan@wnmc.edu.cn,95%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Li Yang,,0%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Keke Di,,0%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Tingting Wang,,0%
https://arxiv.org/pdf/2301.02268.pdf,Restarts subject to approximate sharpness: A parameter-free and optimal scheme for first-order methods,Matthew J. Colbrook,m.colbrook@damtp.cam.ac.uk,82%
https://arxiv.org/pdf/2301.02268.pdf,Restarts subject to approximate sharpness: A parameter-free and optimal scheme for first-order methods,Ben Adcock,,0%
https://arxiv.org/pdf/2301.02268.pdf,Restarts subject to approximate sharpness: A parameter-free and optimal scheme for first-order methods,Maksym Neyra-nesterenko,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Hu Xu,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Saining Xie,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Po-yao Huang,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Licheng Yu,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Russell Howes,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Gargi Ghosh,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Luke Zettlemoyer,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Christoph Feichtenhofer,,0%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Amir Ghodrati,ghodrati@qti.qualcomm.com,78%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Shashanka Venkataramanan,shashanka.venkataramanan@inria.fr,95%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Yuki M. Asano,,0%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Fatih Porikli,,0%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Amirhossein Habibian,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Yu-lun Liu,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Chen Gao,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Andreas Meuleman,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Hung-yu Tseng,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Ayush Saraf,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Changil Kim,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Yung-yu Chuang,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Johannes Kopf,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Jia-bin Huang,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Benjamin Attal,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Jia-bin Huang,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Christian Richardt,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Michael Zollhoefer,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Johannes Kopf,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Matthew O'toole,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Changil Kim,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Jasmine Collins,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Anqi Liang,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Jitendra Malik,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Hao Zhang,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Frédéric Devernay,,0%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Han Hu,hanhu@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Qi Dai,qid@microsoft.com,85%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Chen Li,t-chenli1@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Jia Ning,t-jianing@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Zigang Geng,t-ziganggeng@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Zheng Zhang,zhez@microsoft.com,75%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Kun He,,0%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Ya Zhang,ya zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Yanfeng Wang,wangyanfeng@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Weidi Xie,weidi@sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Chaoyi Wu,,0%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Xiaoman Zhang,,0%
https://arxiv.org/pdf/2301.02217.pdf,EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding,Shuhan Tan,shuhan@cs.utexas.edu,85%
https://arxiv.org/pdf/2301.02217.pdf,EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding,Kristen Grauman,grauman@cs.utexas.edu,78%
https://arxiv.org/pdf/2301.02217.pdf,EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding,Tushar Nagarajan,tushar.nagarajan@cs.utexas.edu,95%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Elijah Cole,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Suzanne Stathatos,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Björn Lütjens,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Tarun Sharma,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Justin Kay,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Jason Parham,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Benjamin Kellenberger,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Sara Beery,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Vikram V. Ramaswamy,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Sing Yu Lin,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Dora Zhao,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Aaron B. Adcock,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Laurens Van Der Maaten,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Deepti Ghadiyaram,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Olga Russakovsky,,0%
https://arxiv.org/pdf/2301.02160.pdf,ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions,Dongwon Lee,dongwon@psu.edu,85%
https://arxiv.org/pdf/2301.02160.pdf,ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions,Aashish Anantha Ramakrishnan,,0%
https://arxiv.org/pdf/2301.02160.pdf,ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions,Sharon X. Huang,,0%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Yuxue Yang,yangyuxue2023@ia.ac.cn,95%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Lue Fan,fanlue2019@ia.ac.cn,95%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Feng Wang,feng.wff@gmail.com,85%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Zhaoxiang Zhang,zhaoxiang.zhang@ia.ac.cn,95%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Usman Muhammad,usman.muhammad@aalto.fi,95%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Jorma Laaksonen,,0%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Djamila Romaissa Beddiar,,0%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Mourad Oussalah,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Carsten T. Lüth,carsten.lueth@dkfz-heidelberg.de,85%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,David Zimmerer,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Gregor Koehler,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Paul F. Jaeger,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Fabian Isensee,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Jens Petersen,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Klaus H. Maier-hein,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Martin Pernuš,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Clinton Fookes,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Vitomir Štruc,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Simon Dobrišek,,0%
https://arxiv.org/pdf/2301.02615.pdf,"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack",Tzvi Lederer,tzvil@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.02615.pdf,"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack",Gallil Maimon,gallil.maimon@mail.huji.ac.il,95%
https://arxiv.org/pdf/2301.02615.pdf,"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack",Lior Rokach,liorrk@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.02092.pdf,DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax,Sadra Safadoust,ssafadoust20@ku.edu.tr,82%
https://arxiv.org/pdf/2301.02092.pdf,DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax,Fatma Güney,fguney@ku.edu.tr,82%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Leonard Bruns,leonardb@kth.se,85%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Patric Jensfelt,patric@kth.se,85%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Fereidoon Zangeneh,,0%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Amit Dekel,,0%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Alessandro Pieropan,,0%
https://arxiv.org/pdf/2301.02074.pdf,Test of Time: Instilling Video-Language Models with a Sense of Time,Piyush Bagad,,0%
https://arxiv.org/pdf/2301.02074.pdf,Test of Time: Instilling Video-Language Models with a Sense of Time,Makarand Tapaswi,,0%
https://arxiv.org/pdf/2301.02074.pdf,Test of Time: Instilling Video-Language Models with a Sense of Time,Cees G. M. Snoek,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,Shixing Cao,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,Nicholas Konz,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,James Duncan,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,Maciej A. Mazurowski,,0%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Ik-jae Lee,ikjae412@yuhs.ac,85%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Jun Won Kim,junwon@yuhs.ac,85%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Sangjoon Park,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Ivan Bilić,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Filip Marić,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Ivan Marković,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Ivan Petrović,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Xiang Li,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Jinshan Pan,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Jinhui Tang,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Jiangxin Dong,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Nina Shvetsova,shvetsov@uni-frankfurt.de,90%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Felix Petersen,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Anna Kukleva,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Hilde Kuehne,,0%
https://arxiv.org/pdf/2301.02008.pdf,Expressive Speech-driven Facial Animation with controllable emotions,Yutong Chen,chenyt19@tsinghua.org.cn,78%
https://arxiv.org/pdf/2301.02008.pdf,Expressive Speech-driven Facial Animation with controllable emotions,Junhong Zhao,junhong.jennifer@gmail.com,85%
https://arxiv.org/pdf/2301.02008.pdf,Expressive Speech-driven Facial Animation with controllable emotions,Wei-qiang Zhang,wqzhang@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Shuailei Ma,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Yuefeng Wang,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Jiaqi Fan,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Ying Wei,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Thomas H. Li,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Hongli Liu,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Fanbing Lv,,0%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Ming Yang,myang@njnu.edu.cn,82%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Lei Yu,yulei@njnu.edu.cn,95%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Shengqi Huang,huangshengqi@njnu.edu.cn,95%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Wanqi Yang,yangwq@njnu.edu.cn,78%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Lei Wang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Yu Zhang,yu@seu.edu.cn,85%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Zihua Wang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Xu Yang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Hanwang Zhang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Haiyang Xu,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Ming Yan,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Fei Huang,,0%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Yu Zhang,zhang yu@seu.edu.cn,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Hanwang Zhang,hanwangzhang@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Fei Huang,f.huang@alibaba-inc.com,82%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Songfang Huang,songfang.hsf@alibaba-inc.com,85%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Qinghao Ye,yeqinghao.yqh@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Xu Yang,xuyang palm@seu.edu.cn,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Zhangzikang Li,lizhangzikang@gmail.com,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Haiyang Xu,,0%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Chenliang Li,,0%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Ming Yan,,0%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Caixia Yuan,yuancx@bupt.edu.cn,78%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Fulong Ye,fulong ye@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Zhuoxin Han,hanzhuoxin@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Xiaojie Wang,xjwang@bupt.edu.cn,82%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Yuxing Long,longyuxing@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Binyuan Hui,,0%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Yanyang Li,,0%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Yongbin Li,,0%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,Brad Mcdanel,bmcdanel@fandm.edu,82%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,Surat Teerapittayanon,surat.tee@nanotec.or.th,85%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,Marcus Comiter,marcuscomiter@post.harvard.edu,95%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,H. T. Kung,kung@harvard.edu,78%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Li Liu,lliu@ee.cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Ang Li,,0%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Jiayi Han,,0%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Yongjian Zhao,,0%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Keyu Li,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Dongdong Chen,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Yuxing Dai,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Lichi Zhang,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Zhihong Zhang,,0%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Hasti Shabani,ha_shabani@sbu.ac.ir,82%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Bahareh Morovati,,0%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Reza Lashgari,,0%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Mojtaba Hajihasani,,0%
https://arxiv.org/pdf/2301.01928.pdf,Event Camera Data Pre-training,Liyuan Pan,liyuan.pan@bit.edu.cn,95%
https://arxiv.org/pdf/2301.01928.pdf,Event Camera Data Pre-training,Yan Yang,Yan.Yang@anu.edu.au,95%
https://arxiv.org/pdf/2301.01928.pdf,Event Camera Data Pre-training,Liu Liu,liuliu33@huawei.com,95%
https://arxiv.org/pdf/2301.01922.pdf,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,Andrew Beng Jin Teoh,bjteoh@yonsei.ac.kr,78%
https://arxiv.org/pdf/2301.01922.pdf,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,Jaewoo Park,julypraise@gmail.com,65%
https://arxiv.org/pdf/2301.01922.pdf,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,Hojin Park,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Ziwei Sun,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Zexi Hua,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Hengcao Li,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Haiyan Zhong,,0%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Michael Cahyadi,michael.cahyadi001@binus.ac.id,95%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,William Shan,william.sitanggang@binus.ac.id,85%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Jurike Moniaga,jurike@binus.edu,85%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Muhammad Rafi,muhammad.rafi007@binus.ac.id,95%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Henry Lucky,henry.lucky@binus.ac.id,95%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Yuqian Chen,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Fan Zhang,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Leo R. Zekelman,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Tengfei Xue,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Chaoyi Zhang,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Yang Song,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Nikos Makris,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Yogesh Rathi,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Weidong Cai,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Lauren J. O'donnell,,0%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Michael Johnston,mjohnstn@amazon.com,65%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Govind Thattai,thattg@amazon.com,76%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Da Yin,da.yin@cs.ucla.edu,95%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Feng Gao,fenggo@amazon.com,85%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Kai-wei Chang,kwchang@cs.ucla.edu,82%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Haoyang Zhang,haoyang.zhang@horizon.ai,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Jian Jia,jiajian2018@ia.ac.cn,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Fei He,hefei2018@ia.ac.cn,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Kaiqi Huang,kaiqi.huang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Yanhu Shan,yanhu.shan@horizon.ai,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Xin Zhao,xzhao@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Naiyu Gao,naiyu01.gao@horizon.ai,95%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Jian Liang,liangjian92@gmail.com,95%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Zhenan Sun,znsun@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Lingxiao He,helingxiao3@jd.com,95%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Boqiang Xu,boqiang.xu@cripac.ia.ac.cn,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Weining Lu,luwn@tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Yu Cheng,yu.cheng@microsoft.com,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Xiang Fang,xfang9508@gmail.com,82%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Xing Di,xing.di@protagolabs.com,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Pan Zhou,panzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Daizong Liu,dzliu@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Zhecheng Wang,zhecheng@stanford.edu,85%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Timothy Dai,timdai@stanford.edu,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Andrew Y. Ng,ang@cs.stanford.edu,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Ram Rajagopal,ramr@stanford.edu,85%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Hesu Yoon,hyoon28@stanford.edu,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Tianyuan Huang,tianyuah@stanford.edu,75%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Hao Sheng,haosheng@stanford.edu,95%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Jackelyn Hwang,jihwang@stanford.edu,82%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Abubakar Sani-mohammed,abubakar.sanimohammed@connect.polyu.hk,85%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Marco Heurich,marco.heurich@npv-bw.bayern.de,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Tsz Chung Wong,tszchung.wong@connect.polyu.hk,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Jinhong Wang,jinhong.wang@connect.polyu.hk,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Wei Yao,wei.hn.yao@polyu.edu.hk,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Puzuo Wang,puzuo.wang@connect.polyu.hk,95%
https://arxiv.org/pdf/2301.01814.pdf,Living Images: A Recursive Approach to Computing the Structural Beauty of Images or the Livingness of Space,Chris De Rijke,chris.de.rijke@hig.se,95%
https://arxiv.org/pdf/2301.01814.pdf,Living Images: A Recursive Approach to Computing the Structural Beauty of Images or the Livingness of Space,Bin Jiang,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Tianjiao Ding,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Shengbang Tong,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Kwan Ho Ryan Chan,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Xili Dai,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Yi Ma,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Benjamin D. Haeffele,,0%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Lingting Ge,lingting.ge@tusimple.ai,95%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Minghan Zhu,minghanz@umich.edu,85%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Panqu Wang,panqu.wang@tusimple.ai,95%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Huei Peng,hpeng@umich.edu,82%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Vignesh Ramanathan,vigneshr@meta.com,85%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Anmol Kalia,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Vladan Petrovic,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Yi Wen,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Baixue Zheng,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Baishan Guo,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Rui Wang,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Aaron Marquez,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Rama Kovvuri,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Abhishek Kadian,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Amir Mousavi,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Yiwen Song,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Abhimanyu Dubey,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Dhruv Mahajan,,0%
https://arxiv.org/pdf/2301.01791.pdf,Fully Automated Artery-Vein ratio and vascular tortuosity measurement in retinal fundus images,Rolando Estrada,restrada1@student.gsu.edu,82%
https://arxiv.org/pdf/2301.01791.pdf,Fully Automated Artery-Vein ratio and vascular tortuosity measurement in retinal fundus images,Aashis Khanal,akhanal1@student.gsu.edu,82%
https://arxiv.org/pdf/2301.01767.pdf,Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,Chao Feng,,0%
https://arxiv.org/pdf/2301.01767.pdf,Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,Ziyang Chen,,0%
https://arxiv.org/pdf/2301.01767.pdf,Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,Andrew Owens,,0%
https://arxiv.org/pdf/2301.01758.pdf,An Ensemble Mobile-Cloud Computing Method for Affordable and Accurate Glucometer Readout,Navidreza Asadi,navidreza.asadi@tum.de,95%
https://arxiv.org/pdf/2301.01758.pdf,An Ensemble Mobile-Cloud Computing Method for Affordable and Accurate Glucometer Readout,Maziar Goudarzi,goudarzi@sharif.edu,78%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Sagnik Majumder,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Hao Jiang,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Pierre Moulon,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Ethan Henderson,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Paul Calamia,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Vamsi Krishna Ithapu,,0%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Zhihan Lv,lvzhihan@gmail.com,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Jinman Kim,man.kim@sydney.edu.au,78%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Tao Huang,tao.huang1@jcu.edu.au,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Zhengmin Kong,zmkong@whu.edu.cn,82%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Yusheng Zhou,yushengzhou@whu.edu.cn,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Euijoon Ahn,euijoon.ahn@jcu.edu.au,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,David Dagan Feng,dagan.feng@sydney.edu.au,82%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Jianan Liu,jianan.liu@vitalent.se,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Hao Li,hao.li@med.uni,95%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Ashkan Ebadi,ashkan.ebadi@nrc-cnrc.gc.ca,95%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Jessy Song,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Adrian Florea,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Pengcheng Xi,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Stéphane Tremblay,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Alexander Wong,,0%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Yao Zhao,yzhao@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Lang Nie,nielang@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Kang Liao,kang liao@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Chunyu Lin,cylin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Zishuo Zheng,,0%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Yuliang Liu,ylliu@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Chunhua Shen,chhshen@gmail.com,82%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Lianwen Jin,eelwjin@scut.edu.cn,78%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Mingxin Huang,huangmingxin21@foxmail.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Dezhi Peng,pengdzscut@foxmail.com,78%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Xinyu Wang,xinyu.wang02@adelaide.edu.au,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Jiaxin Zhang,zhangjiaxin.zjx1995@bytedance.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Xiang Bai,xbai@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Can Huang,can.huang@bytedance.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Jingqun Tang,tangjingqun@bytedance.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Errui Ding,dingerrui@baidu.com,95%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Xiang Bai,xbai@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Xiao Tan,tanxchong@gmail.com,78%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Zhe Liu,zheliu1994@hust.edu.cn,95%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Xiaoqing Ye,,0%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Cécile Favre,cecile.favre@univ-lyon2.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Camille Noûs,camille.nous@cogitamus.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Nouria Harbi,nouria.harbi@univ-lyon2.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Jérôme Darmont,jerome.darmont@univ-lyon2.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Abderrazek Azri,a.azri@univ-lyon2.fr,82%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Marcel Koch,marcel.koch@eah-jena.de,95%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Sören Laue,laue@cs.uni-kl.de,78%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Joachim Giesen,joachim.giesen@uni-jena.de,95%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Matthias Mitterreiter,matthias.mitterreiter@uni-jena.de,95%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Yutaro Shigeto,shigeto@stair.center,78%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Masashi Shimbo,shimbo@stair.center,78%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Yuya Yoshikawa,yoshikawa@stair.center,82%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Akikazu Takeuchi,takeuchi@stair.center,78%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Binod Bhattarai,b.bhattarai@ucl.ac.uk,82%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Tae-kyun Kim,tk.kim@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Danail Stoyanov,danail.stoyanov@ucl.ac.uk,95%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Razvan Caramalau,r.caramalau18@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.01520.pdf,Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy,Diego Marcos,diego.marcos@inria.fr,95%
https://arxiv.org/pdf/2301.01520.pdf,Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy,Dino Ienco,dino.ienco@inrae.fr,95%
https://arxiv.org/pdf/2301.01520.pdf,Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy,Cassio F. Dantas,,0%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Wojciech Niewolski,wojciech.niewolski@orange.com,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Jaroslaw Legierski,jaroslaw.legierski@orange.com,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Maria Ganzha,maria.ganzha@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Przemyslaw Ratuszek,przemyslaw.ratuszek@orange.com,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Piotr Sowinski,piotr.sowinski@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Marcin Paprzycki,marcin.paprzycki@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Kajetan Rachwal,kajetan.rachwal@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Zbigniew Kopertowski,zbigniew.kopertowski@orange.com,95%
https://arxiv.org/pdf/2301.01495.pdf,Beckman Defense,A. V. Subramanyam,subramanyam@iiitd.ac.in,78%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Ralf Dörner,ralf.doerner@hs-rm.de,85%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Christian Geiger,geiger@hs-duesseldorf.de,78%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Rene Ebertowski,rene.ebertowski@hs-duesseldorf.de,95%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Alexander Pech,alexander.pech@hs-duesseldorf.de,95%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Philipp Ladwig,philipp.ladwig@hs-duesseldorf.de,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Jian Cao,caojian_heu@163.com,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Ye Li,liye@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Wei Huo,weihuo@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Zhuoyan Liu,liuzhuoyan@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Yunfeng Li,liyunfeng@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Bo Wang,,0%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Yueming Li,,0%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Qi Dou,qdou@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Yuan Zhong,yzhong22@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Wenlong Deng,dwenlong@ece.ubc.ca,85%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Xiaoxiao Li,xiaoxiao.li@ece.ubc.ca,95%
https://arxiv.org/pdf/2301.11813.pdf,Biomedical Image Reconstruction: A Survey,Samuel Cahyawijaya,scahyawijaya@connect.ust.hk,82%
https://arxiv.org/pdf/2301.01456.pdf,Audio-Visual Efficient Conformer for Robust Speech Recognition,Maxime Burchi,maxime.burchi@uni-wuerzburg.de,95%
https://arxiv.org/pdf/2301.01456.pdf,Audio-Visual Efficient Conformer for Robust Speech Recognition,Radu Timofte,radu.timofte@uni-wuerzburg.de,95%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Viktor Prasanna,prasanna@usc.edu,78%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Bingyi Zhang,bingyizh@usc.edu,85%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Rajgopal Kannan,rajgopal.kannan.civ@army.mil,95%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Carl Busart,carl.e.busart.civ@army.mil,95%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Marshall Burke,mburke@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Stefano Ermon,ermon@cs.stanford.edu,78%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Eun Jee Sung,ejsung@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Chenlin Meng,chenlin@cs.stanford.edu,85%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Sihang Chen,schen22@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Matthew Kolodner,mkolod@stanford.edu,90%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,David Lobell,dlobell@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Enci Liu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Zhilin Zheng,zhilin.zheng95@gmail.com,95%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Chengwei Shao,cwshao@sina.com,82%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Yun Bian,bianyun2012@foxmail.com,95%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Xu Fang,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Jiawen Yao,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Mengmeng Zhu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Le Lu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Lingyun Huang,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Jing Xiao,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Yu Shi,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Hong Lu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Jianping Lu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Ling Zhang,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Weiwei Wan,wan@sys.es.osaka-u.ac.jp,82%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Hao Chen,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Masaki Matsushita,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Takeyuki Kotaka,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Kensuke Harada,,0%
https://arxiv.org/pdf/2301.01431.pdf,Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers,Haojie Yu,yuhaojie02@meituan.com,95%
https://arxiv.org/pdf/2301.01431.pdf,Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers,Xiaoming Xu,xuxiaoming04@meituan.com,95%
https://arxiv.org/pdf/2301.01431.pdf,Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers,Kang Zhao,zhaokang@meituan.com,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Dennis Park,dennis.park@tri.global,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Jiaman Li,jiamanli@stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Huazhe Xu,huazhexu@stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Jiajun Wu,jiajunwu@cs.stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Sifan Ye,sifan.ye@cs.stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Yixing Wang,,0%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,C. Karen Liu,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Yuren Cong,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Martin Renqiang Min,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Li Erran Li,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Bodo Rosenhahn,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Michael Ying Yang,,0%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Tianxu Qin,tianxuq@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Kaihui Zheng,kaihuiz@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Yuqing Ren,yuqingr@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Zixin Shen,zixins@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.01380.pdf,Ego-Only: Egocentric Action Detection without Exocentric Transferring,Huiyu Wang,,0%
https://arxiv.org/pdf/2301.01380.pdf,Ego-Only: Egocentric Action Detection without Exocentric Transferring,Mitesh Kumar Singh,,0%
https://arxiv.org/pdf/2301.01380.pdf,Ego-Only: Egocentric Action Detection without Exocentric Transferring,Lorenzo Torresani,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Pew-thian Yap,ptyap@med.unc.edu,82%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Xiaoyang Chen,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Jinjian Wu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Wenjiao Lyu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Yicheng Zou,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Kim-han Thung,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Siyuan Liu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Ye Wu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Sahar Ahmad,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Daniel H. Pak,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Xiao Chen,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Eric Z. Chen,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Yikang Liu,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Terrence Chen,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Shanhui Sun,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Firas Laakom,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Jenni Raitoharju,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Alexandros Iosifidis,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Moncef Gabbouj,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Shreyansh Daftry,Shreyansh.Daftry@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Brian Coltin,brian.coltin@nasa.gov,95%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Zhanlin Chen,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Yang Cheng,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Scott Tepsuporn,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Ussama Naam,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Lanssie Mingyue Ma,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Shehryar Khattak,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Matthew Deans,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Larry Matthies,,0%
https://arxiv.org/pdf/2301.01343.pdf,Explainability and Robustness of Deep Visual Classification Models,Jindong Gu,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Sucheng Ren,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Fangyun Wei,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Zheng Zhang,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Han Hu,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Junjie Yan,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Yingfei Liu,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Jianjian Sun,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Fan Jia,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Shuailin Li,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Tiancai Wang,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Xiangyu Zhang,,0%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Junayed Mahmud,jmahmud@gmu.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,George Purnell,gwpurnell@email.wm.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Ali Yachnes,ayachnes@email.wm.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Zach H'doubler,pzhdoubler@email.wm.edu,65%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Carlos Bernal-cárdenas,carlosbe@microsoft.com,85%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Michele Tufano,michele.tufano@microsoft.com,95%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Kevin Moran,kpmoran@gmu.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Denys Poshyvanyk,denys@cs.wm.edu,85%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Boqi Chen,,0%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Kevin Thandiackal,,0%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Pushpak Pati,,0%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Orcun Goksel,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Qingyi Pan,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Ning Guo,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Letu Qingge,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Jingyi Zhang,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Pei Yang,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Charlie Hewitt,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Tadas Baltrušaitis,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Erroll Wood,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Lohit Petikam,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Louis Florentin,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Hanz Cuevas Velasquez,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Yue Han,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Jiangning Zhang,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Yabiao Wang,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Yong Liu,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Lu Qi,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Xiangtai Li,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.01149.pdf,I2F: A Unified Image-to-Feature Approach for Domain Adaptive Semantic Segmentation,Xiangru Lin,xrlin2@cs.hku.hk,82%
https://arxiv.org/pdf/2301.01149.pdf,I2F: A Unified Image-to-Feature Approach for Domain Adaptive Semantic Segmentation,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.01149.pdf,I2F: A Unified Image-to-Feature Approach for Domain Adaptive Semantic Segmentation,Haoyu Ma,mahaoyu@connect.hku.hk,95%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Jiangning Zhang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Xiangtai Li,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Jian Li,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Liang Liu,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Zhucun Xue,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Boshen Zhang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Zhengkai Jiang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Tianxin Huang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Yabiao Wang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Shuhao Shi,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Kai Qiao,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Jian Chen,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Shuai Yang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Jie Yang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Baojie Song,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Linyuan Wang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Bin Yan,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Zhisheng Zhong,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Jiequan Cui,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Yibo Yang,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Xiaoyang Wu,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Xiaojuan Qi,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Xiangyu Zhang,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Jiaya Jia,,0%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Jianlong Zhou,Jianlong.Zhou@uts.edu.au,95%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Boyuan Zheng,Boyuan.Zheng-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Chunjie Liu,,0%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Yiqiao Li,,0%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Fang Chen,,0%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Clément Jambon,clement.jambon@polytechnique.edu,95%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,George Drettakis,george.drettakis@inria.fr,95%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Gilles Rainer,gilles.rainer.enst@gmail.com,95%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Thomas Leimkühler,thomas.leimkuehler@mpi-inf.mpg.de,85%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Georgios Kopanas,kopanas@inria.fr,78%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Changjie Fan,fanchangjie@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Zhipeng Hu,zphu@corp.netease.com,82%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Xin Yu,xin.yu@uts.edu.au,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Tangjie Lv,hzlvtangjie@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Yifeng Ma,mayf18@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Suzhen Wang,wangsuzhen@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Yu Ding,dingyu01@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Zhidong Deng,,0%
https://arxiv.org/pdf/2301.01079.pdf,Fine-Grained Hard Negative Mining: Generalizing Mitosis Detection with a Fifth of the MIDOG 2022 Dataset,Maxime W. Lafarge,,0%
https://arxiv.org/pdf/2301.01079.pdf,Fine-Grained Hard Negative Mining: Generalizing Mitosis Detection with a Fifth of the MIDOG 2022 Dataset,Viktor H. Koelzer,,0%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Liqun Lin,lin liqun@fzu.edu.cn,95%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Chengdong Lan,lancd@fzu.edu.cn,78%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Tiesong Zhao,t.zhao@fzu.edu.cn,82%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Weiling Chen,weiling.chen@fzu.edu.cn,95%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Yang Zheng,,0%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Yawei Luo,yaweiluo@zju.edu.cn,95%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Jun Xiao,junx@cs.zju.edu.cn,85%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Fei Gao,feig@zjut.edu.cn,85%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Feifei Shao,,0%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Yi Yang,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Janne Mustaniemi,janne.mustaniemi@oulu.fi,95%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Juho Kannala,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Esa Rahtu,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Li Liu,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Janne Heikkilä,,0%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Tabea-clara Bucher,tabea.bucher@dkfz-heidelberg.de,95%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Titus J. Brinker,titus.brinker@nct-heidelberg.de,95%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Hendrik A. Mehrtens,,0%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Alexander Kurz,,0%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Ligang Liu,lgliu@ustc.edu.cn,82%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Jue Wang,maxjwang@tencent.com,78%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Boyu Zhang,,0%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Hongliang Yuan,,0%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Mingyan Zhu,,0%
https://arxiv.org/pdf/2301.01033.pdf,Dissecting Continual Learning a Structural and Data Analysis,Francesco Pelosin,,0%
https://arxiv.org/pdf/2302.05283.pdf,Deep Learning from Parametrically Generated Virtual Buildings for Real-World Object Recognition,Mohammad Alawadhi,,0%
https://arxiv.org/pdf/2302.05283.pdf,Deep Learning from Parametrically Generated Virtual Buildings for Real-World Object Recognition,Wei Yan,,0%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Sinan Kalkan,skalkan@metu.edu.tr,82%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Emre Akbas,eakbas@metu.edu.tr,82%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Kemal Oksuz,kemal.oksuz@metu.edu.tr,95%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Fehmi Kahraman,fehmi.kahraman 01@metu.edu.tr,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Hui Wan,hwan@us.ibm.com,82%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Marina Danilevsky,mdanile@us.ibm.com,90%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Tejas Indulal Dhamecha,tdhamecha@microsoft.com,82%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Riyaz Bhat,riyaz.bhat@ibm.com,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Rudra Murthy,rmurthyv@in.ibm.com,82%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Siva Sankalp Patel,siva.sankalp.patel@ibm.com,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Danish Contractor,danish.contractor@ibm.com,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Chulaka Gunasekara,chulaka.gunasekara@ibm.com,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Penghao Wu,wupenghao@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Li Chen,lichen@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Junchi Yan,yanjunchi@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Xiaosong Jia,jiaxiaosong@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Hongyang Li,lihongyang@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Yu Qiao,qiaoyu@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Yu-gang Jiang,ygj@fudan.edu.cn,90%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Meng Wang,eric.wangmeng@gmail.com,95%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Hanze Dong,hzdong15@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Leonid Sigal,lsigal@cs.ubc.ca,82%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Xiangyang Xue,xyxue@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Xiaomei Wang,,0%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Yawen Huang,yawenhuang@tencent.com,95%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Yuexiang Li,vicyxli@tencent.com,78%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Nanjun He,nanjunhe@tencent.com,95%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Yefeng Zheng,yefengzheng@tencent.com,95%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Kai Ma,kylekma@tencent.com,82%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Bernard Ghanem,bernard.ghanem@kaust.edu.sa,95%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Fahad Albalawi,falbalawi@sdaia.gov.sa,82%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Hasan Abed Al Kader Hammoud,,0%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Shuming Liu,,0%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Mohammed Alkhrashi,,0%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Sirui Zhao,sirui@mail.ustc.edu.cn,85%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Xinglong Mao,maoxl@mail.ustc.edu.cn,78%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Enhong Chen,cheneh@ustc.edu.cn,78%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Hao Wang,wanghao3@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Yiming Zhang,ymzhang21@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Tong Xu,tongxu@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Huaying Tang,,0%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Shifeng Liu,,0%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Hao Fang,fanghao2021@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Sergio Escalera,sergio@maia.ub.es,85%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Jun Wan,jun.wan@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Stan Z. Li,stan.zq.li@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Ajian Liu,ajian.liu@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Chenxu Zhao,zhaochenxu@sailyond.com,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Xu Zhang,xuzhang0908@mail.bnu.edu.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Zhen Lei,zhen.lei@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Tejas Karkera,tejkar10@gmail.com,75%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Muhammad Saqib,saqib.uet1@gmail.com,78%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Soumi Chattopadhyay,soumi61@gmail.com,85%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Chandranath Adak,adak32@gmail.com,78%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Xu Yan,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Chaoda Zheng,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Ying Xue,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Zhen Li,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Shuguang Cui,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Dengxin Dai,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Zhijing Yang,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Junyang Chen,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Yukai Shi,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Hao Li,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Tianshui Chen,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Liang Lin,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Xiangtai Li,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Shilin Xu,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Yibo Yang,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Haobo Yuan,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Guangliang Cheng,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Yunhai Tong,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Zhouchen Lin,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Dacheng Tao,,0%
https://arxiv.org/pdf/2301.00950.pdf,Class-Continuous Conditional Generative Neural Radiance Field,Minhyeok Lee,mlee@cau.ac.kr,82%
https://arxiv.org/pdf/2301.00950.pdf,Class-Continuous Conditional Generative Neural Radiance Field,Jiwook Kim,,0%
https://arxiv.org/pdf/2301.10293.pdf,A Fast Feature Point Matching Algorithm Based on IMU Sensor,Lu Cao,andrewcao95@pku.edu.cn,78%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Yang Li,yangli@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Yicong Li,,0%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Yang Tan,,0%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Jingyun Yang,,0%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Xiao-ping Zhang,,0%
https://arxiv.org/pdf/2301.00896.pdf,Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos,Wei Xingxing,xxwei@buaa.edu.cn,85%
https://arxiv.org/pdf/2301.00896.pdf,Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos,Wang Songping,,0%
https://arxiv.org/pdf/2301.00896.pdf,Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos,Yan Huanqian,,0%
https://arxiv.org/pdf/2301.00897.pdf,Game of Intelligent Life,Chaytan Inman,chaytan@uw.edu,85%
https://arxiv.org/pdf/2301.00897.pdf,Game of Intelligent Life,Shaun Lee,shauncl8@uw.edu,85%
https://arxiv.org/pdf/2301.00897.pdf,Game of Intelligent Life,Marlene Grieskamp,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Sanghyun Woo,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Shoubhik Debnath,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Ronghang Hu,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Xinlei Chen,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Zhuang Liu,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,In So Kweon,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Saining Xie,,0%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Chen Change Loy,ccloy@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Xiangtai Li,xiangtai.li@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Jianzong Wu,jzwu@stu.pku.edu.cn,82%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Henghui Ding,henghui.ding@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Xia Li,,0%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Guangliang Cheng,,0%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Yunhai Tong,,0%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Harpreet Sawhney,harpreet.sawhney@microsoft.com,95%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Rama Chellappa,rchella4@jhu.edu,65%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Benjamin Lundell,benjamin.lundell@microsoft.com,95%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Anshul Shah,ashah95@jhu.edu,82%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Zongwei Zhou,zzhou82@jh.edu,82%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yucheng Tang,yuchengt@nvidia.com,85%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Jie Liu,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yixiao Zhang,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Jie-neng Chen,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Junfei Xiao,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yongyi Lu,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Bennett A. Landman,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yixuan Yuan,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Alan Yuille,,0%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Chaoqi Chen,cqchen1994@gmail.com,82%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Sibei Yang,yangsb@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Hong-yu Zhou,whuzhouhongyu@gmail.com,95%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Chixiang Lu,luchixiang@gmail.com,95%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Seol Ah Park,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Tamara Sipka,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Zuzana Kriva,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,George Lutfalla,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Mai Nguyen-chi,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Karol Mikula,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Takayuki Nishio,nishio@ict.e.titech.ac.jp,78%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Shoki Ohta,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Riichi Kudo,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Kahoko Takahashi,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Hisashi Nagata,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Luca Casini,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Valentina Orrù,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Andrea Montanucci,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Nicolò Marchetti,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Marco Roccetti,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Sumit Shekhar,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Max Reimann,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Moritz Hilscher,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Amir Semmo,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Jürgen Döllner,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Matthias Trapp,,0%
https://arxiv.org/pdf/2301.00746.pdf,NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,Santhosh Kumar Ramakrishnan,,0%
https://arxiv.org/pdf/2301.00746.pdf,NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,Ziad Al-halah,,0%
https://arxiv.org/pdf/2301.00746.pdf,NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Shuangmei Wang,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Rui Ma,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Tieru Wu,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Yang Cao,,0%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Hao Chen,hao.chen@inria.fr,95%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Benoit Lagadec,benoit.lagadec@esifrance.net,95%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Francois Bremond,cois.bremond@inria.fr,78%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Yaohui Wang,yaohui.wang@inria.fr,95%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Antitza Dantcheva,antitza.dantcheva@inria.fr,95%
https://arxiv.org/pdf/2301.00714.pdf,Learning Road Scene-level Representations via Semantic Region Prediction,Alan Yuille,ayuille1@jhu.edu,82%
https://arxiv.org/pdf/2301.00714.pdf,Learning Road Scene-level Representations via Semantic Region Prediction,Zihao Xiao,zxiao10@jhu.edu,82%
https://arxiv.org/pdf/2301.00714.pdf,Learning Road Scene-level Representations via Semantic Region Prediction,Yi-ting Chen,ychen@cs.nycu.edu.tw,82%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Han Zhang,han@google.com,85%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Dilip Krishnan,dilipkay@google.com,85%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Huiwen Chang,huiwenchang@google.com,95%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Jarred Barber,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Aj Maschinot,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Jose Lezama,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Lu Jiang,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Kevin Murphy,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,William T. Freeman,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Michael Rubinstein,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Yuanzhen Li,,0%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Dennis Eschweiler,dennis.eschweiler@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Ina Laube,ina.laube@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Johannes Stegmaier,johannes.stegmaier@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Rüveyda Yilmaz,rueveyda.yilmaz@lfb.rwth-aachen.de,82%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Rijo Roy,rijo.roy@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Matisse Baumann,matisse.baumann@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Abin Jose,abin.jose@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Daniel Brückner,,0%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Jie Sun,sunjie1979@qut.edu.cn,95%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Lijian Zhou,zhoulijian@qut.edu.cn,95%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Kun Zhao,,0%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Qian Gao,,0%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Siyuan Hao,,0%
https://arxiv.org/pdf/2301.00620.pdf,Dynamically Modular and Sparse General Continual Learning,Elahe Arani,e.arani@tue.nl,82%
https://arxiv.org/pdf/2301.00620.pdf,Dynamically Modular and Sparse General Continual Learning,Arnav Varma,arnav.varma@navinfo.eu,95%
https://arxiv.org/pdf/2301.00620.pdf,Dynamically Modular and Sparse General Continual Learning,Bahram Zonooz,bahram.zonooz@gmail.com,95%
https://arxiv.org/pdf/2301.00618.pdf,An Event-based Algorithm for Simultaneous 6-DOF Camera Pose Tracking and Mapping,Masoud Dayani Najafabadi,,0%
https://arxiv.org/pdf/2301.00618.pdf,An Event-based Algorithm for Simultaneous 6-DOF Camera Pose Tracking and Mapping,Mohammad Reza Ahmadzadeh,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Kristian Muri Knausgård,kristianmk@ieee.org,85%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Ørjan Langøy Olsen,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Tonje Knutsen Sørdalen,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Morten Goodwin,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Ketil Malde,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Kim Tallaksen Halvorsen,,0%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Chiyu Zhang,1alienzhang19961005@gmail.com,78%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Jun Yang,yjun@sicnu.edu.cn,85%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Zaiyan Dai,3daizaiyan@stu.sicnu.edu.cn,95%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Peng Cao,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Fan Zhang,cefzhang@ust.hk,78%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Arianna Salazar Miranda,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Fábio Duarte,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Lawrence Vale,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Gary Hack,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Min Chen,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Yu Liu,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Michael Batty,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Carlo Ratti,,0%
https://arxiv.org/pdf/2301.00555.pdf,Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning,Jisu Shin,,0%
https://arxiv.org/pdf/2301.00555.pdf,Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning,Seunghyun Shin,,0%
https://arxiv.org/pdf/2301.00555.pdf,Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning,Hae-gon Jeon,,0%
https://arxiv.org/pdf/2301.00545.pdf,Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels,Xinwei Sun,sunxinwei@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00545.pdf,Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00545.pdf,Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels,Yikai Wang,yikaiwang19@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Ruimao Zhang,ruimao.zhang@ieee.org,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Zhanglin Peng,zhanglin.peng@connect.hku.hk,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Ziyi Tang,tangziyi@cuhk.edu.cn,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Liang Lin,linliang@ieee.org,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Jinrui Chen,,0%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Sebin Lee,seb.lee@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Woobin Im,iwbn@kaist.ac.kr,60%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Sung-eui Yoon,sungeui@kaist.ac.kr,85%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Jumin Lee,jmlee@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Asma Ahmed Hashmi,asmah17@gmail.com,85%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Aigerim Zhumabayeva,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Nikita Kotelevskii,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Artem Agafonov,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Mohammad Yaqub,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Maxim Panov,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Martin Takáč,,0%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Zichuan Xu,z.xu@dlut.edu.cn,82%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Wenzheng Xu,wenzheng.xu@scu.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Daizong Liu,dzliu@stu.pku.edu.cn,82%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Song Yang,S.Yang@bit.edu.cn,82%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Yao Wan,wanyao@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Jiahao Zhu,jiahaozhu@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Yu Cheng,yu.cheng@microsoft.com,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Xing Di,xing.di@protagolabs.com,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Zeyu Xiong,zeyuxiong@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Pan Zhou,panzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Lichao Sun,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Da Ma,dma@wakehealth.edu,82%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Marinko V. Sarunic,m.sarunic@ucl.ac.uk,82%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Timothy T. Yu,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Jayden Cole,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Myeong Jin Ju,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Mirza F. Beg,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Benjamin Wilson,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,William Qi,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Tanmay Agarwal,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,John Lambert,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Jagjeet Singh,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Siddhesh Khandelwal,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Bowen Pan,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Ratnesh Kumar,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Andrew Hartnett,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Jhony Kaesemodel Pontes,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Deva Ramanan,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Peter Carr,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,James Hays,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Qi Dou,qidou@cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Yonghao Long,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Wang Wei,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Tao Huang,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Yuehao Wang,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,James Batten,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,Matthew Sinclair,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,Ben Glocker,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,Michiel Schaap,,0%
https://arxiv.org/pdf/2301.00436.pdf,Hierarchical Explanations for Video Action Recognition,Sadaf Gulshad,s.gulshad@uva.nl,82%
https://arxiv.org/pdf/2301.00436.pdf,Hierarchical Explanations for Video Action Recognition,Teng Long,t.long@uva.nl,82%
https://arxiv.org/pdf/2301.00436.pdf,Hierarchical Explanations for Video Action Recognition,Nanne Van Noord,n.j.e.vannoord@uva.nl,82%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Mingzhe Chen,mingzhe.chen@miami.edu,95%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Dusit Niyato,dniyato@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Wenjing Zhang,zhangwenjing@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Yining Wang,,0%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Tao Luo,,0%
https://arxiv.org/pdf/2301.00424.pdf,GoogLe2Net: Going Transverse with Convolutions,Yuanpeng He,,0%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Zheng Zhu,zhengzhu@ieee.org,95%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Wenbo Xu,wenbo.xu@phigent.ai,95%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Guan Huang,guan.huang@phigent.ai,95%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Boyu Zhang,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Shizhan Gong,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Cheng Chen,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Yuqi Gong,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Nga Yan Chan,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Wenao Ma,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Calvin Hoi-kwan Mak,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Jill Abrigo,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Qi Dou,,0%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Feihu Xu,feihuxu@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Rui Ding,rding@tju.edu.cn,82%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Qifeng Gao,gaoqifeng 98@tju.edu.cn,95%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Yuping Duan,doveduan@gmail.com,78%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Juntian Ye,,0%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Wenhe Jia,jiawh@bupt.edu.cn,78%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Lu Yang,,0%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Shan Li,,0%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Qing Song,,0%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Jun Wen,wen@hms.harvard.edu,78%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Siheng Chen,sihengc@sjut.edu.cn,85%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Linchao Zhu,zhulinchao@zju.edu.cn,95%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Zenan Huang,,0%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Nenggan Zheng,,0%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Haibin Ling,hling@cs.stonybrook.edu,82%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Heng Fan,heng.fan@unt.edu,95%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Libo Zhang,libo@iscas.ac.cn,85%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Wenzhang Zhou,zhouwenzhang19@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Tiejian Luo,tjluo@ucas.ac.cn,82%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Mukesh Prasad,Mukesh.Prasad@uts.edu.au,95%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Kunal Chaturvedi,,0%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Ali Braytee,,0%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Jun Li,,0%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yanbo Fan,fanyanbo0124@gmail.com,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Fei Yin,yinf20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yujiu Yang,yang.yujiu@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Baoyuan Wu,wubaoyuan@cuhk.edu.cn,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yong Zhang,zhangyong201303@gmail.com,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Jingyi Zhang,jingyi.zhang1995@gmail.com,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yan Feng,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Zhenong Jin,jinzn@umn.edu,78%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Leikun Yin,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Rahul Ghosh,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Chenxi Lin,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,David Hale,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Christoph Weigl,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,James Obarowski,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Junxiong Zhou,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Jessica Till,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Xiaowei Jia,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Troy Mao,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Vipin Kumar,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Huazhu Fu,hzfu@ieee.org,82%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Xuedong Yuan,yxdongdong@163.com,60%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Ke Zou,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Yidi Chen,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Ling Huang,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Xiaojing Shen,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Meng Wang,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Rick Siow Mong Goh,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Yong Liu,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Eva L. Dyer,evadyer@gatech.edu,95%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Lakshmi Sathidevi,lsathidevi3@gatech.edu,82%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Erik C. Johnson,erik.c.johnson@jhuapl.edu,95%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Jorge Quesada,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Ran Liu,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Nauman Ahad,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Joy M. Jackson,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Mehdi Azabou,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Jingyun Xiao,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Christopher Liding,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Matthew Jin,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Carolina Urzay,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,William Gray-roncal,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Fengbei Liu,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Yuanhong Chen,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Chong Wang,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Yu Tain,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Gustavo Carneiro,,0%
https://arxiv.org/pdf/2301.00330.pdf,Efficient On-device Training via Gradient Filtering,Radu Marculescu,radum@utexas.edu,85%
https://arxiv.org/pdf/2301.00330.pdf,Efficient On-device Training via Gradient Filtering,Yuedong Yang,albertyoung@utexas.edu,75%
https://arxiv.org/pdf/2301.00330.pdf,Efficient On-device Training via Gradient Filtering,Guihong Li,,0%
https://arxiv.org/pdf/2301.00326.pdf,Yuille-Poggio's Flow and Global Minimizer of Polynomials through Convexification by Heat Evolution,Qiao Wang,qiaowang@seu.edu.cn,95%
https://arxiv.org/pdf/2301.00314.pdf,Causal Deep Learning,M. Alex O. Vasilescu,,0%
https://arxiv.org/pdf/2303.00138.pdf,Texture-Based Input Feature Selection for Action Recognition,Yalong Jiang,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Yichen Sheng,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Jianming Zhang,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Julien Philip,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Yannick Hold-geoffroy,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Xin Sun,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,He Zhang,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Lu Ling,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Bedrich Benes,,0%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Zhaolin Chen,zhaolin.chen@monash.edu,95%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Mevan Ekanayake,,0%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Kamlesh Pawar,,0%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Gary Egan,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Kai Hendrik Wöhnert,Sven-Jannik.Woehnert@haw-hamburg.de,85%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Sven-jannik Wöhnert,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Eldar Almamedov,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Carsten Frank,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Volker Skwarek,,0%
https://arxiv.org/pdf/2303.00086.pdf,Applying Plain Transformers to Real-World Point Clouds,Lanxiao Li,lanxiao.li@kit.edu,95%
https://arxiv.org/pdf/2303.00086.pdf,Applying Plain Transformers to Real-World Point Clouds,Michael Heizmann,michael.heizmann@kit.edu,95%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Decai Chen,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Haofei Lu,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Ingo Feldmann,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Oliver Schreer,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Peter Eisert,,0%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Hailin Jin,hljin@adobe.com,82%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Shaogang Gong,s.gong@qmul.ac.uk,82%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Yang Liu,yangliu@pku.edu.cn,95%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Jiabo Huang,jiabo.huang@qmul.ac.uk,95%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Dezhao Luo,dezhao.luo@qmul.ac.uk,95%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Lior Yariv,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Peter Hedman,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Christian Reiser,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Dor Verbin,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Pratul P. Srinivasan,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Richard Szeliski,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Jonathan T. Barron,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Ben Mildenhall,,0%
https://arxiv.org/pdf/2302.14835.pdf,Novel Machine Learning Approach for Predicting Poverty using Temperature and Remote Sensing Data in Ethiopia,Om Shah,,0%
https://arxiv.org/pdf/2302.14835.pdf,Novel Machine Learning Approach for Predicting Poverty using Temperature and Remote Sensing Data in Ethiopia,Krti Tallam,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Meshia Cédric Oveneke,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Rucha Vaishampayan,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Deogratias Lukamba Nsadisa,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Jenny Ambukiyenyi Onya,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,Saurabh Saxena,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,Abhishek Kar,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,Mohammad Norouzi,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,David J. Fleet,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Maryam Viqar,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Violeta Madjarova,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Vipul Baghel,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Elena Stoykova,,0%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Sajid Javed,sajid.javed@ku.ac.ae,95%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Majid Khonji,majid.khonji@ku.ac.ae,95%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Mohamed Nagy,mohamed.nagy@ieee.org,95%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Jorge Dias,jorge.dias@ku.ac.ae,95%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Kit Mills Bransby,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Vincenzo Tufaro,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Murat Cap,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Greg Slabaugh,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Christos Bourantas,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Qianni Zhang,,0%
https://arxiv.org/pdf/2302.14794.pdf,Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,Marcel Worring,m.worring@uva.nl,82%
https://arxiv.org/pdf/2302.14794.pdf,Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,Ivona Najdenkoska,,0%
https://arxiv.org/pdf/2302.14794.pdf,Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,Xiantong Zhen,,0%
https://arxiv.org/pdf/2303.12948.pdf,FTSO: Effective NAS via First Topology Second Operator,Lei Chen,leichen@cse.ust.hk,95%
https://arxiv.org/pdf/2303.12948.pdf,FTSO: Effective NAS via First Topology Second Operator,Likang Wang,lwangcg@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14777.pdf,VQA with Cascade of Self- and Co-Attention Blocks,Ashish Anand,anand.ashish@iitg.ac.in,95%
https://arxiv.org/pdf/2302.14777.pdf,VQA with Cascade of Self- and Co-Attention Blocks,Prithwijit Guha,pguha@iitg.ac.in,82%
https://arxiv.org/pdf/2302.14777.pdf,VQA with Cascade of Self- and Co-Attention Blocks,Aakansha Mishra,,0%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Jilin Mei,meijilin@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Yu Hu,huyu@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Jianchao Tan,jianchaotan@kuaishou.com,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Zihao Sun,sunzihao18z@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Chengru Song,songchengru@kuaishou.com,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Shun Lu,lushun19s@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Longxing Yang,yanglongxing20b@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Wei Huang,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Zhiliang Peng,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Li Dong,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Furu Wei,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Jianbin Jiao,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Qixiang Ye,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Voncarlos Marcelo Araújo,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Sébastien Gambs,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Clément Chion,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Robert Michaud,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Léo Schneider,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Hadrien Lautraite,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Laurence Lamant,sylvain.cussat-blanc@irit.fr,92%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Salvatore Valitutti,salvatore.valitutti@inserm.fr,95%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Kévin Cortacero,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Brienne Mckenzie,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Sabina Müller,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Roxana Khazen,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Fanny Lafouresse,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Gaëlle Corsaut,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Nathalie Van Acker,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,François-xavier Frenois,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Nicolas Meyer,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Béatrice Vergier,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Dennis G. Wilson,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Hervé Luga,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Oskar Staufer,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Michael L. Dustin,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Sylvain Cussat-blanc,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Ji Hou,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Xiaoliang Dai,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Zijian He,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Angela Dai,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Matthias Nießner,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Yunpeng Bai,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Cairong Wang,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Shuzhao Xie,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Chao Dong,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Chun Yuan,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Zhi Wang,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Prasun Roy,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Saumik Bhattacharya,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Subhankar Ghosh,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Umapada Pal,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Michael Blumenstein,,0%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Ni Zhang,zhangni_nlc@nec.cn,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Pengyi Zhang,zhang_pengyi@nec.cn,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Peter Wonka,peter.wonka@kaust.edu.sa,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Jian Shi,jian.shi@kaust.edu.sa,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Hakim Ghazzai,hakim.ghazzai@kaust.edu.sa,95%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Samyak Jain,samyakjain.cse18@itbhu.ac.in,95%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Sravanti Addepalli,sravantia@iisc.ac.in,85%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Pawan Sahu,,0%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Priyam Dey,,0%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,R. Venkatesh Babu,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Bo Peng,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Jun Hu,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Jingtao Zhou,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Xuan Gao,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Juyong Zhang,,0%
https://arxiv.org/pdf/2302.14680.pdf,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,Holy Lovenia,hlovenia@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14680.pdf,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,Samuel Cahyawijaya,scahyawijaya@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14680.pdf,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,Pascale Fung,,0%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Shijian Lu,shijian.Lu@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Alex C. Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Wenhan Yang,yangwh@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Yap-peng Tan,eyptan@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Yi Yu,yuyi0010@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Yufei Wang,yufei001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Junwei Zheng,junwei.zheng@kit.edu,95%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Jing Wu,jingwu6@illinois.edu,95%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Ruoyu Sun,sunruoyu@cuhk.edu.cn,95%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Yite Wang,yitew2@illinois.edu,85%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Naira Hovakimyan,nhovakim@illinois.edu,90%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Josua Sassen,,0%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Klaus Hildebrandt,,0%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Martin Rumpf,,0%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Benedikt Wirth,,0%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Rahul Mazumder,rahulmaz@mit.edu,85%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Zhe Zhao,zhezhao@google.com,95%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Hussein Hazimeh,hazimeh@google.com,82%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Riade Benbaki,rbenbaki@mit.edu,82%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Xiang Meng,mengx@mit.edu,78%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Wenyu Chen,wenyu@mit.edu,85%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Natalia Ponomareva,mareva@google.com,90%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Kailun Yang,kailun.yang@hnu.edu.cn,95%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Junwei Zheng,,0%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Jiaming Zhang,,0%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Kunyu Peng,,0%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Rainer Stiefelhagen,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Shoudong Han,shoudonghan@hust.edu.cn,95%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Hao Ren,haoren2000@hust.edu.cn,95%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Huilin Ding,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Ziwen Zhang,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Hongwei Wang,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Faquan Wang,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Kai Zhai,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Qiang Nie,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Bo Ouyang,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Xiang Li,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Shanlin Yang,,0%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Deyu Meng,dymeng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Yawen Huang,yawenhuang@tencent.com,95%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Yuexiang Li,vicyxli@tencent.com,78%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Minghao Zhou,woshizhouminghao@stu.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Hong Wang,hazelhwang@tencent.com,82%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Qian Zhao,timmy.zhaoqian@mail.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Yefeng Zheng,yefengzheng@tencent.com,95%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Markus Eisenbach,markus.eisenbach@tu-ilmenau.de,95%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Jannik Lübberstedt,,0%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Dustin Aganian,,0%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Horst-michael Gross,,0%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Yu Zhu,yuzhu@nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Jinqiu Sun,sunjinqiu@nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Pei Wang,wangpei23@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Yanning Zhang,ynzhang@nwpu.edu.cn,82%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Axi Niu,,0%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Qingsen Yan,,0%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,Om Damani,damani@cse.iitb.ac.in,78%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,Rajiv Kumar,rajiv@cse.iitb.ac.in,85%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,Chintan Tundia,chintan@cse.iitb.ac.in,85%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,G. Sivakumar,siva@iitb.ac.in,90%
https://arxiv.org/pdf/2302.14533.pdf,DEff-GAN: Diverse Attribute Transfer for Few-Shot Image Synthesis,Rajiv Kumar,1rajiv@cse.iitb.ac.in,85%
https://arxiv.org/pdf/2302.14533.pdf,DEff-GAN: Diverse Attribute Transfer for Few-Shot Image Synthesis,G. Sivakumar,,0%
https://arxiv.org/pdf/2302.14522.pdf,AdaptiveShape: Solving Shape Variability for 3D Object Detection with Geometry Aware Anchor Distributions,Michael Walter,michael.walter5@zf.com,95%
https://arxiv.org/pdf/2302.14522.pdf,AdaptiveShape: Solving Shape Variability for 3D Object Detection with Geometry Aware Anchor Distributions,Benjamin Sick,benjamin.sick@zf.com,95%
https://arxiv.org/pdf/2302.14522.pdf,AdaptiveShape: Solving Shape Variability for 3D Object Detection with Geometry Aware Anchor Distributions,Jochen Abhau,jochen.abhau@zf.com,95%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Yong Liu,yongliu@iipc.zju.edu.cn,95%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Guowei Wan,wanguowei@baidu.com,95%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Lin Li,,0%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Wendong Ding,,0%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Yongkun Wen,,0%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Yufei Liang,,0%
https://arxiv.org/pdf/2302.14503.pdf,Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?,Dongheui Lee,dongheui.lee@tuwien.ac.at,95%
https://arxiv.org/pdf/2302.14503.pdf,Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?,Esteve Valls Mascaro,esteve.valls.mascaro@tuwien.ac.at,95%
https://arxiv.org/pdf/2302.14503.pdf,Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?,Hyemin Ahn,hyemin.ahn@unist.ac.kr,95%
https://arxiv.org/pdf/2302.14490.pdf,Estimating Head Motion from MR-Images,Clemens Pollak,,0%
https://arxiv.org/pdf/2302.14490.pdf,Estimating Head Motion from MR-Images,David Kügler,,0%
https://arxiv.org/pdf/2302.14490.pdf,Estimating Head Motion from MR-Images,Martin Reuter,,0%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Abhishek Joshi,abhi.joshi@samsung.com,82%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Sathish Chalasani,sathish.c@samsung.com,85%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Kiran Nanjunda Iyer,kiran.iyer@samsung.com,95%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Sudeep Kumar Sahoo,sudeep.sahoo@samsung.com,95%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Gianluca D'amico,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Mauro Marinoni,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Federico Nesti,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Giulio Rossolini,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Giorgio Buttazzo,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Salvatore Sabina,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Gianluigi Lauro,,0%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Shuo Wang,shawnwang.tech@gmail.com,82%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Tian-zhu Xiang,tianzhu.xiang19@gmail.com,95%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Huan Xiong,huan.xiong.math@gmail.com,95%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Jie Qin,qinjiebuaa@gmail.com,95%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Peng Zheng,zhengpeng0108@gmail.com,95%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Sangwoo Mo,swmo@kaist.ac.kr,82%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Jong-chyi Su,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Chih-yao Ma,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Mido Assran,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Ishan Misra,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Licheng Yu,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Sean Bell,,0%
https://arxiv.org/pdf/2302.14475.pdf,Benchmarking Deepart Detection,Xiaopeng Hong,hongxiaopeng@ieee.org,95%
https://arxiv.org/pdf/2302.14475.pdf,Benchmarking Deepart Detection,Zhiwu Huang,zhiwu.huang@soton.ac.uk,95%
https://arxiv.org/pdf/2302.14475.pdf,Benchmarking Deepart Detection,Yabin Wang,iamwangyabin@stu.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14470.pdf,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,Aleksandra Franz,franzer@in.tum.de,78%
https://arxiv.org/pdf/2302.14470.pdf,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,Barbara Solenthaler,solenthaler@inf.ethz.ch,78%
https://arxiv.org/pdf/2302.14470.pdf,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,Nils Thuerey,nils.thuerey@tum.de,95%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Ričards Marcinkevičs,ricards.marcinkevics@inf.ethz.ch,95%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Patricia Reis Wolfertstetter,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Ugne Klimiene,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Kieran Chin-cheong,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Alyssia Paschke,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Julia Zerres,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Markus Denzinger,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,David Niederberger,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Sven Wellmann,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Ece Ozkan,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Christian Knorr,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Julia E. Vogt,,0%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Xingyu Zeng,zengxingyu@sensetime.com,95%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Kun Wang,wangkun@sensetime.com,95%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Shaobo Lin,linshaobo@sensetime.com,95%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Rui Zhao,zhaorui@sensetime.com,95%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Lichao Wang,l.wang22@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Jiahao Huang,j.huang21@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Xiaodan Xing,x.xing@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14444.pdf,Learning to Estimate Two Dense Depths from LiDAR and Event Data,Julien Moreau,julien.moreau@hds.utc.fr,95%
https://arxiv.org/pdf/2302.14444.pdf,Learning to Estimate Two Dense Depths from LiDAR and Event Data,Vincent Brebion,vincent.brebion@hds.utc.fr,95%
https://arxiv.org/pdf/2302.14444.pdf,Learning to Estimate Two Dense Depths from LiDAR and Event Data,Franck Davoine,franck.davoine@hds.utc.fr,95%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Mingqiang Wei,mqwei@nuaa.edu.cn,82%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Pan Gao,pan.gao@nuaa.edu.cn,95%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Shanshan Li,markli@nuaa.edu.cn,78%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Xiaoyang Tan,x.tan@nuaa.edu.cn,82%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Jianqiang Ren,jianqiang.rjq@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Mengyang Feng,mengyang.fmy@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Xuansong Xie,xingtong.xxs@taobao.com,60%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Biwen Lei,biwen.lbw@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Miaomiao Cui,miaomiao.cmm@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Jinqiao Wang,jqwang@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Wei Li,liwei1@sensetime.com,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Rui Zhao,zhaorui@sensetime.com,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Ming Tang,tangm@nlpr.ia.ac.cn,78%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Zhaowen Li,zhaowen.li@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Zhiyang Chen,zhiyang.chen@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Chaoyang Zhao,chaoyang.zhao@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Yousong Zhu,yousong.zhu@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Arindam Basu,arinbasu@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Chuanlin Lan,chuanlin.lan@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Ziyuan Yin,ziyuanyin3-c@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Rosa H. M. Chan,rosachan@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Yu Zhang,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Junle Yu,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Xiaolin Huang,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Wenhui Zhou,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Ji Hou,,0%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Yanqing Liu,yanqing liu@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Zheng Zhu,zhengzhu@ieee.org,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Wei Jiang,jiangwei zju@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Jianyang Gu,gu jianyang@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Kai Wang,kai.wang@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Yang You,youy@comp.nus.edu.sg,82%
https://arxiv.org/pdf/2302.14415.pdf,Mesh-SORT: Simple and effective location-wise tracker with lost management strategies,Zongtan Li,zongtanli2023@163.com,95%
https://arxiv.org/pdf/2302.14409.pdf,An Adaptive Method for Camera Attribution under Complex Radial Distortion Corrections,Andrea Montibeller,drea.montibeller@unitn.it,78%
https://arxiv.org/pdf/2302.14409.pdf,An Adaptive Method for Camera Attribution under Complex Radial Distortion Corrections,Fernando Pérez-gonzález,fperez@gts.uvigo.es,90%
https://arxiv.org/pdf/2302.14402.pdf,Neural Video Compression with Diverse Contexts,Yan Lu,yanlu@microsoft.com,95%
https://arxiv.org/pdf/2302.14402.pdf,Neural Video Compression with Diverse Contexts,Bin Li,libin@microsoft.com,95%
https://arxiv.org/pdf/2302.14402.pdf,Neural Video Compression with Diverse Contexts,Jiahao Li,li.jiahao@microsoft.com,95%
https://arxiv.org/pdf/2302.14390.pdf,Your time series is worth a binary image: machine vision assisted deep framework for time series forecasting,Xinqi Fan,xinqi.fan@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14390.pdf,Your time series is worth a binary image: machine vision assisted deep framework for time series forecasting,Zijun Zhang,zijzhang@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.14390.pdf,Your time series is worth a binary image: machine vision assisted deep framework for time series forecasting,Luoxiao Yang,luoxiyang2-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Alessandro Achille,aachille@amazon.com,82%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Matthew Trager,mttrager@amazon.com,82%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Pramuditha Perera,pramudi@amazon.com,90%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Parminder Bhatia,parmib@amazon.com,81%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Luca Zancato,zancato@amazon.it,78%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Stefano Soatto,soattos@amazon.com,82%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Wonwoong Cho,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Hareesh Ravi,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Midhun Harikumar,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Vinh Khuc,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Krishna Kumar Singh,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Jingwan Lu,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,David I. Inouye,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Ajinkya Kale,,0%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Sicheng Xu,sichengxu@microsoft.com,95%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Chong Li,chol@microsoft.com,75%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Jiaolong Yang,jiaoyan@microsoft.com,65%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Zhiqi Li,zhiqilicg@gmail.com,95%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Baining Guo,bainguo@microsoft.com,82%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Xin Tong,xtong@microsoft.com,82%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Yizhong Zhang,yizzhan@microsoft.com,65%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Yi Lin,ylinax@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Jieqi Shi,jshias@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Xiaoyang Lyu,xylyu@eee.hku.hk,82%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Dongyu Yan,,0%
https://arxiv.org/pdf/2302.14362.pdf,One-Shot Video Inpainting,Sangjin Lee,,0%
https://arxiv.org/pdf/2302.14362.pdf,One-Shot Video Inpainting,Suhwan Cho,,0%
https://arxiv.org/pdf/2302.14362.pdf,One-Shot Video Inpainting,Sangyoun Lee,,0%
https://arxiv.org/pdf/2302.14354.pdf,Deep Learning for Identifying Iran's Cultural Heritage Buildings in Need of Conservation Using Image Classification and Grad-CAM,Amir Albadvi,albadvi@modares.ac.ir,82%
https://arxiv.org/pdf/2302.14354.pdf,Deep Learning for Identifying Iran's Cultural Heritage Buildings in Need of Conservation Using Image Classification and Grad-CAM,Mahdi Bahrami,mahdi_bahrami@modares.ac.ir,95%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Xianglong Lang,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Zhuming Wang,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Zun Li,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Meng Tian,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Ge Shi,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Lifang Wu,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Liang Wang,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Jihyun Lee,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Minhyuk Sung,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Honggyu Choi,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Tae-kyun Kim,,0%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Zhihao Liang,eezhihaoliang@mail.scut.edu.cn,95%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Kui Jia,kuijia@scut.edu.cn,95%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Zhangjin Huang,eehuangzhangjin@mail.scut.edu.cn,95%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Changxing Ding,chxding@scut.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Wenwen Yu,wenwenyu@hust.edu.cn,95%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Yuliang Liu,ylliu@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Wei Hua,whua hust@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Xiang Bai,xbai@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Deqiang Jiang,dqiangjiang@tencent.com,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Bo Ren,timren@tencent.com,78%
https://arxiv.org/pdf/2302.14337.pdf,UniFLG: Unified Facial Landmark Generator from Text or Speech,Kei Sawada,keisawada@rinna.co.jp,95%
https://arxiv.org/pdf/2302.14337.pdf,UniFLG: Unified Facial Landmark Generator from Text or Speech,Yukiya Hono,yuhono@rinna.co.jp,82%
https://arxiv.org/pdf/2302.14337.pdf,UniFLG: Unified Facial Landmark Generator from Text or Speech,Kentaro Mitsui,kemits@rinna.co.jp,65%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Jianan Zhao,zhaojianan.zjn@antgroup.com,95%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Furong Xu,booyoungxu.xfr@antgroup.com,78%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Yuan Cheng,yuan@fudan.edu.cn,85%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Wei Chu,weichu.cw@antgroup.com,95%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Ruobing Zheng,zhengruobing.zrb@antgroup.com,95%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Wen Li,yinian.lw@antgroup.com,75%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Cheng Zou,wuyou.zc@antgroup.com,65%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Meng Wang,darren.wm@antgroup.com,75%
https://arxiv.org/pdf/2302.14332.pdf,Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer,Florian Richter,frichter@ucsd.edu,82%
https://arxiv.org/pdf/2302.14332.pdf,Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer,Michael C. Yip,yip@ucsd.edu,78%
https://arxiv.org/pdf/2302.14332.pdf,Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer,Jingpei Lu,,0%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Xuan Sun,sunxuan@boe.com.cn,95%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Ziyue Zhang,,0%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Guannan Chen,,0%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Dan Zhu,,0%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Huiliang Shen,shenhl@zju.edu.cn,78%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Yongzhi Fan,tony fan@zju.edu.cn,78%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Siyuan Cao,cao siyuan@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Beinan Yu,yubeinan@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Yixuan Li,yixuanli@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Lun Luo,luolun@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Shuhang Zheng,zhengsh@zju.edu.cn,78%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Shaohui Liu,shliu@hit.edu.cn,82%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Yan Shu,,0%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Honglei Xu,,0%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Feng Jiang,,0%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Alex C. Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Haoliang Li,haoliang.li@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Yap-peng Tan,eyptan@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Siyuan Yang,siyuan005@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Yufei Wang,yufei001@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Chenyu Yi,yich0003@e.ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Xiang Li,xiangli@dase.ecnu.edu.cn,95%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Ming Gao,mgao@dase.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Yunshi Lan,yslan@dase.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Kangyang Luo,,0%
https://arxiv.org/pdf/2302.14306.pdf,CLR-GAM: Contrastive Point Cloud Learning with Guided Augmentation and Feature Mapping,Srikanth Malla,srikanth.m@samsung.com,85%
https://arxiv.org/pdf/2302.14306.pdf,CLR-GAM: Contrastive Point Cloud Learning with Guided Augmentation and Feature Mapping,Yi-ting Chen,ychen@cs.nycu.edu.tw,82%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Hang Su,suhangss@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Chang Liu,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Wenzhao Xiang,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Yuan He,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Hui Xue,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Shibao Zheng,,0%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Hui Xue,hui.xueh@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Hang Su,suhangss@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Wenzhao Xiang,xiangwenzhao22@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Yinpeng Dong,dongyinpeng@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Yuan He,heyuan.hy@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Xiao Yang,yangxiao19@mails.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Yuefeng Chen,yuefeng.chenyf@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Chang Liu,,0%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Jun Zhu,,0%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Shibao Zheng,,0%
https://arxiv.org/pdf/2302.14290.pdf,Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation,Gaurav Patel,gpatel10@purdue.edu,82%
https://arxiv.org/pdf/2302.14290.pdf,Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation,Konda Reddy Mopuri,krmopuri@ai.iith.ac.in,82%
https://arxiv.org/pdf/2302.14290.pdf,Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation,Qiang Qiu,qqiu@purdue.edu,82%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Zhengzhuo Xu,,0%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Shuo Yang,,0%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Xingjun Wang,,0%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Chun Yuan,,0%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Ting Ma,tma@hit.edu.cn,82%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Jiesi Hu,,0%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Yanwu Yang,,0%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Xutao Guo,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Xueyi Liu,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Ji Zhang,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Ruizhen Hu,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Haibin Huang,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,He Wang,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Li Yi,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Jiyuan Liu,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Bingyi Lu,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Mingkang Xiong,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Tao Zhang,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Huilin Xiong,,0%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Di Huang,dhuang@buaa.edu.cn,82%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Ran Qin,,0%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Haoxiang Ma,,0%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Boyang Gao,,0%
https://arxiv.org/pdf/2302.14256.pdf,Remote Sensing Scene Classification with Masked Image Modeling (MIM),Liya Wang,,0%
https://arxiv.org/pdf/2302.14256.pdf,Remote Sensing Scene Classification with Masked Image Modeling (MIM),Alex Tien,,0%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Chaohui Yu,huakun.ych@alibaba-inc.com,65%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Zhibin Wang,zhibin.waz@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Qiang Zhou,jianchong.zq@alibaba-inc.com,75%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Jingliang Li,lijingliang20@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Fan Wang,fan.w@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Jianlong Yuan,,0%
https://arxiv.org/pdf/2302.14239.pdf,"Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images",Li Zhang,yxliu@casm.ac.cn,85%
https://arxiv.org/pdf/2302.14239.pdf,"Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images",Zhongli Fan,fanzhongli@whu.edu.cn,95%
https://arxiv.org/pdf/2302.14239.pdf,"Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images",Yuxuan Liu,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Kay Hutchinson,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Zongyu Li,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Ian Reyes,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Homa Alemzadeh,,0%
https://arxiv.org/pdf/2302.14217.pdf,Global Proxy-based Hard Mining for Visual Place Recognition,Philippe Giguère,philippe.giguere@ift.ulaval.ca,95%
https://arxiv.org/pdf/2302.14217.pdf,Global Proxy-based Hard Mining for Visual Place Recognition,Brahim Chaib-draa,brahim.chaib-draa@ift.ulaval.ca,95%
https://arxiv.org/pdf/2302.14217.pdf,Global Proxy-based Hard Mining for Visual Place Recognition,Amar Ali-bey,amar.ali-bey.1@ulaval.ca,95%
https://arxiv.org/pdf/2303.12700.pdf,ReorientDiff: Diffusion Model based Reorientation for Object Manipulation,Yongxin Chen,yongchen@gatech.edu,82%
https://arxiv.org/pdf/2303.12700.pdf,ReorientDiff: Diffusion Model based Reorientation for Object Manipulation,Utkarsh A. Mishra,umishra31@gatech.edu,82%
https://arxiv.org/pdf/2302.14197.pdf,Image-Based Virtual Try-on System With Clothing-Size Adjustment,Minoru Kuribayashi,,0%
https://arxiv.org/pdf/2302.14197.pdf,Image-Based Virtual Try-on System With Clothing-Size Adjustment,Koki Nakai,,0%
https://arxiv.org/pdf/2302.14197.pdf,Image-Based Virtual Try-on System With Clothing-Size Adjustment,Nobuo Funabiki,,0%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,Shan Liu,shanl@tencent.com,85%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,Jiahao Gu,jiahaogu@usc.edu,95%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,Pranav Kadam,pranavka@usc.edu,85%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,C. -c. Jay Kuo,cckuo@sipi.usc.edu,82%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Xi Peng,pengx.gm@gmail.com,78%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Jianping Fan,jfan1@Lenovo.com,82%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Kui Ren,kuiren@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Jun Yu,yujun@hdu.edu.cn,95%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Buyu Liu,buyu@nec-labs.com,85%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Baojun,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Prashant Pandey,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Mustafa Chasmai,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Monish Natarajan,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Brejesh Lall,,0%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Xiyang Dai,xidai@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Dongdong Chen,dochen@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Ziyu Jiang,jiangziyu@tamu.edu,95%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Lu Yuan,luyuan@microsoft.com,95%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Yinpeng Chen,yiche@microsoft.com,65%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Zicheng Liu,zliu@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Mengchen Liu,mengcliu@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Zhangyang Wang,atlaswang@utexas.edu,78%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Eun Som Jeon,,0%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Hongjun Choi,,0%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Ankita Shukla,,0%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Pavan Turaga,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Tonmoy Hossain,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Zoraiz Qureshi,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Nivetha Jayakumar,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Thomas Eluvathingal Muttikkal,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Sohil Patel,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,David Schiff,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Miaomiao Zhang,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Bijoy Kundu,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Antoine Yang,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Arsha Nagrani,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Paul Hongsuck Seo,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Antoine Miech,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Jordi Pont-tuset,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Ivan Laptev,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Josef Sivic,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Cordelia Schmid,,0%
https://arxiv.org/pdf/2302.14098.pdf,An Embedded and Real-Time Pupil Detection Pipeline,Ankur Raj,,0%
https://arxiv.org/pdf/2302.14098.pdf,An Embedded and Real-Time Pupil Detection Pipeline,Diwas Bhattarai,,0%
https://arxiv.org/pdf/2302.14098.pdf,An Embedded and Real-Time Pupil Detection Pipeline,Kristof Van Laerhoven,,0%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Hao Zhao,zhaohao@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Guyue Zhou,zhouguyue@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Yongliang Shi,Shiyongliang@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Ya-qin Zhang,zhangyaqin@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Jirui Yuan,yuanjirui@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Pengfei Li,li-pf22@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Ruowen Zhao,,0%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Alexander C. Li,derli@cmu.edu,78%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Ellis Brown,ellisbrown@cmu.edu,95%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Alexei A. Efros,,0%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Deepak Pathak,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Shaohan Huang,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Li Dong,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Wenhui Wang,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Yaru Hao,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Saksham Singhal,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Shuming Ma,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Tengchao Lv,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Lei Cui,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Owais Khan Mohammed,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Barun Patra,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Qiang Liu,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Kriti Aggarwal,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Zewen Chi,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Johan Bjorck,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Vishrav Chaudhary,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Subhojit Som,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Xia Song,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Furu Wei,,0%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Yanfeng Wang,wangyanfeng@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Weidi Xie,weidi@sjtu.edu.cn,85%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Ya Zhang,ya_zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Xiaoman Zhang,,0%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Chaoyi Wu,,0%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Cedric Girerd,cedric.girerd@lirmm.fr,95%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Michael C. Yip,yip@ucsd.edu,78%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Fei Liu,f4liu@ucsd.edu,82%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Jingpei Lu,,0%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Anthony Hoogs,anthony.hoogs@kitware.com,95%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Chen Zhao,chen.zhao@kitware.com,95%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Christopher Funk,christopher.funk@kitware.com,95%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Dawei Du,dawei.du@kitware.com,95%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Pheng-ann Heng,pheng@cse.cuhk.edu.hk,95%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Xianzhi Li,xzli@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Ziyu Guo,zyguo@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Renrui Zhang,,0%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Longtian Qiu,,0%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Wenwei Zhang,wenwei001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Wentao Liu,liuwentao@sensetime.com,95%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Size Wu,size001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Sheng Jin,jinsheng@sensetime.com,95%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Chen Change Loy,ccloy@ntu.edu.sg,82%
https://arxiv.org/pdf/2302.13991.pdf,Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays,Taufiq Hasan,taufiq@bme.buet.ac.bd,85%
https://arxiv.org/pdf/2302.13991.pdf,Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays,Md. Aynal Haque,aynal@eee.buet.ac.bd,90%
https://arxiv.org/pdf/2302.13991.pdf,Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays,Mohammad Zunaed,rafizunaed@gmail.com,78%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Zhenwei Zhu,garyzhu1996@gmail.com,78%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Liying Yang,yyliang@must.edu.mo,85%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Ning Li,,0%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Chaohao Jiang,,0%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Yanyan Liang,,0%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Roberto Alcover-couso,roberto.alcover@uam.es,85%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Juan C. Sanmiguel,los.sanmiguel@uam.es,78%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Jose M. Martinez,josem.martinez@uam.es,95%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Marcos Escudero-vinolo,marcos.escudero@uam.es,85%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Hao Cheng,h.cheng-2@utwente.nl,82%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Mengmeng Liu,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Lin Chen,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Hellward Broszio,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Jiangtao Li,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Runjiang Zhao,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Monika Sester,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Michael Ying Yang,,0%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,Robert Platt,r.platt@northeastern.edu,82%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,Ondrej Biza,biza.o@northeastern.edu,78%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,David M. Klee,klee.d@northeastern.edu,78%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,Robin Walters,r.walters@northeastern.edu,82%
https://arxiv.org/pdf/2302.13902.pdf,Language identification as improvement for lip-based biometric visual systems,Lucia Cascone,,0%
https://arxiv.org/pdf/2302.13902.pdf,Language identification as improvement for lip-based biometric visual systems,Michele Nappi,,0%
https://arxiv.org/pdf/2302.13902.pdf,Language identification as improvement for lip-based biometric visual systems,Fabio Narducci,,0%
https://arxiv.org/pdf/2302.13891.pdf,Supervised Virtual-to-Real Domain Adaptation for Object Detection Task using YOLO,Bayu Rahayudi,ubay1@ub.ac.id,60%
https://arxiv.org/pdf/2302.13891.pdf,Supervised Virtual-to-Real Domain Adaptation for Object Detection Task using YOLO,Akbar Satya Nugraha,personal.akbarsn@gmail.com,85%
https://arxiv.org/pdf/2302.13891.pdf,Supervised Virtual-to-Real Domain Adaptation for Object Detection Task using YOLO,Yudistira Novanto,yudistira@ub.ac.id,85%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Jiangang Chen,jgchen@cee.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Menghan Hu,mhhu@ce.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Yiman Liu,LiuyimanSCMC@163.com,95%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Qingli Li,qlli@cs.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Yuqi Zhang,changyuqi@hotmail.com,85%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Bin Dong,dongbin@scmc.com.cn,95%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Tongtong Liang,zhimaliang@163.com,78%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Xiaoxiang Han,,0%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Jiajun Yuan,,0%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Qiaohong Liu,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Leonard Berrada,lberrada | bballe@deepmind.com,82%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Sahra Ghalebikesabi,sahra.ghalebikesabi@univ.ox.ac.uk,95%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Borja Balle,bballe@deepmind.com,82%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Sven Gowal,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Ira Ktena,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Robert Stanforth,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Jamie Hayes,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Soham De,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Samuel L. Smith,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Olivia Wiles,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Yuxiang Wei,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Yabo Zhang,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Zhilong Ji,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Jinfeng Bai,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Lei Zhang,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Wangmeng Zuo,,0%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Canlong Zhang,clzhang@gxnu.edu.cn,82%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Kaijie He,hekaijie123@outlook.com,95%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Sheng Xie,,0%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Zhixin Li,,0%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Zhiwen Wang,,0%
https://arxiv.org/pdf/2302.13838.pdf,Cross-modal Face- and Voice-style Transfer,Naoya Takahashi,Naoya.Takahashi@sony.com,95%
https://arxiv.org/pdf/2302.13838.pdf,Cross-modal Face- and Voice-style Transfer,Yuki Mitsufuji,Yuhki.Mitsufuji@sony.com,82%
https://arxiv.org/pdf/2302.13838.pdf,Cross-modal Face- and Voice-style Transfer,Mayank K. Singh,Mayank.A.Singh@sony.com,95%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Shuang Li,shuangli@bit.edu.cn,95%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Chi Harold Liu,liuchi02@gmail.com,95%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Mixue Xie,mxxie@bit.edu.cn,82%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Rui Zhang,zhangrui20@bit.edu.cn,95%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Long Sun,,0%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Jiangxin Dong,,0%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Jinhui Tang,,0%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Jinshan Pan,,0%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Marco Monforte,marco.monforte@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Arren Glover,arren.glover@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Massimiliano Iacono,massimiliano.iacono@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Chiara Bartolozzi,chiara.bartolozzi@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Luna Gava,luna.gava@iit.it,95%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Pengxiang Xiao,,0%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Shuai He,,0%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Limin Liu,,0%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Anlong Ming,,0%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Bishan Wang,wangbs@whu.edu.cn,78%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Jianzhuang Liu,liu.jianzhuang@huawei.com,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Haijian Zhang,haijian.zhang@whu.edu.cn,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Wen Yang,yangwen@whu.edu.cn,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Xiang Zhang,xiangz@whu.edu.cn,85%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Gui-song Xia,guisong.xia@whu.edu.cn,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Lei Yu,,0%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Shibiao Xu,shibiaoxu@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Weiliang Meng,weiliang.meng@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Rongtao Xu,xurongtao2019@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Jiaxi Sun,sunjiaxi2020@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Xiaopeng Zhang,xiaopeng.zhang@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Changwei Wang,wangchangwei2019@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Xinyang Jiang,xinyangjiang@microsoft.com,95%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Jiaqi Gao,,0%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Yuqing Yang,,0%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Dongsheng Li,,0%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Lili Qiu,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Maheshi Lokumarambage,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Vishnu Gowrisetty,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Hossein Rezaei,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Thushan Sivalingam,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Nandana Rajatheva,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Anil Fernando,,0%
https://arxiv.org/pdf/2302.13700.pdf,Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech,Jiyoung Lee,,0%
https://arxiv.org/pdf/2302.13700.pdf,Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech,Joon Son Chung,,0%
https://arxiv.org/pdf/2302.13700.pdf,Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech,Soo-whan Chung,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Zhenghua Xu,zhenghua.xu@hebut.edu.cn,95%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Xiangtao Wang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Ruizhi Wang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Biao Tian,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Jiaojiao Zhang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Shuo Zhang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Junyang Chen,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Thomas Lukasiewicz,,0%
https://arxiv.org/pdf/2302.13694.pdf,DLOFTBs -- Fast Tracking of Deformable Linear Objects with B-splines,Amadeusz Szymko,name.surname@put.poznan.pl,60%
https://arxiv.org/pdf/2302.13694.pdf,DLOFTBs -- Fast Tracking of Deformable Linear Objects with B-splines,Piotr Kicki,,0%
https://arxiv.org/pdf/2302.13694.pdf,DLOFTBs -- Fast Tracking of Deformable Linear Objects with B-splines,Krzysztof Walas,,0%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Shuicheng Yan,yansc@sea.com,78%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Angela Yao,ayao@comp.nus.edu.sg,82%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Tat-seng Chua,chuats@comp.nus.edu.sg,78%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Pan Zhou,zhoupan@sea.com,95%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Richang Hong,hongrc.hfut@gmail.com,78%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Junbin Xiao,junbin@comp.nus.edu.sg,85%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Yicong Li,liyicong@u.nus.edu,95%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Nikhil J. Dhinagar,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Conor Owens-walton,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Emily Laltoo,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Christina P. Boyle,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Yao-liang Chen,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Philip Cook,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Corey Mcmillan,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Chih-chien Tsai,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,J-j Wang,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Yih-ru Wu,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Ysbrand Van Der Werf,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Paul M. Thompson,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Rahim Entezari,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Mitchell Wortsman,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Olga Saukh,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,M. Moein Shariatnia,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Hanie Sedghi,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Ludwig Schmidt,,0%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Jianqi Ma,jianqi.ma@connect.polyu.hk,95%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Hongwei Yong,hongwei.yong@polyu.edu.hk,95%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Shi Guo,shiguo.guo@connect.polyu.hk,95%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Xindong Zhang,cslzhang@comp.polyu.edu.hk,78%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Lei Zhang,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Wei Wang,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Xuejing Lei,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Yueru Chen,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Ming-sui Lee,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,C. -c. Jay Kuo,,0%
https://arxiv.org/pdf/2302.13594.pdf,Leveraging Video Coding Knowledge for Deep Video Enhancement,Van-quang Nguyen,quang@vision.is.tohoku.ac.jp,85%
https://arxiv.org/pdf/2302.13594.pdf,Leveraging Video Coding Knowledge for Deep Video Enhancement,Thuong Nguyen Canh,ngcthuong@ids.osaka-u.ac.jp,85%
https://arxiv.org/pdf/2302.13594.pdf,Leveraging Video Coding Knowledge for Deep Video Enhancement,Thong Bach,thongbtqm@gmail.com,85%
https://arxiv.org/pdf/2302.13578.pdf,Online Black-Box Confidence Estimation of Deep Neural Networks,Georg Schneider,firstname.surname@zf.com,70%
https://arxiv.org/pdf/2302.13578.pdf,Online Black-Box Confidence Estimation of Deep Neural Networks,Fabian Woitschek,,0%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Xian Wei,xian.wei@tum.de,95%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Arafat Al-jawari,arafatmmj@gmail.com,85%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Xihao Wang,xihaowang2016@gmail.com,95%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Hai Lan,lanhai09@fjirsm.ac.cn,95%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Jiaming Lei,,0%
https://arxiv.org/pdf/2302.13570.pdf,Physical Adversarial Attacks on Deep Neural Networks for Traffic Sign Recognition: A Feasibility Study,Georg Schneider,firstname.surname@zf.com,70%
https://arxiv.org/pdf/2302.13570.pdf,Physical Adversarial Attacks on Deep Neural Networks for Traffic Sign Recognition: A Feasibility Study,Fabian Woitschek,,0%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Jeong Eun Lee,leeje290@gmail.com,78%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Jaeyoung Huh,,0%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Sangjoon Park,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Fabian Woitschek,fabian.woitschek@zf.com,95%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Arndt Von Twickel,arndt.twickel@bsi.bund.de,95%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Devi Padmavathi Alagarswamy,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Christian Berghoff,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Vasilios Danos,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Fabian Langer,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Thora Markert,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Georg Schneider,,0%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Kibo Ote,kibou@crl.hpk.co.jp,85%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Fumio Hashimoto,fumio.hashimoto@crl.hpk.co.jp,95%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Masanobu Ibaraki,iba@akita-noken.jp,90%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Keisuke Matsubara,matsubara@akita-pu.ac.jp,78%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Yuya Onishi,yuya.onishi@hpk.co.jp,95%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Sameera Ramasinghe,,0%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Violetta Shevchenko,,0%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Gil Avraham,,0%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Anton Van Den Hengel,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Ruihang Miao,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Weizhou Liu,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Mingrui Chen,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Zheng Gong,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Weixin Xu,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Chen Hu,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Shuchang Zhou,,0%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Xiaofei Wang,wangxiaofei2022@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Shaohui Mei,meish@nwpu.edu.cn,78%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Jiawei Lian,lianjiawei@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Mingyang Ma,mamingyang@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Yuru Su,,0%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Chaohui Yu,huakun.ych@alibaba-inc.com,65%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Zhibin Wang,zhibin.waz@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Qiang Zhou,jianchong.zq@alibaba-inc.com,75%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Jingliang Li,lijingliang20@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Fan Wang,fan.w@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Yuang Liu,frankliu624@gmail.com,78%
https://arxiv.org/pdf/2303.08670.pdf,Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video,Yong Man Ro,ymro@kaist.ac.kr,82%
https://arxiv.org/pdf/2303.08670.pdf,Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video,Chae Won Kim,chaewonkim@kaist.ac.kr,95%
https://arxiv.org/pdf/2303.08670.pdf,Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video,Minsu Kim,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Shaohui Mei,meish@nwpu.edu.cn,78%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Jiawei Lian,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Xiaofei Wang,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Yuru Su,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Mingyang Ma,,0%
https://arxiv.org/pdf/2302.13434.pdf,Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition,Yifan Jiang,yfjiang@korea.ac.kr,82%
https://arxiv.org/pdf/2302.13434.pdf,Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition,Han Chen,jessicachan@korea.ac.kr,85%
https://arxiv.org/pdf/2302.13434.pdf,Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition,Hanseok Ko,hsko@korea.ac.kr,82%
https://arxiv.org/pdf/2302.13408.pdf,Generative Models for 3D Point Clouds,Lingjie Kong,ljkong@stanford.edu,82%
https://arxiv.org/pdf/2302.13408.pdf,Generative Models for 3D Point Clouds,Siamak Shakeri,siamaks@stanford.edu,85%
https://arxiv.org/pdf/2302.13408.pdf,Generative Models for 3D Point Clouds,Pankaj Rajak,prajak7@stanford.edu,82%
https://arxiv.org/pdf/2302.13392.pdf,NSANet: Noise Seeking Attention Network,Maryam Jameela,,0%
https://arxiv.org/pdf/2302.13392.pdf,NSANet: Noise Seeking Attention Network,Gunho Sohn,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Catarina Moreira,catarina.pintomoreira@data61.csiro.au,95%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Chihcheng Hsieh,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Isabel Blanco Nobre,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Sandra Costa Sousa,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Chun Ouyang,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Margot Brereton,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Jacinto C. Nascimento,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Joaquim Jorge,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Linghao Chen,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Yunzhou Song,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Hujun Bao,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Xiaowei Zhou,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Wayner Barrios,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Mattia Soldan,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Alberto Mario Ceballos-arroyo,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Fabian Caba Heilbron,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Bernard Ghanem,,0%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Jorge Vila-tomás,jorge.vila-tomas@uv.es,95%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Pablo Hernández-cámara,pablo.hernandez-camara@uv.es,95%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Valero Laparra,valero.laparra@uv.es,95%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Jesús Malo,jesus.malo@uv.es,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Didier Hans,didier.hans@chuv.ch,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Zhe Wang,zwang78@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Fang Chen,fangyxy@hactcm.edu.cn,85%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Rachid Jennane,rachid.jennane@univ-orleans.fr,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Fabian Bauer,fabian.bauer@dkfz-heidelberg.de,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Yung Hsin Chen,ychen4@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Yuhua Ru,ruyuhua@163.com,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Mohamed Jarraya,mjarraya@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Aladine Chetouani,aladine.chetouani@univ-paris13.fr,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Liping Zhang,lzhang90@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Haoyu Luo,luohaoyu@stu.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Yihong Gong,ygong@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Xing Wei,weixing@mail.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Yuhang He,chengjie8@huawei.com,78%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Songlin Dong,,0%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Hyunsu Kim,hyunsu1125.kim@navercorp.com,95%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Yoonjeon Kim,yoonkim313@kaist.ac.kr,82%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Yunjey Choi,yunjey.choi@navercorp.com,95%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Junho Kim,jhkim.ai@navercorp.com,82%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Eunho Yang,eunhoy@kaist.ac.kr,85%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Hung Cao,hcao3@unb.ca,82%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Atah Nuh Mih,,0%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Joshua Pickard,,0%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Monica Wachowicz,,0%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Rickey Dubay,,0%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Shoaib Ehsan,sehsan@essex.ac.uk,82%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Mihnea-alexandru Tomita,matomi@essex.ac.uk,90%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Michael Milford,michael.milford@qut.edu.au,95%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Bruno Ferrarini,bferra@essex.ac.uk,90%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Klaus Mcdonald-maier,,0%
https://arxiv.org/pdf/2302.13301.pdf,Pillar R-CNN for Point Cloud 3D Object Detection,Chao Ma,chaoma@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13301.pdf,Pillar R-CNN for Point Cloud 3D Object Detection,Guangsheng Shi,sgsadvance@163.com,60%
https://arxiv.org/pdf/2302.13301.pdf,Pillar R-CNN for Point Cloud 3D Object Detection,Ruifeng Li,,0%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Luisa Verdoliva,verdoliv@unina.it,90%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Marius Pedersen,marius.pedersen@ntnu.no,95%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Ying Xu,,0%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Kiran Raja,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Hatef Otroshi Shahreza,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Pietro Melzi,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Dailé Osorio-roig,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Christian Rathgeb,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Christoph Busch,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Sébastien Marcel,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Ruben Tolosana,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Ruben Vera-rodriguez,,0%
https://arxiv.org/pdf/2302.13279.pdf,Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition,Xingchao Yang,,0%
https://arxiv.org/pdf/2302.13279.pdf,Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition,Takafumi Taketomi,,0%
https://arxiv.org/pdf/2302.13279.pdf,Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition,Yoshihiro Kanamori,,0%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Hongxun Yao,h.yao@hit.edu.cn,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Kuiyuan Yang,kuyang@microsoft.com,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Wei Yu,w.yu@hit.edu.cn,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Yalong Bai,ylbai@mtlab.hit.edu.cn,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Yong Rui,grui@microsoft.com,78%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Haoning Wu,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Liang Liao,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Jingwen Hou,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Chaofeng Chen,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Erli Zhang,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Annan Wang,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Wenxiu Sun,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Qiong Yan,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Weisi Lin,,0%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Ming Wu,wuming@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Shenwei Xie,xieshenwei@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Junli Yang,yangjunli@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Chuang Zhang,zhangchuang@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Wanfeng Zheng,zhengwanfeng@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Zhenglin Xian,,0%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Zhenzhong Chen,zzchen@ieee.org,82%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Yuantong Zhang,,0%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Daiqin Yang,,0%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Wenpeng Ding,,0%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Haoliang Li,haoliang.li@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Jie Liu,wanpeoplejie@gmail.com,85%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Varut Vardhanabhuti,varv@hku.hk,75%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Hong Yan,h.yan@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Victor Ho-fun Lee,vhflee@hku.hk,82%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Kecheng Chen,,0%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Renjie Wan,,0%
https://arxiv.org/pdf/2302.13207.pdf,Stereo X-ray Tomography,Zhenduo Shang,,0%
https://arxiv.org/pdf/2302.13207.pdf,Stereo X-ray Tomography,Thomas Blumensath,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Nchongmaje Ndipenoch,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Alina Miron,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Zidong Wang,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Yongmin Li,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Shaoyan Pan,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Shao-yuan Lo,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Min Huang,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Chaoqiong Ma,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Jacob Wynne,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Tonghe Wang,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Tian Liu,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Xiaofeng Yang,,0%
https://arxiv.org/pdf/2302.13170.pdf,Partial Label Learning for Emotion Recognition from EEG,Ali Etemad,ali.etemad@queensu.ca,95%
https://arxiv.org/pdf/2302.13170.pdf,Partial Label Learning for Emotion Recognition from EEG,Guangyi Zhang,guangyi.zhang@queensu.ca,95%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,Avisek Lahiri,avisek@google.com,85%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,Wan-duo Kurt Ma,mawand@ecs.vuw.ac.nz,95%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,J. P. Lewis,bastiaan.kleijn@vuw.ac.nz,75%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,Thomas Leung,leungt@google.com,78%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,W. Bastiaan Kleijn,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,Tarasha Khurana,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,Peiyun Hu,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,David Held,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,Deva Ramanan,,0%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Galit Fuhrmann Alpert,fuhrmann@bgu.ac.il,90%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Dima Kagan,kagandi@post.bgu.ac.il,78%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Shmuel Horowitz,shmuelho@post.bgu.ac.il,85%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Michael Fire,,0%
https://arxiv.org/pdf/2302.13125.pdf,Non-Intrusive Driver Behavior Characterization From Road-Side Cameras,Pavana Pradeep Kumar,pavana.pradeep@temple.edu,85%
https://arxiv.org/pdf/2302.13125.pdf,Non-Intrusive Driver Behavior Characterization From Road-Side Cameras,Krishna Kant,kkant@temple.edu,82%
https://arxiv.org/pdf/2302.13125.pdf,Non-Intrusive Driver Behavior Characterization From Road-Side Cameras,Amitangshu Pal,amitangshu@cse.iitk.ac.in,85%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Qihan Ren,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Huiqi Deng,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Yunuo Chen,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Siyu Lou,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Quanshi Zhang,,0%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Jingtao Ding,dingjt15@tsinghua.org.cn,78%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Yu Liu,liuyu2419@126.com,95%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Yanxin Xi,yanxin.xi@helsinki.fi,95%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Yong Li,liyong07@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Xin Zhang,zhangxin4087@163.com,95%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Feng Ding,,0%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Jian Jin,,0%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Lili Meng,,0%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Weisi Lin,,0%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Huilin Zhou,zhouhuilin116@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Shih-han Chan,s2chan@ucsd.edu,95%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Huiqi Deng,denghq7@sjtu.edu.cn,78%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Dongrui Liu,drliu96@sjtu.edu.cn,82%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Wen Shen,wen shen@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Hao Zhang,,0%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Quanshi Zhang,,0%
https://arxiv.org/pdf/2302.13084.pdf,RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information,Dong-gyu Lee,dglee@knu.ac.kr,82%
https://arxiv.org/pdf/2302.13084.pdf,RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information,Satyawant Kumar,,0%
https://arxiv.org/pdf/2302.13084.pdf,RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information,Abhishek Kumar,,0%
https://arxiv.org/pdf/2302.13080.pdf,Does a Neural Network Really Encode Symbolic Concepts?,Mingjie Li,,0%
https://arxiv.org/pdf/2302.13080.pdf,Does a Neural Network Really Encode Symbolic Concepts?,Quanshi Zhang,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Martin Sundermeyer,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Tomas Hodan,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Yann Labbe,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Gu Wang,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Eric Brachmann,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Bertram Drost,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Carsten Rother,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Jiri Matas,,0%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Errui Ding,dingerrui@baidu.com,95%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Jian Wang,wangjian33@baidu.com,95%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Desen Zhou,zhouds@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Zhichao Liu,liuzhch@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Rui Fan,fanrui@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Leshan Wang,wanglsh@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Songyang Zhang,,0%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Yang Bai,,0%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Yiqin Yu,yuyiqin@cn.ibm.com,95%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Jing Mei,meijing@cn.ibm.com,95%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Yuan Zhou,zhxyuan@cn.ibm.com,85%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Tanveer Syeda-mahmood,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Lemuel Puglisi,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Frederik Barkhof,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Daniel C. Alexander,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Geoffrey Jm Parker,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Arman Eshaghi,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Daniele Ravì,,0%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Xiaowei Xu,xuxw525@ouc.edu.cn,82%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Leon Bevan Bullock,leonbevanbullock@ouc.edu.cn,95%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Huasong Zhou,zhouhuasong@stu.ouc.edu.cn,95%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Xiaodong Wang,wangxiaodong@ouc.edu.cn,95%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Jawad Muhammad,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Yunlong Wang,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Junxing Hu,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Kunbo Zhang,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Zhenan Sun,,0%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Muhammad Haroon Yousaf,haroon.yousaf@uettaxila.edu.pk,78%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Muhammad Saad Saeed,saad.saeed@uettaxila.edu.pk,78%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Shah Nawaz,shah.nawaz@desy.de,95%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Saqlain Hussain Shah,saqlain.hussain@uettaxila.edu.pk,85%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Lam Pham,lam.pham@ait.ac.at,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Anh Nguyen,AnhNTN34@fsoft.com.vn,85%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Cam Le,cam.levt123@hcmut.edu.vn,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Alexander Schindler,Alexander.Schindler@ait.ac.at,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Jasmin Lampert,Jasmin.Lampert@ait.ac.at,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Ian Mcloughlin,ian.mcloughlin@singaporetech.edu.sg,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Dat Ngo,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Zhengyu Ma,mazhy@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Yonghong Tian,yhtian@pku.edu.cn,82%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Yanqi Chen,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Wei Fang,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Xiawu Zheng,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Zhaofei Yu,,0%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Binbin Lv,lv-bin-bin@outlook.com,78%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Yaqi Liu,liuyaqi@besti.edu.cn,95%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Xin Jin,jinxin@besti.edu.cn,95%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Xiaoyu Chen,,0%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Xiaokun Zhang,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Hao Zhang,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Hongyang Li,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Ailing Zeng,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Feng Li,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Shilong Liu,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Xingyu Liao,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Lei Zhang,,0%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Bihan Wen,bihan.wen@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Alex Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Lap-pui Chau,lap-pui.chau@polyu.edu.hk,95%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Lanqing Guo,lanqing001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Wenhan Yang,yangwh@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Yi Yu,yuyi0010@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Yufei Wang,yufei001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Zhen Lei,zlei@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Benzhi Wang,wangbenzhi2021@ia.ac.cn,95%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Jinlin Wu,jinlin.wu@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Guo-jun Qi,guojunq@gmail.com,85%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Yang Yang,yang.yang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Donald Stewart,dolstewa@ucsc.edu,75%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Alex Pang,pang@soe.ucsc.edu,78%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Gregory Dusek,gregory.dusek@noaa.gov,95%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Akila De Silva,audesilv@ucsc.edu,75%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Mona Zhao,yzhao172@ucsc.edu,78%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Fahim Hasan Khan,fkhan4@ucsc.edu,82%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,James Davis,davis@cs.ucsc.edu,78%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Daniel Racoceanu,firstname.lastname@icm-institute.org,70%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Guanghui Fu,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Gabriel Jimenez,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Sophie Loizillon,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Lydia Chougar,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Didier Dormont,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Romain Valabregue,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Ninon Burgos,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Stéphane Lehéricy,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Olivier Colliot,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,The Iceberg Study Group,,0%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Yongqiang Ma,musayq@xjtu.edu.cn,65%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Guibo Zhu,gbzhu@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Nanning Zheng,nnzheng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Yulong Liu,,0%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Wei Zhou,,0%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Beiying Yang,beiying.yang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Lu Zhou,lu.zhou@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Jinqiao Wang,jqwang@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Guibo Zhu,gbzhu@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Jinzhao Luo,luojinzhao2020@ia.ac.cn,95%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Guojing Ge,guojing.ge@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Jiawei Hou,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Qi Chen,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Yurong Cheng,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Guang Chen,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Xiangyang Xue,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Taiping Zeng,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Jian Pu,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Otilia Stretcu,otiliastr@google.com,85%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Ariel Fuxman,afuxman@google.com,82%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Edward Vendrow,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Kenji Hata,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Krishnamurthy Viswanathan,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Vittorio Ferrari,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Sasan Tavakkol,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Wenlei Zhou,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Aditya Avinash,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Enming Luo,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Neil Gordon Alldrin,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Mohammadhossein Bateni,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Gabriel Berger,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Andrew Bunner,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Chun-ta Lu,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Javier A Rey,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Giulia Desalvo,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Ranjay Krishna,,0%
https://arxiv.org/pdf/2302.12935.pdf,Visual Privacy: Current and Emerging Regulations Around Unconsented Video Analytics in Retail,Scott Pletcher,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Peyman Tahghighi,ptahghig@uoguelph.ca,90%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Nicole Norena,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Eran Ukwatta,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Ryan B Appleby,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Amin Komeili,,0%
https://arxiv.org/pdf/2302.12883.pdf,3D Surface Reconstruction in the Wild by Deforming Shape Priors from Synthetic Data,Jun-jee Chao,chao0107@umn.edu,78%
https://arxiv.org/pdf/2302.12883.pdf,3D Surface Reconstruction in the Wild by Deforming Shape Priors from Synthetic Data,Volkan Isler,isler@umn.edu,78%
https://arxiv.org/pdf/2302.12883.pdf,3D Surface Reconstruction in the Wild by Deforming Shape Priors from Synthetic Data,Nicolai Häni,haeni001@umn.edu,65%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Randall Balestriero,rbalestriero@fb.com,82%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Ahmed Imtiaz Humayun,imtiaz@rice.edu,90%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Richard Baraniuk,richb@rice.edu,78%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Guha Balakrishnan,guha@rice.edu,85%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Vickie Ye,,0%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Georgios Pavlakos,,0%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Jitendra Malik,,0%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Angjoo Kanazawa,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Simone Foti,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Bongjin Koo,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Danail Stoyanov,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Matthew J. Clarkson,,0%
https://arxiv.org/pdf/2302.12772.pdf,FLSea: Underwater Visual-Inertial and Stereo-Vision Forward-Looking Datasets,Yelena Randall,y4randall@gmail.com,82%
https://arxiv.org/pdf/2302.12772.pdf,FLSea: Underwater Visual-Inertial and Stereo-Vision Forward-Looking Datasets,Tali Treibitz,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Siddharth Karamcheti,skaramcheti@cs.stanford.edu,82%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Suraj Nair,surajn@cs.stanford.edu,85%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Annie S. Chen,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Thomas Kollar,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Chelsea Finn,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Dorsa Sadigh,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Percy Liang,,0%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Zhifei Zhang,zzhang@adobe.com,82%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,James Hays,hays@gatech.edu,78%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Krishna Kumar Singh,krishsin@adobe.com,65%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Cusuh Ham,cusuh@gatech.edu,85%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Jingwan Lu,jlu@adobe.com,82%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Tobias Hinz,thinz@adobe.com,82%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Ruchika Chavhan,R.Chavhan@sms.ed.ac.uk,82%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Timothy Hospedales,t.hospedales@ed.ac.uk,82%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Henry Gouk,,0%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Jan Stuehmer,,0%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Calum Heggan,,0%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Mehrdad Yaghoobi,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Haoyi Xiong,haoyi.xiong.fr@ieee.org,95%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Yuxuan Zhang,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Qingzhong Wang,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Jiang Bian,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Yi Liu,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Yanwu Xu,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Dejing Dou,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Tianpeng Deng,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Yanqi Huang,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Guoqiang Han,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Zhenwei Shi,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Jiatai Lin,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Qi Dou,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Zaiyi Liu,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Xiao-jing Guo,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,C. L. Philip Chen,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Chu Han,,0%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Hans Wernher Van De Venn,vhns@zhaw.ch,60%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Charith Munasinghe,,0%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Fatemeh Mohammadi Amin,,0%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Davide Scaramuzza,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Torsten Schlett,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Sebastian Schachner,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Christian Rathgeb,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Juan Tapia,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Christoph Busch,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Vivien Zahs,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Katharina Anders,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Julia Kohns,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Alexander Stark,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Bernhard Höfle,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Arindam Das,firstname.lastname@ul.ie,70%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Sudip Das,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Ganesh Sistu,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Jonathan Horgan,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Ujjwal Bhattacharya,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Edward Jones,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Martin Glavin,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Ciarán Eising,,0%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Yueyang Teng,tengyy@bime.neu.edu.cn,78%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Hengzhi Xue,,0%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Qingqing Fang,,0%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Yudong Yao,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Sungho Suh,sungho.suh@dfki.de,95%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Jwalin Bhatt,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Yaroslav Zharov,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Tilo Baumbach,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Vincent Heuveline,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Paul Lukowicz,,0%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Yanming Guo,guoyanming@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Wei Chen,weichen@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Yu Liu,liuyu8824@dlut.edu.cn,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Cunjuan Zhu,Zhucunjuan@163.com,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Qi Jia,jiaqi@dlut.edu.cn,95%
https://arxiv.org/pdf/2303.12697.pdf,Visual motion analysis of the player's finger,Marco Costanzo,marco.costanzo@mail.polimi.it,95%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Bo Li,libonchu@outlook.com,95%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Bin Liu,nyliubin@nchu.edu.cn,95%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Xiaolin Wei,,0%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Junjie Cao,,0%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Yu-kun Lai,,0%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Simone Saitta,simone.saitta@polimi.it,95%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Marcello Carioni,m.c.carioni@utwente.nl,82%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Subhadip Mukherjee,,0%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Carola-bibiane Schönlieb,,0%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Alberto Redaelli,,0%
https://arxiv.org/pdf/2302.12505.pdf,Spatial Bias for Attention-free Non-local Neural Networks,Junhyung Go,,0%
https://arxiv.org/pdf/2302.12505.pdf,Spatial Bias for Attention-free Non-local Neural Networks,Jongbin Ryu,,0%
https://arxiv.org/pdf/2302.12495.pdf,Data fusion of satellite imagery for generation of daily cloud free images at high resolution level,Petro Martyniuk,petro.martyniyk@eosda.com,85%
https://arxiv.org/pdf/2302.12495.pdf,Data fusion of satellite imagery for generation of daily cloud free images at high resolution level,Natalya Ivanchuk,natalya.ivanchuk@eosda.com,95%
https://arxiv.org/pdf/2302.12495.pdf,Data fusion of satellite imagery for generation of daily cloud free images at high resolution level,Peter Kogut,peter.kogut@eosda.com,95%
https://arxiv.org/pdf/2302.12491.pdf,Joint Learning of Blind Super-Resolution and Crack Segmentation for Realistic Degraded Images,Yuki Kondo,kondo@toyota-ti.ac.jp,78%
https://arxiv.org/pdf/2302.12491.pdf,Joint Learning of Blind Super-Resolution and Crack Segmentation for Realistic Degraded Images,Norimichi Ukita,ukita@toyota-ti.ac.jp,78%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Shumpei Takezaki,,0%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Kiyohito Tanaka,,0%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Seiichi Uchida,,0%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Takeaki Kadota,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Liangqi Zhang,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Yihao Luo,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Xiang Cao,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Haibo Shen,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Tianjiang Wang,,0%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Junghyo Jo,jojunghyo@snu.ac.kr,95%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Youngjung Uh,yj.uh@yonsei.ac.kr,82%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Mingi Kwon,kwonmingi@yonsei.ac.kr,95%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Yong-hyun Park,,0%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Shancong Mou,shancong.mou@gatech.edu,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Ping Huang,huang ping@apple.com,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Jiulong Shan,jlshan@apple.com,82%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Haoping Bai,haoping bai@apple.com,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Xiaoyi Gu,xiaoyigu@gatech.edu,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Jianjun Shi,jianjun.shi@isye.gatech.edu,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Meng Cao,mengcao@apple.com,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Wei Li,liwei089@ieee.org,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Junchuan Yu,yujunchuan@mail.cgs.gov.cn,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Zili Lu,luzili0705@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Yuexing Peng,yxpeng@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Daqing Ge,gedaqing@mail.cgs.gov.cn,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Wei Xiang,w.xiang@latrobe.edu.au,82%
https://arxiv.org/pdf/2302.12416.pdf,A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data,Hayat Rajani,hayat.rajani@udg.edu,95%
https://arxiv.org/pdf/2302.12416.pdf,A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data,Nuno Gracias,ngracias@silver.udg.edu,82%
https://arxiv.org/pdf/2302.12416.pdf,A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data,Rafael Garcia,rafael.garcia@udg.edu,95%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Yaofo Chen,sensc@mail.scut.edu.cn,55%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Mingkui Tan,mingkuitan@scut.edu.cn,95%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Shuaicheng Niu,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Jiaxiang Wu,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Yifan Zhang,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Zhiquan Wen,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Peilin Zhao,,0%
https://arxiv.org/pdf/2302.12393.pdf,Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics,Zhou Wang,zhou.wang@uwaterloo.ca,95%
https://arxiv.org/pdf/2302.12393.pdf,Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics,Wei Zhou,wei.zhou@uwaterloo.ca,95%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Debasmit Das,debadas@qti.qualcomm.com,82%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Hyojin Park,hyojinp@qti.qualcomm.com,85%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Fatih Porikli,fporikli@qti.qualcomm.com,82%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Risheek Garrepalli,rgarrepa@qti.qualcomm.com,90%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Shubhankar Borse,sborse@qti.qualcomm.com,82%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Kambiz Azarian,kambiza@qti.qualcomm.com,85%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Hong Cai,hongcai@qti.qualcomm.com,95%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Junhuan Yang,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Yi Sheng,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Yuzhou Zhang,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Weiwen Jiang,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Lei Yang,,0%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Ryan Goldhahn,goldhahn1@llnl.gov,78%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Bhavya Kailkhura,kailkhura1@llnl.gov,78%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Yize Li,li.yize@northeastern.edu,95%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Xue Lin,xue.lin@northeastern.edu,95%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Pu Zhao,p.zhao@northeastern.edu,82%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Martin Claus,mclaus@geomar.de,82%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Willi Rath,wrath@geomar.de,82%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Peer Kröger,pkr@informatik.uni-kiel.de,90%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Marco Landt-hayen,mlandt-hayen@geomar.de,82%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Avinash C. Kak,kak@purdue.edu,78%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Rahul Deshmukh,deshmuk5@purdue.edu,55%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Constantine J. Roros,croros@purdue.edu,82%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Amith Kashyap,kashyap9@purdue.edu,78%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Shariq Farooq Bhat,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Reiner Birkl,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Diana Wofk,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Peter Wonka,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Matthias Müller,,0%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Yuzhe Yang,yuzhe@mit.edu,85%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Haoran Zhang,,0%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Dina Katabi,,0%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Marzyeh Ghassemi,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Zhixiang Wang,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Yu-lun Liu,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Jia-bin Huang,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Shin'ichi Satoh,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Sizhuo Ma,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Gurunandan Krishnan,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Jian Wang,,0%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Muzammal Naseer,muzammal.naseer@alumni.anu.edu.au,95%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Ahmad Mahmood,,0%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Salman Khan,,0%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Fahad Khan,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Zhiding Yu,zhidingy@nvidia.com,85%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Yiming Li,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Christopher Choy,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Chaowei Xiao,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Jose M. Alvarez,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Sanja Fidler,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Chen Feng,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Anima Anandkumar,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Christian Reiser,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Richard Szeliski,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Dor Verbin,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Pratul P. Srinivasan,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Ben Mildenhall,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Andreas Geiger,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Jonathan T. Barron,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Peter Hedman,,0%
https://arxiv.org/pdf/2302.12248.pdf,Learning Visual Representations via Language-Guided Sampling,Justin Johnson,justincj@umich.edu,85%
https://arxiv.org/pdf/2302.12248.pdf,Learning Visual Representations via Language-Guided Sampling,Mohamed El Banani,mbanani@umich.edu,82%
https://arxiv.org/pdf/2302.12248.pdf,Learning Visual Representations via Language-Guided Sampling,Karan Desai,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Paul Pu Liang,pliang@cs.cmu.edu,82%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Yun Cheng,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Xiang Fan,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Chun Kai Ling,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Suzanne Nie,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Richard Chen,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Zihao Deng,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Nicholas Allen,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Randy Auerbach,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Faisal Mahmood,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Ruslan Salakhutdinov,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Louis-philippe Morency,,0%
https://arxiv.org/pdf/2302.12245.pdf,Set Features for Fine-grained Anomaly Detection,Niv Cohen,nivc@cs.huji.ac.il,85%
https://arxiv.org/pdf/2302.12245.pdf,Set Features for Fine-grained Anomaly Detection,Issar Tzachor,,0%
https://arxiv.org/pdf/2302.12245.pdf,Set Features for Fine-grained Anomaly Detection,Yedid Hoshen,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Mengde Xu,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Zheng Zhang,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Fangyun Wei,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Han Hu,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Xiang Bai,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Chen Geng,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Sida Peng,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Zhen Xu,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Hujun Bao,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Xiaowei Zhou,,0%
https://arxiv.org/pdf/2302.12231.pdf,DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models,Jamie Wynn,,0%
https://arxiv.org/pdf/2302.12231.pdf,DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models,Daniyar Turmukhambetov,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Rinon Gal,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Moab Arar,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Yuval Atzmon,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Amit H. Bermano,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Gal Chechik,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Kimin Lee,kiminl@google.com,85%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Hao Liu,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Moonkyung Ryu,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Olivia Watkins,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Yuqing Du,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Craig Boutilier,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Pieter Abbeel,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Mohammad Ghavamzadeh,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Shixiang Shane Gu,,0%
https://arxiv.org/pdf/2302.12189.pdf,"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales",Michele Cafagna,michele.cafagna@um.edu.mt,95%
https://arxiv.org/pdf/2302.12189.pdf,"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales",Albert Gatt,a.gatt@uu.nl,82%
https://arxiv.org/pdf/2302.12189.pdf,"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales",Kees Van Deemter,c.j.vandeemter@uu.nl,78%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Erkang Chen,ekchen@jmu.edu.cn,82%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Jingxia Jiang,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Jinbin Bai,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Yun Liu,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Junjie Yin,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Sixiang Chen,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Tian Ye,,0%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Leonard Sunwoo,leonard.sunwoo@gmail.com,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Jin-hwa Kim,j1nhwa.kim@navercorp.com,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Edward Choi,edwardchoi@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Da Young Lee,dyan.lee717@gmail.com,82%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Wonjae Kim,wonjae.kim@navercorp.com,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Tackeun Kim,tackeun.kim@snu.ac.kr,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Hyungyung Lee,,0%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Jihang Kim,,0%
https://arxiv.org/pdf/2302.12084.pdf,Dermatological Diagnosis Explainability Benchmark for Convolutional Neural Networks,Raluca Jalaboi,,0%
https://arxiv.org/pdf/2302.12084.pdf,Dermatological Diagnosis Explainability Benchmark for Convolutional Neural Networks,Ole Winther,,0%
https://arxiv.org/pdf/2302.12084.pdf,Dermatological Diagnosis Explainability Benchmark for Convolutional Neural Networks,Alfiia Galimzianova,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Roni Paiss,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Ariel Ephrat,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Omer Tov,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Shiran Zada,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Inbar Mosseri,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Michal Irani,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Tali Dekel,,0%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Liguo Zhou,liguo.zhou@tum.de,95%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Hanzhen Zhang,,0%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Ruining Wang,,0%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Alois Knoll,,0%
https://arxiv.org/pdf/2302.12047.pdf,Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach,Da Li,dali.academic@gmail.com,95%
https://arxiv.org/pdf/2302.12047.pdf,Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach,Minyoung Kim,mikim21@gmail.com,82%
https://arxiv.org/pdf/2302.12047.pdf,Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach,Timothy Hospedales,,0%
https://arxiv.org/pdf/2302.11984.pdf,Unsupervised Domain Adaptation via Distilled Discriminative Clustering,Yaowei Wang,yaoweiwang@pku.edu.cn,95%
https://arxiv.org/pdf/2302.11984.pdf,Unsupervised Domain Adaptation via Distilled Discriminative Clustering,Kui Jia,kuijia@scut.edu.cn,95%
https://arxiv.org/pdf/2302.11984.pdf,Unsupervised Domain Adaptation via Distilled Discriminative Clustering,Hui Tang,eehuitang@mail.scut.edu.cn,95%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Zhenyu Wu,wuzhenyu@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Ziwei Wang,wang-zw18@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Jiwen Lu,lujiwen@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Haibin Yan,eyanhaibin@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Md Awsafur Rahman,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Bishmoy Paul,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Najibul Haque Sarker,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Zaber Ibn Abdul Hakim,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Shaikh Anowarul Fattah,,0%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Tao Li,li.tao@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Xiaolin Huang,xiaolinhuang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Sizhe Chen,sizhe.chen@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Zhengbao He,,0%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Chunqi Tian,tianchunqi@163.com,95%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Yaoru Sun,yaoru@tongji.edu.cn,85%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Maoyu Mao,maomy@tongji.edu.cn,82%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Jun Yang,junyang@tongji.edu.cn,95%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Lizhi Bai,bailizhi@tongji.edu.cn,95%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Guorun Wang,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Ling Li,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Bandara Dissanayake,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Tatsuya Omotezako,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Yunjie Zhong,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Qing Zhang,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Rizhao Cai,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Qian Zheng,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Dennis Sng,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Weisi Lin,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Yufei Wang,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Alex C Kot,,0%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Mohammad Al-sa'd,mohammad.al-sad@tuni.fi,85%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Tuomas Jalonen,tuomas.jalonen@tuni.fi,95%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Serkan Kiranyaz,mkiranyaz@qu.edu.qa,78%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Roope Mellanen,roope.mellanen@konecranes.com,95%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Moncef Gabbouj,cef.gabbouj@tuni.fi,78%
https://arxiv.org/pdf/2302.11929.pdf,A metric to compare the anatomy variation between image time series,Alphin J Thottupattu,alphinj.thottupattu@research.iiit.ac.in,95%
https://arxiv.org/pdf/2302.11929.pdf,A metric to compare the anatomy variation between image time series,Jayanthi Sivaswamy,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Mark Eastwood,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Heba Sailem,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Silviu Tudor,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Xiaohong Gao,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Judith Offman,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Emmanouil Karteris,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Angeles Montero Fernandez,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Danny Jonigk,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,William Cookson,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Miriam Moffatt,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Sanjay Popat,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Fayyaz Minhas,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Jan Lukas Robertus,,0%
https://arxiv.org/pdf/2302.11924.pdf,Crossing Points Detection in Plain Weave for Old Paintings with Deep Learning,J. J. Murillo-fuentes,murillo@us.es,90%
https://arxiv.org/pdf/2302.11924.pdf,Crossing Points Detection in Plain Weave for Old Paintings with Deep Learning,A. Delgado,,0%
https://arxiv.org/pdf/2302.11924.pdf,Crossing Points Detection in Plain Weave for Old Paintings with Deep Learning,L. Alba-carcelén,,0%
https://arxiv.org/pdf/2302.11893.pdf,A framework for benchmarking class-out-of-distribution detection and its application to ImageNet,Ran El-yaniv,rani@cs.technion.ac.il,85%
https://arxiv.org/pdf/2302.11893.pdf,A framework for benchmarking class-out-of-distribution detection and its application to ImageNet,Mohammed Dabbah,m.m.dabbah@gmail.com,82%
https://arxiv.org/pdf/2302.11893.pdf,A framework for benchmarking class-out-of-distribution detection and its application to ImageNet,Ido Galil,idogalil.ig@gmail.com,95%
https://arxiv.org/pdf/2302.11874.pdf,What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers,Ran El-yaniv,rani@cs.technion.ac.il,85%
https://arxiv.org/pdf/2302.11874.pdf,What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers,Mohammed Dabbah,m.m.dabbah@gmail.com,82%
https://arxiv.org/pdf/2302.11874.pdf,What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers,Ido Galil,idogalil.ig@gmail.com,95%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Lin Zhan,,0%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Jiayuan Fan,,0%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Peng Ye,,0%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Jianjian Cao,,0%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Subha Fernando,subhaf@uom.lk,85%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Amirthalingam Ramanan,a.ramanan@univ.jfn.ac.lk,82%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Janani Thangavel,jananitha@univ.jfn.ac.lk,85%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Thanikasalam Kokul,kokul@univ.jfn.ac.lk,78%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Irena Gao,irena@cs.stanford.edu,85%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Shiori Sagawa,ssagawa@cs.stanford.edu,82%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Pang Wei Koh,,0%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Tatsunori Hashimoto,,0%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Percy Liang,,0%
https://arxiv.org/pdf/2302.11850.pdf,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,Angel Villar-corrales,,0%
https://arxiv.org/pdf/2302.11850.pdf,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,Ismail Wahdan,,0%
https://arxiv.org/pdf/2302.11850.pdf,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,Sven Behnke,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Lucas Wannenmacher,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Michael Fitzke,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Diane Wilson,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Andre Dourson,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Chongyi Li,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Chun-le Guo,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Man Zhou,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Zhexin Liang,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Shangchen Zhou,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Ruicheng Feng,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Chen Change Loy,,0%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Jifeng Shen,shenjifeng@ujs.edu.cn,95%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Jianhua Xu,xujianhua@njnu.edu.cn,95%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Wankou Yang,wkyang@seu.edu.cn,82%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Jun Li,lijuncst@njnu.edu.cn,95%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Guangtao Wang,,0%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Zhijian Wu,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Gerard Maggiolino,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Adnan Ahmad,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Jinkun Cao,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Kris Kitani,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Kun Yang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Jing Liu,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Dingkang Yang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Hanqi Wang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Peng Sun,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Yanni Zhang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Yan Liu,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Liang Song,,0%
https://arxiv.org/pdf/2302.11806.pdf,PLU-Net: Extraction of multi-scale feature fusion,Weihu Song,weihusong@buaa.edu.cn,95%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Zhiqi Yu,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Jingjing Li,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Zhekai Du,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Lei Zhu,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Heng Tao Shen,,0%
https://arxiv.org/pdf/2302.11802.pdf,Patch Network for medical image Segmentation,Weihu Song,weihusong@buaa.edu.cn,95%
https://arxiv.org/pdf/2302.11802.pdf,Patch Network for medical image Segmentation,Heng Yu,,0%
https://arxiv.org/pdf/2302.11802.pdf,Patch Network for medical image Segmentation,Jianhua Wu,,0%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Nisha Huang,huangnisha2021@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Fan Tang,tfan.108@gmail.com,85%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Weiming Dong,weiming.dong@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Changsheng Xu,csxu@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Tong-yee Lee,tonylee@mail.ncku.edu.tw,82%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Huazhu Fu,hzfu@ieee.org,82%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Erjian Guo,eguo9622@uni.sydney.edu.au,82%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Dong Xu,dongxu@hku.hk,95%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Luping Zhou,luping.zhou@sydney.edu.au,95%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Nadeem Atif,atif176102103@iitg.ac.in,78%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Debajit Sarma,s.debajit@iitg.ac.in,85%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Saquib Mazhar,saquibmazhar@iitg.ac.in,95%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Shaik Rafi Ahamed,raﬁahamed@iitg.ac.in,78%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,M. K. Bhuyan,,0%
https://arxiv.org/pdf/2302.11767.pdf,Adaptive Approximate Implicitization of Planar Parametric Curves via Weak Gradient Constraints,Minghao Guo,mhguo@ccut.edu.cn,82%
https://arxiv.org/pdf/2302.11767.pdf,Adaptive Approximate Implicitization of Planar Parametric Curves via Weak Gradient Constraints,Yan Gao,,0%
https://arxiv.org/pdf/2302.11767.pdf,Adaptive Approximate Implicitization of Planar Parametric Curves via Weak Gradient Constraints,Zheng Pan,,0%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Liyan Ma,liyanma@shu.edu.cn,95%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Zhenglin Li,zhenglin li@shu.edu.cn,95%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Jinan Yu,,0%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Yan Peng,,0%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Shaorong Xie,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Huaqi Tao,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Bingxi Liu,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Jinqiang Cui,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Hong Zhang,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Yang Chen,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Hexiang Hu,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Yi Luan,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Haitian Sun,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Soravit Changpinyo,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Alan Ritter,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Ming-wei Chang,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Pranav Aggarwal,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Hareesh Ravi,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Naveen Marri,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Sachin Kelkar,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Fengbin Chen,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Vinh Khuc,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Midhun Harikumar,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Ritiz Tambi,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Sudharshan Reddy Kakumanu,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Purvak Lapsiya,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Alvin Ghouas,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Sarah Saber,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Malavika Ramprasad,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Baldo Faieta,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Ajinkya Kale,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Sihan Xu,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Zelong Jiang,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Ruisi Liu,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Kaikai Yang,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Zhijie Huang,,0%
https://arxiv.org/pdf/2302.11703.pdf,fAIlureNotes: Supporting Designers in Understanding the Limits of AI Models for Computer Vision Tasks,Steven Moore,steven.moore@tum.de,95%
https://arxiv.org/pdf/2302.11703.pdf,fAIlureNotes: Supporting Designers in Understanding the Limits of AI Models for Computer Vision Tasks,Q. Vera Liao,veraliao@microsoft.com,78%
https://arxiv.org/pdf/2302.11703.pdf,fAIlureNotes: Supporting Designers in Understanding the Limits of AI Models for Computer Vision Tasks,Hariharan Subramonyam,harihars@stanford.edu,60%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Yi Ru Wang,yiruwang@cs.washington.edu,95%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Yuchi Zhao,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Haoping Xu,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Saggi Eppel,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Alan Aspuru-guzik,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Florian Shkurti,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Animesh Garg,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Chen Guo,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Tianjian Jiang,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Xu Chen,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Jie Song,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Otmar Hilliges,,0%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Adriano Koshiyama,adriano.koshiyama@holisticai.com,95%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Sara Zannone,sara.zannone@holisticai.com,95%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Cristian Muñoz,cristian.munoz@holisticai.com,95%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Umar Mohammed,,0%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Ibrahim Elwarfalli,ieelwarfalli@mix.wvu.edu,82%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Md Shamimul Islam,shamimul435@gmail.com,75%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,A. J. M. Akhtarujjaman Joha,ajmjohamiu@gmail.com,82%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Sohaib Abdullah,sohaib.abdullah2010@gmail.com,95%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Md Nur Hossain,,0%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Md Mahedi Hasan,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Chaoyi Wu,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Xiaoman Zhang,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Yanfeng Wang,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Ya Zhang,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Weidi Xie,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Yilun Du,yilundu@mit.edu,95%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Conor Durkan,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Robin Strudel,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Joshua B. Tenenbaum,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Sander Dieleman,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Rob Fergus,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Jascha Sohl-dickstein,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Arnaud Doucet,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Will Grathwohl,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Fei Xia,xiafei@google.com,95%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Tianhe Yu,tianheyu@google.com,95%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Ted Xiao,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Austin Stone,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Jonathan Tompson,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Anthony Brohan,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Su Wang,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Jaspiar Singh,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Clayton Tan,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Dee M,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Jodilyn Peralta,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Brian Ichter,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Karol Hausman,,0%
https://arxiv.org/pdf/2302.11527.pdf,A study on the invariance in security whatever the dimension of images for the steganalysis by deep-learning,Kévin Planolles,,0%
https://arxiv.org/pdf/2302.11527.pdf,A study on the invariance in security whatever the dimension of images for the steganalysis by deep-learning,Marc Chaumont,,0%
https://arxiv.org/pdf/2302.11527.pdf,A study on the invariance in security whatever the dimension of images for the steganalysis by deep-learning,Frédéric Comby,,0%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Subir Kumar Saha,saha@mech.iitd.ac.in,82%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Deepak Raina,deepak.raina@mech.iitd.ac.in,95%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Kashish Verma,vermakashish888@gmail.com,95%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Sh Chandrashekhara,,0%
https://arxiv.org/pdf/2302.11522.pdf,Evaluation of Extra Pixel Interpolation with Mask Processing for Medical Image Segmentation with Deep Learning,Olivier Rukundo,,0%
https://arxiv.org/pdf/2302.11517.pdf,A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection,Wei Tang,,0%
https://arxiv.org/pdf/2302.11517.pdf,A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection,Kangning Cui,,0%
https://arxiv.org/pdf/2302.11517.pdf,A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection,Raymond H. Chan,,0%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Vishwa S. Parekh,vparekh@som.umaryland.edu,82%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Samson Zhou,samsonzhou@gmail.com,95%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Michael A. Jacobs,Michael.A.Jacobs@uth.tmc.edu,95%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Guangyao Zheng,,0%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Vladimir Braverman,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Pranav Kadam,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Hardik Prajapati,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Min Zhang,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Jintang Xue,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Shan Liu,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,C. -c. Jay Kuo,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Ngoc Long Nguyen,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Jérémy Anger,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Lara Raad,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Bruno Galerne,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Gabriele Facciolo,,0%
https://arxiv.org/pdf/2302.11488.pdf,"Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers",Pranav Jeevan,,0%
https://arxiv.org/pdf/2302.11488.pdf,"Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers",Nikhil Cherian Kurian,,0%
https://arxiv.org/pdf/2302.11488.pdf,"Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers",Amit Sethi,,0%
https://arxiv.org/pdf/2302.11481.pdf,Transformer-Based Sensor Fusion for Autonomous Driving: A Survey,Apoorv Singh,,0%
https://arxiv.org/pdf/2302.11472.pdf,Distilling Calibrated Student from an Uncalibrated Teacher,Ishan Mishra,mishra.10@iitj.ac.in,78%
https://arxiv.org/pdf/2302.11472.pdf,Distilling Calibrated Student from an Uncalibrated Teacher,Deepak Mishra,dmishra@iitj.ac.in,82%
https://arxiv.org/pdf/2302.11472.pdf,Distilling Calibrated Student from an Uncalibrated Teacher,Sethu Vamsi Krishna,,0%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Linqi Song,linqi.song@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Shiqi Wang,shiqwang@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Hanwei Zhu,hanwei.zhu@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Lingyu Zhu,lingyzhu-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Wenhan Yang,yangwh@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Baoliang Chen,blchen6-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Wanli Ouyang,wanli.ouyang@sydney.edu.au,95%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Haiyang Yang,hyyang@smail.nju.edu.cn,82%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Donglian Qi,qidl@zju.edu.cn,78%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Lei Bai,baisanshi@gmail.com,78%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Rui Zhao,zhaorui@sensetime.com,95%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Shixiang Tang,stan3906@uni.sydney.edu.au,65%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Feng Zhu,zhufeng@sensetime.com,95%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Meilin Chen,merlinis@zju.edu.cn,60%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Yizhou Wang,yizhouwang@zju.edu.cn,95%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Dongheui Lee,dongheui.lee@tuwien.ac.at,95%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Manuel Stoiber,firstname.lastname@dlr.de,78%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Mariam Elsayed,,0%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Anne E. Reichert,,0%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Florian Steidle,,0%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Rudolph Triebel,,0%
https://arxiv.org/pdf/2302.11446.pdf,Singular value decomposition based matrix surgery,Sabah Jassim,sabah.jassim@buckingham.ac.uk,95%
https://arxiv.org/pdf/2302.11446.pdf,Singular value decomposition based matrix surgery,Jehan Ghafuri,,0%
https://arxiv.org/pdf/2302.11427.pdf,Enhanced Face Authentication With Separate Loss Functions,Anh-kiet Duong,,0%
https://arxiv.org/pdf/2302.11427.pdf,Enhanced Face Authentication With Separate Loss Functions,Hoang-lan Nguyen,,0%
https://arxiv.org/pdf/2302.11427.pdf,Enhanced Face Authentication With Separate Loss Functions,Toan-thinh Truong,,0%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Guanbin Li,bin@mail.sysu.edu.cn,90%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Xiang Wan,wanxiang@sribd.cn,95%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Wei Lou,weilou@link.cuhk.edu.cn,95%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Feng Gao,gaof57@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Chenghang Li,cli136@connect.hkust-gz.edu.cn,82%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Xiaoying Lou,,0%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Haofeng Li,,0%
https://arxiv.org/pdf/2302.11413.pdf,Gradient Adjusting Networks for Domain Inversion,Erez Sheffi,,0%
https://arxiv.org/pdf/2302.11413.pdf,Gradient Adjusting Networks for Domain Inversion,Michael Rotman,,0%
https://arxiv.org/pdf/2302.11413.pdf,Gradient Adjusting Networks for Domain Inversion,Lior Wolf,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Minzhou Pan,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Yi Zeng,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Lingjuan Lyu,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Xue Lin,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Ruoxi Jia,,0%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Zhenguo Li,li.zhenguo@huawei.com,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Guansong Lu,luguansong@huawei.com,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Yikai Wang,yikaiwang19@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Jianan Wang,jawang19@fudan.edu.cn,82%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Hang Xu,xu.hang@huawei.com,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Wei Zhang,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Paolo Panicucci,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Jérémy Lebreton,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Roland Brochard,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Emmanuel Zenou,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Michel Delpech,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Ahmed Khan,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Minoru Kuribayashi,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Koksheik Wong,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Vishnu Monn Baskaran,,0%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Wei Yi,kussoyi@gmail.com,78%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Haiyi Mao,maohaiyi@std.uestc.edu.cn,95%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Jinping Tang,jptang@std.uestc.edu.cn,82%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Cong Peng,,0%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Yue Liu,,0%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Hua Peng,,0%
https://arxiv.org/pdf/2302.11352.pdf,X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation,Marcel Worring,m.worring@uva.nl,82%
https://arxiv.org/pdf/2302.11352.pdf,X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation,Tom Van Sonsbeek,t.j.vansonsbeek@uva.nl,82%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Sangnie Bhardwaj,sangnie@google.com,85%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Willie Mcclinton,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Tongzhou Wang,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Guillaume Lajoie,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Chen Sun,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Phillip Isola,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Dilip Krishnan,,0%
https://arxiv.org/pdf/2302.11327.pdf,A Gradient Boosting Approach for Training Convolutional and Deep Neural Networks,Gonzalo Martínez-muñoz,gonzalo.martinez@uam.es,85%
https://arxiv.org/pdf/2302.11327.pdf,A Gradient Boosting Approach for Training Convolutional and Deep Neural Networks,Seyedsaman Emami,emami.seyedsaman@uam.es,95%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Chengxi Zeng,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Xinyu Yang,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,David Smithard,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Majid Mirmehdi,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Alberto M Gambaruto,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Tilo Burghardt,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Yibing Song,yibingsong.cv@gmail.com,95%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Hongyu Liu,hliudq@cse.ust.hk,82%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Xintong Han,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Chengbin Jin,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Lihui Qian,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Huawei Wei,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Zhe Lin,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Faqiang Wang,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Haoye Dong,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Jia Xu,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Qifeng Chen,,0%
https://arxiv.org/pdf/2302.11301.pdf,View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation,Xu Zhao,zhaoxu@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11301.pdf,View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation,Xiaoyue Wan,,0%
https://arxiv.org/pdf/2302.11301.pdf,View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation,Zhuo Chen,,0%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Gen Luo,luogen@stu.xmu.edu.cn,95%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Xiaoshuai Sun,xssun@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Yiyi Zhou,zhouyiyi@xmu.edu.cn,95%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Rongrong Ji,rrji@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Lei Jin,,0%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Jingxiang Qu,qujx@whut.edu.cn,78%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Yisheng Lv,yisheng.lv@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Fenghua Zhu,fenghua.zhu@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Yuxu Lu,yuxulu@whut.edu.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Yu Guo,yuguo@whut.edu.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Ryan Wen Liu,wenliu@whut.edu.cn,78%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Meng Liu,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Kong Aik Lee,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Longbiao Wang,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Hanyi Zhang,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Chang Zeng,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Jianwu Dang,,0%
https://arxiv.org/pdf/2302.11252.pdf,Focusing On Targets For Improving Weakly Supervised Visual Grounding,Viet-quoc Pham,,0%
https://arxiv.org/pdf/2302.11252.pdf,Focusing On Targets For Improving Weakly Supervised Visual Grounding,Nao Mishima,,0%
https://arxiv.org/pdf/2302.11244.pdf,Considering Layerwise Importance in the Lottery Ticket Hypothesis,Benjamin Vandersmissen,benjamin.vandersmissen@uantwerpen.be,95%
https://arxiv.org/pdf/2302.11244.pdf,Considering Layerwise Importance in the Lottery Ticket Hypothesis,Jose Oramas,,0%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Soravit Changpinyo,schangpi@google.com,90%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Vittorio Ferrari,vittoferrari@google.com,82%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Jordi Pont-tuset,jponttuset@google.com,65%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Radu Soricut,rsoricut@google.com,82%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Paul Voigtlaender,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Shannan Guan,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Xin Yu,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Wei Huang,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Gengfa Fang,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Haiyan Lu,,0%
https://arxiv.org/pdf/2302.11208.pdf,KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer,Norimichi Ukita,ukita@toyota-ti.ac.jp,78%
https://arxiv.org/pdf/2302.11208.pdf,KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer,Kaikai Zhao,zhaokaikai@toyota-ti.ac.jp,95%
https://arxiv.org/pdf/2302.11200.pdf,Semi-Supervised Segmentation of Multi-vendor and Multi-center Cardiac MRI using Histogram Matching,Ilkay Oksuz,oksuzilkay@itu.edu.tr,95%
https://arxiv.org/pdf/2302.11200.pdf,Semi-Supervised Segmentation of Multi-vendor and Multi-center Cardiac MRI using Histogram Matching,Mahyar Bolhassani,bolhassani19@itu.edu.tr,78%
https://arxiv.org/pdf/2302.11184.pdf,A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.11184.pdf,A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning,Jin Zhu,zhujin1121@gmail.com,95%
https://arxiv.org/pdf/2302.11184.pdf,A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning,Pietro Lio,,0%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Dejan Vucinic,dejan.vucinic@wdc.com,95%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Jaco Hofmann,jaco.hofmann@wdc.com,95%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Minghai Qin,minghai.qin@wdc.com,95%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Chao Sun,chao.sun@wdc.com,95%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Hexiang Hu,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Yi Luan,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Yang Chen,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Urvashi Khandelwal,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Mandar Joshi,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Kenton Lee,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Kristina Toutanova,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Ming-wei Chang,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Zhenghua Xu,zhenghua.xu@hebut.edu.cn,95%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Hexiang Zhang,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Dan Yao,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Shuo Zhang,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Junyang Chen,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Thomas Lukasiewicz,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Haiyu Wu,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Grace Bezold,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Aman Bhatta,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Kevin W. Bowyer,,0%
https://arxiv.org/pdf/2302.11097.pdf,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,Fei Yin,fyin@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.11097.pdf,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,Ming-liang Zhang,zhangmingliang2018@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11097.pdf,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,Cheng-lin Liu,liucl@nlpr.ia.ac.cn,78%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Quanjin Liu,liuquanjin@aqnu.edu.cn,95%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Yu Ren,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Guoli Wang,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Pingping Wang,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Kunmeng Liu,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Hongfu Sun,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Xiang Li,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Benzheng Wei,,0%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Yifei Zhou,zhou@berkeley.edu,78%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Ser-nam Lim,sernam@ucf.edu,85%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Juntao Ren,,0%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Fengyu Li,,0%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Ramin Zabih,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Guoli Wang,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Pingping Wang,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Jinyu Cong,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Kunmeng Liu,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Benzheng Wei,,0%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Rob Timpe,rob@xwing.com,85%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Maxime Gariel,maxime@xwing.com,85%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Evan Wilson,evan@xwing.com,85%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Brian Shimanuki,brian@xwing.com,85%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Neil Chulpongsatorn,thobthai.chulpongsat@ucalgary.ca,75%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Aman Parnami,aman@iiitd.ac.in,85%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Ryo Suzuki,ryo.suzuki@ucalgary.ca,95%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Kyzyl Monteiro,kyzyl17296@iiitd.ac.in,85%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Ritik Vatsal,ritik19321@iiitd.ac.in,85%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Labib Ahmed Siddique,1labib.ahmed.siddique@g.bracu.ac.bd,95%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Tanvir Rahman,5rtanvir@udel.edu,85%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Rabita Junhai,2rabita.junhai@g.bracu.ac.bd,95%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Tanzim Reza,3tanzim.reza@bracu.ac.bd,95%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Salman Sayeed Khan,4salman.sayeed@bracu.ac.bd,85%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Ramneet Kaur,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Xiayan Ji,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Souradeep Dutta,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Michele Caprio,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Yahan Yang,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Elena Bernardis,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Oleg Sokolsky,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Insup Lee,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Uddeshya Upadhyay,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Jae Myung Kim,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Cordelia Schmidt,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Bernhard Schölkopf,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Zeynep Akata,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Nikita Morozov,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Denis Rakitin,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Oleg Desheulin,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Dmitry Vetrov,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Kirill Struminsky,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Mahdi Ghafourian,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Bilgesu Sumer,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Ruben Vera-rodriguez,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Julian Fierrez,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Ruben Tolosana,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Aythami Moralez,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Els Kindt,,0%
https://arxiv.org/pdf/2302.10873.pdf,Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,Ioannis Karamouzas,ioannis@clemson.edu,85%
https://arxiv.org/pdf/2302.10873.pdf,Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,Jean-bernard Hayet,jbhayet@cimat.mx,82%
https://arxiv.org/pdf/2302.10873.pdf,Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,Pei Xu,peix@clemson.edu,85%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Rafsanjany Kushol,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Collin C. Luk,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Avyarthana Dey,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Michael Benatar,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Hannah Briemberg,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Annie Dionne,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Nicolas Dupré,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Richard Frayne,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Angela Genge,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Summer Gibson,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Simon J. Graham,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Lawrence Korngut,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Peter Seres,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Robert C. Welsh,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Alan Wilman,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Lorne Zinman,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Sanjay Kalra,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Yee-hong Yang,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Peichao Li,peichao.2.li@kcl.ac.uk,95%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Muhammad Asad,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Conor Horgan,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Oscar Maccormac,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Jonathan Shapey,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Tom Vercauteren,,0%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Paolo Fiorini,paolo.ﬁorini@univr.it,95%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Nicolas Padoy,npadoy@unistra.fr,82%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Pietro Mascagni,p.mascagni@unistra.fr,82%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Tong Yu,tyu@unistra.fr,82%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Diego Dall'alba,diego.dallalba@univr.it,85%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Sanat Ramesh,sanat.ramesh@univr.it,95%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Cristians Gonzalez,,0%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Didier Mutter,,0%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Jacques Marescaux,,0%
https://arxiv.org/pdf/2302.10820.pdf,Device Tuning for Multi-Task Large Model,Penghao Jiang,,0%
https://arxiv.org/pdf/2302.10820.pdf,Device Tuning for Multi-Task Large Model,Xuanchen Hou,,0%
https://arxiv.org/pdf/2302.10820.pdf,Device Tuning for Multi-Task Large Model,Yinsi Zhou,,0%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Jiahao Zhu,jiahaozhu@hust.edu.cn,95%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Zeyu Xiong,zeyuxiong@hust.edu.cn,95%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Daizong Liu,dzliu@stu.pku.edu.cn,82%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Pan Zhou,panzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2302.10808.pdf,Bokeh Rendering Based on Adaptive Depth Calibration Network,Yuhan Dong,dongyuhan@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.10808.pdf,Bokeh Rendering Based on Adaptive Depth Calibration Network,Lei Zhou,leizhou.astro@gmail.com,95%
https://arxiv.org/pdf/2302.10808.pdf,Bokeh Rendering Based on Adaptive Depth Calibration Network,Lu Liu,l-liu20@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Zhengyuan Yang,zhengyang@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Nan Duan,nanduan@microsoft.com,95%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Lijuan Wang,lijuanw@microsoft.com,85%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Minheng Ni,t-mni@microsoft.com,78%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Linjie Li,Lindsey.Li@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Shengming Yin,v-sheyin@microsoft.com,78%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Zicheng Liu,zliu@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Jianfeng Wang,jianfw@microsoft.com,81%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Chenfei Wu,chewu@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Fan Yang,fanyang@microsoft.com,95%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Xiaodong Wang,,0%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Yuejian Fang,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Timilehin B. Aderinola,timi.aderinola@insight-centre.org,82%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Hananeh Younesian,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Darragh Whelan,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Brian Caulfield,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Georgiana Ifrim,,0%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Víctor M. Brea,victor.brea@usc.es,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Saqib Nazir,saqib.nazir@upb.ro,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Manuel Mucientes,manuel.mucientes@usc.es,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Lorenzo Vaquero,lorenzo.vaquero.otal@usc.es,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Daniela Coltuc,daniela.coltuc@upb.ro,95%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Leonardo Rossi,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Vittorio Bernuzzi,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Tomaso Fontanini,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Massimo Bertozzi,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Andrea Prati,,0%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Jingxing Zhou,jingxing.zhou@porsche-engineering.de,95%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Niket Ahuja,niket.ahuja@porsche-engineering.de,95%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Tobias Kalb,tobias.kalb@porsche-engineering.de,95%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Jürgen Beyerer,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Huaping Liu,hpliu@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Yuhong Deng,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Xiaofeng Guo,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Yixuan Wei,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Kai Lu,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Bin Fang,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Di Guo,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Fuchun Sun,,0%
https://arxiv.org/pdf/2302.10698.pdf,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,Orcun Goksel,orcun.goksel@it.uu.se,95%
https://arxiv.org/pdf/2302.10698.pdf,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,Lin Zhang,,0%
https://arxiv.org/pdf/2302.10698.pdf,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,Tiziano Portenier,,0%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Peng Chen,chenpeng@zjut.edu.cn,95%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Binwei Xu,xubinwei@zjut.edu.cn,95%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Weihua Gong,whgong@zjut.edu.cn,82%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Ronghua Liang,rhliang@zjut.edu.cn,82%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Haoran Liang,haoran@zjut.edu.cn,85%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Zhijie Deng,zhijied@sjtu.edu.cn,85%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Tianyu Pang,tianyupang@sea.com,95%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Shuicheng Yan,yansc@sea.com,78%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Chao Du,duchao@sea.com,95%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Cheng Lu,lucheng.lc15@gmail.com,95%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Min Lin,linmin@sea.com,95%
