Paper URL,Title,Author,Email,Confidence
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Lei Li,LI-LEI@USTC.EDU,95%
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Zhongfeng Kang,KANGZHF@GMAIL.COM,78%
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Wenhan Zhang,WENHANZHANG430@GMAIL.COM,95%
https://arxiv.org/pdf/2302.00123.pdf,Design and Implementation of A Soccer Ball Detection System with Multiple Cameras,Tianfang Zhang,,0%
https://arxiv.org/pdf/2302.00117.pdf,Real Estate Property Valuation using Self-Supervised Vision Transformers,Mahdieh Yazdani,,0%
https://arxiv.org/pdf/2302.00117.pdf,Real Estate Property Valuation using Self-Supervised Vision Transformers,Maziar Raissi,,0%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Ching-yao Chuang,cychuang@mit.edu,82%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Antonio Torralba,torralba@mit.edu,78%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Yuanzhen Li,yzli@google.com,82%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Varun Jampani,varunjampani@google.com,95%
https://arxiv.org/pdf/2302.00070.pdf,Debiasing Vision-Language Models via Biased Prompts,Stefanie Jegelka,,0%
https://arxiv.org/pdf/2302.00059.pdf,NASiam: Efficient Representation Learning using Neural Architecture Search for Siamese Networks,Hedi Tabia,hedi.tabia@univ-evry.fr,95%
https://arxiv.org/pdf/2302.00059.pdf,NASiam: Efficient Representation Learning using Neural Architecture Search for Siamese Networks,Hichem Arioui,hichem.arioui@univ-evry.fr,95%
https://arxiv.org/pdf/2302.00059.pdf,NASiam: Efficient Representation Learning using Neural Architecture Search for Siamese Networks,Alexandre Heuillet,alexandre.heuillet@univ-evry.fr,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Hongbin Zha,zha@cis.pku.edu.cn,78%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Hao Zhao,hao.zhao@intel.com,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Guyue Zhou,zhouguyue@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Pengfei Li,li-pf22@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Yurong Chen,yurong.chen@intel.com,95%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Xiaoxue Chen,chenxx21@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Huan-ang Gao,,0%
https://arxiv.org/pdf/2301.13865.pdf,From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds,Beiwen Tian,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,N. Joseph Tatro,joseph.tatro@str.us,78%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Brandon B. May,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Dylan Walker,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Piyush Kumar,,0%
https://arxiv.org/pdf/2301.13862.pdf,Salient Conditional Diffusion for Defending Against Backdoor Attacks,Nathan Shnidman,,0%
https://arxiv.org/pdf/2301.13823.pdf,Grounding Language Models to Images for Multimodal Inputs and Outputs,Jing Yu Koh,jingyuk@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.13823.pdf,Grounding Language Models to Images for Multimodal Inputs and Outputs,Ruslan Salakhutdinov,,0%
https://arxiv.org/pdf/2301.13823.pdf,Grounding Language Models to Images for Multimodal Inputs and Outputs,Daniel Fried,,0%
https://arxiv.org/pdf/2301.13838.pdf,Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression,Martha Larson,m.larson@cs.ru.nl,82%
https://arxiv.org/pdf/2301.13838.pdf,Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression,Zhuoran Liu,z.liu@cs.ru.nl,82%
https://arxiv.org/pdf/2301.13838.pdf,Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression,Zhengyu Zhao,zhengyu.zhao@cispa.de,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Yael Vinker,yael.vinker@mail.huji.ac.il,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Hila Chefer,hilach70@gmail.com,85%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Lior Wolf,liorwolf@gmail.com,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Yuval Alaluf,yuvalalaluf@gmail.com,95%
https://arxiv.org/pdf/2301.13826.pdf,Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Deepak K. Gupta,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Gowreesh Mago,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Arnav Chavan,,0%
https://arxiv.org/pdf/2301.13817.pdf,Patch Gradient Descent: Training Neural Networks on Very Large Images,Dilip K. Prasad,,0%
https://arxiv.org/pdf/2301.13809.pdf,A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control,Mahesh Raveendranatha Panicker,mahesh@iitpkd.ac.in,85%
https://arxiv.org/pdf/2301.13809.pdf,A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control,Ayush Singh,,0%
https://arxiv.org/pdf/2301.13809.pdf,A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control,Pisharody Harikrishnan Gopalkrishnan,,0%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Dongxiao Zhu,dzhu@wayne.edu,82%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Chengyin Li,cyli@wayne.edu,82%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Prashant Khanduri,khanduri.prashant@wayne.edu,95%
https://arxiv.org/pdf/2301.13803.pdf,Fairness-aware Vision Transformer via Debiased Self-Attention,Yao Qiang,yao@wayne.edu,85%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Daniel Capellán-martín,daniel.capellan@upm.es,85%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Elisa López-varela,mj.ledesma@upm.es,60%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Juan J. Gómez-valverde,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Ramon Sanchez-jacob,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,David Bermejo-peláez,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Lara García-delgado,,0%
https://arxiv.org/pdf/2301.13786.pdf,Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis,Maria J. Ledesma-carbayo,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Zihao Wang,zihao.wang@ieee.org,95%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Yingyu Yang,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Maxime Sermesant,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Hervé Delingette,,0%
https://arxiv.org/pdf/2301.13743.pdf,Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion,Ona Wu,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Chun Yuan,yuanc@sz.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Dachuan Shi,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Chaofan Tao,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Ying Jin,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Zhendong Yang,,0%
https://arxiv.org/pdf/2301.13741.pdf,UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers,Jiaqi Wang,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Samuel Hurault,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Antonin Chambolle,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Arthur Leclaire,,0%
https://arxiv.org/pdf/2301.13731.pdf,A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser,Nicolas Papadakis,,0%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Tao Yang,wang-yuwang@mail.tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Nanning Zheng,nnzheng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Yuwang Wang,,0%
https://arxiv.org/pdf/2301.13721.pdf,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,Yan Lv,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Eva Schnider,eva.schnider@unibas.ch,95%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Julia Wolleb,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Antal Huck,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Mireille Toranelli,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Georg Rauter,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Magdalena Müller-gerbl,,0%
https://arxiv.org/pdf/2301.13674.pdf,Improved distinct bone segmentation in upper-body CT through multi-resolution networks,Philippe C. Cattin,,0%
https://arxiv.org/pdf/2301.13670.pdf,What Makes Good Examples for Visual In-Context Learning?,Ziwei Liu,ziwei.liu@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.13670.pdf,What Makes Good Examples for Visual In-Context Learning?,Yuanhan Zhang,,0%
https://arxiv.org/pdf/2301.13670.pdf,What Makes Good Examples for Visual In-Context Learning?,Kaiyang Zhou,,0%
https://arxiv.org/pdf/2301.13659.pdf,Spyker: High-performance Library for Spiking Deep Neural Networks,Shahriar Rezghi Shirsavar,shahriar.rezghi@ut.ac.ir,85%
https://arxiv.org/pdf/2301.13659.pdf,Spyker: High-performance Library for Spiking Deep Neural Networks,Mohammad-reza A. Dehaqani,dehaqani@ut.ac.ir,78%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Raphael Sulzer,raphaelsulzer@gmx.de,95%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Renaud Marlet,,0%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Bruno Vallet,,0%
https://arxiv.org/pdf/2301.13656.pdf,A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds,Loic Landrieu,,0%
https://arxiv.org/pdf/2301.13622.pdf,Learning Data Representations with Joint Diffusion Models,Kamil Deja,kamil.deja@pw.edu.pl,95%
https://arxiv.org/pdf/2301.13622.pdf,Learning Data Representations with Joint Diffusion Models,Tomasz Trzcinski,tomasz.trzcinski@pw.edu.pl,95%
https://arxiv.org/pdf/2301.13622.pdf,Learning Data Representations with Joint Diffusion Models,Jakub M. Tomczak,j.m.tomczak@tue.nl,82%
https://arxiv.org/pdf/2301.13592.pdf,Priors are Powerful: Improving a Transformer for Multi-camera 3D Detection with 2D Priors,Di Feng,fengdi1015@gmail.com,95%
https://arxiv.org/pdf/2301.13592.pdf,Priors are Powerful: Improving a Transformer for Multi-camera 3D Detection with 2D Priors,Francesco Ferroni,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Boris Mansencal,mediaeval.sport.task@diff.u-bordeaux.fr,70%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Pierre-etienne Martin,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Jordan Calandre,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Jenny Benois-pineau,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Renaud Péteri,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Laurent Mascarilla,,0%
https://arxiv.org/pdf/2301.13576.pdf,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,Julien Morlier,,0%
https://arxiv.org/pdf/2301.13569.pdf,NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning,Thomas Lukasiewicz,thomas.lukasiewicz@cs.ox.ac.uk,95%
https://arxiv.org/pdf/2301.13569.pdf,NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning,Xiaolin Hu,xlhu@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.13569.pdf,NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning,Jianfeng Wang,jianfeng.wang@cs.ox.ac.uk,95%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Hang Su,suhangss@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Liyuan Wang,,0%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Xingxing Zhang,,0%
https://arxiv.org/pdf/2302.00487.pdf,"A Comprehensive Survey of Continual Learning: Theory, Method and Application",Jun Zhu,,0%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Federico Tombar,tombari@in.tum.de,78%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Artem Savkin,artem.savkin@tum.de,95%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Yida Wang,,0%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Sebastian Wirkert,,0%
https://arxiv.org/pdf/2301.13558.pdf,Lidar Upsampling with Sliced Wasserstein Distance,Nassir Navab,,0%
https://arxiv.org/pdf/2301.13554.pdf,NoiseTransfer: Image Noise Generation with Contrastive Embeddings,Seunghwan Lee,seunghwanlee@hanyang.ac.kr,95%
https://arxiv.org/pdf/2301.13554.pdf,NoiseTransfer: Image Noise Generation with Contrastive Embeddings,Tae Hyun Kim,taehyunkim@hanyang.ac.kr,95%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Michał Grochowski,michal.grochowski@pg.edu.pl,82%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Zuzanna Klawikowska,zuzanna.klawikowska@pg.edu.pl,95%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Maria Ferlin,maria.ferlin@pg.edu.pl,95%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Edyta Szurowska,eszurowska@gumed.edu.pl,82%
https://arxiv.org/pdf/2301.13549.pdf,Review of methods for automatic cerebral microbleeds detection,Małgorzata Grzywińska,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Jun Li,lijuncst@njnu.edu.cn,95%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Guang Yang,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Yin Tang,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Jianhua Xu,,0%
https://arxiv.org/pdf/2301.13538.pdf,AMD: Adaptive Masked Distillation for Object Detection,Xili Wan,,0%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Ofir Lindenbaum,ofirlin@gmail.com,85%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Amit Rozner,,0%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Barak Battash,,0%
https://arxiv.org/pdf/2301.13530.pdf,Domain-Generalizable Multiple-Domain Clustering,Lior Wolf,,0%
https://arxiv.org/pdf/2301.13514.pdf,Fourier Sensitivity and Regularization of Computer Vision Models,Chuan-sheng Foo,foo_chuan_sheng@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.13514.pdf,Fourier Sensitivity and Regularization of Computer Vision Models,Kiran Krishnamachari,kirank@u.nus.edu,85%
https://arxiv.org/pdf/2301.13514.pdf,Fourier Sensitivity and Regularization of Computer Vision Models,See-kiong Ng,seekiong@nus.edu.sg,95%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Siyu Zhu,siting.zsy@alibaba-inc.com,60%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Weihao Yuan,,0%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Xiaodong Gu,,0%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Heng Li,,0%
https://arxiv.org/pdf/2301.13510.pdf,3D Former: Monocular Scene Reconstruction with 3D SDF Transformers,Zilong Dong,,0%
https://arxiv.org/pdf/2301.13504.pdf,Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease,Maha M. Alwuthaynani,maha.alwuthaynani@bristol.ac.uk,95%
https://arxiv.org/pdf/2301.13504.pdf,Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease,Zahraa S. Abdallah,zahraa.abdallah@bristol.ac.uk,95%
https://arxiv.org/pdf/2301.13504.pdf,Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease,Raul Santos-rodriguez,,0%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Zhiyuan Cheng,cheng443@purdue.edu,78%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Xiangyu Zhang,xyzhang@cs.purdue.edu,82%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Guanhong Tao,taog@purdue.edu,78%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,Dongfang Liu,dongfang.liu@rit.edu,95%
https://arxiv.org/pdf/2301.13487.pdf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,James Liang,,0%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Swagat Kumar,kumars@edgehill.ac.uk,78%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Darshita Jain,,0%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Anima Majumder,,0%
https://arxiv.org/pdf/2301.13473.pdf,CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning,Samrat Dutta,,0%
https://arxiv.org/pdf/2301.13459.pdf,Learning Generalized Hybrid Proximity Representation for Image Recognition,Anca Ralescu,ralescal@ucmail.uc.edu,65%
https://arxiv.org/pdf/2301.13459.pdf,Learning Generalized Hybrid Proximity Representation for Image Recognition,Zhiyuan Li,li3z3@mail.uc.edu,78%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Asef Islam,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Anthony Wu,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Jay Mandavilli,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Wojtek Zbijewski,,0%
https://arxiv.org/pdf/2303.08105.pdf,Image Guidance for Robot-Assisted Ankle Fracture Repair,Jeff Siewerdsen,,0%
https://arxiv.org/pdf/2301.13445.pdf,A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics,Naveed Akhtar,naveed.akhtar@uwa.edu.au,95%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Min-kook Choi,mkchoi@hutom.io,82%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Seungbum Hong,,0%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Jihun Yoon,,0%
https://arxiv.org/pdf/2301.13444.pdf,Rethinking Soft Label in Label Distribution Learning Perspective,Bogyu Park,,0%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Jinzheng He,jinzhenghe@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Zhenhui Ye,zhenhuiye@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Jinglin Liu,jinglinliu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Ziyue Jiang,jiangziyue@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Zhou Zhao,zhaozhou@zju.edu.cn,95%
https://arxiv.org/pdf/2301.13430.pdf,GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,Yi Ren,ren.yi@bytedance.com,95%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Yonggang Li,liyonggang@zjxu.edu.cn,95%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Xiangbin Zhu,zhuxb@zjnu.cn,78%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Yingjian Li,liyingjian@zjnu.edu.cn,95%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Yuqi Chen,,0%
https://arxiv.org/pdf/2301.13428.pdf,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,Haojie Fang,,0%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Jingtao Li,JingtaoLi@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Hengwei Zhao,whu_zhaohw@whu.edu.cn,78%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Yanfei Zhong,zhongyanfei@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Xinyu Wang,wangxinyu@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13422.pdf,Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,Shaoyu Wang,wangshaoyu@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jiayi Yuan,jiayiyuan@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jianjun Qian,csjqian@njust.edu.cn,78%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jun Li,junli@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Xiang Li,xiang.li.implus@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Jian Yang,jiang.hao.bo@njust.edu.cn,85%
https://arxiv.org/pdf/2301.13419.pdf,Recurrent Structure Attention Guidance for Depth Super-Resolution,Haobo Jiang,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Chong Wang,chong.wang@adelaide.edu.au,95%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Yuanhong Chen,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Yuyuan Liu,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Michael Elliott,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Chun Fung Kwok,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Carlos Pena-solorzano,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Yu Tian,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Fengbei Liu,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Helen Frazer,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Davis J. Mccarthy,,0%
https://arxiv.org/pdf/2301.13418.pdf,BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations,Gustavo Carneiro,,0%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jiayi Yuan,jiayiyuan@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jianjun Qian,csjqian@njust.edu.cn,78%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jun Li,junli@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Xiang Li,xiang.li.implus@njust.edu.cn,95%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Jian Yang,jiang.hao.bo@njust.edu.cn,85%
https://arxiv.org/pdf/2301.13416.pdf,Structure Flow-Guided Network for Real Depth Super-Resolution,Haobo Jiang,,0%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Ke Yan,kerwinyan@tencent.com,95%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Jiaming Han,hanjiaming@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Yuqiang Ren,condiren@tencent.com,78%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Gui-song Xia,guisong.xia@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13411.pdf,Few-Shot Object Detection via Variational Feature Aggregation,Jian Ding,jian.ding@whu.edu.cn,95%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Mohsen Dorodchi,mdorodch@uncc.edu,90%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Ayman Ali,,0%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Ekkasit Pinyoanuntapong,,0%
https://arxiv.org/pdf/2301.13403.pdf,A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose,Pu Wang,,0%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Bingchuan Li,libingchuan@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Qian He,heqian@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Peng Zhang,zhangpeng.ucas@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Wei Liu,liuwei.jikun@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Tianxiang Ma,matianxiang.724@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Miao Hua,huamiao@bytedance.com,95%
https://arxiv.org/pdf/2301.13402.pdf,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,Zili Yi,yizili@bytedance.com,95%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Chung-i Huang,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Wei-yu Chen,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Wei Jan Ko,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Jih-sheng Chang,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Chen-kai Sun,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Hui Hung Yu,,0%
https://arxiv.org/pdf/2301.13385.pdf,Fisheye traffic data set of point center markers,Fang-pang Lin,,0%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Minwoo Lee,minwoo.lee@uncc.edu,95%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Zhi Sun,qzhisun@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Ekkasit Pinyoanuntapong,epinyoan@uncc.edu,90%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Chen Chen,chen.chen@crcv.ucf.edu,95%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Pu Wang,pwang13@uncc.edu,82%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Ayman Ali,aali26@uncc.edu,82%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Kalvik Jakkala,kjakkala@uncc.edu,82%
https://arxiv.org/pdf/2301.13384.pdf,GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition,Qucheng Peng,qucheng.peng@knights.ucf.edu,95%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Li Yi,jli3779@uwo.ca,85%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Boyu Wang,bwang@csd.uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Pengcheng Xu,pxu67@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Ruizhi Pu,rpu2@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,A. Ian Mcleod,aimcleod@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Charles Ling,charles.ling@uwo.ca,95%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Gezheng Xu,gxu86@uwo.ca,82%
https://arxiv.org/pdf/2301.13381.pdf,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,Jiaqi Li,,0%
https://arxiv.org/pdf/2301.13376.pdf,Quantized Neural Networks for Low-Precision Accumulation with Guaranteed Overflow Avoidance,Ian Colbert,ian.colbert@amd.com,95%
https://arxiv.org/pdf/2301.13376.pdf,Quantized Neural Networks for Low-Precision Accumulation with Guaranteed Overflow Avoidance,Alessandro Pappalardo,,0%
https://arxiv.org/pdf/2301.13376.pdf,Quantized Neural Networks for Low-Precision Accumulation with Guaranteed Overflow Avoidance,Jakoba Petri-koenig,,0%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Hamed Hassani,hassani@seas.upenn.edu,82%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Xinmeng Huang,xinmengh@sas.upenn.edu,85%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Behrad Moniri,bemoniri@seas.upenn.edu,82%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Edgar Dobriban,dobriban@wharton.upenn.edu,78%
https://arxiv.org/pdf/2301.13371.pdf,Demystifying Disagreement-on-the-Line in High Dimensions,Donghwan Lee,,0%
https://arxiv.org/pdf/2301.13366.pdf,CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects,Murray Loew,loew@gwu.edu,78%
https://arxiv.org/pdf/2301.13366.pdf,CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects,Shuyue Guan,frankshuyueguan@gwu.edu,95%
https://arxiv.org/pdf/2301.13366.pdf,CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects,Ange Lou,ange.lou@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Yi Zhu,zhuyi36@huawei.com,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Jianzhuang Liu,liu.jianzhuang@huawei.com,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Pengzhen Ren,pzhren@foxmail.com,82%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Guangrun Wang,wanggrun@gmail.com,78%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Changlin Li,changlinli.ai@gmail.com,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Xiaodan Liang,xdliang328@gmail.com,82%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Xiaojun Chang,xiaojun.chang@uts.edu.au,95%
https://arxiv.org/pdf/2302.10307.pdf,ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,Hang Xu,xu.hang@huawei.com,95%
https://arxiv.org/pdf/2301.13361.pdf,Iterative Loop Method Combining Active and Semi-Supervised Learning for Domain Adaptive Semantic Segmentation,Xue Yuan,xyuan@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.13361.pdf,Iterative Loop Method Combining Active and Semi-Supervised Learning for Domain Adaptive Semantic Segmentation,Licong Guan,,0%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Mohsen Dorodchi,mdorodch@uncc.edu,90%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Ayman Ali,,0%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Ekkasit Pinyoanuntapong,,0%
https://arxiv.org/pdf/2301.13360.pdf,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),Pu Wang,,0%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Jinbao Wang,jasoncjwang@tencent.com,82%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Yaochu Jin,jinyaochu@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Jiaqi Liu,chaosliu@tencent.com,78%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Jiayi Lyu,lyujiayi21@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Feng Zheng,f.zheng@ieee.org,82%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Guoyang Xie,guoyang.xie@ieee.org,95%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Yong Liu,,0%
https://arxiv.org/pdf/2301.13359.pdf,IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,Hu Chen,huchen@scu.edu.cn,95%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,Yi Zhang,yzhang@scu.edu.cn,82%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,Wenchao Du,wenchaodu.scu@gmail.com,95%
https://arxiv.org/pdf/2301.13358.pdf,Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond,H. Yang,yanghongyu@scu.edu.cn,78%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Hugo Lemarchant,hugo@is.ids.osaka-u.ac.jp,85%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Yuta Nakashima,n-yuta@ids.osaka-u.ac.jp,85%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Hajime Nagahara,nagahara@ids.osaka-u.ac.jp,78%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Yiming Qian,yimingqian@ids.osaka-u.ac.jp,95%
https://arxiv.org/pdf/2301.13356.pdf,Inference Time Evidences of Adversarial Attacks for Forensic on Transformers,Liangzi Li,li@ids.osaka-u.ac.jp,82%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Jun Sakuma,jun@cs.tsukuba.ac.jp,85%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Rei Sato,reisato@bbo.cs.tsukuba.ac.jp,95%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Kazuto Fukuchi,fukuchi@cs.tsukuba.ac.jp,78%
https://arxiv.org/pdf/2301.13343.pdf,Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning,Youhei Akimoto,akimoto@cs.tsukuba.ac.jp,78%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,David Van Dijk,david.vandijk@yale.edu,95%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,Antonio H. De O. Fonseca,,0%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,Emanuele Zappala,,0%
https://arxiv.org/pdf/2301.13338.pdf,Continuous Spatiotemporal Transformers,Josue Ortega Caro,,0%
https://arxiv.org/pdf/2301.13335.pdf,Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Jian Zhu,jianzhu@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.13335.pdf,Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Miaojing Shi,mshi@tongji.edu.cn,82%
https://arxiv.org/pdf/2301.13335.pdf,Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Hanli Wang,hanliwang@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Deepika Bablani,deepika.bablani@ibm.com,95%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Jeffrey L. Mckinstry,,0%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Steven K. Esser,,0%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Rathinakumar Appuswamy,,0%
https://arxiv.org/pdf/2301.13330.pdf,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",Dharmendra S. Modha,,0%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Klaus H. Maier-hein,klaus.maier-hein@dkfz-heidelberg.de,95%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Karol Gotkowski,karol.gotkowski@dkfz.de,95%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Shuvam Gupta,c.guimaraes-da-silva-tochtrop@hzdr.de,85%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Jose R. A. Godinho,j.godinho@hzdr.de,82%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Fabian Isensee,f.isensee@dkfz-heidelberg.de,82%
https://arxiv.org/pdf/2301.13319.pdf,ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling,Camila G. S. Tochtrop,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Erik Wijmans,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Manolis Savva,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Irfan Essa,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Stefan Lee,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Ari S. Morcos,,0%
https://arxiv.org/pdf/2301.13261.pdf,Emergence of Maps in the Memories of Blind Navigation Agents,Dhruv Batra,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Travis Driver,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Kento Tomita,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Koki Ho,,0%
https://arxiv.org/pdf/2301.13254.pdf,Deep Monocular Hazard Detection for Safe Small Body Landing,Panagiotis Tsiotras,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Haonan Chang,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Dhruv Metha Ramesh,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Shijie Geng,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Yuqiu Gan,,0%
https://arxiv.org/pdf/2301.13244.pdf,Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction,Abdeslam Boularias,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Yan Zhang,yan@cyan.zone,85%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,David W. Zhang,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Simon Lacoste-julien,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Gertjan J. Burghouts,,0%
https://arxiv.org/pdf/2301.13197.pdf,Unlocking Slot Attention by Changing Optimal Transport Costs,Cees G. M. Snoek,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Fuzhao Xue,xuefuzhao24@gmail.com,95%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Mostafa Dehghani,dehghani@google.com,78%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Valerii Likhosherstov,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Anurag Arnab,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Neil Houlsby,,0%
https://arxiv.org/pdf/2301.13195.pdf,Adaptive Computation with Elastic Input Sequence,Yang You,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Yiran Zhong,zhongyiran@gmail.com,95%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Meng Wang,eric.mengwang@gmail.com,95%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jinxing Zhou,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Xuyang Shen,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jianyuan Wang,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jiayi Zhang,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Weixuan Sun,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Jing Zhang,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Stan Birchfield,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Dan Guo,,0%
https://arxiv.org/pdf/2301.13190.pdf,Audio-Visual Segmentation with Semantics,Lingpeng Kong,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Nicholas Carlini,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Jamie Hayes,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Milad Nasr,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Matthew Jagielski,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Vikash Sehwag,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Florian Tramèr,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Borja Balle,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Daphne Ippolito,,0%
https://arxiv.org/pdf/2301.13188.pdf,Extracting Training Data from Diffusion Models,Eric Wallace,,0%
https://arxiv.org/pdf/2301.13186.pdf,Accurate Gaze Estimation using an Active-gaze Morphable Model,Hao Sun,,0%
https://arxiv.org/pdf/2301.13186.pdf,Accurate Gaze Estimation using an Active-gaze Morphable Model,Nick Pears,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Yao-chih Lee,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Ji-ze Genevieve Jang,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Yi-ting Chen,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Elizabeth Qiu,,0%
https://arxiv.org/pdf/2301.13173.pdf,Shape-aware Text-driven Layered Video Editing,Jia-bin Huang,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Xin Eric Wang,xwang366@ucsc.edu,82%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Kaiwen Zhou,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Kaizhi Zheng,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Connor Pryor,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Yilin Shen,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Hongxia Jin,,0%
https://arxiv.org/pdf/2301.13166.pdf,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,Lise Getoor,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Li Zhang,lizhangfd@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Qiang Wan,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Zilong Huang,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Jiachen Lu,,0%
https://arxiv.org/pdf/2301.13156.pdf,SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,Gang Yu,,0%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Hong-yu Zhou,whuzhouhongyu@gmail.com,95%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Chenyu Lian,cylian@stu.xmu.edu.cn,82%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Liansheng Wang,lswang@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.13155.pdf,Advancing Radiograph Representation Learning with Masked Record Modeling,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Duaa Alsaeed,dalsaeed@ksu.edu.sa,82%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Remy Peyret,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Fouad Khelifi,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Nadia Al-ghreimil,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Heyam Al-baity,,0%
https://arxiv.org/pdf/2301.13151.pdf,Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study,Ahmed Bouridane,,0%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Talha Qaiser,talha.qaiser@warwick.ac.uk,95%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Shan E Ahmed Raza,shan.raza@warwick.ac.uk,95%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Raja Muhammad Saad Bashir,saad.bashir@warwick.ac.uk,78%
https://arxiv.org/pdf/2301.13141.pdf,Consistency Regularisation in Varying Contexts and Feature Perturbations for Semi-Supervised Semantic Segmentation of Histology Images,Nasir M. Rajpoot,n.m.rajpoot@warwick.ac.uk,82%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Nicolas Nerrienet,nicolas.n@primaalab.com,85%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Rémy Peyret,,0%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Marie Sockeel,,0%
https://arxiv.org/pdf/2301.13128.pdf,Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology,Stéphane Sockeel,,0%
https://arxiv.org/pdf/2301.13104.pdf,Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models,Florian A. Hölzl,florian.hoelzl@tum.de,85%
https://arxiv.org/pdf/2301.13104.pdf,Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models,Georgios Kaissis,g.kaissis@tum.de,82%
https://arxiv.org/pdf/2301.13104.pdf,Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models,Daniel Rueckert,daniel.rueckert@tum.de,95%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Mengyun Qiao,m.qiao21@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Shuo Wang,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Huaqi Qiu,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Antonio De Marvao,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Declan P. O'regan,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Daniel Rueckert,,0%
https://arxiv.org/pdf/2301.13098.pdf,CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy,Wenjia Bai,,0%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Zhanhao Hu,huzhanha17@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Xiaolin Hu,xlhu@mail.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Wei Zhang,zhang-w19@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Xiao Li,lixiao20@mails.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Yining Liu,,0%
https://arxiv.org/pdf/2301.13096.pdf,Language-Driven Anchors for Zero-Shot Adversarial Robustness,Bo Zhang,,0%
https://arxiv.org/pdf/2301.13090.pdf,Action Capsules: Human Skeleton Action Recognition,Hamid D. Taghirad,taghirad@kntu.ac.ir,78%
https://arxiv.org/pdf/2301.13090.pdf,Action Capsules: Human Skeleton Action Recognition,Ali Farajzadeh Bavil,,0%
https://arxiv.org/pdf/2301.13090.pdf,Action Capsules: Human Skeleton Action Recognition,Hamed Damirchi,,0%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yisheng Yuan,y.yuan@hw.ac.uk,82%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Zhang Luo,luozhang@seafogai.com,95%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yang Xu,yang.xu@sdsu.edu,95%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yue Wang,wang.yue.f07@kyoto-u.jp,95%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Wei Pang,w.pang@hw.ac.uk,82%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Zuhao Yang,,0%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Huajun Bai,,0%
https://arxiv.org/pdf/2301.13082.pdf,PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy,Yingfang Yuan,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Yinfei Yang,feiy@apple.com,90%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Bowen Zhang,zhang4@apple.com,78%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Chen Chen,chen999@apple.com,95%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Liangliang Cao,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Jiguang Shen,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Tom Gunter,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Albin Madappally Jose,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Alexander Toshev,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Jonathon Shlens,,0%
https://arxiv.org/pdf/2301.13081.pdf,STAIR: Learning Sparse Text and Image Representation in Grounded Tokens,Ruoming Pang,,0%
https://arxiv.org/pdf/2301.13018.pdf,DELTA: degradation-free fully test-time adaptation,Shu-tao Xia,xiast@sz.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.13018.pdf,DELTA: degradation-free fully test-time adaptation,Chen Chen,chen1634chen@gmail.com,95%
https://arxiv.org/pdf/2301.13018.pdf,DELTA: degradation-free fully test-time adaptation,Bowen Zhao,,0%
https://arxiv.org/pdf/2301.12995.pdf,FedFA: Federated Feature Augmentation,Ender Konukoglu,kender@vision.ee.ethz.ch,85%
https://arxiv.org/pdf/2301.12995.pdf,FedFA: Federated Feature Augmentation,Tianfei Zhou,tiazhou@vision.ee.ethz.ch,82%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Florian Stimberg,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Ayan Chakrabarti,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Chun-ta Lu,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Hussein Hazimeh,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Otilia Stretcu,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Wei Qiao,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Yintao Liu,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Merve Kaya,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Cyrus Rashtchian,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Ariel Fuxman,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Mehmet Tek,,0%
https://arxiv.org/pdf/2301.12993.pdf,Benchmarking Robustness to Adversarial Image Obfuscations,Sven Gowal,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Sunghwan Yoo,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Yeongjeong Jeong,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Maryam Jameela,,0%
https://arxiv.org/pdf/2301.12972.pdf,Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scene,Gunho Sohn,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Ming Tao,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Bing-kun Bao,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Hao Tang,,0%
https://arxiv.org/pdf/2301.12959.pdf,GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis,Changsheng Xu,,0%
https://arxiv.org/pdf/2301.13648.pdf,CSDN: Combing Shallow and Deep Networks for Accurate Real-time Segmentation of High-definition Intravascular Ultrasound Images,Feng Yang,yangf@smu.edu.cn,78%
https://arxiv.org/pdf/2301.13648.pdf,CSDN: Combing Shallow and Deep Networks for Accurate Real-time Segmentation of High-definition Intravascular Ultrasound Images,Shaofeng Yuan,shaofeng.yuan.smu@gmail.com,95%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Nikhil S. Narayan,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Shashanka B. R.,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Rohit Damodaran,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Chandrashekhar Jayaram,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,M. A. Kareem,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Mamta P.,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Saravanan K. R.,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Monu Krishnan,,0%
https://arxiv.org/pdf/2301.12943.pdf,Factors that affect Camera based Self-Monitoring of Vitals in the Wild,Raja Indana,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Alexandros Kalimeris,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Ioannis Psarros,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Giorgos Giannopoulos,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Manolis Terrovitis,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,George Papastefanatos,,0%
https://arxiv.org/pdf/2301.12939.pdf,Data-driven soiling detection in PV modules,Gregory Kotsis,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Shengming Li,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Luping Liu,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Runnan Li,,0%
https://arxiv.org/pdf/2301.12935.pdf,ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models,Xu Tan,,0%
https://arxiv.org/pdf/2301.12914.pdf,PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks,Alexandros Iosifidis,ai@ece.au.dk,90%
https://arxiv.org/pdf/2301.12914.pdf,PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks,Arian Bakhtiarnia,arianbakh@ece.au.dk,85%
https://arxiv.org/pdf/2301.12914.pdf,PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks,Qi Zhang,qz@ece.au.dk,90%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Xinyin Ma,maxinyin@u.nus.edu,95%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Xinchao Wang,xinchao@nus.edu.sg,85%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Gongfan Fang,gongfan@u.nus.edu,85%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Mingli Song,,0%
https://arxiv.org/pdf/2301.12900.pdf,DepGraph: Towards Any Structural Pruning,Michael Bi Mi,,0%
https://arxiv.org/pdf/2301.12891.pdf,Half of an image is enough for quality assessment,Junyong You,,0%
https://arxiv.org/pdf/2301.12891.pdf,Half of an image is enough for quality assessment,Yuan Lin,,0%
https://arxiv.org/pdf/2301.12891.pdf,Half of an image is enough for quality assessment,Jari Korhonen,,0%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Shiqi Wang,shiqwang@cityu.edu.hk,82%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Anderson Rocha,anderson.rocha@ic.unicamp.br,95%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Haoliang Li,haoliang.li@cityu.edu.hk,95%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Chenqi Kong,cqkong2-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Yibing Liu,lyibing112@gmail.com,85%
https://arxiv.org/pdf/2301.12831.pdf,M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System,Kexin Zheng,kzhengaj@connect.ust.hk,82%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Marc Gebauer,gebaumar@b-tu.de,75%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Toni Schneidereit,schneton@b-tu.de,60%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Slavomira Schneidereit,,0%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Ashkan Mansouri Yarahmadi,,0%
https://arxiv.org/pdf/2301.12827.pdf,YOLO-based Object Detection in Industry 4.0 Fischertechnik Model Environment,Michael Breuß,,0%
https://arxiv.org/pdf/2301.12799.pdf,Eye Image-based Algorithms to Estimate Percentage Closure of Eye and Saccadic Ratio for Alertness Detection,Supratim Gupta,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Huazhu Fu,hzfu@ieee.org,82%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Yong Liu,liuyong@ihpc.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Meng Wang,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Kai Yu,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Chun-mei Feng,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Yiming Qian,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Ke Zou,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Lianyu Wang,,0%
https://arxiv.org/pdf/2301.12798.pdf,Reliable Federated Disentangling Network for Non-IID Domain Feature,Rick Siow Mong Goh,,0%
https://arxiv.org/pdf/2301.12796.pdf,Rendering the Directional TSDF for Tracking and Multi-Sensor Registration with Point-To-Plane Scale ICP,Malte Splietker,,0%
https://arxiv.org/pdf/2301.12796.pdf,Rendering the Directional TSDF for Tracking and Multi-Sensor Registration with Point-To-Plane Scale ICP,Sven Behnke,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Muhammad Al-barham,muhammadal-barham@ieee.org,95%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Adham Alsharkawi,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Musa Al-yaman,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Mohammad Al-fetyani,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Ashraf Elnagar,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Ahmad Abu Saaleek,,0%
https://arxiv.org/pdf/2301.11932.pdf,RGB Arabic Alphabets Sign Language Dataset,Mohammad Al-odat,,0%
https://arxiv.org/pdf/2301.12744.pdf,PointSmile: Point Self-supervised Learning via Curriculum Mutual Information,Songcan Chen,s.chen@nuaa.edu.cn,82%
https://arxiv.org/pdf/2301.12744.pdf,PointSmile: Point Self-supervised Learning via Curriculum Mutual Information,Mingqiang Wei,mingqiang.wei@gmail.com,95%
https://arxiv.org/pdf/2301.12744.pdf,PointSmile: Point Self-supervised Learning via Curriculum Mutual Information,Xin Li,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Nan Li,chuanqil@sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Weijie Lv,lvweijie@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Xuan Xia,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Xing He,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Chuanqi Liu,,0%
https://arxiv.org/pdf/2301.12739.pdf,FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation,Ning Ding,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Penghao Jiang,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Xin Ke,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Zifeng Wang,,0%
https://arxiv.org/pdf/2301.12698.pdf,Robust Meta Learning for Image based tasks,Chunxi Li,,0%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Younggun Cho,yg.cho@inha.ac.kr,82%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Myung-hwan Jeon,myunghwan.jeon@snu.ac.kr,95%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Dong-guw Lee,,0%
https://arxiv.org/pdf/2301.12689.pdf,Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels,Ayoung Kim,,0%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Linning Xu,linningxu@ie.cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Yuwei Guo,guoyuwei@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Libiao Jin,libiao@cuc.edu.cn,85%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Anyi Rao,anyirao@stanford.edu,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Bo Dai,daibo@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Lei Yang,yanglei@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Xuekun Jiang,jiangxuekun@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.12688.pdf,Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production,Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Naoki Murata,naoki.murata@sony.com,95%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Koichi Saito,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Chieh-hsin Lai,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Yuhta Takida,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Toshimitsu Uesaka,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Yuki Mitsufuji,,0%
https://arxiv.org/pdf/2301.12686.pdf,GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,Stefano Ermon,,0%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,M. Sohel Rahman,msrahman@cse.buet.ac.bd,82%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,Mohimenul Kabir,,0%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,Jaiaid Mobin,,0%
https://arxiv.org/pdf/2301.12682.pdf,Image Contrast Enhancement using Fuzzy Technique with Parameter Determination using Metaheuristics,Ahmad Hassanat,,0%
https://arxiv.org/pdf/2301.12667.pdf,NeSyFOLD: Neurosymbolic Framework for Interpretable Image Classification,Parth Padalkar,parth.padalkar@utdallas.edu,95%
https://arxiv.org/pdf/2301.12667.pdf,NeSyFOLD: Neurosymbolic Framework for Interpretable Image Classification,Huaduo Wang,,0%
https://arxiv.org/pdf/2301.12667.pdf,NeSyFOLD: Neurosymbolic Framework for Interpretable Image Classification,Gopal Gupta,,0%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Lijian Lin,ljlin@stu.xmu.edu.cn,82%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Jin Ma,majin01@mail.ustc.edu.cn,95%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Yizhen Chen,grayyzchen@tencent.com,78%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Zhongang Qi,zhongangqi@tencent.com,95%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Ying Shan,yingsshan@tencent.com,95%
https://arxiv.org/pdf/2301.12644.pdf,Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval,Jie Wang,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Yabin Zhang,csybzhang@comp.polyu.edu.hk,78%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Bin Deng,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Ruihuang Li,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Kui Jia,,0%
https://arxiv.org/pdf/2301.12643.pdf,Adversarial Style Augmentation for Domain Generalization,Lei Zhang,,0%
https://arxiv.org/pdf/2301.12637.pdf,Lateralized Learning for Multi-Class Visual Classification Tasks,Abubakar Siddique,,0%
https://arxiv.org/pdf/2301.12637.pdf,Lateralized Learning for Multi-Class Visual Classification Tasks,Will N. Browne,,0%
https://arxiv.org/pdf/2301.12637.pdf,Lateralized Learning for Multi-Class Visual Classification Tasks,Gina M. Grimshaw,,0%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Curtis Langlotz,langlotz@stanford.edu,78%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Akshay Chaudhari,akshaysc@stanford.edu,85%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Rogier Van Der Sluijs,sluijs@stanford.edu,78%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Daniel Rubin,dlrubin@stanford.edu,82%
https://arxiv.org/pdf/2301.12636.pdf,Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays,Nandita Bhaskhar,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Gunnar A. Sigurdsson,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Jesse Thomason,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Gaurav S. Sukhatme,,0%
https://arxiv.org/pdf/2301.12614.pdf,RREx-BoT: Remote Referring Expressions with a Bag of Tricks,Robinson Piramuthu,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Bingbing Ni,nibingbing@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Xiaoyang Huang,huangxiaoyang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Yanjun Wang,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Yang Liu,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Wenjun Zhang,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Jinxian Liu,,0%
https://arxiv.org/pdf/2301.12613.pdf,AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio,Teng Li,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Junnan Li,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Dongxu Li,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Silvio Savarese,,0%
https://arxiv.org/pdf/2301.12597.pdf,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Steven Hoi,,0%
https://arxiv.org/pdf/2301.12592.pdf,Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition,Ross Greer,regreer@ucsd.edu,82%
https://arxiv.org/pdf/2301.12592.pdf,Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.12589.pdf,Confidence-Aware Calibration and Scoring Functions for Curriculum Learning,Shuang Ao,,0%
https://arxiv.org/pdf/2301.12589.pdf,Confidence-Aware Calibration and Scoring Functions for Curriculum Learning,Stefan Rueger,,0%
https://arxiv.org/pdf/2301.12589.pdf,Confidence-Aware Calibration and Scoring Functions for Curriculum Learning,Advaith Siddharthan,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Bahare Samadi,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Maxime Raison,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Philippe Mahaudens,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Christine Detrembleur,,0%
https://arxiv.org/pdf/2301.12588.pdf,Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study),Sofiane Achiche,,0%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Yatong Bai,yatong_bai@berkeley.edu,95%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Somayeh Sojoudi,sojoudi@berkeley.edu,82%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Aerin Kim,aerinykim@gmail.com,95%
https://arxiv.org/pdf/2301.12554.pdf,Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing,Brendon G. Anderson,bganderson@berkeley.edu,82%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Matt Fredrikson,mfredrik@cs.cmu.edu,90%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Klas Leino,kleino@cs.cmu.edu,82%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Zifan Wang,zifan@safe.ai,85%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Kai Hu,kaihu@andrew.cmu.edu,95%
https://arxiv.org/pdf/2301.12549.pdf,Unlocking Deterministic Robustness Certification on ImageNet,Andy Zou,andyzou@cmu.edu,95%
https://arxiv.org/pdf/2301.12541.pdf,Supervised and Contrastive Self-Supervised In-Domain Representation Learning for Dense Prediction Problems in Remote Sensing,Ali Ghanbarzade,,0%
https://arxiv.org/pdf/2301.12541.pdf,Supervised and Contrastive Self-Supervised In-Domain Representation Learning for Dense Prediction Problems in Remote Sensing,Hossein Soleimani,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Yiming Zhou,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Callen Macphee,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Madhuri Suthar,,0%
https://arxiv.org/pdf/2301.12531.pdf,PhyCV: The First Physics-inspired Computer Vision Library,Bahram Jalali,,0%
https://arxiv.org/pdf/2301.12527.pdf,"Diverse, Difficult, and Odd Instances (D2O): A New Test Set for Object Classification",Ali Borji,aliborji@gmail.com,95%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Jin Fang,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Dingfu Zhou,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Jingjing Zhao,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Chenming Wu,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Chulin Tang,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Cheng-zhong Xu,,0%
https://arxiv.org/pdf/2301.12515.pdf,LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection,Liangjun Zhang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Yangguang Li,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Bin Huang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Zeren Chen,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Yufeng Cui,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Feng Liang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Mingzhu Shen,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Fenggang Liu,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Enze Xie,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Lu Sheng,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Wanli Ouyang,,0%
https://arxiv.org/pdf/2301.12511.pdf,Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline,Jing Shao,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Danyang Hou,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Liang Pang,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Yanyan Lan,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Huawei Shen,,0%
https://arxiv.org/pdf/2301.13606.pdf,Multi-video Moment Ranking with Multimodal Clue,Xueqi Cheng,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Isaac Osei Agyemang,ioagyemang@std.uestc.edu.cn,82%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Isaac Adjei Mensah,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Sophyani Banaamwini Yussif,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Fiasam Linda Delali,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Bernard Cobinnah Mawuli,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Bless Lord Y. Agbley,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Collins Sey,,0%
https://arxiv.org/pdf/2301.12470.pdf,Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators,Joshua Berkohd,,0%
https://arxiv.org/pdf/2301.12459.pdf,The Influences of Color and Shape Features in Visual Contrastive Learning,Xiaoqi Zhuang,xiaoqizhuang@outlook.com,95%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Wenjie Ruan,w.ruan@exeter.ac.uk,82%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Peipei Xu,peipei.xu@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Xiaowei Huang,xiaowei.huang@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.12456.pdf,Towards Verifying the Geometric Robustness of Large-scale Neural Networks,Fu Wang,,0%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Qixiang Ye,qxye@ucas.ac.cn,82%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Jiahan Li,han.li@cumt.edu.cn,78%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Qiong Wu,qiong@stu.xmu.edu.cn,85%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Pingyang Dai,pydai@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Liujuan Cao,caoliujuan@xmu.edu.cn,95%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Rongrong Ji,rrji@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.12439.pdf,Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning,Yongjian Wu,,0%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Yi Cheng,cheng yi@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Ying Sun,suny@i2r.a-star.edu.sg,78%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Qianli Xu,qxu@i2r.a-star.edu.sg,82%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Hao Xuan Woon,haoxuan.woon@u.nus.edu,95%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Dongyun Lin,lin dongyun@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12436.pdf,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022,Fen Fang,fang fen@i2r.a-star.edu.sg,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Beier Zhu,beier002@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Hanwang Zhang,hanwangzhang@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Saeil Lee,saeil.lee@hmgics.com,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Yulei Niu,yn.yuleiniu@gmail.com,95%
https://arxiv.org/pdf/2301.12429.pdf,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,Minhoe Hur,minhoe.hur@hyundai.com,95%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Ming Tang,tangm@nlpr.ia.ac.cn,78%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Xiaomei Zhang,xiaomei.zhang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Xiangyu Zhu,xiangyu.zhu.chen@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2301.12416.pdf,Deep Learning for Human Parsing: A Survey,Zhen Lei,zlei@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Kan Ren,kan.ren@microsoft.com,95%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Ziyue Li,litzy0619owned@gmail.com,78%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Yifan Yang,,0%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Xinyang Jiang,,0%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Yuqing Yang,,0%
https://arxiv.org/pdf/2301.12378.pdf,Towards Inference Efficient Deep Ensemble Learning,Dongsheng Li,,0%
https://arxiv.org/pdf/2301.12356.pdf,Exploiting High Performance Spiking Neural Networks with Efficient Spiking Patterns,Yi Zeng,yi.zeng@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12356.pdf,Exploiting High Performance Spiking Neural Networks with Efficient Spiking Patterns,Guobin Shen,shenguobin2021@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12356.pdf,Exploiting High Performance Spiking Neural Networks with Efficient Spiking Patterns,Dongcheng Zhao,zhaodongcheng2016@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Jialin Yuan,yuanjial@oregonstate.edu,78%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Hung Nguyen,nguyehu5@oregonstate.edu,85%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Jay Patravali,patravaj@oregonstate.edu,65%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Li Fuxin,lif@oregonstate.edu,85%
https://arxiv.org/pdf/2301.12352.pdf,Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation,Chanho Kim,kimchanh@oregonstate.edu,78%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Weihua Zhou,whzhou@mtu.edu,82%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Ni Yao,niceday1987@hotmail.com,85%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Yanhui Tian,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Daniel Gama Das Neves,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Chen Zhao,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Claudio Tinoco Mesquita,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Wolney De Andrade Martins,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Alair Augusto Sarmet Moreira Damas Dos Santos,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Yanting Li,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Chuang Han,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Fubao Zhu,,0%
https://arxiv.org/pdf/2301.12340.pdf,Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection,Neng Dai,,0%
https://arxiv.org/pdf/2301.12334.pdf,Don't Play Favorites: Minority Guidance for Diffusion Models,Suhyeon Lee,suhyeon.lee@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12334.pdf,Don't Play Favorites: Minority Guidance for Diffusion Models,Soobin Um,sum@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.12334.pdf,Don't Play Favorites: Minority Guidance for Diffusion Models,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Peng Qiao,pengqiao@nudt.edu.cn,95%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Sidun Liu,,0%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Tao Sun,,0%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Ke Yang,,0%
https://arxiv.org/pdf/2301.12332.pdf,Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration,Yong Dou,,0%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Zeba Karishma,zebakarishma@gmail.com,95%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Jian Wu,jwu@cs.odu.edu,82%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Shaurya Rohatgi,,0%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,Kavya Shrinivas Puranik,,0%
https://arxiv.org/pdf/2301.12293.pdf,ACL-Fig: A Dataset for Scientific Figure Classification,C. Lee Giles,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jieneng Chen,jienengchen01@gmail.com,95%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Zaiyi Liu,zyliu@163.com,82%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yingda Xia,yingda.xia@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jiawen Yao,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Ke Yan,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jianpeng Zhang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Le Lu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Fakai Wang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Bo Zhou,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Mingyan Qiu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Qihang Yu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Mingze Yuan,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Wei Fang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yuxing Tang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Minfeng Xu,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jian Zhou,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yuqian Zhao,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Qifeng Wang,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Xianghua Ye,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Xiaoli Yin,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Yu Shi,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Xin Chen,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Jingren Zhou,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Alan Yuille,,0%
https://arxiv.org/pdf/2301.12291.pdf,"CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",Ling Zhang,,0%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Bartosz Zieliński,bartosz.zielinski@uj.edu.pl,95%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Jacek Tabor,lukasz.struski;jacek.tabor;bartosz.zielinski@uj.edu.pl,95%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Dawid Rymarczyk,dawid.rymarczyk@doctoral.uj.edu.pl,95%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Mikołaj Sacha,,0%
https://arxiv.org/pdf/2301.12276.pdf,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,Łukasz Struski,,0%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Sonia Moshfeghi,smoshfeghi@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Jiannan Zhai,jzhai@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,David Newman,dnewma@fau.edu,90%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Kwangsoo Yang,yangk@fau.edu,78%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Monica Rosselli,mrossell@fau.edu,90%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Borko Furht,bfurht@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Jinwoo Jang,jangj@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Ruth Tappen,rtappen@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Muhammad Tanveer Jan,mjan2021@fau.edu,82%
https://arxiv.org/pdf/2301.12269.pdf,Methods and Tools for Monitoring Driver's Behavior,Joshua William Conniff,fau.jconniff@health.fau.edu,78%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Xiaoyu Wang,fanghuaxue@gmail.com,65%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Nannan Wang,nnwang@xidian.edu.cn,82%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Ruoyu Zhao,royzhao@stu.xidian.edu.cn,82%
https://arxiv.org/pdf/2301.12257.pdf,Few-shot Face Image Translation via GAN Prior Distillation,Mingrui Zhu,mrzhu@xidian.edu.cn,82%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Manuel Brack,brack@cs.tu-darmstadt.de,78%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Felix Friedrich,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Dominik Hintersdorf,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Lukas Struppek,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Patrick Schramowski,,0%
https://arxiv.org/pdf/2301.12247.pdf,SEGA: Instructing Text-to-Image Models using Semantic Guidance,Kristian Kersting,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Jingkuan Song,jingkuan.song@gmail.com,95%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Xu Luo,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Hao Wu,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Ji Zhang,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Lianli Gao,,0%
https://arxiv.org/pdf/2301.12246.pdf,A Closer Look at Few-shot Classification Again,Jing Xu,,0%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Junyou Wang,wangjunyou@stu.scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Jianwei Zhang,zhangjianwei@stu.scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Lei Zhang,leizhang@scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Xin Wei,weixin@stu.scu.edu.cn,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Jiaqi Li,lijiaqicd@gmail.com,95%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Wenjie Liu,liuwj@stu.scu.edu.cn,78%
https://arxiv.org/pdf/2301.12219.pdf,Towards Accurate Acne Detection via Decoupled Sequential Detection Head,Xian Jiang,,0%
https://arxiv.org/pdf/2301.12176.pdf,Neural Gas Network Image Features and Segmentation for Brain Tumor Detection Using Magnetic Resonance Imaging Data,S. Muhammad Hossein Mousavi,mosavi.a.i.buali@gmail.com,85%
https://arxiv.org/pdf/2301.12171.pdf,ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12171.pdf,ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts,Yujin Oh,yujin.oh@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12171.pdf,ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts,Kwanyoung Kim,,0%
https://arxiv.org/pdf/2301.12168.pdf,"Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits",Matteo Matteucci,matteo.matteucci@polimi.it,95%
https://arxiv.org/pdf/2301.12168.pdf,"Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits",Simone Sarti,simone.sarti@mail.polimi.it,95%
https://arxiv.org/pdf/2301.12168.pdf,"Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits",Eugenio Lomurno,eugenio.lomurno@polimi.it,95%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Jianqiang Wang,,0%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Dandan Ding,,0%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Hao Chen,,0%
https://arxiv.org/pdf/2301.12165.pdf,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,Zhan Ma,,0%
https://arxiv.org/pdf/2301.12159.pdf,ClusterFuG: Clustering Fully connected Graphs by Multicut,Ahmed Abbas,ahmed.abbas@mpi-inf.mpg.de,95%
https://arxiv.org/pdf/2301.12159.pdf,ClusterFuG: Clustering Fully connected Graphs by Multicut,Paul Swoboda,,0%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Aibin Huang,huangaibin@hdu.edu.cn,95%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Yuanqi Chang,yuanqichang@hdu.edu.cn,95%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Jiawei Mao,jiaweima0@hdu.edu.cn,85%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Binling Nie,binlingnie@hdu.edu.cn,95%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Xuesong Yin,yinxs@hdu.edu.cn,78%
https://arxiv.org/pdf/2301.12149.pdf,POSTER++: A simpler and stronger facial expression recognition network,Rui Xu,,0%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Xiaoya Yang,yangxiaoya@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Dongxv Liu,liudongxv@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Pu Cao,caopu@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Tianrui Huang,huangtianrui@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Lu Yang,,0%
https://arxiv.org/pdf/2301.12141.pdf,What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,Qing Song,,0%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Gim Hee Lee,gimhee.lee@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Jianming Li,jianming.li@ninebot.com,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Shu Song,songshu0905@gmail.com,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Yu Chen,chenyu@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Tianning Yu,tianning.yu@rlm.segway.com,95%
https://arxiv.org/pdf/2301.12135.pdf,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,Zihao Yu,yuzihao@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.12093.pdf,Local Contrast and Global Contextual Information Make Infrared Small Object Salient Again,Chenyi Wang,Nanjing@qq.com,55%
https://arxiv.org/pdf/2301.12093.pdf,Local Contrast and Global Contextual Information Make Infrared Small Object Salient Again,Huan Wang,,0%
https://arxiv.org/pdf/2301.12093.pdf,Local Contrast and Global Contextual Information Make Infrared Small Object Salient Again,Peiwen Pan,,0%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Feng Zheng,zhengf@sustech.edu.cn,78%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Guoyang Xie,guoyang.xie@surrey.ac.uk,95%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Yaochu Jin,yaochu.jin@uni-bielefeld.de,95%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Jiaqi Liu,liujq32021@mail.sustech.edu.cn,78%
https://arxiv.org/pdf/2301.12082.pdf,Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore,Jinbao Wang,,0%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Mingyu Xu,xumingyu2021@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Zheng Lian,lianzheng2016@ia.ac.cn,95%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Lei Feng,,0%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Bin Liu,,0%
https://arxiv.org/pdf/2301.12077.pdf,ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,Jianhua Tao,,0%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Lia Coleman,liac@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Peter Schaldenbrand,pschalde@andrew.cmu.edu,90%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Youeun Shin,youeuns@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Jihie Kim,jihie.kim@dgu.edu,95%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Jean Oh,jeanoh@cmu.edu,95%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Zhixuan Liu,zhixuan2@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Youngsik Yun,youngsiy@andrew.cmu.edu,75%
https://arxiv.org/pdf/2301.12073.pdf,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,Beverley-claire Okogwu,bokogwu@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Lav R. Varshney,varshney@illinois.edu,78%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Moulik Choraria,moulikc2@illinois.edu,85%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Ibtihal Ferwana,iferwna2@illinois.edu,65%
https://arxiv.org/pdf/2301.12067.pdf,Learning Optimal Features via Partial Invariance,Ankur Mani,amani@umn.edu,82%
https://arxiv.org/pdf/2301.12058.pdf,Aerial Image Object Detection With Vision Transformer Detector (ViTDet),Liya Wang,,0%
https://arxiv.org/pdf/2301.12058.pdf,Aerial Image Object Detection With Vision Transformer Detector (ViTDet),Alex Tien,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Kaijie Zhao,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Haitao Zhao,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Zhongze Wang,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Jingchao Peng,,0%
https://arxiv.org/pdf/2301.12057.pdf,Object Preserving Siamese Network for Single Object Tracking on Point Clouds,Zhengwei Hu,,0%
https://arxiv.org/pdf/2301.12053.pdf,Weakly Supervised Image Segmentation Beyond Tight Bounding Box Annotations,Bin Xia,b.xia@sibionics.com,82%
https://arxiv.org/pdf/2301.12053.pdf,Weakly Supervised Image Segmentation Beyond Tight Bounding Box Annotations,Juan Wang,wangjuan313@gmail.com,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yue Bai,bai.yue@northeastern.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yun Fu,yunfu@ece.neu.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yi Xu,xu.yi@northeastern.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Xu Ma,ma.xu1@northeastern.edu,95%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Can Qin,qin.ca@northeastern.edu,78%
https://arxiv.org/pdf/2301.12048.pdf,Making Reconstruction-based Method Great Again for Video Anomaly Detection,Yizhou Wang,,0%
https://arxiv.org/pdf/2301.12046.pdf,Semantic Adversarial Attacks on Face Recognition through Significant Attributes,Yasmeen M. Khedr,yasmeenkhedr@hust.edu.cn,95%
https://arxiv.org/pdf/2301.12046.pdf,Semantic Adversarial Attacks on Face Recognition through Significant Attributes,Yifeng Xiong,xiongyf@hust.edu.cn,78%
https://arxiv.org/pdf/2301.12046.pdf,Semantic Adversarial Attacks on Face Recognition through Significant Attributes,Kun He,,0%
https://arxiv.org/pdf/2301.12032.pdf,BinaryVQA: A Versatile Test Set to Evaluate the Out-of-Distribution Generalization of VQA Models,Ali Borji,aliborji@gmail.com,95%
https://arxiv.org/pdf/2301.12025.pdf,Cross-Architectural Positive Pairs improve the effectiveness of Self-Supervised Learning,Pranav Singh,,0%
https://arxiv.org/pdf/2301.12025.pdf,Cross-Architectural Positive Pairs improve the effectiveness of Self-Supervised Learning,Jacopo Cirrone,,0%
https://arxiv.org/pdf/2301.12006.pdf,Improved knowledge distillation by utilizing backward pass knowledge in neural networks,Aref Jafari,aref.jafari@uwaterloo.ca,95%
https://arxiv.org/pdf/2301.12006.pdf,Improved knowledge distillation by utilizing backward pass knowledge in neural networks,Mehdi Rezagholizadeh,mehdi.rezagholizadeh@huawei.com,95%
https://arxiv.org/pdf/2301.12006.pdf,Improved knowledge distillation by utilizing backward pass knowledge in neural networks,Ali Ghodsi,ali.ghodsi@uwaterloo.ca,95%
https://arxiv.org/pdf/2301.12003.pdf,Minimizing Trajectory Curvature of ODE-based Generative Models,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.12003.pdf,Minimizing Trajectory Curvature of ODE-based Generative Models,Sangyun Lee,,0%
https://arxiv.org/pdf/2301.12003.pdf,Minimizing Trajectory Curvature of ODE-based Generative Models,Beomsu Kim,,0%
https://arxiv.org/pdf/2301.11990.pdf,Alignment with human representations supports robust few-shot learning,Ilia Sucholutsky,,0%
https://arxiv.org/pdf/2301.11990.pdf,Alignment with human representations supports robust few-shot learning,Thomas L. Griffiths,,0%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Hamid Hassanpour,h.hassanpour@shahroodut.ac.ir,82%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Soroush Hashemifar,hashemifar_soroush@cmps2.iust.ac.ir,95%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Javad Hassannataj Joloudari,javad.hassannataj@birjand.ac.ir,85%
https://arxiv.org/pdf/2301.11986.pdf,Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction,Abdolreza Marefat,rzamarefat@gmail.com,78%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Jie Zhu,zhujie@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Jiyang Qi,jiyangqi@hust.edu.cn,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Mingyu Ding,myding@berkeley.edu,82%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Ping Luo,pluo@cs.hku.hk,82%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Jingdong Wang,wangjingdong@outlook.com,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Leye Wang,leyewang@pku.edu.cn,95%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Wenyu Liu,liuwy@hust.edu.cn,78%
https://arxiv.org/pdf/2301.11915.pdf,Understanding Self-Supervised Pretraining with Part-Aware Representation Learning,Xiaokang Chen,,0%
https://arxiv.org/pdf/2301.11752.pdf,Inter-View Depth Consistency Testing in Depth Difference Subspace,Markus Flierl,markus.ﬂierl@kth.se,95%
https://arxiv.org/pdf/2301.11752.pdf,Inter-View Depth Consistency Testing in Depth Difference Subspace,Pravin Kumar Rana,pravin.rana@tobii.com,95%
https://arxiv.org/pdf/2301.11892.pdf,Streaming LifeLong Learning With Any-Time Inference,Vinay Kumar Verma,vinaykumar.verma@duke.edu,95%
https://arxiv.org/pdf/2301.11892.pdf,Streaming LifeLong Learning With Any-Time Inference,Soumya Banerjee,soumyab@cse.iitk.ac.in,85%
https://arxiv.org/pdf/2301.11892.pdf,Streaming LifeLong Learning With Any-Time Inference,Vinay P. Namboodiri,,0%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Yan Yan,yyan34@iit.edu,95%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Bin Duan,bduan2@hawk.iit.edu,82%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Keshav Bhandari,,0%
https://arxiv.org/pdf/2301.11880.pdf,"Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application",Gaowen Liu,,0%
https://arxiv.org/pdf/2301.11843.pdf,Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking,Oana Cocarascu,oana.cocarascu@kcl.ac.uk,95%
https://arxiv.org/pdf/2301.11843.pdf,Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking,Elena Simperl,elena.simperl@kcl.ac.uk,95%
https://arxiv.org/pdf/2301.11843.pdf,Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking,Mubashara Akhtar,mubashara.akhtar@kcl.ac.uk,95%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Erdi Kara,erdikara@spelman.edu,95%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,George Zhang,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Joseph J. Williams,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Gonzalo Ferrandez-quinto,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Leviticus J. Rhoden,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Maximilian Kim,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,J. Nathan Kutz,,0%
https://arxiv.org/pdf/2302.05425.pdf,Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments,Aminur Rahman,,0%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Zahra Arjmandi,zahraarj@yorku.com,85%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Yujia Zhang,zhang89@yorku.ca,78%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Mostafa Ahmadi,ahmadism@yorku.ca,78%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Amin Alizadeh Naeini,naeini@yorku.ca,78%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Gunho Sohn,gsohn@yorku.ca,82%
https://arxiv.org/pdf/2301.11823.pdf,HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera,Mohammad Moein Sheikholeslami,,0%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,Floran De Putter,f.a.m.d.putter@tue.nl,82%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,David Van Son,d.v.son@tue.nl,82%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,Sebastian Vogel,sebastian.vogel@nxp.com,95%
https://arxiv.org/pdf/2301.11810.pdf,BOMP-NAS: Bayesian Optimization Mixed Precision NAS,Henk Corporaal,h.corporaal@tue.nl,82%
https://arxiv.org/pdf/2301.11806.pdf,PCV: A Point Cloud-Based Network Verifier,Matthew B. Dwyer,matthewbdwyer@virginia.edu,95%
https://arxiv.org/pdf/2301.11806.pdf,PCV: A Point Cloud-Based Network Verifier,Arup Kumar Sarker,,0%
https://arxiv.org/pdf/2301.11806.pdf,PCV: A Point Cloud-Based Network Verifier,Farzana Yasmin Ahmad,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Sumukh Aithal,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Anirudh Goyal,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Alex Lamb,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Yoshua Bengio,,0%
https://arxiv.org/pdf/2301.11790.pdf,Leveraging the Third Dimension in Contrastive Learning,Michael Mozer,,0%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Jiajing Zhang,zhangjj83@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Yifeng Guo,guoyf25@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Zhifan Gao,gaozhifan@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Tieyong Zeng,zeng@math.cuhk.edu.hk,78%
https://arxiv.org/pdf/2302.10309.pdf,Hierarchical Perception Adversarial Learning Framework for Compressed Sensing MRI,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.11753.pdf,Détection d'Objets dans les documents numérisés par réseaux de neurones profonds,Mélodie Boillet,,0%
https://arxiv.org/pdf/2301.11745.pdf,Side Auth: Synthesizing Virtual Sensors for Authentication,Kevin Fu,k.fu@northeastern.edu,82%
https://arxiv.org/pdf/2301.11745.pdf,Side Auth: Synthesizing Virtual Sensors for Authentication,Yan Long,yanlong@umich.edu,95%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Mang Ning,m.ning@uu.nl,82%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Enver Sangineto,,0%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Angelo Porrello,,0%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Simone Calderara,,0%
https://arxiv.org/pdf/2301.11706.pdf,Input Perturbation Reduces Exposure Bias in Diffusion Models,Rita Cucchiara,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Ziwei Luo,ziwei.luo@it.uu.se,95%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Fredrik K. Gustafsson,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Zheng Zhao,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Jens Sjölund,,0%
https://arxiv.org/pdf/2301.11699.pdf,Image Restoration with Mean-Reverting Stochastic Differential Equations,Thomas B. Schön,,0%
https://arxiv.org/pdf/2301.11663.pdf,Deep Residual Compensation Convolutional Network without Backpropagation,Richard Wilson,richard.wilson@york.ac.uk,95%
https://arxiv.org/pdf/2301.11663.pdf,Deep Residual Compensation Convolutional Network without Backpropagation,Mubarakah Alotaibi,,0%
https://arxiv.org/pdf/2301.11650.pdf,Fast Region of Interest Proposals on Maritime UAVs,Andreas Zell,prename.surname@uni-tuebingen.de,60%
https://arxiv.org/pdf/2301.11650.pdf,Fast Region of Interest Proposals on Maritime UAVs,Benjamin Kiefer,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Przemysław Spurek,myslaw.spurek@uj.edu.pl,78%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Adam Kania,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Artur Kasymov,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Jakub Kościukiewicz,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Artur Górak,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Marcin Mazur,,0%
https://arxiv.org/pdf/2301.11631.pdf,HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,Maciej Zięba,,0%
https://arxiv.org/pdf/2301.11630.pdf,Joint Geometry and Attribute Upsampling of Point Clouds Using Frequency-Selective Models with Overlapped Support,Viktoria Heimann,viktoria.heimann@fau.de,95%
https://arxiv.org/pdf/2301.11630.pdf,Joint Geometry and Attribute Upsampling of Point Clouds Using Frequency-Selective Models with Overlapped Support,André Kaup,andre.kaup@fau.de,95%
https://arxiv.org/pdf/2301.11630.pdf,Joint Geometry and Attribute Upsampling of Point Clouds Using Frequency-Selective Models with Overlapped Support,Andreas Spruck,,0%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Jason Gu,jason.gu@dal.ca,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Tiefeng Li,litiefeng@zju.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yi Ren,even.renyi@huawei.com,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Xiaowen Chu,xwchu@hkust-gz.edu.cn,82%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Piaopiao Jin,piaopiaojin@zju.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Penglei Sun,psun012@connect.hkust-gz.edu.cn,82%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yaoxian Song,songyaoxian@zju.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yue Zhang,zhangyue@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Zhixu Li,zhixuli@ruc.edu.cn,95%
https://arxiv.org/pdf/2301.11564.pdf,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,Yu Zheng,yu.zheng3@ubtrobot.com,95%
https://arxiv.org/pdf/2301.11558.pdf,Accelerating Guided Diffusion Sampling with Splitting Numerical Methods,Suttisak Wizadwongsa,suttisak.w s19@vistec.ac.th,85%
https://arxiv.org/pdf/2301.11558.pdf,Accelerating Guided Diffusion Sampling with Splitting Numerical Methods,Supasorn Suwajanakorn,supasorn.s@vistec.ac.th,85%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Hossein Kashiani,h_asgariandehkordi@elec.iust.ac.ir,85%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Shahriar Baradaran Shokouhi,bshokouhi@iust.ac.ir,78%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Omid Nejati Manzari,omid_nejaty@alumni.iust.ac.ir,85%
https://arxiv.org/pdf/2301.11553.pdf,Robust Transformer with Locality Inductive Bias and Feature Normalization,Hojat Asgarian Dehkordi,,0%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Farzad Beizaee,farzad.beizaee.1@ens.etsmtl.ca,95%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Christian Desrosiers,,0%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Gregory A. Lodygensky,,0%
https://arxiv.org/pdf/2301.11551.pdf,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,Jose Dolz,,0%
https://arxiv.org/pdf/2301.11525.pdf,Mixed Attention Network for Hyperspectral Image Denoising,Ying Fu,fuying@bit.edu.cn,95%
https://arxiv.org/pdf/2301.11525.pdf,Mixed Attention Network for Hyperspectral Image Denoising,Zeqiang Lai,laizeqiang@bit.edu.cn,95%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Saulo Abraham Gante,sganted1500@ipn.mx,82%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Juan Irving Vasquez,,0%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Marco Antonio Valencia,,0%
https://arxiv.org/pdf/2301.11522.pdf,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,Mauricio Olguín Carbajal,,0%
https://arxiv.org/pdf/2301.11520.pdf,SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning,H. Jin Kim,hjinkim@snu.ac.kr,82%
https://arxiv.org/pdf/2301.11520.pdf,SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning,Dongseok Shim,,0%
https://arxiv.org/pdf/2301.11520.pdf,SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning,Seungjae Lee,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Jiaqi Liu,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Guoyang Xie,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Jinbao Wang,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Shangnian Li,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Feng Zheng,,0%
https://arxiv.org/pdf/2301.11514.pdf,Deep Industrial Image Anomaly Detection: A Survey,Yaochu Jin,,0%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Chunhui Li,lich@smail.nju.edu.cn,78%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Zhiling Yan,zhilingyan724@outlook.com,95%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Guanglei Zhang,guangleizhang@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Yunlu Feng,yunluf@icloud.com,85%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Nan Ying,rain986532@126.com,60%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Tianyi Zhang,,0%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Yanli Lei,,0%
https://arxiv.org/pdf/2301.11513.pdf,CellMix: A General Instance Relationship based Method for Data Augmentation Towards Pathology Image Classification,Yu Zhao,,0%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Minjoon Seo,minjoon@kaist.ac.kr,85%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Sungdong Kim,,0%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Jin-hwa Kim,,0%
https://arxiv.org/pdf/2301.11507.pdf,Semi-Parametric Video-Grounded Text Generation,Jiyoung Lee,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Dong Sun,medsun@cityu.edu.hk,78%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Fei Pan,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Yutong Wu,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Kangning Cui,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Shuxun Chen,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Yanfang Li,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Yaofang Liu,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Adnan Shakoor,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Han Zhao,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Beijia Lu,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Shaohua Zhi,,0%
https://arxiv.org/pdf/2301.11499.pdf,Dual-View Selective Instance Segmentation Network for Unstained Live Adherent Cells in Differential Interference Contrast Images,Raymond Chan,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Fenggen Yu,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Qimin Chen,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Maham Tanveer,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Ali Mahdavi Amiri,,0%
https://arxiv.org/pdf/2301.11497.pdf,D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts,Hao Zhang,,0%
https://arxiv.org/pdf/2301.11495.pdf,Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks,Lei Lyu,lvlei@sdnu.edu.cn,85%
https://arxiv.org/pdf/2301.11495.pdf,Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks,Chen Pang,,0%
https://arxiv.org/pdf/2301.11495.pdf,Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks,Xuequan Lu,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Yitong Deng,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Hong-xing Yu,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Jiajun Wu,,0%
https://arxiv.org/pdf/2301.11494.pdf,Learning Vortex Dynamics for Fluid Inference and Prediction,Bo Zhu,,0%
https://arxiv.org/pdf/2302.08901.pdf,Exploring External Knowledge for Accurate modeling of Visual and Language Problems,Xuewen Yang,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Jacob Mallet,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Laura Pryor,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Rushit Dave,,0%
https://arxiv.org/pdf/2302.10280.pdf,Deepfake Detection Analyzing Hybrid Dataset Utilizing CNN and SVM,Mounika Vanamala,,0%
https://arxiv.org/pdf/2301.11468.pdf,Multi-limb Split Learning for Tumor Classification on Vertically Distributed Data,Mayar M. Alfares,mayar.mohamed@guc.edu.eg,85%
https://arxiv.org/pdf/2301.11468.pdf,Multi-limb Split Learning for Tumor Classification on Vertically Distributed Data,Mohammed A. -m. Salem,mohammed.salem@guc.edu.eg,95%
https://arxiv.org/pdf/2301.11468.pdf,Multi-limb Split Learning for Tumor Classification on Vertically Distributed Data,Omar S. Ads,omar.ads@student.guc.edu.eg,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Shu Hu,shuhu@cmu.edu,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Xi Wu,xi.wu@imde.ac.cn,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Siwei Lyu,siweilyu@buffalo.edu,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Yunxu Xie,xieyunxu@imde.ac.cn,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Xin Wang,xwang264@buffalo.edu,82%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Bin Zhu,binzhu@microsoft.com,95%
https://arxiv.org/pdf/2301.11457.pdf,Attacking Important Pixels for Anchor-free Detectors,Quanyu Liao,,0%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Bibek Aryal,baryal@miners.utep.edu,82%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Katie E. Miles,,0%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Sergio A. Vargas Zesati,,0%
https://arxiv.org/pdf/2301.11454.pdf,Boundary Aware U-Net for Glacier Segmentation,Olac Fuentes,,0%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Matthias Niessner,niessner@tum.de,78%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Jiapeng Tang,jiapeng.tang@tum.de,95%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Biao Zhang,biao.zhang@kaust.edu.sa,95%
https://arxiv.org/pdf/2301.11445.pdf,3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models,Peter Wonka,,0%
https://arxiv.org/pdf/2301.11431.pdf,Semidefinite Relaxations for Robust Multiview Triangulation,Daniel Cremers,cremers@tum.de,78%
https://arxiv.org/pdf/2301.11431.pdf,Semidefinite Relaxations for Robust Multiview Triangulation,Linus Härenstam-nielsen,linus.nielsen@tum.de,85%
https://arxiv.org/pdf/2301.11431.pdf,Semidefinite Relaxations for Robust Multiview Triangulation,Niclas Zeller,niclas.zeller@h-ka.de,95%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Masoud Zarepisheh,zarepism@mskcc.org,65%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Donghoon Lee,leed10@mskcc.org,78%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Yu-chi Hu,huj@mskcc.org,78%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Saad Nadeem,nadeems@mskcc.org,78%
https://arxiv.org/pdf/2301.11422.pdf,RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans,Ellen Yorke,yorkee@mskcc.org,78%
https://arxiv.org/pdf/2301.11418.pdf,Parkinson gait modelling from an anomaly deep representation,Edgar Rangel,edgar.rangel@correo.uis.edu.co,95%
https://arxiv.org/pdf/2301.11418.pdf,Parkinson gait modelling from an anomaly deep representation,Fabio Martinez,,0%
https://arxiv.org/pdf/2301.11417.pdf,Are Labels Needed for Incremental Instance Learning?,Joaquin Vanschoren,j.vanschoren@tue.nl,82%
https://arxiv.org/pdf/2301.11417.pdf,Are Labels Needed for Incremental Instance Learning?,Mert Kilickaya,kilickayamert@gmail.com,95%
https://arxiv.org/pdf/2301.11405.pdf,Discriminative Entropy Clustering and its Relation to K-means and SVM,Zhongwen Zhang,z889zhan@uwaterloo.ca,65%
https://arxiv.org/pdf/2301.11405.pdf,Discriminative Entropy Clustering and its Relation to K-means and SVM,Yuri Boykov,yboykov@uwaterloo.ca,82%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Qingsong Xu,qingsong.xu@tum.de,95%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Xin Yuan,xyuan@westlake.edu.cn,82%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Xiao Xiang Zhu,xiaoxiang.zhu@tum.de,95%
https://arxiv.org/pdf/2301.11387.pdf,Universal Domain Adaptation for Remote Sensing Image Scene Classification,Yilei Shi,yilei.shi@tum.de,95%
https://arxiv.org/pdf/2301.11367.pdf,Style-Aware Contrastive Learning for Multi-Style Image Captioning,Guodong Long,guodong.long@uts.edu.au,95%
https://arxiv.org/pdf/2301.11367.pdf,Style-Aware Contrastive Learning for Multi-Style Image Captioning,Yucheng Zhou,yucheng.zhou-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.11362.pdf,Improving Cross-modal Alignment for Text-Guided Image Inpainting,Guodong Long,guodong.long@uts.edu.au,95%
https://arxiv.org/pdf/2301.11362.pdf,Improving Cross-modal Alignment for Text-Guided Image Inpainting,Yucheng Zhou,yucheng.zhou-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.11360.pdf,The Power of Linear Combinations: Learning with Random Convolutions,Paul Gavrikov,paul.gavrikov@hs-offenburg.de,95%
https://arxiv.org/pdf/2301.11360.pdf,The Power of Linear Combinations: Learning with Random Convolutions,Janis Keuper,janis.keuper@hs-offenburg.de,95%
https://arxiv.org/pdf/2301.11357.pdf,Multimodal Event Transformer for Image-guided Story Ending Generation,Guodong Long,guodong.long@uts.edu.au,95%
https://arxiv.org/pdf/2301.11357.pdf,Multimodal Event Transformer for Image-guided Story Ending Generation,Yucheng Zhou,yucheng.zhou-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Malte Hoffmann,mhoffmann@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Andrew Hoopes,,0%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Douglas N. Greve,,0%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Bruce Fischl,,0%
https://arxiv.org/pdf/2301.11329.pdf,Anatomy-aware and acquisition-agnostic joint registration with SynthMorph,Adrian V. Dalca,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Aliaksandr Siarohin,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Willi Menapace,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Ivan Skorokhodov,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Kyle Olszewski,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Jian Ren,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Hsin-ying Lee,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Menglei Chai,,0%
https://arxiv.org/pdf/2301.11326.pdf,Unsupervised Volumetric Animation,Sergey Tulyakov,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Xudong Wang,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Rohit Girdhar,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Stella X. Yu,,0%
https://arxiv.org/pdf/2301.11320.pdf,Cut and Learn for Unsupervised Object Detection and Instance Segmentation,Ishan Misra,,0%
https://arxiv.org/pdf/2301.11316.pdf,Open Problems in Applied Deep Learning,Maziar Raissi,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Mingquan Lin,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Yuyun Xiao,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Bojian Hou,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Tingyi Wanyan,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Mohit Manoj Sharma,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Zhangyang Wang,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Fei Wang,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Sarah Van Tassel,,0%
https://arxiv.org/pdf/2301.11315.pdf,Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations,Yifan Peng,,0%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Luca De Luigi,luca.deluigi4@unibo.it,95%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Pierluigi Zama Ramirez,pierluigi.zama@unibo.it,85%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Samuele Salti,samuele.salti@unibo.it,95%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Alessio Tonioni,alessiot@google.com,85%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Luigi Di Stefano,luigi.distefano@unibo.it,95%
https://arxiv.org/pdf/2301.11310.pdf,Learning Good Features to Transfer Across Tasks and Domains,Adriano Cardace,adriano.cardace2@unibo.it,95%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Uriel Singer,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Shelly Sheynin,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Adam Polyak,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Oron Ashual,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Iurii Makarov,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Filippos Kokkinos,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Naman Goyal,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Andrea Vedaldi,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Devi Parikh,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Justin Johnson,,0%
https://arxiv.org/pdf/2301.11280.pdf,Text-To-4D Dynamic Scene Generation,Yaniv Taigman,,0%
https://arxiv.org/pdf/2301.11274.pdf,Self-Supervised RGB-T Tracking with Cross-Input Consistency,Xingchen Zhang,xingchen.zhang@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.11274.pdf,Self-Supervised RGB-T Tracking with Cross-Input Consistency,Yiannis Demiris,y.demiris@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Xianglong Liu,xlliu@buaa.edu.cn,82%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Haotong Qin,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Mingyuan Zhang,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Yifu Ding,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Aoyu Li,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Zhongang Cai,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Ziwei Liu,,0%
https://arxiv.org/pdf/2301.11233.pdf,BiBench: Benchmarking and Analyzing Network Binarization,Fisher Yu,,0%
https://arxiv.org/pdf/2301.11201.pdf,Relative-Interior Solution for the (Incomplete) Linear Assignment Problem with Applications to the Quadratic Assignment Problem,Bogdan Savchynskyy,bogdan.savchynskyy@iwr.uni-heidelberg.de,95%
https://arxiv.org/pdf/2301.11201.pdf,Relative-Interior Solution for the (Incomplete) Linear Assignment Problem with Applications to the Quadratic Assignment Problem,Tomáš Dlask,dlaskto2@fel.cvut.cz,78%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Derek Gloudemans,derek.gloudemans@vanderbilt.edu,95%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Yanbing Wang,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Junyi Ji,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Gergely Zachar,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Will Barbour,,0%
https://arxiv.org/pdf/2301.11198.pdf,I-24 MOTION: An instrument for freeway traffic science,Daniel B. Work,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Matthew J. Muckley,mmuckley@meta.com,82%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Alaaeldin El-nouby,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Karen Ullrich,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Hervé Jégou,,0%
https://arxiv.org/pdf/2301.11189.pdf,Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models,Jakob Verbeek,,0%
https://arxiv.org/pdf/2301.11180.pdf,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,Mingbao Lin,linmb001@outlook.com,78%
https://arxiv.org/pdf/2301.11180.pdf,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,Ziran Qin,qinziran@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.11180.pdf,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,Weiyao Lin,wylin@sjtu.edu.cn,82%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,Tae-hyun Oh,taehyun@postech.ac.kr,85%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,Dong-jin Kim,jinsc37@kaist.ac.kr,85%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,In So Kweon,iskweon77@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.11174.pdf,Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data,Jinsoo Choi,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Michal Kawulok,michal.kawulok@polsl.pl,95%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Jakub Nalepa,jakub.nalepa@polsl.pl,95%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Tomasz Tarasiewicz,tomasz.tarasiewicz@polsl.pl,95%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Reuben A. Farrugia,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Gianluca Valentino,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Mang Chen,,0%
https://arxiv.org/pdf/2301.11154.pdf,Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images,Johann A. Briffa,,0%
https://arxiv.org/pdf/2301.11145.pdf,Learning from Mistakes: Self-Regularizing Hierarchical Representations in Point Cloud Semantic Segmentation,Umberto Michieli,umberto.michieli@dei.unipd.it,95%
https://arxiv.org/pdf/2301.11145.pdf,Learning from Mistakes: Self-Regularizing Hierarchical Representations in Point Cloud Semantic Segmentation,Elena Camuffo,elena.camuffo@dei.unipd.it,95%
https://arxiv.org/pdf/2301.11145.pdf,Learning from Mistakes: Self-Regularizing Hierarchical Representations in Point Cloud Semantic Segmentation,Simone Milani,simone.milani@dei.unipd.it,95%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Jingjia Huang,huangjingjia@bytedance.com,95%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Jiashi Feng,jshfeng@bytedance.com,82%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Xinglong Wu,wuxinglong@bytedance.com,95%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Thomas H. Li,thomas@.pku.edu.cn,85%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Ruyang Liu,,0%
https://arxiv.org/pdf/2301.11116.pdf,Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring,Ge Li,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Sangwoo Mo,swmo@umich.edu,82%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Younghyun Kim,younghyun.kim@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Minkyu Kim,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Kyungmin Lee,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Jaeho Lee,,0%
https://arxiv.org/pdf/2301.11104.pdf,Discovering and Mitigating Visual Biases through Keyword Explanation,Jinwoo Shin,,0%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Melissa Hall,melissahall@meta.com,95%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Candace Ross,ccross@meta.com,82%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Laura Gustafson,,0%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Aaron Adcock,,0%
https://arxiv.org/pdf/2301.11100.pdf,Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities,Ishan Misra,,0%
https://arxiv.org/pdf/2301.11093.pdf,Simple diffusion: End-to-end diffusion for high resolution images,Emiel Hoogeboom,emielh@google.com,85%
https://arxiv.org/pdf/2301.11093.pdf,Simple diffusion: End-to-end diffusion for high resolution images,Jonathan Heek,,0%
https://arxiv.org/pdf/2301.11093.pdf,Simple diffusion: End-to-end diffusion for high resolution images,Tim Salimans,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Penghao Jiang,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Ke Xin,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Zifeng Wang,,0%
https://arxiv.org/pdf/2301.11779.pdf,Invariant Meta Learning for Out-of-Distribution Generalization,Chunxi Li,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Hyeongji Kim,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Pekka Parviainen,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Terje Berge,,0%
https://arxiv.org/pdf/2301.11065.pdf,Inspecting class hierarchies in classification-based metric learning models,Ketil Malde,,0%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Abhishek Kumar,abhishek.ai@knu.ac.kr,85%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Athul Shibu,athulshibu@knu.ac.kr,95%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Dong-gyu Lee,dglee@knu.ac.kr,82%
https://arxiv.org/pdf/2301.11063.pdf,Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning,Heechul Jung,heechul@knu.ac.kr,85%
https://arxiv.org/pdf/2302.01394.pdf,Understanding and contextualising diffusion models,Stefano Scotta,,0%
https://arxiv.org/pdf/2302.01394.pdf,Understanding and contextualising diffusion models,Alberto Messina,,0%
https://arxiv.org/pdf/2301.11022.pdf,Semantic Segmentation Enhanced Transformer Model for Human Attention Prediction,Shuo Zhang,,0%
https://arxiv.org/pdf/2301.11015.pdf,Explore the Power of Dropout on Few-shot Learning,Shaobo Lin,,0%
https://arxiv.org/pdf/2301.11015.pdf,Explore the Power of Dropout on Few-shot Learning,Xingyu Zeng,,0%
https://arxiv.org/pdf/2301.11015.pdf,Explore the Power of Dropout on Few-shot Learning,Rui Zhao,,0%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Yao Zhao,yzhao@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Shangrong Yang,sr yang@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Kang Liao,kang liao@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.11785.pdf,Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization,Chunyu Lin,cylin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.10972.pdf,On the Importance of Noise Scheduling for Diffusion Models,Ting Chen,iamtingchen@google.com,95%
https://arxiv.org/pdf/2301.10958.pdf,Learning Large Scale Sparse Models,Atul Dhingra,atul.dhingra@rutgers.edu,95%
https://arxiv.org/pdf/2301.10958.pdf,Learning Large Scale Sparse Models,Jie Shen,,0%
https://arxiv.org/pdf/2301.10958.pdf,Learning Large Scale Sparse Models,Nicholas Kleene,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Atul Dhingra,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Adeboye A. Adejare,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Adam Fendler,,0%
https://arxiv.org/pdf/2301.10957.pdf,Neurorehab: An Interface for Rehabilitation,Roopeswar Kommalapati,,0%
https://arxiv.org/pdf/2301.10951.pdf,Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images,Ali Vosoughi,mvosough@ece.rochester.edu,55%
https://arxiv.org/pdf/2301.10951.pdf,Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images,Nathan Hadjiyski,,0%
https://arxiv.org/pdf/2301.10951.pdf,Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images,Axel Wismueller,,0%
https://arxiv.org/pdf/2301.10941.pdf,GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,Jiuhn Song,song@korea.ac.kr,78%
https://arxiv.org/pdf/2301.10941.pdf,GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,Seungryong Kim,kim@korea.ac.kr,78%
https://arxiv.org/pdf/2301.10941.pdf,GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,Min-seop Kwak,mskwak01@korea.ac.kr,82%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Sachit Menon,sachit.menon@columbia.edu,95%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Scott Geng,scott.geng@columbia.edu,95%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Revant Teotia,,0%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Purva Tendulkar,,0%
https://arxiv.org/pdf/2301.10939.pdf,Affective Faces for Goal-Driven Dyadic Communication,Carl Vondrick,,0%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Junqing Yu,yjqing@hust.edu.cn,75%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Zikai Song,skyesong@hust.edu.cn,78%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Yi-ping Phoebe Chen,phoebe.chen@latrobe.edu.au,78%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Wei Yang,weiyangcs@hust.edu.cn,95%
https://arxiv.org/pdf/2301.10938.pdf,Compact Transformer Tracker with Correlative Masked Modeling,Run Luo,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Fanman Meng,fmmeng@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Qingbo Wu,qbwu@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Linfeng Xu,lfxu@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Hongliang Li,hlli@uestc.edu.cn,82%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Lili Pan,lilipan@uestc.edu.cn,95%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Chiyuan He,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Hanxin Wang,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Shaoxu Cheng,,0%
https://arxiv.org/pdf/2301.10931.pdf,Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning,Yu Dai,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Gui-song Xia,guisong.xia@whu.edu.cn,95%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Chao Pang,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Jiang Wu,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Jian Ding,,0%
https://arxiv.org/pdf/2301.10922.pdf,Detecting Building Changes with Off-Nadir Aerial Images,Can Song,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Ran Tao,taoran1@cmu.edu,95%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Marios Savvides,marioss@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Hao Chen,haoc3@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Jindong Wang,jindong.wang@microsoft.com,95%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Yue Fan,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Yidong Wang,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Xing Xie,,0%
https://arxiv.org/pdf/2301.10921.pdf,SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning,Bhiksha Raj,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Yunpeng Bai,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Jiayue Liu,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Chao Dong,,0%
https://arxiv.org/pdf/2301.10916.pdf,ITstyler: Image-optimized Text-based Style Transfer,Chun Yuan,,0%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,James Bailey,baileyj@unimelb.edu.au,78%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,Xingjun Ma,xingjunma@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,Sarah Erfani,sarah.erfani@unimelb.edu.au,95%
https://arxiv.org/pdf/2301.10908.pdf,Distilling Cognitive Backdoor Patterns within an Image,Hanxun Huang,hanxunh@student.unimelb.edu.au,85%
https://arxiv.org/pdf/2301.10906.pdf,Facial Expression Recognition using Squeeze and Excitation-powered Swin Transformers,Arpita Vats,avats@scu.edu,82%
https://arxiv.org/pdf/2301.10906.pdf,Facial Expression Recognition using Squeeze and Excitation-powered Swin Transformers,Aman Chadha,,0%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Errui Ding,dingerrui@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Jian Wang,wangjian33@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Jingdong Wang,wangjingdong@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Haocheng Feng,fenghaocheng@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Xiaohu Huang,huangxiaohu@hust.edu.cn,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Junyu Han,hanjunyu@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Hao Zhou,zhouhao14@baidu.com,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Bin Feng,fengbin@hust.edu.cn,95%
https://arxiv.org/pdf/2301.10900.pdf,Graph Contrastive Learning for Skeleton-based Action Recognition,Wenyu Liu,liuwy@hust.edu.cn,78%
https://arxiv.org/pdf/2301.10877.pdf,The Projection-Enhancement Network (PEN),Bo Sun,sunb@onid.orst.edu,78%
https://arxiv.org/pdf/2301.10877.pdf,The Projection-Enhancement Network (PEN),Christopher Z. Eddy,,0%
https://arxiv.org/pdf/2301.10877.pdf,The Projection-Enhancement Network (PEN),Austin Naylor,,0%
https://arxiv.org/pdf/2301.10876.pdf,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,Rohitash Chandra,rohitash.chandra@unsw.edu.au,95%
https://arxiv.org/pdf/2301.10876.pdf,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,Saharsh Barve,saharshbarve3@gmail.com,95%
https://arxiv.org/pdf/2301.10876.pdf,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,Jody M. Webster,jody.webster@sydney.edu.au,95%
https://arxiv.org/pdf/2301.10863.pdf,Shape Reconstruction from Thoracoscopic Images using Self-supervised Virtual Learning,Tomoki Oya,t-oya@sys.i.kyoto-u.ac.jp,82%
https://arxiv.org/pdf/2301.10863.pdf,Shape Reconstruction from Thoracoscopic Images using Self-supervised Virtual Learning,Megumi Nakao,,0%
https://arxiv.org/pdf/2301.10863.pdf,Shape Reconstruction from Thoracoscopic Images using Self-supervised Virtual Learning,Tetsuya Matsuda,,0%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Yiwei Jia,yiwei.jia@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Reza Azad,reza.azad; yiwei.jia@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Julien Cohen-adad,jcohen@polymtl.ca,90%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Dorit Merhof,dorit.merhof@ur.de,95%
https://arxiv.org/pdf/2301.10847.pdf,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach,Ehsan Khodapanah Aghdam,ehsan.khpaghdam@gmail.com,95%
https://arxiv.org/pdf/2301.10829.pdf,TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction,Zeynel A. Samak,,0%
https://arxiv.org/pdf/2301.10829.pdf,TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction,Philip Clatworthy,,0%
https://arxiv.org/pdf/2301.10829.pdf,TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction,Majid Mirmehdi,,0%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Zichao Li,zli489@ucsc.edu,82%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Shaoyuan Xie,shaoyux@uci.edu,84%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Cihang Xie,cixie@ucsc.edu,82%
https://arxiv.org/pdf/2301.10766.pdf,On the Adversarial Robustness of Camera-based 3D Object Detection,Zeyu Wang,zwang615@ucsc.edu,82%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Stavros Tsogkas,stavros.t@samsung.com,85%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Alex Levinshtein,alex.lev@samsung.com,85%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Fengjia Zhang,f.zhang2@samsung.com,82%
https://arxiv.org/pdf/2301.10759.pdf,Efficient Flow-Guided Multi-frame De-fencing,Allan Jepson,allan.jepson@samsung.com,95%
https://arxiv.org/pdf/2301.10750.pdf,Out of Distribution Performance of State of Art Vision Model,Salman Rahman,salman@nyu.edu,85%
https://arxiv.org/pdf/2301.10750.pdf,Out of Distribution Performance of State of Art Vision Model,Wonkwon Lee,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Aotian Wu,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Pan He,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Xiao Li,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Ke Chen,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Sanjay Ranka,,0%
https://arxiv.org/pdf/2301.10732.pdf,An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation,Anand Rangarajan,,0%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Iván De Andrés Tamé,ivan.andrest@estudiante.uam.es,85%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Marcos Escudero-viñolo,marcos.escudero@uam.es,85%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Pablo Carballeira,pablo.carballeira@uam.es,95%
https://arxiv.org/pdf/2301.10687.pdf,Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification,Kirill Sirotkin,kirill.sirotkin@uam.es,95%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Yunpeng Bai,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Zihan Zhong,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Chao Dong,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Weichen Zhang,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Guowei Xu,,0%
https://arxiv.org/pdf/2301.10670.pdf,Towards Arbitrary Text-driven Image Manipulation via Space Alignment,Chun Yuan,,0%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Carsten T. Lüth,carsten.lueth@dkfz-heidelberg.de,85%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Till J. Bungert,,0%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Lukas Klein,,0%
https://arxiv.org/pdf/2301.10625.pdf,Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment,Paul F. Jaeger,,0%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Enjie Ghorbel,enjie.ghorbel@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Djamila Aouada,djamila.aouada@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Arunkumar Rathinam,arunkumar.rathinam@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Anis Kacem,anis.kacem@uni.lu,95%
https://arxiv.org/pdf/2301.10611.pdf,Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification,Indel Pal Singh,inder.singh@uni.lu,82%
https://arxiv.org/pdf/2301.10608.pdf,Connecting metrics for shape-texture knowledge in computer vision,Tiago Oliveira,,0%
https://arxiv.org/pdf/2301.10608.pdf,Connecting metrics for shape-texture knowledge in computer vision,Tiago Marques,,0%
https://arxiv.org/pdf/2301.10608.pdf,Connecting metrics for shape-texture knowledge in computer vision,Arlindo L. Oliveira,,0%
https://arxiv.org/pdf/2302.10801.pdf,Deep Generative Neural Embeddings for High Dimensional Data Visualization,Gerardo Hermosillo Valadez,gerardo.hermosillovaladez@siemens-healthineers.com,95%
https://arxiv.org/pdf/2302.10801.pdf,Deep Generative Neural Embeddings for High Dimensional Data Visualization,Halid Ziya Yerebakan,halid.yerebakan@siemens-healthineers.com,95%
https://arxiv.org/pdf/2301.10593.pdf,Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition,Denis Coquenet,denis.coquenet@lecnam.net,95%
https://arxiv.org/pdf/2301.10593.pdf,Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition,Thierry Paquet,thierry.paquet@litislab.eu,95%
https://arxiv.org/pdf/2301.10593.pdf,Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition,Clément Chatelain,clement.chatelain@litislab.eu,95%
https://arxiv.org/pdf/2301.10584.pdf,A Method For Eliminating Contour Errors In Self-Encoder Reconstructed Images,Hao Zhang,haozhang@stu.sicnu.edu.cn,95%
https://arxiv.org/pdf/2301.10584.pdf,A Method For Eliminating Contour Errors In Self-Encoder Reconstructed Images,Yonggang Li,,0%
https://arxiv.org/pdf/2301.10583.pdf,An Efficient Approximate Method for Online Convolutional Dictionary Learning,Farshad G. Veshki,,0%
https://arxiv.org/pdf/2301.10583.pdf,An Efficient Approximate Method for Online Convolutional Dictionary Learning,Sergiy A. Vorobyov,,0%
https://arxiv.org/pdf/2301.10575.pdf,Trainable Loss Weights in Super-Resolution,Arash Chaichi Mellatshahi,,0%
https://arxiv.org/pdf/2301.10575.pdf,Trainable Loss Weights in Super-Resolution,Shohreh Kasaei,,0%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Chamath Abeysinghe,chamath.abeysinghe@monash.edu,95%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Bernd Meyer,bernd.meyer@monash.edu,95%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Chris Reid,chris.reid@mq.edu.au,95%
https://arxiv.org/pdf/2301.10559.pdf,Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking,Hamid Rezatofighi,hamid.rezatoﬁghi@monash.edu,95%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Mingle Xu,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Jaehwan Lee,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Sook Yoon,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Hyongsuk Kim,,0%
https://arxiv.org/pdf/2301.10551.pdf,Variation-Aware Semantic Image Synthesis,Dong Sun Park,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,David W. Romero,d.w.romeroguzman@vu.nl,82%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,David M. Knigge,d.m.knigge@uva.nl,82%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Albert Gu,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Efstratios Gavves,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Erik J. Bekkers,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Jakub M. Tomczak,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Mark Hoogendoorn,,0%
https://arxiv.org/pdf/2301.10540.pdf,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,Jan-jakob Sonke,,0%
https://arxiv.org/pdf/2301.10531.pdf,3D Tooth Mesh Segmentation with Simplified Mesh Cell Representation,Ananya Jana,,0%
https://arxiv.org/pdf/2301.10531.pdf,3D Tooth Mesh Segmentation with Simplified Mesh Cell Representation,Hrebesh Molly Subhash,,0%
https://arxiv.org/pdf/2301.10531.pdf,3D Tooth Mesh Segmentation with Simplified Mesh Cell Representation,Dimitris N. Metaxas,,0%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Nassir Navab,nassir.navab@tum.de,95%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Mehrdad Salehi,mehrdad.salehi@tum.de,95%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Benjamin Busam,b.busam@tum.de,82%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Christine Eilers,christine.eilers@tum.de,95%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Mohammad Farid Azampour,mf.azampour@tum.de,82%
https://arxiv.org/pdf/2301.10520.pdf,Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging,Magdalena Wysocki,magdalena.wysocki@tum.de,95%
https://arxiv.org/pdf/2302.10306.pdf,Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet Integration,Masoud Shahraki Mohammadi,mahdavi@mshdiau.ac.ir,65%
https://arxiv.org/pdf/2302.10306.pdf,Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet Integration,Seyed Javad Seyed Mahdavi Chabok,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Yushan Zhang,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Andreas Robinson,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Maria Magnusson,,0%
https://arxiv.org/pdf/2301.10492.pdf,Flow-guided Semi-supervised Video Object Segmentation,Michael Felsberg,,0%
https://arxiv.org/pdf/2301.10473.pdf,Aircraft Skin Inspections: Towards a New Model for Dent Evaluation,Pasquale Lafiosca,pasquale.lafiosca@cranfield.ac.uk,95%
https://arxiv.org/pdf/2301.10473.pdf,Aircraft Skin Inspections: Towards a New Model for Dent Evaluation,Ip-shing Fan,,0%
https://arxiv.org/pdf/2301.10473.pdf,Aircraft Skin Inspections: Towards a New Model for Dent Evaluation,Nicolas P. Avdelidis,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Fenggen Yu,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Yiming Qian,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Francisca Gil-ureta,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Brian Jackson,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Eric Bennett,,0%
https://arxiv.org/pdf/2301.10460.pdf,HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling,Hao Zhang,,0%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Pengwei Zhang,zhangpengwei@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Chengqian Ma,machengqian01@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Long Zheng,zhenglong@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Chunlei Cai,caichunlei@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Yi Wang,wangyi@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Chao Chen,chenchao02@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Zhiqiang Wu,wuzhiqiang01@bilibili.com,95%
https://arxiv.org/pdf/2301.10455.pdf,Rate-Perception Optimized Preprocessing for Video Coding,Quan Zhou,zhouquan@bilibili.com,95%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Arman Zarei,arman.zarei@sharif.edu,95%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Mohammad Azizmalayeri,m.azizmalayeri@sharif.edu,82%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Mohammad Hossein Rohban,rohban@sharif.edu,78%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Alireza Isavand,alireza.isavand@sharif.edu,95%
https://arxiv.org/pdf/2301.10454.pdf,A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection,Mohammad Taghi Manzuri,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Tongzhi Niu,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Bin Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Kai Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Yufeng Lin,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Yuwei Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Weifeng Li,,0%
https://arxiv.org/pdf/2301.10441.pdf,Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection,Zhenrong Wang,,0%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Angela Yao,ayao@comp.nus.edu.sg,82%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Kerui Gu,keruigu@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Linlin Yang,yangll@comp.nus.edu.sg,78%
https://arxiv.org/pdf/2301.10431.pdf,Bias-Compensated Integral Regression for Human Pose Estimation,Michael Bi Mi,,0%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Chen Sun,chen.sun@sony.com,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Lixu Wang,lixuwang2025@u.northwestern.edu,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Lingjuan Lyu,Lingjuan.Lv@sony.com,85%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Xiao Wang,wangxiao@northwestern.edu,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Chenxi Liu,chenxiliu2020@u.northwestern.edu,95%
https://arxiv.org/pdf/2301.10418.pdf,DEJA VU: Continual Model Generalization For Unseen Domains,Qi Zhu,qzhu@northwestern.edu,82%
https://arxiv.org/pdf/2301.10413.pdf,Local Feature Extraction from Salient Regions by Feature Map Transformation,Nur Suriza Syazwany Binti Ahmad Nizam,surizasyazwany@inha.edu,90%
https://arxiv.org/pdf/2301.10413.pdf,Local Feature Extraction from Salient Regions by Feature Map Transformation,Sang-chul Lee,sclee@inha.ac.kr,82%
https://arxiv.org/pdf/2301.10413.pdf,Local Feature Extraction from Salient Regions by Feature Map Transformation,Yerim Jung,,0%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Neel Dey,dey@mit.edu,78%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Nalini M. Singh,nmsingh@mit.edu,82%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Bruce Fischl,bfischl@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Adrian V. Dalca,adalca@mit.edu,82%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Polina Golland,polina@csail.mit.edu,85%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Elfar Adalsteinsson,elfar@mit.edu,85%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Robert Frost,srfrost@mgh.harvard.edu,78%
https://arxiv.org/pdf/2301.10365.pdf,Data Consistent Deep Rigid MRI Motion Correction,Malte Hoffmann,mhoffmann@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,John Lagergren,lagergrenjh@ornl.gov,78%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Hari B. Chhetri,streichjc@ornl.gov,75%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Daniel Jacobson,jacobsonda@ornl.gov,78%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Mirko Pavicic,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Larry M. York,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,P. Doug Hyatt,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,David Kainer,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Erica M. Rutter,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Kevin Flores,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Jack Bailey-bale,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Marie Klein,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Gail Taylor,,0%
https://arxiv.org/pdf/2301.10351.pdf,Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa,Jared Streich,,0%
https://arxiv.org/pdf/2301.10327.pdf,Generating Multidimensional Clusters With Support Lines,Nuno Fachada,nuno.fachada@ulusofona.pt,95%
https://arxiv.org/pdf/2301.10327.pdf,Generating Multidimensional Clusters With Support Lines,Diogo De Andrade,diogo.andrade@ulusofona.pt,95%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Benjamin Recht,brecht@berkeley.edu,82%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Angjoo Kanazawa,kanazawa@berkeley.edu,78%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Giacomo Meanti,giacomo.meanti@iit.it,95%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Sara Fridovich-keil,sfk@berkeley.edu,98%
https://arxiv.org/pdf/2301.10241.pdf,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",Frederik Warburg,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Angelika Ando,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Spyros Gidaris,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Andrei Bursuc,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Gilles Puy,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Alexandre Boulch,,0%
https://arxiv.org/pdf/2301.10222.pdf,RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving,Renaud Marlet,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,Xueshen Li,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,Yu Gan,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,David Duan,,0%
https://arxiv.org/pdf/2301.10218.pdf,Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope,Xiao Yang,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Junyu Wang,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Shijie Wang,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Wenyu Liu,,0%
https://arxiv.org/pdf/2301.10208.pdf,A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction,Zengqiang Zheng,,0%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Min Xian,mxian@uidaho.edu,82%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Sujata Butte,,0%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Haotian Wang,,0%
https://arxiv.org/pdf/2301.10187.pdf,Enhanced Sharp-GAN For Histopathology Image Synthesis,Aleksandar Vakanski,,0%
https://arxiv.org/pdf/2301.10134.pdf,Bipartite Graph Diffusion Model for Human Interaction Generation,Baptiste Chopin,,0%
https://arxiv.org/pdf/2301.10134.pdf,Bipartite Graph Diffusion Model for Human Interaction Generation,Hao Tang,,0%
https://arxiv.org/pdf/2301.10134.pdf,Bipartite Graph Diffusion Model for Human Interaction Generation,Mohamed Daoudi,,0%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Lennart Svensson,lennart.svensson@chalmers.se,95%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Lars Hammarstrand,lars.hammarstrand@chalmers.se,95%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Erik Wallin,walline@chalmers.se,78%
https://arxiv.org/pdf/2301.10127.pdf,Improving Open-Set Semi-Supervised Learning with Self-Supervision,Fredrik Kahl,fredrik.kahl@chalmers.se,95%
https://arxiv.org/pdf/2301.10100.pdf,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,Gilles Puy,,0%
https://arxiv.org/pdf/2301.10100.pdf,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,Alexandre Boulch,,0%
https://arxiv.org/pdf/2301.10100.pdf,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,Renaud Marlet,,0%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Charles Dansereau,charles.dansereau@polymtl.ca,95%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Mehdi Zalai,mehdi.zalai@polymtl.ca,95%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Milo Sobral,milo.sobral@polymtl.ca,95%
https://arxiv.org/pdf/2301.10092.pdf,Model soups to increase inference without increasing compute time,Maninder Bhogal,maninder.bhogal@polymtl.ca,95%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Romain Xu-darme,romain.xu-darme@cea.fr,95%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Zakaria Chihani,zakaria.chihani@cea.fr,95%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Julien Girard-satabin,julien.girard2@cea.fr,85%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Darryl Hond,,0%
https://arxiv.org/pdf/2302.10303.pdf,Interpretable Out-Of-Distribution Detection Using Pattern Identification,Gabriele Incorvaia,,0%
https://arxiv.org/pdf/2301.10057.pdf,Planar Object Tracking via Weighted Optical Flow,Jiri Matas,matas@fel.cvut.cz,78%
https://arxiv.org/pdf/2301.10057.pdf,Planar Object Tracking via Weighted Optical Flow,Jonas Serych,serycjon@fel.cvut.cz,75%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Blas Kojusner,bkojusner@ufl.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Pirouz Naghavi,pnaghavi@ufl.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Kevin Butler,butler@ufl.edu,78%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Sara Rampazzi,srampazzi@ufl.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Kevin Fu,k.fu@northeastern.edu,82%
https://arxiv.org/pdf/2301.10056.pdf,Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,Yan Long,yanlong@umich.edu,95%
https://arxiv.org/pdf/2301.10052.pdf,Event Detection in Football using Graph Convolutional Networks,Aditya Sangram Singh Rana,adityasangramsingh.rana@e-campus.uab.cat,95%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Zanjia Tong,,0%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Yuhang Chen,,0%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Zewei Xu,,0%
https://arxiv.org/pdf/2301.10051.pdf,Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism,Rong Yu,,0%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Jingjing Fu,jifu@microsoft.com,82%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Dong Liu,dongeliu@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Kaidong Zhang,,0%
https://arxiv.org/pdf/2301.10048.pdf,Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting,Jialun Peng,,0%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Naye Ji,jinaye@cuz.edu.cn,95%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Fan Zhang,fanzhang@cuz.edu.cn,95%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Yongping Li,liyongping@nbufe.edu.cn,95%
https://arxiv.org/pdf/2301.10047.pdf,DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,Fuxing Gao,fuxing@cuz.edu.cn,85%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Peijie Dong,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Xin Niu,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Zhiliang Tian,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Lujun Li,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Xiaodong Wang,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Zimian Wei,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Hengyue Pan,,0%
https://arxiv.org/pdf/2301.10038.pdf,Progressive Meta-Pooling Learning for Lightweight Image Classification Model,Dongsheng Li,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Xiao He,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Mingrui Zhu,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Nannan Wang,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Xinbo Gao,,0%
https://arxiv.org/pdf/2301.10008.pdf,Few-shot Font Generation by Learning Style Difference and Similarity,Heng Yang,,0%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Li Liu,dreamliu2010@gmail.com,95%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Wanxia Deng,dengwanxia14@nudt.edu.cn,95%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Yawen Cui,,0%
https://arxiv.org/pdf/2301.09964.pdf,Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning,Haoyu Chen,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Verena Jasmin Hallitschke,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Tobias Schlumberger,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Philipp Kataliakos,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Zdravko Marinov,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Moon Kim,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Lars Heiliger,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Constantin Seibold,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Jens Kleesiek,,0%
https://arxiv.org/pdf/2301.09914.pdf,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,Rainer Stiefelhagen,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Mathias Zinnen,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Prathmesh Madhu,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Peter Bell,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Andreas Maier,,0%
https://arxiv.org/pdf/2301.09906.pdf,Transfer Learning for Olfactory Object Detection,Vincent Christlein,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Azadeh Fakhrzadeh,fakhrzadeh@irandoc.ac.ir,78%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Pouya Karimian,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Mahsa Meyari,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Cris L. Luengo Hendriks,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Lena Holm,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Christian Sonne,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Rune Dietz,,0%
https://arxiv.org/pdf/2301.09887.pdf,Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue,Ellinor Spörndly-nees,,0%
https://arxiv.org/pdf/2301.09879.pdf,Data Augmentation Alone Can Improve Adversarial Training,Lin Li,michael.spratling@kcl.ac.uk,95%
https://arxiv.org/pdf/2301.09879.pdf,Data Augmentation Alone Can Improve Adversarial Training,Michael Spratling,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Mathias Zinnen,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Prathmesh Madhu,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Ronak Kosti,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Peter Bell,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Andreas Maier,,0%
https://arxiv.org/pdf/2301.09878.pdf,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,Vincent Christlein,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Jinpeng Shi,jinpeeeng.s@gmail.com,75%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Hui Li,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Tianle Liu,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Yulong Liu,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Mingjian Zhang,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Jinchen Zhu,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Ling Zheng,,0%
https://arxiv.org/pdf/2301.09869.pdf,Image Super-Resolution using Efficient Striped Window Transformer,Shizhuang Weng,,0%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Edouard Yvinec,ey@datakalab.com,90%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Arnaud Dapogny,,0%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Matthieu Cord,,0%
https://arxiv.org/pdf/2301.09858.pdf,PowerQuant: Automorphism Search for Non-Uniform Quantization,Kevin Bailly,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Peijie Dong,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Xin Niu,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Lujun Li,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Zhiliang Tian,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Xiaodong Wang,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Zimian Wei,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Hengyue Pan,,0%
https://arxiv.org/pdf/2301.09850.pdf,RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies,Dongsheng Li,,0%
https://arxiv.org/pdf/2301.09799.pdf,LDMIC: Learning-based Distributed Multi-view Image Coding,Xinjie Zhang,xinjie.zhang@connect.ust.hk,95%
https://arxiv.org/pdf/2301.09799.pdf,LDMIC: Learning-based Distributed Multi-view Image Coding,Jiawei Shao,jiawei.shao@connect.ust.hk,95%
https://arxiv.org/pdf/2301.09799.pdf,LDMIC: Learning-based Distributed Multi-view Image Coding,Jun Zhang,,0%
https://arxiv.org/pdf/2301.11726.pdf,GAN-Based Object Removal in High-Resolution Satellite Images,Hadi Mansourifar,,0%
https://arxiv.org/pdf/2301.11726.pdf,GAN-Based Object Removal in High-Resolution Satellite Images,Steven J. Simske,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Joseph Bentsman,jbentsma@illinois.edu,90%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Martin Ostoja-starzewski,martinos@illinois.edu,85%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Leonardo P. Chamorro,lpchamo@illinois.edu,65%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Richard Berlin,pubs-permissions@ieee.org,65%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Junren Ran,jran2@illinois.edu,82%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Hamza El-kebir,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Yongseok Lee,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Gabriela M. Aguiluz Cornejo,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Enrico Benedetti,,0%
https://arxiv.org/pdf/2301.09733.pdf,Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography,Pier C. Giulianotti,,0%
https://arxiv.org/pdf/2301.09724.pdf,Long-tail Detection with Effective Class-Margins,Jang Hyun Cho,janghyuncho7@utexas.edu,95%
https://arxiv.org/pdf/2301.09724.pdf,Long-tail Detection with Effective Class-Margins,Philipp Krähenbühl,philkr@cs.utexas.edu,60%
https://arxiv.org/pdf/2301.09702.pdf,Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification,Jiaqi Guo,guo498@purdue.edu,78%
https://arxiv.org/pdf/2301.09702.pdf,Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification,Amy R. Reibman,reibman@purdue.edu,78%
https://arxiv.org/pdf/2301.09702.pdf,Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification,Edward J. Delp,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Amir Ghasemi,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Nasrin Bayat,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Fatemeh Mottaghian,,0%
https://arxiv.org/pdf/2301.09667.pdf,Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans,Akram Bayat,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Chieh Hubert Lin,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Hsin-ying Lee,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Willi Menapace,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Menglei Chai,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Aliaksandr Siarohin,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.09637.pdf,InfiniCity: Infinite-Scale City Synthesis,Sergey Tulyakov,,0%
https://arxiv.org/pdf/2301.09632.pdf,HexPlane: A Fast Representation for Dynamic Scenes,Ang Cao,ancao@umich.edu,82%
https://arxiv.org/pdf/2301.09632.pdf,HexPlane: A Fast Representation for Dynamic Scenes,Justin Johnson,justincj@umich.edu,85%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Qiuhong Anna Wei,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Sijie Ding,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Jeong Joon Park,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Rahul Sajnani,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Adrien Poulenard,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Srinath Sridhar,,0%
https://arxiv.org/pdf/2301.09629.pdf,LEGO-Net: Learning Regular Rearrangements of Objects in Rooms,Leonidas Guibas,,0%
https://arxiv.org/pdf/2301.09624.pdf,Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images,Piotr Keller,,0%
https://arxiv.org/pdf/2301.09624.pdf,Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images,Muhammad Dawood,,0%
https://arxiv.org/pdf/2301.09624.pdf,Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images,Fayyaz Ul Amir Afsar Minhas,,0%
https://arxiv.org/pdf/2301.09620.pdf,Tracking the industrial growth of modern China with high-resolution panchromatic imagery: A sequential convolutional approach,Ethan Brewer,ethan.brewer@nyu.edu,95%
https://arxiv.org/pdf/2301.09620.pdf,Tracking the industrial growth of modern China with high-resolution panchromatic imagery: A sequential convolutional approach,Dan Runfola,danr@wm.edu,85%
https://arxiv.org/pdf/2301.09620.pdf,Tracking the industrial growth of modern China with high-resolution panchromatic imagery: A sequential convolutional approach,Zhonghui Lv,zlv@wm.edu,82%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Sophia J. Wagner,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Daniel Reisenbüchler,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Nicholas P. West,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Jan Moritz Niehues,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Gregory Patrick Veldhuizen,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Philip Quirke,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Heike I. Grabsch,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Piet A. Van Den Brandt,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Gordon G. A. Hutchins,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Susan D. Richman,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Tanwei Yuan,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Rupert Langer,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Josien Christina Anna Jenniskens,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Kelly Offermans,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Wolfram Mueller,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Richard Gray,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Stephen B. Gruber,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Joel K. Greenson,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Gad Rennert,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Joseph D. Bonner,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Daniel Schmolze,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Jacqueline A. James,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Maurice B. Loughrey,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Manuel Salto-tellez,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Hermann Brenner,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Michael Hoffmeister,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Daniel Truhn,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Julia A. Schnabel,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Melanie Boxberg,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Tingying Peng,,0%
https://arxiv.org/pdf/2301.09617.pdf,Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study,Jakob Nikolas Kather,,0%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Jesus Angulo,jesus.angulo@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Etienne Decencière,etienne.decenciere@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Santiago Velasco-forero,santiago.velasco@minesparis.psl.eu,85%
https://arxiv.org/pdf/2301.09602.pdf,Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation,Joao P. C. Bertoldo,jpcbertoldo@minesparis.psl.eu,82%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Adrià Recasens,arecasens@google.com,82%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Jason Lin,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Joāo Carreira,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Drew Jaegle,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Luyu Wang,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Jean-baptiste Alayrac,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Pauline Luc,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Antoine Miech,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Lucas Smaira,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Ross Hemsley,,0%
https://arxiv.org/pdf/2301.09595.pdf,Zorro: the masked multimodal transformer,Andrew Zisserman,,0%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Nathalie Majcherczyk,majcherc@amazon.com,90%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Xuewei Qi,qixuewei@amazon.com,95%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Rajasimman Madhivanan,rajasimm@amazon.com,90%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Wenhao Ding,wenhaod@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Arnie Sen,senarnie@amazon.com,95%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Ding Zhao,dingzhao@andrew.cmu.edu,95%
https://arxiv.org/pdf/2301.09544.pdf,Learning to View: Decision Transformers for Active Object Detection,Mohit Deshpande,deshmohi@amazon.com,60%
https://arxiv.org/pdf/2301.09542.pdf,Improving Presentation Attack Detection for ID Cards on Remote Verification Systems,Sebastian Gonzalez,sebastian.gonzalez@tocbiometrics.com,95%
https://arxiv.org/pdf/2301.09542.pdf,Improving Presentation Attack Detection for ID Cards on Remote Verification Systems,Juan Tapia,juan.tapia-farias@h-da.de,95%
https://arxiv.org/pdf/2301.09525.pdf,DeepFEL: Deep Fastfood Ensemble Learning for Histopathology Image Analysis,Nima Hatami,hatami@creatis.insa-lyon.fr,78%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Dengyu Wu,dengyu.wu@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Xiaowei Huang,xiaowei.huang@liverpool.ac.uk,95%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Gaojie Jin,,0%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Han Yu,,0%
https://arxiv.org/pdf/2301.09522.pdf,Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff,Xinping Yi,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Axel Sauer,a.sauer@uni-tuebingen.de,82%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Tero Karras,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Samuli Laine,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Andreas Geiger,,0%
https://arxiv.org/pdf/2301.09515.pdf,StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis,Timo Aila,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Keyan Chen,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Xiaolong Jiang,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Yao Hu,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Xu Tang,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Yan Gao,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Jianqi Chen,,0%
https://arxiv.org/pdf/2301.09506.pdf,OvarNet: Towards Open-vocabulary Object Attribute Recognition,Weidi Xie,,0%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Liyan Zhang,zhangliyan@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Xiangbo Shu,shuxb@njust.edu.cn,78%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Fei Shen,feishen@njust.edu.cn,95%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Xiaoyu Du,duxy@njust.edu.cn,78%
https://arxiv.org/pdf/2301.09498.pdf,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,Jinhui Tang,jinhuitang@njust.edu.cn,95%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Alessandro Flaborea,flaborea@di.uniroma1.it,78%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Guido D'amely,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Stefano D'arrigo,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Marco Aurelio Sterpa,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Alessio Sampieri,,0%
https://arxiv.org/pdf/2301.09489.pdf,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,Fabio Galasso,,0%
https://arxiv.org/pdf/2301.09461.pdf,Study on the identification limits of craniofacial superimposition,Óscar Ibáñez,oscar.ibanez@udc.es,95%
https://arxiv.org/pdf/2301.09461.pdf,Study on the identification limits of craniofacial superimposition,Enrique Bermejo,,0%
https://arxiv.org/pdf/2301.09461.pdf,Study on the identification limits of craniofacial superimposition,Andrea Valsecchi,,0%
https://arxiv.org/pdf/2301.09460.pdf,HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images,Kun Li,,0%
https://arxiv.org/pdf/2301.09460.pdf,HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images,George Vosselman,,0%
https://arxiv.org/pdf/2301.09460.pdf,HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images,Michael Ying Yang,,0%
https://arxiv.org/pdf/2301.09451.pdf,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,Ishan Misra,imisra@meta.com,82%
https://arxiv.org/pdf/2301.09451.pdf,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,Nicolas Ballas,ballasn@meta.com,78%
https://arxiv.org/pdf/2301.09451.pdf,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,Quentin Duval,qduval@meta.com,82%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Thibaut Eloy,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Etienne Baudrier,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Marine Laporte,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Virginie Hamel,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Paul Guichard,,0%
https://arxiv.org/pdf/2301.09452.pdf,Fast and robust single particle reconstruction in 3D fluorescence microscopy,Denis Fortun,,0%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Shuaicheng Liu,liushuaicheng@uestc.edu.cn,95%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Haipeng Li,,0%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Kunming Luo,,0%
https://arxiv.org/pdf/2301.10018.pdf,GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning,Bing Zeng,,0%
https://arxiv.org/pdf/2301.09431.pdf,Multi-domain stain normalization for digital pathology: A cycle-consistent adversarial network for whole slide images,Tabea-clara Bucher,tabea.bucher@dkfz.de,95%
https://arxiv.org/pdf/2301.09431.pdf,Multi-domain stain normalization for digital pathology: A cycle-consistent adversarial network for whole slide images,Martin J. Hetz,martinjoachim.hetz@dkfz.de,95%
https://arxiv.org/pdf/2301.09431.pdf,Multi-domain stain normalization for digital pathology: A cycle-consistent adversarial network for whole slide images,Titus J. Brinker,titus.brinker@dkfz.de,95%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Yiyang Shen,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Mingqiang Wei,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Yongzhen Wang,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Xueyang Fu,,0%
https://arxiv.org/pdf/2301.09430.pdf,Rethinking Real-world Image Deraining via An Unpaired Degradation-Conditioned Diffusion Model,Jing Qin,,0%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Jiahao Huang,j.huang21@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Angelica Aviles-rivero,,0%
https://arxiv.org/pdf/2302.10273.pdf,ViGU: Vision GNN U-Net for Fast MRI,Carola-bibiane Schonlieb,,0%
https://arxiv.org/pdf/2302.10272.pdf,Is Autoencoder Truly Applicable for 3D CT Super-Resolution?,Weixun Luo,,0%
https://arxiv.org/pdf/2302.10272.pdf,Is Autoencoder Truly Applicable for 3D CT Super-Resolution?,Xiaodan Xing,,0%
https://arxiv.org/pdf/2302.10272.pdf,Is Autoencoder Truly Applicable for 3D CT Super-Resolution?,Guang Yang,,0%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Hao Wen,wenhao@tju.edu.cn,95%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Haozhe Lin,linhz@tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Huili Cui,huilicui 1@tju.edu.cn,95%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Kun Li,lik@tju.edu.cn,78%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Yukun Lai,LaiY4@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Lu Fang,fanglu@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.09376.pdf,Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,Jing Huang,,0%
https://arxiv.org/pdf/2301.09339.pdf,Computer Vision for a Camel-Vehicle Collision Mitigation System,Khalid Alnujaidi,,0%
https://arxiv.org/pdf/2301.09339.pdf,Computer Vision for a Camel-Vehicle Collision Mitigation System,Ghadah Alhabib,,0%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Eva Vandersmissen,eva.vandersmissen@agfa.com,95%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Annemiek Snoeckx,Annemiek.Snoeckx@uza.be,95%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Dimitrios Lenis,lenis@vrvis.at,78%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Jeroen Cant,jeroen.cant@agfa.com,95%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Maria Wimmer,mwimmer@vrvis.at,82%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Astrid Berg,berg@vrvis.at,78%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Theresa Neubauer,tneubauer@vrvis.at,82%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,David Major,major@vrvis.at,78%
https://arxiv.org/pdf/2301.09338.pdf,Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods,Katja Bühler,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Neus Rodeja Ferrer,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Malini Vendela Sagar,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Kiril Vadimovic Klein,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Christina Kruuse,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Mads Nielsen,,0%
https://arxiv.org/pdf/2301.09322.pdf,Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19,Mostafa Mehdipour Ghazi,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Johannes Jakubik,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Michal Muszynski,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Michael Vössing,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Niklas Kühl,,0%
https://arxiv.org/pdf/2301.09318.pdf,Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation,Thomas Brunschwiler,,0%
https://arxiv.org/pdf/2301.09315.pdf,AI-Based Framework for Understanding Car Following Behaviors of Drivers in A Naturalistic Driving Environment,Yaw Adu-gyamfi,adugyamfiym@missouri.edu,75%
https://arxiv.org/pdf/2301.09315.pdf,AI-Based Framework for Understanding Car Following Behaviors of Drivers in A Naturalistic Driving Environment,Armstrong Aboah,,0%
https://arxiv.org/pdf/2301.09315.pdf,AI-Based Framework for Understanding Car Following Behaviors of Drivers in A Naturalistic Driving Environment,Abdul Rashid Mussah,,0%
https://arxiv.org/pdf/2301.09299.pdf,Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay,Yinheng Li,,0%
https://arxiv.org/pdf/2301.09299.pdf,Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay,Han Ding,,0%
https://arxiv.org/pdf/2301.09299.pdf,Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay,Shaofei Wang,,0%
https://arxiv.org/pdf/2301.09282.pdf,Classification of Luminal Subtypes in Full Mammogram Images Using Transfer Learning,Adarsh Bhandary Panambur,,0%
https://arxiv.org/pdf/2301.09282.pdf,Classification of Luminal Subtypes in Full Mammogram Images Using Transfer Learning,Prathmesh Madhu,,0%
https://arxiv.org/pdf/2301.09282.pdf,Classification of Luminal Subtypes in Full Mammogram Images Using Transfer Learning,Andreas Maier,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Brian Li,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Steven Palayew,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Francis Li,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Saad Abbasi,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Saeejith Nair,,0%
https://arxiv.org/pdf/2301.09268.pdf,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,Alexander Wong,,0%
https://arxiv.org/pdf/2301.09266.pdf,FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing Flows,Aditya Kallappa,aditya.kallappa@research.iiit.ac.in,95%
https://arxiv.org/pdf/2301.09266.pdf,FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing Flows,Sandeep Nagar,sandeep.nagar@research.iiit.ac.in,95%
https://arxiv.org/pdf/2301.09266.pdf,FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing Flows,Girish Varma,girish.varma@iiit.ac.in,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Mahdi Zolnouri,mahdi.zolnouri@huawei.com,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Sébastien Le Digabel,sebastien.le.digabel@gerad.ca,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Eyyüb Sari,eyyub.sari@huawei.com,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Christophe Tribes,christophe.tribes@polymtl.ca,95%
https://arxiv.org/pdf/2301.09264.pdf,Efficient Training Under Limited Resources,Dounia Lakhmiri,dounia.lakhmiri@polymtl.ca,95%
https://arxiv.org/pdf/2301.09257.pdf,Real-Time Simultaneous Localization and Mapping with LiDAR intensity,Giovanni Beltrame,giovanni.beltrame@polymtl.ca,95%
https://arxiv.org/pdf/2301.09257.pdf,Real-Time Simultaneous Localization and Mapping with LiDAR intensity,Wenqiang Du,wenqiang.du@polymtl.ca,95%
https://arxiv.org/pdf/2301.09255.pdf,Combined Use of Federated Learning and Image Encryption for Privacy-Preserving Image Classification with Vision Transformer,Hitoshi Kiya,kiya@tmu.ac.jp,78%
https://arxiv.org/pdf/2301.09255.pdf,Combined Use of Federated Learning and Image Encryption for Privacy-Preserving Image Classification with Vision Transformer,Teru Nagamori,nagamori-teru@ed.tmu.ac.jp,95%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Peter A. Beerel,pabeerel@usc.edu,82%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Jacqueline Liu,jtliu@usc.edu,82%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Souvik Kundu,souvikk.kundu@intel.com,95%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Shunlin Lu,shunlinlu@usc.edu,95%
https://arxiv.org/pdf/2301.09254.pdf,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,Yuke Zhang,yukezhan@usc.edu,85%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Huan Lei,,0%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Ruitao Leng,,0%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Liang Zheng,,0%
https://arxiv.org/pdf/2301.09253.pdf,CircNet: Meshing 3D Point Clouds with Circumcenter Detection,Hongdong Li,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Yadan Luo,y.luo@uq.edu.au,82%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Zhuoxiao Chen,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Zijian Wang,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Xin Yu,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Zi Huang,,0%
https://arxiv.org/pdf/2301.09249.pdf,Exploring Active 3D Object Detection from a Generalization Perspective,Mahsa Baktashmotlagh,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Chandana Raju,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Sumedh Vilas Datar,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Kushala Hari,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Kavin Vijay,,0%
https://arxiv.org/pdf/2301.09219.pdf,Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images,Suma Ningappa,,0%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,Nikolaos Stathoulopoulos,niksta@ltu.se,60%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,Anton Koval,,0%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,Ali-akbar Agha-mohammadi,,0%
https://arxiv.org/pdf/2301.09213.pdf,FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for Egocentric multi-robot exploration,George Nikolakopoulos,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Razvan-george Pasca,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Alexey Gavryushin,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Muhammad Hamza,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Yen-ling Kuo,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Kaichun Mo,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Luc Van Gool,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Otmar Hilliges,,0%
https://arxiv.org/pdf/2301.09209.pdf,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,Xi Wang,,0%
https://arxiv.org/pdf/2301.09190.pdf,Apples and Oranges? Assessing Image Quality over Content Recognition,Junyong You,,0%
https://arxiv.org/pdf/2301.09190.pdf,Apples and Oranges? Assessing Image Quality over Content Recognition,Zheng Zhang,,0%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Luis F. Gomez,luisf.gomez@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Aythami Morales,aythami.morales@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Julian Fierrez,julian.ﬁerrez@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Ruben Tolosana,ruben.tolosana@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Ruth Cobos,ruth.cobos@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Roberto Daza,roberto.daza@uam.es,95%
https://arxiv.org/pdf/2301.09174.pdf,MATT: Multimodal Attention Level Estimation for e-learning Platforms,Javier Ortega-garcia,javier.ortega@uam.es,85%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Kevin Mcguinness,kevin.mcguinness@dcu.ie,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Eric Arazo,eric.arazo@insight-centre.org,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Alexandru Drimbarean,Alexandru.Drimbarean@xperi.com,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Paul Albert,paul.albert@insight-centre.org,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Tarun Krishna,tarun.krishna2@mail.dcu.ie,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Noel E O'connor,noel.oconnor@dcu.ie,85%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Alan F Smeaton,alan.smeaton@dcu.ie,95%
https://arxiv.org/pdf/2301.09164.pdf,Unifying Synergies between Self-supervised Learning and Dynamic Computation,Ayush K Rai,ayush.rai3@mail.dcu.ie,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Mihir Tale,4mihir.tale18@vit.edu,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Sandeep Shinde,1sandeep.shinde@vit.edu,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Aniket Ghorpade,3aniket.ghorpade18@vit.edu,95%
https://arxiv.org/pdf/2301.09123.pdf,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,Tejas Pradhan,2tejas.pradhan18@vit.edu,95%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Jilan Xu,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Junlin Hou,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Yuejie Zhang,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Rui Feng,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Yi Wang,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Yu Qiao,,0%
https://arxiv.org/pdf/2301.09121.pdf,Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision,Weidi Xie,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Minjung Shin,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Yunji Seo,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Jeongmin Bae,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Young Sun Choi,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Hyunsu Kim,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Hyeran Byun,,0%
https://arxiv.org/pdf/2301.09091.pdf,BallGAN: 3D-aware Image Synthesis with a Spherical Background,Youngjung Uh,,0%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Yixuan Yuan,yxyuan@ee.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Junhui Hou,jh.hou@cityu.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Qijian Zhang,qijizhang3-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Yifan Zhang,yzhang3362-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2301.09077.pdf,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,Guoliang Xing,glxing@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Wenqiao Zhang,wenqiao@nus.edu.sg,85%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Linchao Zhu,zhulinchao@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Fei Wu,wufei@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Siliang Tang,siliang@zju.edu.cn,85%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Yi Yang,yangyics@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Juncheng Li,junchengli@zju.edu.cn,95%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Tat-seng Chua,,0%
https://arxiv.org/pdf/2301.09071.pdf,Variational Cross-Graph Reasoning and Adaptive Structured Semantics Learning for Compositional Temporal Grounding,Yueting Zhuang,,0%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Jihong Zhu,jhzhu@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Eksan Firkat,eksan@stu.xju.edu.cn.com,85%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Askar Hamdulla,askar@xju.edu.cn,85%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Yucheng Huang,,0%
https://arxiv.org/pdf/2301.09063.pdf,DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for Improved Siamese Tracking,Ziwang Xiao,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Basilio Caruso,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Trupti Mahendrakar,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Van Minh Nguyen,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Ryan T. White,,0%
https://arxiv.org/pdf/2301.09060.pdf,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,Todd Steffen,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Trupti Mahendrakar,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Steven Holmberg,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Andrew Ekblad,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Emma Conti,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Ryan T. White,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Markus Wilde,,0%
https://arxiv.org/pdf/2301.09059.pdf,Autonomous Rendezvous with Non-cooperative Target Objects with Swarm Chasers and Observers,Isaac Silver,,0%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Nathan Fischer,nfischer2018@my.fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Isaac Silver,isaac@energymanagementaero.com,85%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Markus Wilde,mwilde@fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Ryan T. White,rwhite@my.fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Brian Kish,bkish@fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Andrew Ekblad,aekblad2019@my.fit.edu,82%
https://arxiv.org/pdf/2301.09056.pdf,Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation around Non-Cooperative Targets,Trupti Mahendrakar,tmahendrakar2020@my.fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Ryan T. White,rwhite@fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Brooke Wheeler,bwheeler@fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Isaac Silver,isaac@energymanagementaero.com,85%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Markus Wilde,mwilde@fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Andrew Ekblad,aekblad2019@my.fit.edu,82%
https://arxiv.org/pdf/2301.09055.pdf,Resource-constrained FPGA Design for Satellite Component Feature Extraction,Trupti Mahendrakar,Tmahendrakar2020@my.fit.edu,82%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Shengyi Gao,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Zhe Chen,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Guo Chen,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Wenhai Wang,,0%
https://arxiv.org/pdf/2301.09045.pdf,Champion Solution for the WSDM2023 Toloka VQA Challenge,Tong Lu,,0%
https://arxiv.org/pdf/2301.09015.pdf,E$^3$Pose: Energy-Efficient Edge-assisted Multi-camera System for Multi-human 3D Pose Estimation,Jie Xu,jiexu@miami.edu,95%
https://arxiv.org/pdf/2301.09015.pdf,E$^3$Pose: Energy-Efficient Edge-assisted Multi-camera System for Multi-human 3D Pose Estimation,Letian Zhang,,0%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Yash Patel,yspatel@uwm.edu,82%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Zeyun Yu,yuz@uwm.edu,78%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Hosein Barzekar,barzekar@uwm.edu,78%
https://arxiv.org/pdf/2301.09007.pdf,MultiNet with Transformers: A Model for Cancer Diagnosis Using Images,Ling Tong,ltong@uwm.edu,82%
https://arxiv.org/pdf/2302.08503.pdf,Unpaired Image-to-Image Translation with Limited Data to Reveal Subtle Phenotypes,Anis Bourou,anis.bourou@ens.fr,95%
https://arxiv.org/pdf/2302.08503.pdf,Unpaired Image-to-Image Translation with Limited Data to Reveal Subtle Phenotypes,Auguste Genovesio,auguste.genovesio@ens.psl.eu,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,William Ljungbergh,william.ljungbergh@liu.se,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,Christoffer Petersson,christoffer.petersson@zenseact.com,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,Joakim Johnander,joakim.johnander@zenseact.com,95%
https://arxiv.org/pdf/2301.08965.pdf,Raw or Cooked? Object Detection on RAW Images,Michael Felsberg,michael.felsberg@liu.se,95%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Xiaofeng Liu,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Fangxu Xing,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Hanna K. Gaggin,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,C. -c. Jay Kuo,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Georges El Fakhri,,0%
https://arxiv.org/pdf/2301.08959.pdf,Successive Subspace Learning for Cardiac Disease Classification with Two-phase Deformation Fields from Cine MRI,Jonghye Woo,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Saeed Anwar,saeed.anwar@kfupm.edu.sa,95%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Muhammad Ibrahim,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Naveed Akhtar,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Michael Wise,,0%
https://arxiv.org/pdf/2301.08957.pdf,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,Ajmal Mian,,0%
https://arxiv.org/pdf/2301.08951.pdf,Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction,Bin Li,libin@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.08951.pdf,Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction,Chengmin Gao,,0%
https://arxiv.org/pdf/2301.08939.pdf,Counterfactual Explanation and Instance-Generation using Cycle-Consistent Generative Adversarial Networks,Tehseen Zia,tehseen.zia@comsats.edu.pk,95%
https://arxiv.org/pdf/2301.08939.pdf,Counterfactual Explanation and Instance-Generation using Cycle-Consistent Generative Adversarial Networks,Zeeshan Nisar,,0%
https://arxiv.org/pdf/2301.08939.pdf,Counterfactual Explanation and Instance-Generation using Cycle-Consistent Generative Adversarial Networks,Shakeeb Murtaza,,0%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Ping Tan,pingtan@ust.hk,95%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Luwei Yang,luweiy@sfu.ca,85%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Weihao Yuan,qianmu.ywh@alibaba-inc.com,65%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Zilong Dong,dadong.gxd@alibaba-inc.com,78%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Heng Li,lh.heng.li@connect.ust.hk,95%
https://arxiv.org/pdf/2301.08930.pdf,Dense RGB SLAM with Neural Implicit Maps,Xiaodong Gu,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Shihao Zhang,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Linlin Yang,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Michael Bi Mi,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Xiaoxu Zheng,,0%
https://arxiv.org/pdf/2301.08915.pdf,Improving Deep Regression with Ordinal Entropy,Angela Yao,,0%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Houqiang Li,lihq@ustc.edu.cn,78%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Keyi Zhou,kyzhou2000@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Jiajun Deng,jiajun.deng@adelaide.edu.au,95%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Hao Feng,haof@mail.ustc.edu.cn,85%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Yufei Yin,yinyufei@mail.ustc.edu.cn,95%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Wengang Zhou,,0%
https://arxiv.org/pdf/2301.08898.pdf,Recurrent Generic Contour-based Instance Segmentation with Progressive Learning,Qi Sun,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Fouzia Altaf,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Syed M. S. Islam,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Naeem K. Janjua,,0%
https://arxiv.org/pdf/2301.08888.pdf,Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection,Naveed Akhtar,,0%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Chi-man Pun,cmpun@umac.mo,82%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Zinuo Li,,0%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Xuhang Chen,,0%
https://arxiv.org/pdf/2301.08880.pdf,A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement,Shuqiang Wang,,0%
https://arxiv.org/pdf/2301.08874.pdf,Improving Zero-Shot Action Recognition using Human Instruction with Text Description,Hiroshi Kera,kera@chiba-u.jp,78%
https://arxiv.org/pdf/2301.08874.pdf,Improving Zero-Shot Action Recognition using Human Instruction with Text Description,Kazuhiko Kawamoto,kawa@faculty.chiba-u.jp,90%
https://arxiv.org/pdf/2301.08874.pdf,Improving Zero-Shot Action Recognition using Human Instruction with Text Description,Nan Wu,gonan@chiba-u.jp,85%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Eric Z. Chen,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Chi Zhang,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Xiao Chen,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Yikang Liu,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Terrence Chen,,0%
https://arxiv.org/pdf/2301.08868.pdf,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,Shanhui Sun,,0%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Weixuan Liang,weixuanliang@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Siwei Wang,wangsiwei13@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Jiyuan Liu,liujiyuan13@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Xinhang Wan,wanxinhang@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Lu Zhou,lu.zhou@nuaa.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Yi Wen,wenyi21@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,En Zhu,enzhu@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Xinwang Liu,xinwangliu@nudt.edu.cn,95%
https://arxiv.org/pdf/2303.01983.pdf,Auto-weighted Multi-view Clustering for Large-scale Data,Zhe Liu,zhe.liu@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.08849.pdf,CADA-GAN: Context-Aware GAN with Data Augmentation,Sofie Daniels,,0%
https://arxiv.org/pdf/2301.08849.pdf,CADA-GAN: Context-Aware GAN with Data Augmentation,Jiugeng Sun,,0%
https://arxiv.org/pdf/2301.08849.pdf,CADA-GAN: Context-Aware GAN with Data Augmentation,Jiaqing Xie,,0%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Tao Qin,taoqin@microsoft.com,95%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Jiang Bian,jiabia@microsoft.com,65%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Yoshua Bengio,2yoshua.bengio@mila.quebec,95%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Tie-yan Liu,tyliu@microsoft.com,82%
https://arxiv.org/pdf/2301.08846.pdf,Regeneration Learning: A Learning Paradigm for Data Generation,Xu Tan,xuta@microsoft.com,85%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Md Selim,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Jie Zhang,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Michael A. Brooks,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Ge Wang,,0%
https://arxiv.org/pdf/2301.08815.pdf,DiffusionCT: Latent Diffusion Model for CT Image Standardization,Jin Chen,,0%
https://arxiv.org/pdf/2301.08802.pdf,Impact of PCA-based preprocessing and different CNN structures on deformable registration of sonograms,Christian Schmidt,christian.schmidt@w-hs.de,95%
https://arxiv.org/pdf/2301.08802.pdf,Impact of PCA-based preprocessing and different CNN structures on deformable registration of sonograms,Heinrich Martin Overhoff,heinrich-martin.overhoff@w-hs.de,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Rui Kou,kourui.mun@gmail.com,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Satish Kumar,satishkumar@ucsb.edu,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Vikram Jayaram,Vikram.Jayaram@pxd.com,95%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Henry Hill,,0%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Jake Lempges,,0%
https://arxiv.org/pdf/2301.08800.pdf,In-situ Water quality monitoring in Oil and Gas operations,Eric Qian,,0%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Yunan Wu,yunanwu2020@u.northwestern.edu,95%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Amil Dravid,,0%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Ramsey Michael Wehbe,,0%
https://arxiv.org/pdf/2301.08798.pdf,DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels,Aggelos K. Katsaggelos,,0%
https://arxiv.org/pdf/2301.08794.pdf,"Robot Skill Learning Via Classical Robotics-Based Generated Datasets: Advantages, Disadvantages, and Future Improvement",Batu Kaan Oezen,ozenbatukaan@gmail.com,85%
https://arxiv.org/pdf/2301.08784.pdf,Visual Semantic Relatedness Dataset for Image Captioning,Ahmed Sabir,,0%
https://arxiv.org/pdf/2301.08784.pdf,Visual Semantic Relatedness Dataset for Image Captioning,Francesc Moreno-noguer,,0%
https://arxiv.org/pdf/2301.08784.pdf,Visual Semantic Relatedness Dataset for Image Captioning,Lluís Padró,,0%
https://arxiv.org/pdf/2301.08783.pdf,An Asynchronous Intensity Representation for Framed and Event Video Sources,Andrew C. Freeman,acfreeman@cs.unc.edu,82%
https://arxiv.org/pdf/2301.08783.pdf,An Asynchronous Intensity Representation for Framed and Event Video Sources,Montek Singh,montek@cs.unc.edu,85%
https://arxiv.org/pdf/2301.08783.pdf,An Asynchronous Intensity Representation for Framed and Event Video Sources,Ketan Mayer-patel,kmp@cs.unc.edu,98%
https://arxiv.org/pdf/2301.08782.pdf,Estimation of mitral valve hinge point coordinates -- deep neural net for echocardiogram segmentation,Christian Schmidt,christian.schmidt@w-hs.de,95%
https://arxiv.org/pdf/2301.08782.pdf,Estimation of mitral valve hinge point coordinates -- deep neural net for echocardiogram segmentation,Heinrich Martin Overhoff,heinrich-martin.overhoff@w-hs.de,95%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Zhijian Liu,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Xinyu Yang,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Haotian Tang,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Shang Yang,,0%
https://arxiv.org/pdf/2301.08739.pdf,FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer,Song Han,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Changan Chen,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Alexander Richard,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Roman Shapovalov,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Vamsi Krishna Ithapu,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Natalia Neverova,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.08730.pdf,Novel-View Acoustic Synthesis,Andrea Vedaldi,,0%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Sourav Saha,souravsaha0152@gmail.com,95%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Trina Chakraborty,trinasustcse41@gmail.com,85%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Tithi Paul,tithi.cse3.bu@gmail.com,85%
https://arxiv.org/pdf/2302.10277.pdf,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,Rejwan Bin Sulaiman,rejwan.binsulaiman@gmail.com,95%
https://arxiv.org/pdf/2302.06389.pdf,Deep-Learning Quantitative Structural Characterization in Additive Manufacturing,Amra Peles,pelesa@ornl.gov,78%
https://arxiv.org/pdf/2302.06389.pdf,Deep-Learning Quantitative Structural Characterization in Additive Manufacturing,Vincent C. Paquit,,0%
https://arxiv.org/pdf/2302.06389.pdf,Deep-Learning Quantitative Structural Characterization in Additive Manufacturing,Ryan R. Dehoff,,0%
https://arxiv.org/pdf/2301.08669.pdf,Holistically Explainable Vision Transformers,Moritz Böhle,,0%
https://arxiv.org/pdf/2301.08669.pdf,Holistically Explainable Vision Transformers,Mario Fritz,,0%
https://arxiv.org/pdf/2301.08669.pdf,Holistically Explainable Vision Transformers,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Liang Mi,liangmi@smail.nju.edu.cn,95%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Haipeng Dai,haipengdai@nju.edu.cn,95%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Xiaoming Fu,fu@cs.uni-goettingen.de,78%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Tingting Yuan,tingting.yuan@cs.uni-goettingen.de,95%
https://arxiv.org/pdf/2301.08664.pdf,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,Weijun Wang,weijun.wang@cs.uni-goettingen.de,95%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Justyna P. Zwolak,jpzwolak@nist.gov,82%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Joshua Ziegler,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Florian Luthi,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Mick Ramsey,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Felix Borjans,,0%
https://arxiv.org/pdf/2301.08654.pdf,Automated extraction of capacitive coupling for quantum dot systems,Guoji Zheng,,0%
https://arxiv.org/pdf/2301.08647.pdf,Image Memorability Prediction with Vision Transformers,Thomas Hagen,,0%
https://arxiv.org/pdf/2301.08647.pdf,Image Memorability Prediction with Vision Transformers,Thomas Espeseth,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Romain Xu-darme,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Georges Quénot,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Zakaria Chihani,,0%
https://arxiv.org/pdf/2302.08508.pdf,Sanity checks and improvements for patch visualisation in prototype-based image classification,Marie-christine Rousset,,0%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Laurent Ferro-famil,laurent.ferro-famil@isae-supaero.fr,95%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Yue Huang,yhuang228@gmail.com,82%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Loïc Denis,loic.denis@univ-st-etienne.fr,95%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Zoé Berenger,,0%
https://arxiv.org/pdf/2301.08605.pdf,A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas,Florence Tupin,,0%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Pinar Duygulu,pinar@cs.hacettepe.edu.tr,85%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Emre Akbas,emre@ceng.metu.edu.tr,85%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Nermin Samet,nermin.samet@enpc.fr,95%
https://arxiv.org/pdf/2301.08590.pdf,Improving Sketch Colorization using Adversarial Segmentation Consistency,Samet Hicsonmez,samethicsonmez@hacettepe.edu.tr,95%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Bernt Schiele,schiele@mpi-inf.mpg.de,78%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Vera Demberg,vera@coli.uni-saarland.de,85%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Xudong Hong,xhong@coli.uni-saarland.de,82%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Asad Sayeed,asad.sayeed@gu.se,95%
https://arxiv.org/pdf/2301.08571.pdf,Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,Khushboo Mehra,kmehra@coli.uni-saarland.de,82%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Zoltan Galaz,xgalaz00@gmail.com,78%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Jiri Mekyska,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Jan Mucha,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Vojtech Zvoncak,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Zdenek Smekal,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Marcos Faundez-zanuy,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Lubos Brabenec,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Ivona Moravkova,,0%
https://arxiv.org/pdf/2301.08534.pdf,Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties,Irena Rektorova,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Irena Rektorova,irena.rektorova@fnusa.cz,95%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Jan Mucha,mucha@vut.cz,78%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Zoltan Galaz,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Jiri Mekyska,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Marcos Faundez-zanuy,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Vojtech Zvoncak,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Zdenek Smekal,,0%
https://arxiv.org/pdf/2301.08529.pdf,Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis,Lubos Brabenec,,0%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Rizwan Ahmed Khan,Rizwan.Khan@shu.edu.pk,95%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Wardah Ali,,0%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Eesha Qureshi,,0%
https://arxiv.org/pdf/2301.08479.pdf,Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance,Omama Ahmed Farooqi,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Jianyuan Wang,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Lalit Bhagat,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Ceyuan Yang,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Yinghao Xu,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Yujun Shen,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Hongdong Li,,0%
https://arxiv.org/pdf/2301.08455.pdf,Spatial Steerability of GANs via Self-Supervision from Discriminator,Bolei Zhou,,0%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Seogkyu Jeon,jone9312@yonsei.ac.kr,55%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Hyeran Byun,hrbyun@yonsei.ac.kr,82%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Sunhee Hwang,sunheehwang@lguplus.co.kr,95%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Pilhyeon Lee,,0%
https://arxiv.org/pdf/2301.08448.pdf,Source-free Subject Adaptation for EEG-based Visual Recognition,Minjung Shin,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Dongsik Yoon,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Jeong-gi Kwak,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Yuanming Li,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,David Han,,0%
https://arxiv.org/pdf/2301.08443.pdf,DIFAI: Diverse Facial Inpainting using StyleGAN Inversion,Hanseok Ko,,0%
https://arxiv.org/pdf/2301.08433.pdf,Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction,Nan Meng,nanmeng@hku.hk,95%
https://arxiv.org/pdf/2301.08433.pdf,Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction,Shansi Zhang,sszhang@eee.hku.hk,82%
https://arxiv.org/pdf/2301.08433.pdf,Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction,Edmund Y. Lam,elam@eee.hku.hk,82%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Zhenghao Zhang,zhangzhenghao.zzh@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Siyu Zhu,siting.zsy@alibaba-inc.com,60%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Zuozhuo Dai,zuozhuo.dzz@alibaba-inc.com,85%
https://arxiv.org/pdf/2301.09416.pdf,Towards Robust Video Instance Segmentation with Temporal-Aware Transformer,Fangtao Shao,shaofangtao.sft@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Yong Liu,yongliu@iipc.zju.edu.cn,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Junyu Zhu,junyuzhu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Hongbo Zhang,zhanghongbo888@huawei.com,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Wanlong Li,liwanlong@huawei.com,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Lina Liu,linaliu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.08414.pdf,FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,Feng Wen,wenfeng3@huawei.com,95%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Yan Li,yanli@shnu.edu.cn,95%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Wenming Cao,wmcao@szu.edu.cn,82%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Hong Wang,wanghong1@cetc.com.cn,95%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Guitao Cao,gtcao@sei.ecnu.edu.cn,82%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Chunwei Wu,,0%
https://arxiv.org/pdf/2301.08413.pdf,Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation,Xidong Xi,,0%
https://arxiv.org/pdf/2301.09420.pdf,On Multi-Agent Deep Deterministic Policy Gradients and their Explainability for SMARTS Environment,Aditya Malte,malte@usc.edu,78%
https://arxiv.org/pdf/2301.09420.pdf,On Multi-Agent Deep Deterministic Policy Gradients and their Explainability for SMARTS Environment,Ansh Mittal,anshm@usc.edu,85%
https://arxiv.org/pdf/2301.08408.pdf,Identity masking effectiveness and gesture recognition: Effects of eye enhancement in seeing through the mask,Madeline Rachow,mrachow@uark.edu,82%
https://arxiv.org/pdf/2301.08408.pdf,Identity masking effectiveness and gesture recognition: Effects of eye enhancement in seeing through the mask,Thomas Karnowski,karnowskitp@ornl.gov,78%
https://arxiv.org/pdf/2301.08408.pdf,Identity masking effectiveness and gesture recognition: Effects of eye enhancement in seeing through the mask,Alice J. O'toole,otoole@utdallas.edu,55%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Miao Yin,miao.yin@rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Yang Sui,yang.sui@rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Dingwen Tao,ditao@iu.edu,82%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Lizhi Xiang,lizhi.xiang@wsu.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Jinqi Xiao,jinqi.xiao@rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Bo Yuan,bo.yuan@soe.rutgers.edu,95%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Chengming Zhang,,0%
https://arxiv.org/pdf/2301.09422.pdf,HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks,Yu Gong,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Malik Boudiaf,lik.boudiaf.1@etsmtl.net,78%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Etienne Bennequin,etienneb@sicara.com,85%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Myriam Tami,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Antoine Toubhans,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Pablo Piantanida,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Céline Hudelot,,0%
https://arxiv.org/pdf/2301.08390.pdf,Open-Set Likelihood Maximization for Few-Shot Learning,Ismail Ben Ayed,,0%
https://arxiv.org/pdf/2301.08387.pdf,Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree Canopies,George Kantor,kantor@andrew.cmu.edu,78%
https://arxiv.org/pdf/2301.08387.pdf,Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree Canopies,Chung Hee Kim,chunghek@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,George Yiasemis,,0%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,Clara I. Sánchez,,0%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,Jan-jakob Sonke,,0%
https://arxiv.org/pdf/2301.08365.pdf,On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction,Jonas Teuwen,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Antanas Kascenas,antanas.kascenas@mre.medical.canon,95%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Pedro Sanchez,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Patrick Schrempf,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Chaoyang Wang,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,William Clackett,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Shadia S. Mikhael,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Jeremy P. Voisey,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Keith Goatman,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Alexander Weir,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Nicolas Pugeault,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Sotirios A. Tsaftaris,,0%
https://arxiv.org/pdf/2301.08330.pdf,The role of noise in denoising models for anomaly detection in medical images,Alison Q. O'neil,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Chiara Di Vece,chiara.divece.20@ucl.ac.uk,95%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Maela Le Lous,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Brian Dromey,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Francisco Vasconcelos,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Anna L David,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Donald Peebles,,0%
https://arxiv.org/pdf/2301.08317.pdf,Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain,Danail Stoyanov,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Chao-yuan Wu,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Justin Johnson,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Jitendra Malik,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Christoph Feichtenhofer,,0%
https://arxiv.org/pdf/2301.08247.pdf,Multiview Compressive Coding for 3D Reconstruction,Georgia Gkioxari,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Pierluigi Zama Ramirez,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Alex Costanzino,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Fabio Tosi,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Matteo Poggi,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Samuele Salti,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Stefano Mattoccia,,0%
https://arxiv.org/pdf/2301.08245.pdf,Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces,Luigi Di Stefano,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Mahmoud Assran,massran@meta.com,82%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Quentin Duval,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Ishan Misra,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Piotr Bojanowski,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Pascal Vincent,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Michael Rabbat,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Yann Lecun,,0%
https://arxiv.org/pdf/2301.08243.pdf,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Nicolas Ballas,,0%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,Feng Cheng,fengchan@cs.unc.edu,85%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,Xizi Wang,xiziwang@iu.edu,95%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,Gedas Bertasius,gedas@cs.unc.edu,85%
https://arxiv.org/pdf/2301.08237.pdf,LoCoNet: Long-Short Context Network for Active Speaker Detection,David Crandall,,0%
https://arxiv.org/pdf/2301.08229.pdf,Estimating Remaining Lifespan from the Face,Amir Fekrazad,afekrazad@tamusa.edu,82%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Mihir Durve,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Sibilla Orsini,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Adriano Tiribocchi,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Andrea Montessori,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Jean-michel Tucny,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Marco Lauricella,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Andrea Camposeo,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Dario Pisignano,,0%
https://arxiv.org/pdf/2301.08189.pdf,Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications,Sauro Succi,,0%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,George Deligiannidis,deligian@stats.ox.ac.uk,90%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Fabian Falck,fabian.falck@stats.ox.ac.uk,95%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Dominic Danks,ddanks@turing.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Matthew Willetts,mwilletts@turing.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Christopher Williams,williams@stats.ox.ac.uk,78%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Christopher Yau,cyau@turing.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Chris Holmes,cholmes@stats.ox.ac.uk,82%
https://arxiv.org/pdf/2301.08187.pdf,A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs,Arnaud Doucet,doucet@stats.ox.ac.uk,78%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Alistair Weld,a.weld20@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Michael Dyck,michael.dyck@dlr.de,95%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Julian Klodmann,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Giulio Anichini,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Luke Dixon,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Sophie Camp,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Alin Albu-schäffer,,0%
https://arxiv.org/pdf/2301.08174.pdf,Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery,Stamatia Giannarou,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Huafeng Liu,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Pai Peng,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Tao Chen,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Qiong Wang,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Yazhou Yao,,0%
https://arxiv.org/pdf/2301.08160.pdf,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,Xian-sheng Hua,,0%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Luigi Manfredi,l.manfredi@dundee.ac.uk,82%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Alwyn Mathew,,0%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Ludovic Magerand,,0%
https://arxiv.org/pdf/2301.08157.pdf,SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots,Emanuele Trucco,,0%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Yunzhao Zeng,zengyunzhao@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Li Chen,chenli.phd@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Ming Wu,wuming@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Hao Yang,yang.hao@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Min Zheng,zhengmin.666@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Shizun Wang,wangshizun@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Xu Wang,wangxu.ailab@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Chuang Zhang,zhangchuang@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Yi Yuan,yuanyi.cv@bytedance.com,95%
https://arxiv.org/pdf/2301.08153.pdf,SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines,Weihong Zeng,zengweihong@bytedance.com,95%
https://arxiv.org/pdf/2301.08147.pdf,"RGB-D-Based Categorical Object Pose and Shape Estimation: Methods, Datasets, and Evaluation",Leonard Bruns,leonardb@kth.se,85%
https://arxiv.org/pdf/2301.08147.pdf,"RGB-D-Based Categorical Object Pose and Shape Estimation: Methods, Datasets, and Evaluation",Patric Jensfelt,patric@kth.se,85%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Alistair Weld,a.weld20@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Joao Cartucho,,0%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Chi Xu,,0%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Joseph Davids,,0%
https://arxiv.org/pdf/2301.08140.pdf,Regularising disparity estimation via multi task learning with structured light reconstruction,Stamatia Giannarou,,0%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Conghao Xiong,chxiong21@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Irwin King,king@cse.cuhk.edu.hk,78%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Joseph J. Y. Sung,josephsung@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.08125.pdf,Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification,Hao Chen,,0%
https://arxiv.org/pdf/2301.08113.pdf,Soft Thresholding for Visual Image Enhancement,Christoph Dalitz,christoph.dalitz@hsnr.de,95%
https://arxiv.org/pdf/2301.08092.pdf,RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation,Yingzhen Yang,yyang409@asu.edu,82%
https://arxiv.org/pdf/2301.08092.pdf,RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation,Utkarsh Nath,unath@asu.edu,82%
https://arxiv.org/pdf/2301.08092.pdf,RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation,Yancheng Wang,ywan1053@asu.edu,65%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Leyuan Fang,fangleyuan@gmail.com,95%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Yue Deng,yuedeng.thu@gmail.com,95%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Shaobo Xia,shaobo.xia@csust.edu.cn,95%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Jiayi Ma,jyma2010@gmail.com,82%
https://arxiv.org/pdf/2301.08072.pdf,Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models,Jun Yue,,0%
https://arxiv.org/pdf/2301.08067.pdf,Interpreting CNN Predictions using Conditional Generative Adversarial Networks,R T Akash Guna,sikhakrishnanunni@gmail.com,85%
https://arxiv.org/pdf/2301.08067.pdf,Interpreting CNN Predictions using Conditional Generative Adversarial Networks,Raul Benitez,raul.benitez@upc.edu,95%
https://arxiv.org/pdf/2301.08067.pdf,Interpreting CNN Predictions using Conditional Generative Adversarial Networks,O K Sikha,,0%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Julia Wolleb,julia.wolleb@unibas.ch,95%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Florentin Bieder,florentin.bieder@unibas.ch,95%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Philippe C. Cattin,philippe.cattin@unibas.ch,95%
https://arxiv.org/pdf/2301.08064.pdf,Position Regression for Unsupervised Anomaly Detection,Robin Sandkühler,robin.sandkuehler@unibas.ch,85%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Dongsik Yoon,kevinds1106@korea.ac.kr,60%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Youngsaeng Jin,youngsjin@korea.ac.kr,82%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Hanseok Ko,hsko@korea.ac.kr,82%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Jeonggi Kwak,,0%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,Yuanming Li,,0%
https://arxiv.org/pdf/2301.08044.pdf,Reference Guided Image Inpainting using Facial Attributes,David Han,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Rosalba Calvini,rosalba.calvini@unimore.it,95%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Veronica Ferrari,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Bas Boom,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Camilla Menozzi,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Aravind Krishnaswamy Rangarajan,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Lara Maistrello,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Peter Offermans,,0%
https://arxiv.org/pdf/2301.08252.pdf,Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug,Alessandro Ulrici,,0%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Sylwia Majchrowska,sylwia.majchrowska@ai.se,95%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Jakub Nalepa,jnalepa@ieee.org,82%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Maria Ferlin,maria.ferlin@pg.edu.pl,95%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Alicja Kwaśniwska,alicja@sima.ai,85%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Marta Plantykow,m.plantykow@gmail.com,82%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Agnieszka Mikołajczyk-bareła,agnieszka.mikolajczyk@voicelab.ai,85%
https://arxiv.org/pdf/2302.10768.pdf,On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study,Milena Olech,milena.w.olech@gmail.com,95%
https://arxiv.org/pdf/2301.08555.pdf,Hybrid Open-set Segmentation with Synthetic Negative Data,Siniša Šegvić,sinisa.segvic@fer.hr,95%
https://arxiv.org/pdf/2301.08555.pdf,Hybrid Open-set Segmentation with Synthetic Negative Data,Matej Grcić,matej.grcic@fer.hr,95%
https://arxiv.org/pdf/2301.07969.pdf,Fast Inference in Denoising Diffusion Models via MMD Finetuning,Emanuele Aiello,name.surname@polito.it,60%
https://arxiv.org/pdf/2301.07969.pdf,Fast Inference in Denoising Diffusion Models via MMD Finetuning,Diego Valsesia,,0%
https://arxiv.org/pdf/2301.07969.pdf,Fast Inference in Denoising Diffusion Models via MMD Finetuning,Enrico Magli,,0%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Xiaoguang Han,hanxiaoguang@cuhk.edu.cn,95%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Bingchen Gong,gongbingchen@gmail.com,95%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Qi Dou,qidou@cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.07958.pdf,RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes,Yuehao Wang,yhwang@link.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Boris Mocialov,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Eirik Eythorsson,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Reza Parseh,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Hoang Tran,,0%
https://arxiv.org/pdf/2301.07947.pdf,Point Cloud Data Simulation and Modelling with Aize Workspace,Vegard Flovik,,0%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Mengmeng Wang,mengmengwang@zju.edu.cn,95%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Boyu Mu,muboyu@zju.edu.cn,95%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Jiazheng Xing,jiazhengxing@zju.edu.cn,95%
https://arxiv.org/pdf/2301.07944.pdf,Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition,Yong Liu,yongliu@iipc.zju.edu.cn,95%
https://arxiv.org/pdf/2301.07927.pdf,Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning,Jun Huang,huangj@sari.ac.cn,78%
https://arxiv.org/pdf/2301.07927.pdf,Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning,Shuzhen Rao,,0%
https://arxiv.org/pdf/2301.07927.pdf,Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning,Zengming Tang,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Snehashis Majhi,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Rui Dai,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Quan Kong,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Lorenzo Garattoni,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Gianpiero Francesca,,0%
https://arxiv.org/pdf/2301.07923.pdf,Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection,Francois Bremond,,0%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Xiuen Wu,xiuen_wu@163.com,95%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Tao Wang,twang@mju.edu.cn,82%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Fum Yew Ching,fyching@student.usm.my,82%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Lingyu Liang,eelyliang@scut.edu.cn,78%
https://arxiv.org/pdf/2301.07921.pdf,Spatio-Temporal Context Modeling for Road Obstacle Detection,Zuoyong Li,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Barak Or,barak@almatechnologies.com,85%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Areej Eweida,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Nimord Segol,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Maxim Freydin,,0%
https://arxiv.org/pdf/2302.12720.pdf,Surface Recognition for e-Scooter Using Smartphone IMU Sensor,Niv Sfaradi,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Dongdong Liu,ddliu@nyu.edu,82%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Hang Zhang,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Rongguang Wang,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Jinwei Zhang,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Chao Li,,0%
https://arxiv.org/pdf/2301.07895.pdf,Spatially Covariant Lesion Segmentation,Jiahao Li,,0%
https://arxiv.org/pdf/2301.07879.pdf,Unposed: Unsupervised Pose Estimation based Product Image Recommendations,Faizan Ahemad,ahemf@amazon.com,73%
https://arxiv.org/pdf/2301.07879.pdf,Unposed: Unsupervised Pose Estimation based Product Image Recommendations,Saurabh Sharma,sharsar@amazon.com,65%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Bin Huang,huangbin1@senseauto.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Yangguang Li,liyangguang@sensetime.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Fenggang Liu,liufenggang@senseauto.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Luya Wang,wangluya@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Enze Xie,xieenze@connect.hku.hk,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Ping Luo,pluo@cs.hku.hk,82%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Mingzhu Shen,shenmingzhu@sensetime.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Jing Shao,shaojing@senseauto.com,95%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Feng Liang,,0%
https://arxiv.org/pdf/2301.07870.pdf,Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception,Tianqi Wang,,0%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Bowen Zhang,zhangbowen.17@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Xiaojie Jin,jinxiaojie@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Kai Xu,xukai.1993@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Peng Wang,peng.wang@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Jiashi Feng,jshfeng@bytedance.com,82%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Weibo Gong,gongweibo@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Xiaohui Shen,shenxiaohui.kevin@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Xueqing Deng,xueqingdeng@bytedance.com,95%
https://arxiv.org/pdf/2301.07868.pdf,MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval,Zhao Zhang,,0%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Junde Wu,jundewu@ieee.org,95%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Yanwu Xu,ywxu@ieee.org,82%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Min Xu,xumin100@gmail.com,95%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Wei Ji,,0%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Huazhu Fu,,0%
https://arxiv.org/pdf/2301.11798.pdf,MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer,Yueming Jin,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Yue Han,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Sri Kalyan Yarlagadda,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Tonmoy Ghosh,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Fengqing Zhu,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Edward Sazonov,,0%
https://arxiv.org/pdf/2301.07861.pdf,Improving Food Detection For Images From a Wearable Egocentric Camera,Edward J. Delp,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Qiuhao Zeng,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Wei Wang,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Fan Zhou,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Charles Ling,,0%
https://arxiv.org/pdf/2301.07845.pdf,Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment,Boyu Wang,,0%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Yinfei Yang,yinfei yang@apple.com,95%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Angelos Katharopoulos,a katharopoulos@apple.com,82%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Vaishaal Shankar,vaishaal shankar@apple.com,95%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Tom Gunter,tom gunter@apple.com,95%
https://arxiv.org/pdf/2301.07836.pdf,Masked Autoencoding Does Not Help Natural Language Supervision at Scale,Floris Weers,fweers@apple.com,82%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Allan Zhou,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Moo Jin Kim,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Lirui Wang,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Pete Florence,,0%
https://arxiv.org/pdf/2301.08556.pdf,NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis,Chelsea Finn,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Renjie Li,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Chun Yu Lao,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Rebecca St. George,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Katherine Lawler,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Saurabh Garg,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Son N. Tran,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Quan Bai,,0%
https://arxiv.org/pdf/2302.08505.pdf,Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning,Jane Alty,,0%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Ruben Coen-cagli,ruben.coen-cagli@einsteinmed.edu,95%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Jonathan Vacher,jonathan.vacher@u-paris.fr,95%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Claire Launay,,0%
https://arxiv.org/pdf/2301.07807.pdf,Measuring uncertainty in human visual segmentation,Pascal Mamassian,,0%
https://arxiv.org/pdf/2301.07805.pdf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,Hsiang-wei Huang,hwhuang@uw.edu,82%
https://arxiv.org/pdf/2301.07805.pdf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,Cheng-yen Yang,cycyang@uw.edu,82%
https://arxiv.org/pdf/2301.07805.pdf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,Jenq-neng Hwang,hwang@uw.edu,78%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Megan M. Baker,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Alexander New,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Mario Aguilar-simon,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ziad Al-halah,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Sébastien M. R. Arnold,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ese Ben-iwhiwhu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Andrew P. Brna,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ethan Brooks,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ryan C. Brown,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Zachary Daniels,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Anurag Daram,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Fabien Delattre,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Ryan Dellana,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Eric Eaton,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Haotian Fu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Jesse Hostetler,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Shariq Iqbal,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Cassandra Kent,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Nicholas Ketz,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Soheil Kolouri,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,George Konidaris,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Dhireesha Kudithipudi,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Erik Learned-miller,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Seungwon Lee,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Michael L. Littman,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Sandeep Madireddy,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Jorge A. Mendez,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Eric Q. Nguyen,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Christine D. Piatko,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Praveen K. Pilly,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Aswin Raghavan,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Abrar Rahman,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Santhosh Kumar Ramakrishnan,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Neale Ratzlaff,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Andrea Soltoggio,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Peter Stone,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Indranil Sur,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Zhipeng Tang,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Saket Tiwari,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Kyle Vedder,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Felix Wang,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Zifan Xu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Angel Yanguas-gil,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Harel Yedidsion,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Shangqun Yu,,0%
https://arxiv.org/pdf/2301.07799.pdf,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Gautam K. Vallabha,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Zifan Shi,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Yujun Shen,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Yinghao Xu,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Sida Peng,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Yiyi Liao,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Sheng Guo,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Qifeng Chen,,0%
https://arxiv.org/pdf/2301.07702.pdf,Learning 3D-aware Image Synthesis with Unknown Pose Distribution,Dit-yan Yeung,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Ziyu Su,zsu@wakehealth.edu,82%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Mostafa Rezapour,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Usama Sajjad,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Metin Nafi Gurcan,,0%
https://arxiv.org/pdf/2301.07700.pdf,Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images,Muhammad Khalid Khan Niazi,,0%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Xiao Fu,fuxiao@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Ziwei Liu,ziwei.liu@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Jiawei Ren,jiawei011@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Yuxin Wang,ywangom@connect.ust.hk,82%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Chen Qian,qianchen@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Lei Yang,yanglei@sensetime.com,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Wayne Wu,wuwenyan0503@gmail.com,82%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Jiaqi Wang,wangjiaqi@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Liang Pan,liang.pan@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Tong Wu,,0%
https://arxiv.org/pdf/2301.07525.pdf,"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation",Jiarui Zhang,,0%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Guanghui Yue,yueguanghui@szu.edu.cn,95%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Hantao Liu,liuh35@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Wei Zhou,wei.zhou@uwaterloo.ca,95%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Yipeng Qin,qiny16@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.07681.pdf,Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection,Ruizeng Zhang,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Xingyi He,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Jiaming Sun,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Yuang Wang,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Di Huang,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Hujun Bao,,0%
https://arxiv.org/pdf/2301.07673.pdf,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,Xiaowei Zhou,,0%
https://arxiv.org/pdf/2301.07670.pdf,Active learning for medical image segmentation with stochastic batches,Mélanie Gaillochet,,0%
https://arxiv.org/pdf/2301.07670.pdf,Active learning for medical image segmentation with stochastic batches,Christian Desrosiers,,0%
https://arxiv.org/pdf/2301.07670.pdf,Active learning for medical image segmentation with stochastic batches,Hervé Lombaert,,0%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Daniel Cremers,cremers@tum.de,78%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Nan Yang,nan.yang@tum.de,95%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Felix Wimbauer,felix.wimbauer@tum.de,95%
https://arxiv.org/pdf/2301.07668.pdf,Behind the Scenes: Density Fields for Single View Reconstruction,Christian Rupprecht,chrisr@robots.ox.ac.uk,81%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,Suya You,suya.you.civ@army.mil,95%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,A S M Iftekhar,,0%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,Raphael Ruschel,,0%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,Satish Kumar,,0%
https://arxiv.org/pdf/2301.07666.pdf,DDS: Decoupled Dynamic Scene-Graph Generation Network,B. S. Manjunath,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Yangang Wang,yangangwang@seu.edu.cn,95%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Wei Xie,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Zhipeng Yu,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Zimeng Zhao,,0%
https://arxiv.org/pdf/2301.07652.pdf,HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects,Binghui Zuo,,0%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Diana Carolina Lopez-medina,diana.lopezme@campusucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Renato Zambrano-cruz,renato.zambrano@ucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Victor H. Aristizabal-tique,victor.aristizabalt@campusucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Marcela Henao-perez,marcela.henaop@campusucc.edu.co,85%
https://arxiv.org/pdf/2301.07650.pdf,Facial Thermal and Blood Perfusion Patterns of Human Emotions: Proof-of-Concept,Gloria Diaz-londoñod,,0%
https://arxiv.org/pdf/2301.07634.pdf,Training Semantic Segmentation on Heterogeneous Datasets,Panagiotis Meletis,p.c.meletis@tue.nl,82%
https://arxiv.org/pdf/2301.07634.pdf,Training Semantic Segmentation on Heterogeneous Datasets,Gijs Dubbelman,g.dubbelman@tue.nl,82%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Huadeng Wang,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Hao Xu,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Bingbing Li,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Xipeng Pan,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Lingqi Zeng,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Rushi Lan,,0%
https://arxiv.org/pdf/2301.07627.pdf,A novel dataset and a two-stage mitosis nuclei detection method based on hybrid anchor branch,Xiaonan Luo,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Muhammad Ali Farooq,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Waseem Shariff,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Faisal Khan,,0%
https://arxiv.org/pdf/2301.07613.pdf,"Development, Optimization, and Deployment of Thermal Forward Vision Systems for Advance Vehicular Applications on Edge Devices",Peter Corcoran,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Henry Zheng,jh-zheng22@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Gao Huang,gaohuang@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Shiji Song,shijis@tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Rui Huang,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Xuran Pan,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Haojun Jiang,,0%
https://arxiv.org/pdf/2301.07584.pdf,Joint Representation Learning for Text and 3D Point Cloud,Zhifeng Xie,,0%
https://arxiv.org/pdf/2301.07583.pdf,A Survey of Advanced Computer Vision Techniques for Sports,Tiago Mendes-neves,,0%
https://arxiv.org/pdf/2301.07583.pdf,A Survey of Advanced Computer Vision Techniques for Sports,Luís Meireles,,0%
https://arxiv.org/pdf/2301.07583.pdf,A Survey of Advanced Computer Vision Techniques for Sports,João Mendes-moreira,,0%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Jitka Kostkova,kostkova@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Jan Flusser,ﬂusser@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Matej Lebl,lebl@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Filip Sroubek,sroubekf@utia.cas.cz,78%
https://arxiv.org/pdf/2301.07581.pdf,Blur Invariants for Image Recognition,Matteo Pedone,,0%
https://arxiv.org/pdf/2301.07565.pdf,Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using a New Frame Selection Policy and Gating Mechanism,Vasileios Mezaris,bmezaris@iti.gr,78%
https://arxiv.org/pdf/2301.07565.pdf,Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using a New Frame Selection Policy and Gating Mechanism,Dimitrios Daskalakis,dimidask@iti.gr,65%
https://arxiv.org/pdf/2301.07565.pdf,Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using a New Frame Selection Policy and Gating Mechanism,Nikolaos Gkalelis,gkalelis@iti.gr,78%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Michele Buzzicotti,michele.buzzicotti@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Luca Biferale,luca.biferale@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Fabio Bonaccorso,fabio.bonaccorso@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07541.pdf,Generative Adversarial Networks to infer velocity components in rotating turbulent flows,Tianyi Li,tianyi.li@roma2.infn.it,95%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Tao Wang,twang@mju.edu.cn,82%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Yuanzheng Cai,yuanzheng_cai@mju.edu.cn,95%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Lingyu Liang,eelyliang@scut.edu.cn,78%
https://arxiv.org/pdf/2301.07533.pdf,A Multi-Scale Framework for Out-of-Distribution Detection in Dermoscopic Images,Zhongzheng Huang,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Yuanyuan Peng,pengmi467347713@126.com,78%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Lin Pan,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Pengpeng Luan,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Hongbin Tu,,0%
https://arxiv.org/pdf/2301.07475.pdf,Curvilinear object segmentation in medical images based on ODoS filter and deep learning network,Xiong Li,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Jérémy Chopin,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Jean-baptiste Fasquel,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Harold Mouchère,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Rozenn Dahyot,,0%
https://arxiv.org/pdf/2301.07468.pdf,Model-based inexact graph matching on top of CNNs for semantic scene understanding,Isabelle Bloch,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Aviad Aberdam,aaberdam@amazon.com,82%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,David Bensaïd,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Alona Golts,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Roy Ganz,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Oren Nuriel,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Royee Tichauer,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Shai Mazor,,0%
https://arxiv.org/pdf/2301.07464.pdf,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,Ron Litman,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Jiashi Feng,jshfeng@bytedance.com,82%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Fan Ma,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Xiaojie Jin,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Heng Wang,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Jingjia Huang,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Linchao Zhu,,0%
https://arxiv.org/pdf/2301.07463.pdf,Temporal Perceiving Video-Language Pre-training,Yi Yang,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Carlo Metta,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Riccardo Guidotti,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Yuan Yin,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Patrick Gallinari,,0%
https://arxiv.org/pdf/2302.03033.pdf,"Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling",Salvatore Rinzivillo,,0%
https://arxiv.org/pdf/2301.07431.pdf,Sharp Eyes: A Salient Object Detector Working The Same Way as Human Visual Characteristics,Yahong Guo,guoyh@qlu.edu.cn,78%
https://arxiv.org/pdf/2301.07431.pdf,Sharp Eyes: A Salient Object Detector Working The Same Way as Human Visual Characteristics,Jinbao Li,jinb@sdas.org,90%
https://arxiv.org/pdf/2301.07431.pdf,Sharp Eyes: A Salient Object Detector Working The Same Way as Human Visual Characteristics,Ge Zhu,zhuge@hlju.edu.cn,95%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Yushu Zhang,yushu@nuaa.edu.cn,85%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Xiaochun Cao,caoxiaochun@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Chao Wang,c.wang@nuaa.edu.cn,82%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Yong Xiang,yong.xiang@deakin.edu.au,95%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Tao Xiang,txiang@cqu.edu.cn,82%
https://arxiv.org/pdf/2301.07409.pdf,Representing Noisy Image Without Denoising,Shuren Qi,,0%
https://arxiv.org/pdf/2301.07407.pdf,TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks,Vasileios Mezaris,bmezaris@iti.gr,78%
https://arxiv.org/pdf/2301.07407.pdf,TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks,Mariano Ntrougkas,ntrougkas@iti.gr,78%
https://arxiv.org/pdf/2301.07407.pdf,TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks,Nikolaos Gkalelis,gkalelis@iti.gr,78%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Cédric Demonceaux,cedric.demonceaux@u-bourgogne.fr,95%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Guillaume Allibert,allibert@i3s.unice.fr,78%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Chao Ma,chaoma@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Zongwei Wu,,0%
https://arxiv.org/pdf/2301.07405.pdf,HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness,Fabrice Meriaudeau,,0%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Aviad Aberdam,aaberdam@amazon.com,82%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Roy Ganz,ganz@cs.technion.ac.il,78%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Yair Kittenplon,yairk@amazon.com,85%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Shai Mazor,smazor@amazon.com,82%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Ron Litman,litmanr@amazon.com,78%
https://arxiv.org/pdf/2301.07389.pdf,Towards Models that Can See and Read,Oren Nuriel,,0%
https://arxiv.org/pdf/2301.07385.pdf,Three-dimensional reconstruction and characterization of bladder deformations,Stanislas Rapacchi,stanislas.rapacchi@univ-amu.fr,95%
https://arxiv.org/pdf/2301.07385.pdf,Three-dimensional reconstruction and characterization of bladder deformations,Augustin C. Ogier,augustin.ogier@gmail.com,95%
https://arxiv.org/pdf/2301.07385.pdf,Three-dimensional reconstruction and characterization of bladder deformations,Marc-emmanuel Bellemare,marc-emmanuel.bellemare@univ-amu.fr,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Benedikt Wiestler,b.wiestler@tum.de,82%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Jiancheng Yang,jiancheng.yang@epfl.ch,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Hongwei Bran Li,hongwei.li@tum.de,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Suprosana Shit,suprosanna.shit@tum.de,82%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Bjoern Menze,bjoern.menze@uzh.ch,95%
https://arxiv.org/pdf/2301.07382.pdf,ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations,Chinmay Prabhakar,chinmay.prabhakar@uzh.ch,95%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Munan Ning,munanning@pku.edu.cn,95%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Shuicheng Yan,yansc@sea.com,78%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Li Yuan,yuanli-ece@pku.edu.cn,95%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Yonghong Tian,yhtian@pku.edu.cn,82%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Donghuan Lu,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Yujia Xie,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Dongdong Chen,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Dong Wei,,0%
https://arxiv.org/pdf/2301.07354.pdf,MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation,Yefeng Zheng,,0%
https://arxiv.org/pdf/2301.07340.pdf,Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant,Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.07340.pdf,Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant,Ying Jin,,0%
https://arxiv.org/pdf/2301.07340.pdf,Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant,Jiaqi Wang,,0%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Dinh Phung,dinh.phung@monash.edu,95%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Son Duy Dao,duy.dao@monash.edu,78%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Jianfei Cai,jianfei.cai@monash.edu,95%
https://arxiv.org/pdf/2301.07336.pdf,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,Hengcan Shi,hengcan.shi@monash.edu,95%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Jinkyu Kim,jinkyukim@korea.ac.kr,95%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Gyeongrok Oh,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Sungjune Kim,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Heon Gu,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Sang Ho Yoon,,0%
https://arxiv.org/pdf/2301.07330.pdf,FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment,Sangpil Kim,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jimmy Ren,pubs-permissions@ieee.org,55%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jiawei Zhang,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jinshan Pan,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Daoye Wang,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Shangchen Zhou,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Xing Wei,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Furong Zhao,,0%
https://arxiv.org/pdf/2301.07329.pdf,Deep Dynamic Scene Deblurring from Optical Flow,Jianbo Liu,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Xiaoye Qian,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Youbao Tang,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Ning Zhang,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Mei Han,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Jing Xiao,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Ming-chun Huang,,0%
https://arxiv.org/pdf/2301.07322.pdf,HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation,Ruei-sung Lin,,0%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Jianfeng Weng,jwen0609@uni.sydney.edu.au,65%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Jingya Wang,wangjingya@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Zhiyong Wang,zhiyong.wang@sydney.edu.au,95%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Kun Hu,,0%
https://arxiv.org/pdf/2301.07320.pdf,Robust Knowledge Adaptation for Federated Unsupervised Person ReID,Tingting Yao,,0%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Kanghao Chen,kanec9707@gmail.com,65%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Ruixuan Wang,wangruix5@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Wei-shi Zheng,wszheng@ieee.org,82%
https://arxiv.org/pdf/2301.07316.pdf,Adaptively Integrated Knowledge Distillation and Prediction Uncertainty for Continual Learning,Sijia Liu,liusj56@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.07315.pdf,Face Recognition in the age of CLIP & Billion image datasets,Aaditya Bhat,aadityaubhat@gmail.com,95%
https://arxiv.org/pdf/2301.07315.pdf,Face Recognition in the age of CLIP & Billion image datasets,Shrey Jain,,0%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Jun Shu,xjtushujun@gmail.com,95%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Kehui Ding,,0%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Deyu Meng,,0%
https://arxiv.org/pdf/2301.07306.pdf,Improve Noise Tolerance of Robust Loss via Noise-Awareness,Zongben Xu,,0%
https://arxiv.org/pdf/2301.07301.pdf,PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection,Tianyun Zhao,zhaoty@nwpu.edu.cn,78%
https://arxiv.org/pdf/2301.07301.pdf,PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection,Rui Wan,,0%
https://arxiv.org/pdf/2301.07301.pdf,PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection,Wei Zhao,,0%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Aswathnarayan Radhakrishnan,radhakrishnan.39@osu.edu,78%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Jim Davis,davis.1719@osu.edu,78%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Matthew Scherreik,matthew.scherreik.1@us.af.mil,95%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Roman Ilin,roman.ilin.1@us.af.mil,95%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Benjamin Lewis,benjamin.lewis.13@us.af.mil,95%
https://arxiv.org/pdf/2301.07294.pdf,Enhancing Self-Training Methods,Zachary Rabin,rabin.30@osu.edu,78%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Cecilia Morales,cgmorale@andrew.cmu.edu,65%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Howie Choset,choset@andrew.cmu.edu,78%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Tejas Rane,tejasr@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Robert Edman,redman@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Jason Yao,jlyao@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.07286.pdf,Reslicing Ultrasound Images for Data Augmentation and Vessel Reconstruction,Artur Dubrawski,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Andrej Janda,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Brandon Wagstaff,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Edwin G. Ng,,0%
https://arxiv.org/pdf/2301.07283.pdf,Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data,Jonathan Kelly,,0%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Guohang Yan,yanguohang@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Zhuochun Liu,liuzhuochun@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Yikang Li,liyikang@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07279.pdf,SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios,Zhaotong Luo,luozhaotong@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Gang Chen,chengang08@semi.ac.cn,95%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Jixing Li,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Xiaozhou Guo,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Benzhe Dai,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Guoliang Gong,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Min Jin,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Wenyu Mao,,0%
https://arxiv.org/pdf/2301.07266.pdf,ACQ: Improving Generative Data-free Quantization Via Attention Correction,Huaxiang Lu,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Olivia Weng,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Gabriel Marcano,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Vladimir Loncar,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Alireza Khodamoradi,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Nojan Sheybani,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Andres Meza,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Farinaz Koushanfar,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Kristof Denolf,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Javier Mauricio Duarte,,0%
https://arxiv.org/pdf/2301.07247.pdf,Tailor: Altering Skip Connections for Resource-Efficient Inference,Ryan Kastner,,0%
https://arxiv.org/pdf/2301.07236.pdf,Effective End-to-End Vision Language Pretraining with Semantic Visual Loss,Fayao Liu,fayaoliu@gmail.com,95%
https://arxiv.org/pdf/2301.07236.pdf,Effective End-to-End Vision Language Pretraining with Semantic Visual Loss,Guosheng Lin,gslin@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.07236.pdf,Effective End-to-End Vision Language Pretraining with Semantic Visual Loss,Xiaofeng Yang,xiaofeng001@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Aaron Carass,carass@jhu.edu,78%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Fangxu Xing,fxing1@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Muhan Shao,muhan@jhu.edu,85%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Zhangxing Bian,zbian4@jhu.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Yihao Liu,yliu236@jhu.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jiachen Zhuo,jzhuo@umm.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jinglun Yu,jyu146@jhu.edu,82%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jerry L. Prince,prince@jhu.edu,78%
https://arxiv.org/pdf/2301.07234.pdf,DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue,Jonghye Woo,jwoo@mgh.harvard.edu,82%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Bipasha Sen,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Aditya Agarwal,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Gaurav Singh,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Brojeshwar B.,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Srinath Sridhar,,0%
https://arxiv.org/pdf/2301.07213.pdf,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,Madhava Krishna,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Shervin Dehghani,shervin.dehghani@tum.de,95%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Michael Sommersperger,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Peiyao Zhang,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Alejandro Martin-gomez,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Benjamin Busam,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Peter Gehlbach,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Nassir Navab,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,M. Ali Nasseri,,0%
https://arxiv.org/pdf/2301.07204.pdf,Robotic Navigation Autonomy for Subretinal Injection via Intelligent Real-Time Virtual iOCT Volume Slicing,Iulian Iordachita,,0%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Renell Castro,renell@yukoai.com,85%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Shubhra Aich,saich@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Jean Marie Uwabeza Vianney,jean@yukoai.com,85%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Sajith Rajapaksa,sajith@yukoai.com,85%
https://arxiv.org/pdf/2301.07178.pdf,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,Farzad Khalvati,farzad.khalvati@utoronto.ca,95%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Marcella Cornia,firstname.lastname@unimore.it,70%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Roberto Bigazzi,,0%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Silvia Cascianelli,,0%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Lorenzo Baraldi,,0%
https://arxiv.org/pdf/2301.07150.pdf,Embodied Agents for Efficient Exploration and Smart Scene Description,Rita Cucchiara,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Manthan Patel,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Marco Karrer,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Philipp Bänninger,,0%
https://arxiv.org/pdf/2301.07147.pdf,COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM,Margarita Chli,,0%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Ce Liu,ce.liu@microsoft.com,95%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Kilho Son,kilhoson@microsoft.com,95%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Jianfeng Gao,jfgao@microsoft.com,82%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Chunyuan Li,chunyl@microsoft.com,81%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Jianwei Yang,jianwyan@microsoft.com,65%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Yong Jae Lee,yongjaelee@cs.wisc.edu,95%
https://arxiv.org/pdf/2301.07094.pdf,Learning Customized Visual Models with Retrieval-Augmented Knowledge,Haotian Liu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Yuheng Li,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Haotian Liu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Qingyang Wu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Fangzhou Mu,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Jianwei Yang,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Jianfeng Gao,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Chunyuan Li,,0%
https://arxiv.org/pdf/2301.07093.pdf,GLIGEN: Open-Set Grounded Text-to-Image Generation,Yong Jae Lee,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Bingchen Zhao,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Quan Cui,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Hao Wu,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Osamu Yoshie,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Cheng Yang,,0%
https://arxiv.org/pdf/2301.07088.pdf,Vision Learners Meet Web Image-Text Pairs,Oisin Mac Aodha,,0%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Vishwa S. Parekh,vparekh@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Adway U. Kanhere,akanhere@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Pranav Kulkarni,pkulkarni@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07074.pdf,SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations,Paul H. Yi,pyi@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Joël L. Lavanchy,joel.lavanchy@ihu-strasbourg.eu,95%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Armine Vardazaryan,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Pietro Mascagni,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Ai4safechole Consortium,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Didier Mutter,,0%
https://arxiv.org/pdf/2301.07053.pdf,Preserving Privacy in Surgical Video Analysis Using Artificial Intelligence: A Deep Learning Classifier to Identify Out-of-Body Scenes in Endoscopic Videos,Nicolas Padoy,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,H. Ayoobi,h.ayoobi@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,H. Kasaei,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,M. Cao,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,R. Verbrugge,,0%
https://arxiv.org/pdf/2301.07037.pdf,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,B. Verheij,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Hanwei Zhang,zhanghanwei0912@gmail.com,95%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Felipe Torres,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Ronan Sicre,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Yannis Avrithis,,0%
https://arxiv.org/pdf/2301.07002.pdf,Opti-CAM: Optimizing saliency maps for interpretability,Stephane Ayache,,0%
https://arxiv.org/pdf/2301.06975.pdf,Vision Based Machine Learning Algorithms for Out-of-Distribution Generalisation,Hamza Riaz,hamza.riaz2@mail.dcu.ie,95%
https://arxiv.org/pdf/2301.06975.pdf,Vision Based Machine Learning Algorithms for Out-of-Distribution Generalisation,Alan F. Smeaton,,0%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Xiang-li Li,lixl19@mails.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Ralph R. Martin,martinrr@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Tai-jiang Mu,taijiang@tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Shi-min Hu,shimin@tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.06962.pdf,Long Range Pooling for 3D Large-Scale Scene Understanding,Meng-hao Guo,,0%
https://arxiv.org/pdf/2301.06961.pdf,Composite Deep Network with Feature Weighting for Improved Delineation of COVID Infection in Lung CT,Pallabi Dutta,duttapallabi2907@gmail.com,95%
https://arxiv.org/pdf/2301.06961.pdf,Composite Deep Network with Feature Weighting for Improved Delineation of COVID Infection in Lung CT,Sushmita Mitra,sushmita@isical.ac.in,85%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Xinggang Wang,xgwang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Shusheng Yang,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Yixiao Ge,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Kun Yi,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Dian Li,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Ying Shan,,0%
https://arxiv.org/pdf/2301.06958.pdf,RILS: Masked Visual Reconstruction in Language Semantic Space,Xiaohu Qie,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Yi Yang,yi@bit.edu.cn,85%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Yu Gao,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Xi Xu,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Tianji Jiang,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Siyuan Chen,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Yufeng Yue,,0%
https://arxiv.org/pdf/2301.06944.pdf,"DR-WLC: Dimensionality Reduction cognition for object detection and pose estimation by Watching, Learning and Checking",Mengyin Fu,,0%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Peng Cao,caopeng@mail.neu.edu.cn,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Osmar R. Zaiane,zaiane@cs.ualberta.ca,78%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Xiaoli Liu,liuxiaoli.lxl@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Qingshan Hou,houqingshancv@gmail.com,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Jinzhu Yang,yangjinzhu@cse.neu.edu.cn,95%
https://arxiv.org/pdf/2301.06943.pdf,Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement,Jiaqi Wang,,0%
https://arxiv.org/pdf/2301.06910.pdf,BSNet: Lane Detection via Draw B-spline Curves Nearby,Haoxin Chen,,0%
https://arxiv.org/pdf/2301.06910.pdf,BSNet: Lane Detection via Draw B-spline Curves Nearby,Mengmeng Wang,,0%
https://arxiv.org/pdf/2301.06910.pdf,BSNet: Lane Detection via Draw B-spline Curves Nearby,Yong Liu,,0%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Zhaohong Deng,dengzhaohong@jiangnan.edu.cn,95%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Yuanyuan Wang,wxwangst@aliyun.com,78%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Kup-sze Choi,thomasks.choi@polyu.edu.hk,78%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Qiongdan Lou,,0%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Shudong Hu,,0%
https://arxiv.org/pdf/2301.06892.pdf,Cooperation Learning Enhanced Colonic Polyp Segmentation Based on Transformer-CNN Fusion,Shitong Wang,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Benjamin Tams,benjamin.tams@secunet.com,95%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Christian Rathgeb,christian.rathgeb@secunet.com,95%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Johannes Merkle,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Vanessa Nesterowicz,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Ulrike Korte,,0%
https://arxiv.org/pdf/2301.06882.pdf,Multi-Biometric Fuzzy Vault based on Face and Fingerprints,Matthias Neu,,0%
https://arxiv.org/pdf/2301.06874.pdf,Training Methods of Multi-label Prediction Classifiers for Hyperspectral Remote Sensing Images,Salma Haidar,salma.haidar@uantwerpen.be,95%
https://arxiv.org/pdf/2301.06874.pdf,Training Methods of Multi-label Prediction Classifiers for Hyperspectral Remote Sensing Images,José Oramas,,0%
https://arxiv.org/pdf/2301.06871.pdf,Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks,Anna Midgley,amidgley@g.harvard.edu,82%
https://arxiv.org/pdf/2301.06871.pdf,Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks,Lars Lien Ankile,larsankile@g.harvard.edu,95%
https://arxiv.org/pdf/2301.06871.pdf,Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks,Sebastian Weisshaar,sweisshaar@g.harvard.edu,82%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Junjie Zhou,zhoujunjie@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Chinwai Chiu,chiuchinwai@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Yongping Xiong,ypxiong@bupt.edu.cn,82%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Xiangyang Gong,xygong@bupt.edu.cn,82%
https://arxiv.org/pdf/2301.06869.pdf,SAT: Size-Aware Transformer for 3D Point Cloud Semantic Segmentation,Fangyu Liu,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Aniket Agarwal,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Alex Zhang,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Karthik Narasimhan,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Igor Gilitschenski,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Vishvak Murahari,,0%
https://arxiv.org/pdf/2301.06866.pdf,Building Scalable Video Understanding Benchmarks through Sports,Yash Kant,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Manasi Muglikar,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Leonard Bauersfeld,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Diederik Paul Moeys,,0%
https://arxiv.org/pdf/2301.06855.pdf,Event-based Shape from Polarization,Davide Scaramuzza,,0%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Zhong Ji,jizhong@tju.edu.cn,95%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Xuelong Li,li@nwpu.edu.cn,78%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Di Wang,wangdi2015@tju.edu.cn,95%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Yan Zhang,yzhang1995@tju.edu.cn,82%
https://arxiv.org/pdf/2301.06844.pdf,USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval,Yanwei Pang,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,A. V. Dobshik,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,S. K. Verbitskiy,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,I. A. Pestunov,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,K. M. Sherman,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,Yu. N. Sinyavskiy,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,A. A. Tulupov,,0%
https://arxiv.org/pdf/2301.06793.pdf,Acute ischemic stroke lesion segmentation in non-contrast CT images using 3D convolutional neural networks,V. B. Berikov,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Chongshan Lu,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Fukun Yin,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Xin Chen,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Tao Chen,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Gang Yu,,0%
https://arxiv.org/pdf/2301.06782.pdf,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,Jiayuan Fan,,0%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Jiawan Zhang,jwzhang@tju.edu.cn,82%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Xiaojie Guo,xj.max.guo@gmail.com,82%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Meng Wang,,0%
https://arxiv.org/pdf/2301.06733.pdf,Face Inverse Rendering via Hierarchical Decoupling,Wenjing Dai,,0%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Shehroz S. Khan,shehroz.khan@uhn.ca,95%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Chinchu Thomas,chinchu.thomas@iiitb.ac.in,95%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Dinesh Babu Jayagopi,jdinesh@iiitb.ac.in,85%
https://arxiv.org/pdf/2301.06730.pdf,Bag of States: A Non-sequential Approach to Video-based Engagement Measurement,Ali Abedi,ali.abedi@uhn.ca,95%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Peng Tu,yh.peng.tu@gmail.com,95%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Xu Xie,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Guo Ai,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Yuexiang Li,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Yawen Huang,,0%
https://arxiv.org/pdf/2301.06719.pdf,FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs,Yefeng Zheng,,0%
https://arxiv.org/pdf/2301.06715.pdf,SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network,H. Jin Kim,hjinkim@snu.ac.kr,82%
https://arxiv.org/pdf/2301.06715.pdf,SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network,Dongseok Shim,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Jing Li,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Di Kang,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Wenjie Pei,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Xuefei Zhe,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Ying Zhang,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Linchao Bao,,0%
https://arxiv.org/pdf/2301.06690.pdf,Audio2Gestures: Generating Diverse Gestures from Audio,Zhenyu He,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Yuchen Wu,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Kun Song,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Fangzheng Zhao,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Jiansheng Chen,,0%
https://arxiv.org/pdf/2301.06685.pdf,Distribution Aligned Feature Clustering for Zero-Shot Sketch-Based Image Retrieval,Huimin Ma,,0%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Paul H. Yi,paul.yi@stjude.org,95%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Pranav Kulkarni,pkulkarni@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Adway Kanhere,akanhere@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.06683.pdf,From Isolation to Collaboration: Federated Class-Heterogeneous Learning for Chest X-Ray Classification,Vishwa S. Parekh,vparekh@som.umaryland.edu,82%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Hengrong Lan,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Lijie Huang,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Zhiqiang Li,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Jing Lv,,0%
https://arxiv.org/pdf/2301.06681.pdf,Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction,Jianwen Luo,,0%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Prateek Chhikara,prateek.chhikara@housing.com,95%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Harshul Kuhar,harshul.kuhar@housing.com,95%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Anil Goyal,anil.goyal@housing.com,95%
https://arxiv.org/pdf/2301.06680.pdf,DIGITOUR: Automatic Digital Tours for Real-Estate Properties,Chirag Sharma,chirag.sharma@housing.com,95%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Changqun Xia,xiachq@pcl.ac.cn,78%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Jia Li,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Shengye Qiao,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Zhirui Zhao,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Chenxi Xie,,0%
https://arxiv.org/pdf/2301.06679.pdf,Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff,Xiaowu Chen,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Fintan O'sullivan,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Kirita-rose Escott,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Rachael C. Shaw,,0%
https://arxiv.org/pdf/2301.06678.pdf,Feature-based Image Matching for Identifying Individual Kākā,Andrew Lensen,,0%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Parsa Delavari,parsadlr@student.ubc.ca,85%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Gulcenur Ozturan,,0%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Ozgur Yilmaz,,0%
https://arxiv.org/pdf/2301.06675.pdf,Artificial intelligence as a gateway to scientific discovery: Uncovering features in retinal fundus images,Ipek Oruc,,0%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Hai-dang Nguyen,nhdang@selab.hcmus.edu.vn,85%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Minh-triet Tran,tmtriet@fit.hcmus.edu.vn,85%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Trong-hieu Nguyen Mau,,0%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Quoc-huy Trinh,,0%
https://arxiv.org/pdf/2301.06673.pdf,Multi Kernel Positional Embedding ConvNeXt for Polyp Segmentation,Nhat-tan Bui,,0%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Ramzi Majaj,majaj@umass.edu,78%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Kaidong Chai,kchai@umass.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Haowen Yu,hyu@cs.umass.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Edward Wang,ejaywang@eng.ucsd.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Donghyun Kim,donghyunkim@cs.umass.edu,95%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Upal Mahbub,upalmahbub@yahoo.com,95%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Hava Siegelmann,hava@umass.edu,85%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Tauhidur Rahman,trahman@ucsd.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Francesca Walsh,fnwalsh@umass.edu,82%
https://arxiv.org/pdf/2301.06648.pdf,Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment,Zhongyang Zhang,,0%
https://arxiv.org/pdf/2301.06629.pdf,Diverse Multimedia Layout Generation with Multi Choice Learning,David D. Nguyen,d.d.nguyen@unsw.edu.au,82%
https://arxiv.org/pdf/2301.06629.pdf,Diverse Multimedia Layout Generation with Multi Choice Learning,Salil S. Kanhere,salil.kanhere@unsw.edu.au,95%
https://arxiv.org/pdf/2301.06629.pdf,Diverse Multimedia Layout Generation with Multi Choice Learning,Surya Nepal,surya.nepal@data61.csiro.au,95%
https://arxiv.org/pdf/2301.06624.pdf,TAAL: Test-time Augmentation for Active Learning in Medical Image Segmentation,Mélanie Gaillochet,melanie.gaillochet.1@ens.etsmtl.ca,95%
https://arxiv.org/pdf/2301.06624.pdf,TAAL: Test-time Augmentation for Active Learning in Medical Image Segmentation,Christian Desrosiers,,0%
https://arxiv.org/pdf/2301.06624.pdf,TAAL: Test-time Augmentation for Active Learning in Medical Image Segmentation,Hervé Lombaert,,0%
https://arxiv.org/pdf/2301.06567.pdf,Scalable Surface Water Mapping up to Fine-scale using Geometric Features of Water from Topographic Airborne LiDAR Data,Hunsoo Song,,0%
https://arxiv.org/pdf/2301.06567.pdf,Scalable Surface Water Mapping up to Fine-scale using Geometric Features of Water from Topographic Airborne LiDAR Data,Jinha Jung,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Jannes Gladrow,jannes.gladrow@microsoft.com,95%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Joowon Lim,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Douglas Kelly,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Greg O'shea,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Govert Verkes,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Ioan Stefanovici,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Sebastian Nowozin,,0%
https://arxiv.org/pdf/2301.06496.pdf,Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning,Benn Thomsen,,0%
https://arxiv.org/pdf/2301.06489.pdf,Simplex Autoencoders,David Naccache,david.naccache@ens.fr,95%
https://arxiv.org/pdf/2301.06489.pdf,Simplex Autoencoders,Aymene Mohammed Bouayed,Aymene.Bouayed@ens.fr,95%
https://arxiv.org/pdf/2301.06443.pdf,Sparse resultant based minimal solvers in computer vision and their connection with the action matrix,Zuzana Kukelova,kukelova@cmp.felk.cvut.cz,78%
https://arxiv.org/pdf/2301.06443.pdf,Sparse resultant based minimal solvers in computer vision and their connection with the action matrix,Snehal Bhayani,,0%
https://arxiv.org/pdf/2301.06443.pdf,Sparse resultant based minimal solvers in computer vision and their connection with the action matrix,Janne Heikkilä,,0%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Xiaotong Li,lixiaotong@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Ling-yu Duan,lingyu@pku.edu.cn,85%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Zixuan Hu,hzxuan@pku.edu.cn,75%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Yixiao Ge,aoge@tencent.com,78%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Yongxing Dai,dai@pku.edu.cn,78%
https://arxiv.org/pdf/2301.06442.pdf,Modeling Uncertain Feature Representation for Domain Generalization,Jun Liu,,0%
https://arxiv.org/pdf/2303.11223.pdf,Monocular Cyclist Detection with Convolutional Neural Networks,Charles Tang,ctang5@wpi.edu,82%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Zhichao Wei,,0%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Xiaohao Chen,,0%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Mingqiang Chen,,0%
https://arxiv.org/pdf/2301.06429.pdf,Linguistic Query-Guided Mask Generation for Referring Image Segmentation,Siyu Zhu,,0%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Junmo Kim,junmo.kim@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Jiwan Hur,jiwan.hur@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Jaehyun Choi,chlwogus@kaist.ac.kr,55%
https://arxiv.org/pdf/2301.06392.pdf,I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,Jae Young Lee,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Hyung-min Park,hpark@sogang.ac.kr,82%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Jeongkyun Park,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Jung-wook Hwang,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Kwanghee Choi,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Seung-hyun Lee,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Jun Hwan Ahn,,0%
https://arxiv.org/pdf/2301.06375.pdf,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,Rae-hong Park,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Yassir Bendou,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Lucas Drumetz,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Vincent Gripon,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Giulia Lioi,,0%
https://arxiv.org/pdf/2301.06372.pdf,Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach,Bastien Pasdeloup,,0%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Anuja Vats,anuja.vats@ntnu.no,95%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Marius Pedersen,,0%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Ahmed Mohammed,,0%
https://arxiv.org/pdf/2301.06366.pdf,Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,Øistein Hovde,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Andrea Coletta,coletta@di.uniroma1.it,78%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Flavio Giorgi,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Gaia Maselli,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Matteo Prata,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Domenicomichele Silvestri,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Jonathan Ashdown,,0%
https://arxiv.org/pdf/2301.06363.pdf,A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems,Francesco Restuccia,,0%
https://arxiv.org/pdf/2301.07502.pdf,Multimodal Side-Tuning for Document Classification,Stefano Pio Zingaro,,0%
https://arxiv.org/pdf/2301.07502.pdf,Multimodal Side-Tuning for Document Classification,Giuseppe Lisanti,,0%
https://arxiv.org/pdf/2301.07502.pdf,Multimodal Side-Tuning for Document Classification,Maurizio Gabbrielli,,0%
https://arxiv.org/pdf/2301.06358.pdf,Post-Train Adaptive U-Net for Image Segmentation,Kostiantyn Khabarlak,habarlack@gmail.com,65%
https://arxiv.org/pdf/2301.06324.pdf,Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification,Youngjae Song,yuong13@skku.edu,65%
https://arxiv.org/pdf/2301.06324.pdf,Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification,Kwang-su Kim,kim.kwangsu@skku.edu,95%
https://arxiv.org/pdf/2301.06324.pdf,Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification,Sung Kuk Shyn,davidshyn@skku.edu,78%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Jingdong Wang,wangjingdong@baidu.com,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Wenhao Wu,wenhao.wu@sydney.edu.au,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Chang Liu,liuchang2022@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Yuxin Song,songyuxin02@baidu.com,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Xiangyang Ji,xyji@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Xiangbo Shu,shuxb@njust.edu.cn,78%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Weiping Wang,wangweiping@iie.ac.cn,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Yu Zhou,zhouyu@iie.ac.cn,95%
https://arxiv.org/pdf/2301.06309.pdf,UATVR: Uncertainty-Adaptive Text-Video Retrieval,Bo Fang,fangbo@iie.ac.cn,95%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Jeroen Van Der Laak,Jeroen.vanderLaak@radboudumc.nl,95%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Yiping Jiao,ping@nuist.edu.cn,90%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Francesco Ciompi,francesco.ciompi@radboudumc.nl,95%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Shadi Albarqouni,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Zhang Li,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Tao Tan,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Abhir Bhalerao,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Jiabo Ma,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Jiamei Sun,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Johnathan Pocock,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Josien P. W. Pluim,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Navid Alemi Koohbanani,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Raja Muhammad Saad Bashir,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Shan E Ahmed Raza,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Sibo Liu,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Simon Graham,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Suzanne Wetstein,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Syed Ali Khurram,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Thomas Watson,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Nasir Rajpoot,,0%
https://arxiv.org/pdf/2301.06304.pdf,LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset,Mitko Veta,,0%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Lucas Heublein,heublels@iis.fraunhofer.de,65%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Christopher Mutschler,christopher.mutschler@iis.fraunhofer.de,95%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Felix Ott,felix.ott@iis.fraunhofer.de,95%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,Bernd Bischl,bernd.bischl@stat.uni-muenchen.de,95%
https://arxiv.org/pdf/2301.06293.pdf,Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition,David Rügamer,david.ruegamer@stat.uni-muenchen.de,85%
https://arxiv.org/pdf/2301.06286.pdf,Meta Generative Attack on Person Reidentification,A V Subramanyam,subramanyam@iiitd.ac.in,95%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Youxin Pang,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Yong Zhang,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Weize Quan,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Yanbo Fan,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Xiaodong Cun,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Ying Shan,,0%
https://arxiv.org/pdf/2301.06281.pdf,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,Dong-ming Yan,,0%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Jinli Suo,jlsuo@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Zhihong Zhang,zhangzh19@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Yuchen Guo,yuchen.w.guo@gmail.com,95%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Qionghai Dai,qhdai@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Bo Zhang,b-zhang18@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Jiayi Xie,xiejiayi97@163.com,95%
https://arxiv.org/pdf/2301.06269.pdf,DarkVision: A Benchmark for Low-light Image/Video Perception,Runzhao Yang,yangrz20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Zhiyi Kuang,zkuang@cs.cmu.edu,82%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Deepak Pathak,dpathak@cs.cmu.edu,82%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Zhiqiu Lin,zhiqiul@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Deva Ramanan,deva@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.06267.pdf,Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,Samuel Yu,samuelyu@cs.cmu.edu,95%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Yushan Han,yushanhan@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Congyan Lang,cylang@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Yi Jin,yjin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Hui Zhang,huizhang1@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Yidong Li,ydli@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06262.pdf,"Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges",Huifang Li,,0%
https://arxiv.org/pdf/2301.06230.pdf,Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems,Giovanni Beltrame,giovanni.beltrame@polymtl.ca,95%
https://arxiv.org/pdf/2301.06230.pdf,Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems,Pierre-yves Lajoie,pierre-yves.lajoie@polymtl.ca,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Ujjwal Baid,baidujjwal@sggs.ac.in,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Sanjay Talbar,sntalbar@sggs.ac.in,82%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Prasad Dutande,prasad.dutande@sggs.ac.in,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Bhakti Baheti,bahetibhakti@sggs.ac.in,95%
https://arxiv.org/pdf/2301.06226.pdf,Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis,Shubham Innani,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,André Santos,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,João Dinis Ferreira,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,Onur Mutlu,,0%
https://arxiv.org/pdf/2301.06193.pdf,RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs,Gabriel Falcao,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Lei Li,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Tianfang Zhang,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Stefan Oehmcke,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Fabian Gieseke,,0%
https://arxiv.org/pdf/2301.06190.pdf,BuildSeg: A General Framework for the Segmentation of Buildings,Christian Igel,,0%
https://arxiv.org/pdf/2301.06187.pdf,CNN-Based Action Recognition and Pose Estimation for Classifying Animal Behavior from Videos: A Survey,Michael Perez,,0%
https://arxiv.org/pdf/2301.06187.pdf,CNN-Based Action Recognition and Pose Estimation for Classifying Animal Behavior from Videos: A Survey,Corey Toler-franklin,,0%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Tian Guo,tian@wpi.edu,85%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Yiqin Zhao,yzhao11@wpi.edu,82%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Chongyang Ma,chongyangm@gmail.com,85%
https://arxiv.org/pdf/2301.06184.pdf,LitAR: Visually Coherent Lighting for Mobile Augmented Reality,Haibin Huang,jackiehuanghaibin@gmail.com,95%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Pedro Machado,pedro.machado@ntu.ac.uk,95%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Isibor Kennedy Ihianle,isibor.ihianle@ntu.ac.uk,95%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Nicholas Murray-hill,,0%
https://arxiv.org/pdf/2301.06180.pdf,Secure Video Streaming Using Dedicated Hardware,Laura Fontes,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Shu Zhong,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Miriam Ribul,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Youngjun Cho,,0%
https://arxiv.org/pdf/2301.06160.pdf,TextileNet: A Material Taxonomy-based Fashion Textile Dataset,Marianna Obrist,,0%
https://arxiv.org/pdf/2301.06152.pdf,Inpainting borehole images using Generative Adversarial Networks,Rachid Belmeskine,rachid@convergaince.com,85%
https://arxiv.org/pdf/2301.06152.pdf,Inpainting borehole images using Generative Adversarial Networks,Abed Benaichouche,abed@convergaince.com,85%
https://arxiv.org/pdf/2301.06143.pdf,Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality,Yiqin Zhao,yzhao11@wpi.edu,82%
https://arxiv.org/pdf/2301.06143.pdf,Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality,Tian Guo,tian@wpi.edu,85%
https://arxiv.org/pdf/2301.06143.pdf,Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality,Sean Fanello,seanfa@google.com,85%
https://arxiv.org/pdf/2301.06133.pdf,Improving Reliability of Fine-tuning with Block-wise Optimisation,Basel Barakat,,0%
https://arxiv.org/pdf/2301.06133.pdf,Improving Reliability of Fine-tuning with Block-wise Optimisation,Qiang Huang,,0%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Deyu Meng,dymeng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Huanqiang Zeng,zeng0043@hqu.edu.cn,78%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Hui Liu,h2liu@sfu.edu.hk,82%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Jinhui Hou,jh.hou@cityu.edu.hk,82%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Zhiyu Zhu,zhiyuzhu2@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2301.06132.pdf,Deep Diversity-Enhanced Feature Representation of Hyperspectral Images,Junhui Hou,,0%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Yiming Lei,ymlei@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Zilong Li,zilongli21@m.fudan.edu.cn,95%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Hongming Shan,hmshan@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Junping Zhang,jpzhang@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06122.pdf,CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation,Yangyang Li,yyli@amss.ac.cn,82%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Federico Pernici,federico.pernici@unifi.it,95%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Alberto Del Bimbo,alberto.delbimbo@unifi.it,95%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Claudio Baecchi,claudio.baecchi@unifi.it,95%
https://arxiv.org/pdf/2301.06116.pdf,Maximally Compact and Separated Features with Regular Polytope Networks,Matteo Bruni,matteo.bruni@unifi.it,95%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Chuanmin Jia,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Feng Ye,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Huifang Sun,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Siwei Ma,,0%
https://arxiv.org/pdf/2301.06115.pdf,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,Wen Gao,,0%
https://arxiv.org/pdf/2301.06103.pdf,Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,Ajmal Mian,ajmal.mian@uwa.edu.au,95%
https://arxiv.org/pdf/2301.06103.pdf,Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,Ghulam Mubashar Hassan,ghulam.hassan@uwa.edu.au,95%
https://arxiv.org/pdf/2301.06103.pdf,Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,Sania Zahan,sania.zahan@research.uwa.edu.au,95%
https://arxiv.org/pdf/2301.06077.pdf,MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation,Junichiro Fujii,jn-fujii@yachiyo-eng.co.jp,82%
https://arxiv.org/pdf/2301.06077.pdf,MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation,Takato Yasuno,tk-yasuno@yachiyo-eng.co.jp,82%
https://arxiv.org/pdf/2301.06077.pdf,MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation,Masahiro Okano,ms-okano@yachiyo-eng.co.jp,82%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Linchao Bao,linchaobao@gmail.com,95%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Haoxian Zhang,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Yue Qian,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Tangli Xue,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Changhai Chen,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Xuefei Zhe,,0%
https://arxiv.org/pdf/2301.06059.pdf,Learning Audio-Driven Viseme Dynamics for 3D Face Animation,Di Kang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Jianrong Zhang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Yangsong Zhang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Xiaodong Cun,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Shaoli Huang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Yong Zhang,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Hongwei Zhao,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Hongtao Lu,,0%
https://arxiv.org/pdf/2301.06052.pdf,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,Xi Shen,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Sen Wang,wangsen31@huawei.com,95%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Chen Shi,schiele@mpi-inf.mpg.de,75%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Shaoshuai Shi,sshi@mpi-inf.mpg.de,82%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Haiyang Wang,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Meng Lei,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Di He,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.06051.pdf,DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets,Liwei Wang,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Sihan Wang,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Fuping Wu,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Lei Li,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Zheyao Gao,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Byung-woo Hong,,0%
https://arxiv.org/pdf/2301.06043.pdf,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,Xiahai Zhuang,,0%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Yebin Liu,liuyebin@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Hongwen Zhang,zhanghongwen@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Kai Jia,kajia@umich.edu,82%
https://arxiv.org/pdf/2301.06020.pdf,Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery,Liang An,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Xiaojie Jin,jinxiaojie@bytedance.com,95%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Cheng-ze Lu,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Zhicheng Huang,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Qibin Hou,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Ming-ming Cheng,,0%
https://arxiv.org/pdf/2301.06018.pdf,CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition,Jiashi Feng,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Wei Liang,liangwei@bit.edu.cn,95%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Siyuan Huang,syhuang@bigai.ai,82%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Zan Wang,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Puhao Li,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Baoxiong Jia,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Tengyu Liu,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Yixin Zhu,,0%
https://arxiv.org/pdf/2301.06015.pdf,"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",Song-chun Zhu,,0%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Weiyang Ding,dingwy@fudan.edu.cn,78%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Jianfeng Feng,jffeng@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Jiayi Han,,0%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Longbin Zeng,,0%
https://arxiv.org/pdf/2301.06013.pdf,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,Liang Du,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Chen Li,lichen@bmie.neu.edu.cn,95%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Feng-lei Fan,fengleifan@cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Ao Chen,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Jinghua Zhang,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Md Mamunur Rahaman,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Hongzan Sun,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,M. D.,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Tieyong Zeng,,0%
https://arxiv.org/pdf/2301.06002.pdf,ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos,Marcin Grzegorzek,,0%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Yuexian Zou,zouyx@pku.edu.cn,78%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Hongxiang Li,lihongxiang@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Zhihong Zhu,zhihongzhu@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Xuxin Cheng,chengxx@stu.pku.edu.cn,78%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Meng Cao,mengcao@pku.edu.cn,95%
https://arxiv.org/pdf/2301.05997.pdf,Exploiting Auxiliary Caption for Video Grounding,Yaowei Li,,0%
https://arxiv.org/pdf/2301.05994.pdf,Min-Max-Jump distance and its applications,Gangli Liu,gl-liu13@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Iván Vallés-pérez,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Emilio Soria-olivas,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Marcelino Martínez-sober,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Antonio J. Serrano-lópez,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Joan Vila-francés,,0%
https://arxiv.org/pdf/2301.05993.pdf,Empirical study of the modulus as activation function in computer vision applications,Juan Gómez-sanchís,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Zhaohui Zheng,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Yuming Chen,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Qibin Hou,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Xiang Li,,0%
https://arxiv.org/pdf/2301.05957.pdf,Towards Spatial Equilibrium Object Detection,Ming-ming Cheng,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Andy N. D. Nguyen,Nghia.D.Nguyen@uth.tmc.edu,78%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Kareem Allam,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Xiaohong Iris Wang,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Songlin Zhang,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Jianmin Ding,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Kevin Chiu,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Karan Saluja,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Amer Wahed,,0%
https://arxiv.org/pdf/2301.05938.pdf,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,Hongxia Sun,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Enrique Vidal,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Alejandro H. Toselli,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Antonio Ríos-vila,,0%
https://arxiv.org/pdf/2301.05935.pdf,End-to-End Page-Level Assessment of Handwritten Text Recognition,Jorge Calvo-zaragoza,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Xin Jin,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Wu Zhou,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Jinyu Wang,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Duo Xu,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Yiqing Rong,,0%
https://arxiv.org/pdf/2301.05908.pdf,An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,Shuai Cui,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xinghui Li,li.xinghui@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Erik Isai Valle Salgado,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Haoxin Yan,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Yue Hong,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Peiyuan Zhu,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Shidong Zhu,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Chengwei Liao,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Yanxiang Wen,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xiu Li,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xiang Qian,,0%
https://arxiv.org/pdf/2301.05897.pdf,Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy,Xiaohao Wang,,0%
https://arxiv.org/pdf/2301.05892.pdf,Object Detection performance variation on compressed satellite image datasets with iquaflow,Katalin Takats,katalin.takats@satellogic.com,95%
https://arxiv.org/pdf/2301.05892.pdf,Object Detection performance variation on compressed satellite image datasets with iquaflow,Pau Gallés,pau.galles@satellogic.com,95%
https://arxiv.org/pdf/2301.05892.pdf,Object Detection performance variation on compressed satellite image datasets with iquaflow,Javier Marin,jmarin@satellogic.com,82%
https://arxiv.org/pdf/2301.05871.pdf,Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes,Songchun Zhang,,0%
https://arxiv.org/pdf/2301.05871.pdf,Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes,Chunhui Zhao,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Erland Hilman Fuadi,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Aristo Renaldo Ruslim,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Putu Wahyu Kusuma Wardhana,,0%
https://arxiv.org/pdf/2301.05865.pdf,Gated Self-supervised Learning For Improving Supervised Learning,Novanto Yudistira,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,Jinyang Wang,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,Tao Wang,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,Min Gan,,0%
https://arxiv.org/pdf/2301.05858.pdf,Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking,George Hadjichristofi,,0%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Jian Guan,j.guan@hrbeu.edu.cn,82%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Youtian Lin,linyoutian.loyot@gmail.com,95%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Pengming Feng,p.feng.cn@outlook.com,82%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Mingjie Xie,xiemingjie@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.05856.pdf,EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images,Guangjun He,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Zhe Sheng,shengzhe.sz@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Meng Li,list.dzl@alibaba-inc.com,78%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Senbo Wang,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Weihao Yuan,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Weichao Shen,,0%
https://arxiv.org/pdf/2301.05845.pdf,${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface,Zilong Dong,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Ross Greer,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Lulua Rakla,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Samveed Desai,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Afnan Alofi,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Akshay Gopalkrishnan,,0%
https://arxiv.org/pdf/2301.05842.pdf,"CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems",Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05839.pdf,NCP: Neural Correspondence Prior for Effective Unsupervised Shape Matching,Maks Ovsjanikov,maks@lix.polytechnique.fr,85%
https://arxiv.org/pdf/2301.05839.pdf,NCP: Neural Correspondence Prior for Effective Unsupervised Shape Matching,Souhaib Attaiki,attaiki@lix.polytechnique.fr,78%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Ross Greer,,0%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Lulua Rakla,,0%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Anish Gopalan,,0%
https://arxiv.org/pdf/2301.05838.pdf,(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Kundan Patil,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Shrushti Kale,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Jaivanti Dhokey,,0%
https://arxiv.org/pdf/2301.05819.pdf,Deepfake Detection using Biological Features: A Survey,Abhishek Gulhane,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Ross Greer,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Nachiket Deo,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Akshay Rangesh,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Pujitha Gunaratne,,0%
https://arxiv.org/pdf/2301.05805.pdf,Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Ross Greer,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Akshay Gopalkrishnan,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Nachiket Deo,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Akshay Rangesh,,0%
https://arxiv.org/pdf/2301.05804.pdf,Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context,Mohan Trivedi,,0%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Chen Zhou,chen.zhou@gatech.edu,95%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Ghassan Alregib,alregib@gatech.edu,78%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Armin Parchami,,0%
https://arxiv.org/pdf/2301.05796.pdf,Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior,Kunjan Singh,,0%
https://arxiv.org/pdf/2301.05792.pdf,RMM: Reinforced Memory Management for Class-Incremental Learning,Yaoyao Liu,yaoyao.liu@mpi-inf.mpg.de,95%
https://arxiv.org/pdf/2301.05792.pdf,RMM: Reinforced Memory Management for Class-Incremental Learning,Bernt Schiele,schiele@mpi-inf.mpg.de,78%
https://arxiv.org/pdf/2301.05792.pdf,RMM: Reinforced Memory Management for Class-Incremental Learning,Qianru Sun,qianrusun@smu.edu.sg,95%
https://arxiv.org/pdf/2301.05776.pdf,Young Labeled Faces in the Wild (YLFW): A Dataset for Children Faces Recognition,Nuno Gonçalves,nunogon@deec.uc.pt,85%
https://arxiv.org/pdf/2301.05776.pdf,Young Labeled Faces in the Wild (YLFW): A Dataset for Children Faces Recognition,Iurii Medvedev,iurii.medvedev@isr.uc.pt,95%
https://arxiv.org/pdf/2301.05776.pdf,Young Labeled Faces in the Wild (YLFW): A Dataset for Children Faces Recognition,Farhad Shadmand,farhad.shadmand@isr.uc.pt,95%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Berton Earnshaw,berton.earnshaw@recursion.com,95%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Maciej Sypetkowski,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Morteza Rezanejad,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Saber Saberian,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Oren Kraus,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,John Urbanik,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,James Taylor,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Ben Mabey,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Mason Victors,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Jason Yosinski,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Alborz Rezazadeh Sereshkeh,,0%
https://arxiv.org/pdf/2301.05768.pdf,RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods,Imran Haque,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Pol Moreno,polc@deepmind.com,85%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Adam R. Kosiorek,adamrk@deepmind.com,85%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Heiko Strathmann,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Daniel Zoran,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Rosalia G. Schneider,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Björn Winckler,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Larisa Markeeva,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Théophane Weber,,0%
https://arxiv.org/pdf/2301.05747.pdf,Laser: Latent Set Representations for 3D Generative Modeling,Danilo J. Rezende,,0%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Matthew Hagen,mathage@amazon.com,65%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Unaiza Ahsan,unaiza_ahsan@homedepot.com,95%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Yuanbo Wang,,0%
https://arxiv.org/pdf/2301.07499.pdf,A Comprehensive Review of Modern Object Segmentation Approaches,Hanyan Li,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Dalí F. D. Dos Santos,dalifreire@gmail.com,85%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Paulo R. De Faria,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Adriano M. Loyola,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Sérgio V. Cardoso,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Bruno A. N. Travençolo,,0%
https://arxiv.org/pdf/2303.10172.pdf,Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset,Marcelo Z. Do Nascimento,,0%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Hung-kuo Chu,hkchu@cs.nthu.edu.tw,82%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Chao-chen Gao,,0%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Cheng-hsiu Chen,,0%
https://arxiv.org/pdf/2301.05624.pdf,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,Jheng-wei Su,,0%
https://arxiv.org/pdf/2301.05623.pdf,Reworking geometric morphometrics into a methodology of transformation grids,Fred L. Bookstein,fred.bookstein@univie.ac.at,95%
https://arxiv.org/pdf/2301.05609.pdf,Co-manipulation of soft-materials estimating deformation from depth images,Giorgio Nicola,giorgio.nicola@stiima.cnr.it,95%
https://arxiv.org/pdf/2301.05609.pdf,Co-manipulation of soft-materials estimating deformation from depth images,Enrico Villagrossi,,0%
https://arxiv.org/pdf/2301.05609.pdf,Co-manipulation of soft-materials estimating deformation from depth images,Nicola Pedrocchi,,0%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Meng Cheng,chengmeng05@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Zaidan Ke,kezaidan@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Xiangxiang Chu,chuxiangxiang@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Xiaoming Xu,xuxiaoming04@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Hongliang Jiang,jianghongliang02@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Chuyi Li,lichuyi@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Lulu Li,lilulu05@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Bo Zhang,zhangbo97@meituan.com,95%
https://arxiv.org/pdf/2301.05586.pdf,YOLOv6 v3.0: A Full-Scale Reloading,Yifei Geng,gengyifei@meituan.com,95%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Cristina P. Santos,cristina@dei.uminho.pt,85%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Carolina Gonçalves,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,João M. Lopes,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Sara Moccia,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Daniele Berardini,,0%
https://arxiv.org/pdf/2301.05575.pdf,Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation,Lucia Migliorelli,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,Li Xiang,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,He Miao,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,Luo Haibo,,0%
https://arxiv.org/pdf/2301.05565.pdf,DINF: Dynamic Instance Noise Filter for Occluded Pedestrian Detection,Xiao Jiajie,,0%
https://arxiv.org/pdf/2301.07030.pdf,Computational Pathology for Brain Disorders,Daniel Racoceanu,daniel.racoceanu@sorbonne-universite.fr,95%
https://arxiv.org/pdf/2301.07030.pdf,Computational Pathology for Brain Disorders,Gabriel Jimenez,,0%
https://arxiv.org/pdf/2301.05528.pdf,Development of a Prototype Application for Rice Disease Detection Using Convolutional Neural Networks,Arpee Callejo-arruejo,arpee.callejo@unp.edu.ph,85%
https://arxiv.org/pdf/2301.05528.pdf,Development of a Prototype Application for Rice Disease Detection Using Convolutional Neural Networks,Harold Costales,hlcostales@unp.edu.ph,82%
https://arxiv.org/pdf/2301.05528.pdf,Development of a Prototype Application for Rice Disease Detection Using Convolutional Neural Networks,Noel Rafanan,noel.rafanan@unp.edu.ph,95%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Qi Zhao,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Shuchang Lyu,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Binghao Liu,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Lijiang Chen,,0%
https://arxiv.org/pdf/2301.05526.pdf,Self-Training Guided Disentangled Adaptation for Cross-Domain Remote Sensing Image Semantic Segmentation,Hongbo Zhao,,0%
https://arxiv.org/pdf/2301.05506.pdf,On the feasibility of attacking Thai LPR systems with adversarial examples,Norrathep Rattanavipanon,norrathep.r@phuket.psu.ac.th,85%
https://arxiv.org/pdf/2301.05506.pdf,On the feasibility of attacking Thai LPR systems with adversarial examples,Jakapan Suaboot,jakapan.su@phuket.psu.ac.th,85%
https://arxiv.org/pdf/2301.05506.pdf,On the feasibility of attacking Thai LPR systems with adversarial examples,Chissanupong Jiamsuchon,,0%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Zengxin Qi,qizengxin@huashan.org.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Ying Mao,maoying@huashan.org.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Xiangyu Zhao,xiangyu.zhao@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Qian Wang,qianwang@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Lichi Zhang,lichizhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Sheng Wang,wsheng@sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.05500.pdf,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation,Xuehai Wu,wuxuehai2013@163.com,95%
https://arxiv.org/pdf/2301.05499.pdf,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,Martin Engilberge,firstname.lastname@epfl.ch,70%
https://arxiv.org/pdf/2301.05499.pdf,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,Vidit Vidit,,0%
https://arxiv.org/pdf/2301.05499.pdf,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,Mathieu Salzmann,,0%
https://arxiv.org/pdf/2301.05496.pdf,Learning Transformations To Reduce the Geometric Shift in Object Detection,Martin Engilberge,firstname.lastname@epfl.ch,70%
https://arxiv.org/pdf/2301.05496.pdf,Learning Transformations To Reduce the Geometric Shift in Object Detection,Vidit Vidit,,0%
https://arxiv.org/pdf/2301.05496.pdf,Learning Transformations To Reduce the Geometric Shift in Object Detection,Mathieu Salzmann,,0%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Guillaume Sautière,gsautie@qti.qualcomm.com,90%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Tianlin Xu,tianlin.xu1@gmail.com,95%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Auke Wiggers,auke@qti.qualcomm.com,85%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Jens Petersen,jpeterse@qti.qualcomm.com,90%
https://arxiv.org/pdf/2301.05489.pdf,A Residual Diffusion Model for High Perceptual Quality Codec Augmentation,Noor Fathima Ghouse,noor@qti.qualcomm.com,85%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Julian Schön,julian.e.s@di.ku.dk,85%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Raghavendra Selvan,,0%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Lotte Nygård,,0%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Ivan Richter Vogelius,,0%
https://arxiv.org/pdf/2301.05465.pdf,Explicit Temporal Embedding in Deep Generative Latent Models for Longitudinal Medical Image Synthesis,Jens Petersen,,0%
https://arxiv.org/pdf/2301.05440.pdf,Learnable Heterogeneous Convolution: Learning both topology and strength,Zhenzhi Wu,zhenzhi.wu@lynxi.com,95%
https://arxiv.org/pdf/2301.05440.pdf,Learnable Heterogeneous Convolution: Learning both topology and strength,Qikun Zhang,qikun.zhang@lynxi.com,95%
https://arxiv.org/pdf/2301.05440.pdf,Learnable Heterogeneous Convolution: Learning both topology and strength,Rongzhen Zhao,rongzhen.zhao@lynxi.com,95%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Marian Bittner,mbittner.work@gmail.com,82%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Wei-tse Yang,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Xucong Zhang,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Ajay Seth,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Jan Van Gemert,,0%
https://arxiv.org/pdf/2301.05435.pdf,Towards Single Camera Human 3D-Kinematics,Frans C. T. Van Der Helm,,0%
https://arxiv.org/pdf/2301.05434.pdf,LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility,Pratik Narang,pratik.narang@pilani.bits-pilani.ac.in,95%
https://arxiv.org/pdf/2301.05434.pdf,LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility,Esha Pahwa,achleshl@andrew.cmu.edu,60%
https://arxiv.org/pdf/2301.05434.pdf,LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility,Achleshwar Luthra,,0%
https://arxiv.org/pdf/2301.05421.pdf,Anti-aliasing Predictive Coding Network for Future Video Frame Prediction,Chaofan Ling,wichaofan@mail.scut.edu.cn,85%
https://arxiv.org/pdf/2301.05421.pdf,Anti-aliasing Predictive Coding Network for Future Video Frame Prediction,Junpei Zhong,joni.zhong@polyu.edu.hk,82%
https://arxiv.org/pdf/2301.05421.pdf,Anti-aliasing Predictive Coding Network for Future Video Frame Prediction,Weihua Li,,0%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Jiajun Deng,djiajun1206@gmail.com,85%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Houqiang Li,lihq@ustc.edu.cn,78%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Jianmin Ji,jianmin@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Yu Zhang,yuzhang@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Yanyong Zhang,yanyongz@ustc.edu.cn,85%
https://arxiv.org/pdf/2301.05711.pdf,OA-DET3D: Embedding Object Awareness as a General Plug-in for Multi-Camera 3D Object Detection,Xiaomeng Chu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Kaiwen Wan,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Lei Li,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Dengqiang Jia,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Shangqi Gao,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Wei Qian,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Yingzhi Wu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Huandong Lin,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Xiongzheng Mu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Xin Gao,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Sijia Wang,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Fuping Wu,,0%
https://arxiv.org/pdf/2301.05392.pdf,Multi-Target Landmark Detection with Incomplete Images via Reinforcement Learning and Shape Prior,Xiahai Zhuang,,0%
https://arxiv.org/pdf/2301.05372.pdf,Text to Point Cloud Localization with Relation-Enhanced Transformer,Hehe Fan,hehe.fan@nus.edu.sg,95%
https://arxiv.org/pdf/2301.05372.pdf,Text to Point Cloud Localization with Relation-Enhanced Transformer,Guangzhi Wang,guangzhi.wang@u.nus.edu,95%
https://arxiv.org/pdf/2301.05372.pdf,Text to Point Cloud Localization with Relation-Enhanced Transformer,Mohan Kankanhalli,mohan@comp.nus.edu.sg,85%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Scott Howland,scott.howland@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Keerti Kappagantula,keertisahithi.kappagantula@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Tegan Emerson,tegan.emerson@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Lara Kassab,lara.kassab@pnnl.gov,95%
https://arxiv.org/pdf/2302.08495.pdf,"Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing",Henry Kvinge,henry.kvinge@pnnl.gov,95%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Miao Yin,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Burak Uzkent,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Yilin Shen,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Hongxia Jin,,0%
https://arxiv.org/pdf/2301.05345.pdf,GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,Bo Yuan,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Simbarashe Nyatsanga,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Taras Kucherenko,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Chaitanya Ahuja,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Gustav Eje Henter,,0%
https://arxiv.org/pdf/2301.05339.pdf,A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,Michael Neff,,0%
https://arxiv.org/pdf/2301.05323.pdf,Salient Object Detection for Images Taken by People With Vision Impairments,Jarek Reynolds,,0%
https://arxiv.org/pdf/2301.05323.pdf,Salient Object Detection for Images Taken by People With Vision Impairments,Chandra Kanth Nagesh,,0%
https://arxiv.org/pdf/2301.05323.pdf,Salient Object Detection for Images Taken by People With Vision Impairments,Danna Gurari,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Yinghao Xu,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Yujun Shen,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Jiapeng Zhu,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Ceyuan Yang,,0%
https://arxiv.org/pdf/2301.05315.pdf,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,Bolei Zhou,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Anas Mahmoud,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Jordan S. K. Hu,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Tianshu Kuai,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Ali Harakeh,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Liam Paull,,0%
https://arxiv.org/pdf/2301.05709.pdf,Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss,Steven L. Waslander,,0%
https://arxiv.org/pdf/2301.05246.pdf,Online Class-Incremental Learning For Real-World Food Image Classification,Jiangpeng He,he416@purdue.edu,78%
https://arxiv.org/pdf/2301.05246.pdf,Online Class-Incremental Learning For Real-World Food Image Classification,Fengqing Zhu,zhu0@purdue.edu,78%
https://arxiv.org/pdf/2301.05246.pdf,Online Class-Incremental Learning For Real-World Food Image Classification,Siddeshwar Raghavan,raghav12@purdue.edu,55%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Zhenfang Chen,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Qinhong Zhou,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Yikang Shen,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Yining Hong,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Hao Zhang,,0%
https://arxiv.org/pdf/2301.05226.pdf,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Chuang Gan,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Yotam Nitzan,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Michaël Gharbi,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Richard Zhang,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Taesung Park,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Jun-yan Zhu,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2301.05225.pdf,Domain Expansion of Image Generators,Eli Shechtman,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Ziyi Li,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Qinye Zhou,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Xiaoyun Zhang,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Ya Zhang,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Yanfeng Wang,,0%
https://arxiv.org/pdf/2301.05221.pdf,Open-vocabulary Object Segmentation with Diffusion Models,Weidi Xie,,0%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Huan Wang,wang.huan@northeastern.edu,95%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Can Qin,,0%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Yue Bai,,0%
https://arxiv.org/pdf/2301.05219.pdf,"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",Yun Fu,,0%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Artem Moskalev,a.moskalev@uva.nl,82%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Ivan Sosnovik,i.sosnovik@uva.nl,82%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Arnold Smeulders,a.w.m.smeulders@uva.nl,82%
https://arxiv.org/pdf/2301.05213.pdf,Learning to Summarize Videos by Contrasting Clips,Cees Kaandorp,cees.kaandorp@gmail.com,95%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Hong-xing Yu,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Samir Agarwala,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Charles Herrmann,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Richard Szeliski,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Noah Snavely,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Jiajun Wu,,0%
https://arxiv.org/pdf/2301.05211.pdf,Accidental Light Probes,Deqing Sun,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Xiaoping Hong,thongxpu@sustech.edu.cn,78%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Fu Zhang,fuzhangu@connect.hku.hk,95%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Jiarong Lin,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Chongjiang Yuan,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Yixi Cai,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Haotian Li,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Yunfan Ren,,0%
https://arxiv.org/pdf/2301.05206.pdf,ImMesh: An Immediate LiDAR Localization and Meshing Framework,Yuying Zou,,0%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Daniel Gehrig,dgehrig@ifi.uzh.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Jingyun Liang,jinliang@vision.ee.ethz.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Christos Sakaridis,csakaridis@vision.ee.ethz.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Lei Sun,sun@zju.edu.cn,78%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Luc Van Gool,vangool@vision.ee.ethz.ch,78%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Mathias Gehrig,mgehrig@ifi.uzh.ch,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Peng Sun,pengsunr@zju.edu.cn,95%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Davide Scaramuzza,sdavide@ifi.uzh.ch,85%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Zhijie Xu,z.xu@hud.ac.uk,82%
https://arxiv.org/pdf/2301.05191.pdf,A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild,Kaiwei Wang,wangkaiwei@zju.edu.cn,95%
https://arxiv.org/pdf/2301.05180.pdf,Effective Decision Boundary Learning for Class Incremental Learning,Kunchi Li,likunchi2020@ia.ac.cn,95%
https://arxiv.org/pdf/2301.05180.pdf,Effective Decision Boundary Learning for Class Incremental Learning,Jun Wan,,0%
https://arxiv.org/pdf/2301.05180.pdf,Effective Decision Boundary Learning for Class Incremental Learning,Shan Yu,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Diogo Luvizon,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Marc Habermann,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Vladislav Golyanik,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Adam Kortylewski,,0%
https://arxiv.org/pdf/2301.05175.pdf,Scene-Aware 3D Multi-Human Motion Capture from a Single Camera,Christian Theobalt,,0%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Maarten De Rijke,m.derijke@uva.nl,82%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Mariya Hendriksen,m.hendriksen@uva.nl,82%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Ernst Kuiper,ekuiper@bol.com,82%
https://arxiv.org/pdf/2301.05174.pdf,Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,Svitlana Vakulenko,,0%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Dominik Zietlow,ZIETLD@AMAZON.DE,76%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Francesco Locatello,LOCATELF@AMAZON.DE,65%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Bernhard Schölkopf,BS@TUEBINGEN.MPG.DE,90%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Yuejiang Liu,YUEJIANG.LIU@EPFL.CH,95%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Max Horn,HORNMAX@AMAZON.DE,95%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Alexandre Alahi,ALEXANDRE.ALAHI@EPFL.CH,95%
https://arxiv.org/pdf/2301.05169.pdf,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,Chris Russell,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Jovana Mitrovic,mitrovic@deepmind.com,78%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Pierre H. Richemond,richemond@deepmind.com,78%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Matko Bošnjak,matko@deepmind.com,85%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Nenad Tomasev,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Florian Strub,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Jacob C. Walker,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Felix Hill,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Lars Holger Buesing,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Razvan Pascanu,,0%
https://arxiv.org/pdf/2301.05158.pdf,SemPPL: Predicting pseudo-labels for better contrastive representations,Charles Blundell,,0%
https://arxiv.org/pdf/2301.05124.pdf,Poses of People in Art: A Data Set for Human Pose Estimation in Digital Art History,Ricarda Vollmer,ricarda.vollmer@campus.lmu.de,95%
https://arxiv.org/pdf/2301.05124.pdf,Poses of People in Art: A Data Set for Human Pose Estimation in Digital Art History,Stefanie Schneider,stefanie.schneider@itg.uni-muenchen.de,95%
https://arxiv.org/pdf/2301.05106.pdf,Forgetful Active Learning with Switch Events: Efficient Sampling for Out-of-Distribution Data,Ghassan Alregib,alregib@gatech.edu,78%
https://arxiv.org/pdf/2301.05106.pdf,Forgetful Active Learning with Switch Events: Efficient Sampling for Out-of-Distribution Data,Ryan Benkert,rbenkert3@gatech.edu,82%
https://arxiv.org/pdf/2301.05106.pdf,Forgetful Active Learning with Switch Events: Efficient Sampling for Out-of-Distribution Data,Mohit Prabhushankar,,0%
https://arxiv.org/pdf/2301.05070.pdf,Wildfire Smoke Detection with Computer Vision,Eldan R. Daniel,deldanr@gmail.com,85%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Hang Li,xszhang0320@gmail.com,85%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Xinsong Zhang,,0%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Yan Zeng,,0%
https://arxiv.org/pdf/2301.05065.pdf,"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",Jipeng Zhang,,0%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Simon Mangold,simon.mangold@kit.edu,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Alexander Cebulla,alexander.cebulla@kit.edu,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2301.05033.pdf,Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,Xuelei Bi,xuelei.bi@student.kit.edu,95%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Siddharth Ravi,siddharth.ravi@gcloud.ua.es,95%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Martin Kampel,martin.kampel@tuwien.ac.at,95%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Sophie Noiret,snoiret@cvl.tuwien.ac.at.com,82%
https://arxiv.org/pdf/2301.05012.pdf,Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms,Francisco Florez-revuelta,,0%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Lianghua He,helianghua@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Longzhen Yang,yanglongzhen@tongji.edu.cn,95%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Yitao Peng,,0%
https://arxiv.org/pdf/2301.04970.pdf,Hierarchical Dynamic Masks for Visual Explanation of Neural Networks,Yihang Liu,,0%
https://arxiv.org/pdf/2301.04956.pdf,Graph Laplacian for Semi-Supervised Learning,Or Streicher,orr.shtr@gmail.com,85%
https://arxiv.org/pdf/2301.04956.pdf,Graph Laplacian for Semi-Supervised Learning,Guy Gilboa,guy.gilboa@ee.technion.ac.il,95%
https://arxiv.org/pdf/2301.04954.pdf,Reaching the Edge of the Edge: Image Analysis in Space,Robert Bayer,,0%
https://arxiv.org/pdf/2301.04954.pdf,Reaching the Edge of the Edge: Image Analysis in Space,Julian Priest,,0%
https://arxiv.org/pdf/2301.04954.pdf,Reaching the Edge of the Edge: Image Analysis in Space,Pınar Tözün,,0%
https://arxiv.org/pdf/2301.04944.pdf,ViTs for SITS: Vision Transformers for Satellite Image Time Series,Erik Chavez,erik.chavez@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.04944.pdf,ViTs for SITS: Vision Transformers for Satellite Image Time Series,Stefanos Zafeiriou,s.zafeiriou@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.04944.pdf,ViTs for SITS: Vision Transformers for Satellite Image Time Series,Michail Tarasiou,michail.tarasiou10@imperial.ac.uk,95%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Giovanna Castellano,,0%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Eugenio Cotardo,,0%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Corrado Mencar,,0%
https://arxiv.org/pdf/2301.04937.pdf,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,Gennaro Vessio,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Runnan Chen,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Youquan Liu,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Lingdong Kong,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Xinge Zhu,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yuexin Ma,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yikang Li,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yuenan Hou,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Yu Qiao,,0%
https://arxiv.org/pdf/2301.04926.pdf,CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP,Wenping Wang,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Guanbin Li,liguanbin@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Ruifei Zhang,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Peiwen Lai,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Xiang Wan,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,De-jun Fan,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Feng Gao,,0%
https://arxiv.org/pdf/2301.04904.pdf,Lesion-aware Dynamic Kernel for Polyp Segmentation,Xiao-jian Wu,,0%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Itsumi Saito,itsumi.saito.df@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Kyosuke Nishida,kyosuke.nishida.rx@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Ryota Tanaka,ryouta.tanaka.rg@hco.ntt.co.jp,82%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Taku Hasegawa,taku.hasegawa.ps@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Kuniko Saito,kuniko.saito.ku@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04883.pdf,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,Kosuke Nishida,kosuke.nishida.ap@hco.ntt.co.jp,95%
https://arxiv.org/pdf/2301.04882.pdf,ZScribbleSeg: Zen and the Art of Scribble Supervised Medical Image Segmentation,Ke Zhang,,0%
https://arxiv.org/pdf/2301.04882.pdf,ZScribbleSeg: Zen and the Art of Scribble Supervised Medical Image Segmentation,Xiahai Zhuang,,0%
https://arxiv.org/pdf/2301.04875.pdf,Color-NeuraCrypt: Privacy-Preserving Color-Image Classification Using Extended Random Neural Networks,Hitoshi Kiya,kiya@tmu.ac.jp,78%
https://arxiv.org/pdf/2301.04875.pdf,Color-NeuraCrypt: Privacy-Preserving Color-Image Classification Using Extended Random Neural Networks,Zheng Qi,,0%
https://arxiv.org/pdf/2301.04875.pdf,Color-NeuraCrypt: Privacy-Preserving Color-Image Classification Using Extended Random Neural Networks,Aprilpyone Maungmaung,,0%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Changxin Gao,cgao@hust.edu.cn,82%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Nong Sang,nsang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Aoyan Li,aoyanli@hust.edu.cn,95%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Changqian Yu,changqianyu@meituan.com,95%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Zilin Guo,zilin guo@hust.edu.cn,95%
https://arxiv.org/pdf/2301.04870.pdf,Semantic Segmentation via Pixel-to-Center Similarity Calculation,Dongyue Wu,dongyue wu@hust.edu.cn,95%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Guanbin Li,liguanbin@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Ruifei Zhang,,0%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Sishuo Liu,,0%
https://arxiv.org/pdf/2301.04866.pdf,Self-Supervised Correction Learning for Semi-Supervised Biomedical Image Segmentation,Yizhou Yu,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Xiaogang Wang,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Yuhang Cheng,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Liang Wang,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Jiangbo Lu,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Kai Xu,,0%
https://arxiv.org/pdf/2301.04860.pdf,Edge Preserving Implicit Surface Representation of Point Clouds,Guoqiang Xiao,,0%
https://arxiv.org/pdf/2301.04847.pdf,Real-time FPGA implementation of the Semi-Global Matching stereo vision algorithm for a 4K/UHD video stream,Mariusz Grabowski,grabowski@student.agh.edu.pl,78%
https://arxiv.org/pdf/2301.04847.pdf,Real-time FPGA implementation of the Semi-Global Matching stereo vision algorithm for a 4K/UHD video stream,Tomasz Kryjak,tomasz.kryjak@agh.edu.pl,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Ling Li,lingl@njust.edu.cn,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Lin Zhao,linzhao@njust.edu.cn,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Jie Xu,jiexu@njust.edu.cn,95%
https://arxiv.org/pdf/2301.04842.pdf,Towards High Performance One-Stage Human Pose Estimation,Linhao Xu,,0%
https://arxiv.org/pdf/2301.04811.pdf,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,Lei Fan,Lei.Fan@xjtlu.edu.cn,95%
https://arxiv.org/pdf/2301.04811.pdf,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,Yang Zhao,,0%
https://arxiv.org/pdf/2301.04811.pdf,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,Hyungjoon Seo,,0%
https://arxiv.org/pdf/2301.04805.pdf,DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention,Zewei He,zeweihe@zju.edu.cn,95%
https://arxiv.org/pdf/2301.04805.pdf,DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention,Zixuan Chen,,0%
https://arxiv.org/pdf/2301.04805.pdf,DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention,Zhe-ming Lu,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Mohamed Akrout,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Bálint Gyepesi,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Péter Holló,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Adrienn Poór,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Blága Kincső,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Stephen Solis,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Katrina Cirone,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Jeremy Kawahara,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Dekker Slade,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Latif Abid,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,Máté Kovács,,0%
https://arxiv.org/pdf/2301.04802.pdf,Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images,István Fazekas,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Guanbin Li,liguanbin@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Ruifei Zhang,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Zhen Li,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Shuguang Cui,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Dahong Qian,,0%
https://arxiv.org/pdf/2301.04799.pdf,Adaptive Context Selection for Polyp Segmentation,Yizhou Yu,,0%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Binbin Chen,chenbinbin8@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Weijie Chen,chenweijie5@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Di Xie,xiedi@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Shiliang Pu,pushiliang.hri@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Yueting Zhuang,yzhuang@zju.edu.cn,82%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Wei Zhao,zhaowei29@hikvision.com,95%
https://arxiv.org/pdf/2301.04796.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,Shicai Yang,yangshicai@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Yueting Zhuang,yzhuang@zju.edu.cn,82%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Weijie Chen,chenweijie5@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Di Xie,xiedi@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Shiliang Pu,pushiliang.hri@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Yilu Guo,guoyilu5@hikvision.com,95%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Xingyue Shi,shixy@stu.pku.edu.cn,78%
https://arxiv.org/pdf/2301.04795.pdf,1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track,Shicai Yang,yangshicai@hikvision.com,95%
https://arxiv.org/pdf/2301.04791.pdf,Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction,Khai Nguyen,khainb@utexas.edu,85%
https://arxiv.org/pdf/2301.04791.pdf,Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction,Dang Nguyen,,0%
https://arxiv.org/pdf/2301.04791.pdf,Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction,Nhat Ho,,0%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Alexander Carballo,alex@gifu-u.ac.jp,90%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Robin Karlsson,karlsson.robin@g.sp.m.is.nagoya-u.ac.jp,95%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Kazuya Takeda,kazuya.takeda@nagoya-u.jp,95%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Keisuke Fujii,fujii@i.nagoya-u.ac.jp,78%
https://arxiv.org/pdf/2301.04783.pdf,Predictive World Models from Real-World Partial Observations,Kento Ohtani,ohtani.kento@g.sp.m.is.nagoya-u.ac.jp,95%
https://arxiv.org/pdf/2301.04751.pdf,Artificial Intelligence Generated Coins for Size Comparison,Gerald Artner,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Zhihua Liu,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Bin Yang,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Yan Shen,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Xuejun Ni,,0%
https://arxiv.org/pdf/2301.04748.pdf,LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking,Huiyu Zhou,,0%
https://arxiv.org/pdf/2301.04746.pdf,Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning,Eduardo Alonso,e.alonso@city.ac.uk,82%
https://arxiv.org/pdf/2301.04746.pdf,Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning,Chi-hang Suen,chi.suen@city.ac.uk,95%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Han Lin,hanhlin@gene.com,95%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Hai Ngu,hain@gene.com,85%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Mohsen Hejrati,hejratis@gene.com,78%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Sarah Chu,chus18@gene.com,78%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Baris Bingol,barisb@gene.com,85%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Fatemeh Haghighi,fhaghigh@asu.edu,90%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Soumitra Ghosh,ghoshs29@gene.com,78%
https://arxiv.org/pdf/2301.08141.pdf,Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease,Somaye Hashemifar,,0%
https://arxiv.org/pdf/2301.04742.pdf,HADA: A Graph-based Amalgamation Framework in Image-text Retrieval,Manh-duy Nguyen,,0%
https://arxiv.org/pdf/2301.04742.pdf,HADA: A Graph-based Amalgamation Framework in Image-text Retrieval,Binh T. Nguyen,,0%
https://arxiv.org/pdf/2301.04742.pdf,HADA: A Graph-based Amalgamation Framework in Image-text Retrieval,Cathal Gurrin,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Weihua Zhou,whzhou@mtu.edu,82%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Chen Zhao,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Zhihui Xu,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Jingfeng Jiang,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Michele Esposito,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Drew Pienta,,0%
https://arxiv.org/pdf/2301.04733.pdf,AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms,Guang-uei Hung,,0%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Ruta Desai,rutadesai@meta.com,95%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Nitin Kamra,nitinkamra@meta.com,95%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Weichao Mao,weichao2@illinois.edu,85%
https://arxiv.org/pdf/2302.05330.pdf,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,Michael Louis Iuzzolino,mliuzzolino@meta.com,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Lijun Qian,liqian@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Pamela Obiomon,phobiomon@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Taoreed Akinola,takinola2@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Richard Wilkins,rtwilkins@pvamu.edu,82%
https://arxiv.org/pdf/2301.04705.pdf,Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation,Xiangfang Li,xili@pvamu.edu,82%
https://arxiv.org/pdf/2301.04695.pdf,Learning Continuous Mesh Representation with Spherical Implicit Surface,Zhongpai Gao,,0%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Euntai Kim,etkim@yonsei.ac.kr,82%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Suhyeon Lee,hyeon93@yonsei.ac.kr,60%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Kyoungwon Min,minkw@keti.re.kr,78%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Hongje Seong,hjseong@yonsei.ac.kr,82%
https://arxiv.org/pdf/2301.04685.pdf,SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation,Seokbeom Song,,0%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Mayank Agarwal,mayankag@cmu.edu,85%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Naveen Venkat,nvenkat@cmu.edu,82%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Maneesh Singh,dr.maneesh.singh@ieee.org,95%
https://arxiv.org/pdf/2301.04650.pdf,Geometry-biased Transformers for Novel View Synthesis,Shubham Tulsiani,,0%
https://arxiv.org/pdf/2301.04648.pdf,Head-Free Lightweight Semantic Segmentation with Linear Transformer,Fan Wang,fan.w@alibaba-inc.com,85%
https://arxiv.org/pdf/2301.04648.pdf,Head-Free Lightweight Semantic Segmentation with Linear Transformer,Pichao Wang,pichaowang@gmail.com,95%
https://arxiv.org/pdf/2301.04648.pdf,Head-Free Lightweight Semantic Segmentation with Linear Transformer,Bo Dong,bo.dong.cst@gmail.com,95%
https://arxiv.org/pdf/2301.04647.pdf,EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata,Chenhao Zheng,,0%
https://arxiv.org/pdf/2301.04647.pdf,EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata,Ayush Shrivastava,,0%
https://arxiv.org/pdf/2301.04647.pdf,EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata,Andrew Owens,,0%
https://arxiv.org/pdf/2301.04644.pdf,Does progress on ImageNet transfer to real-world datasets?,Simon Kornblith,skornblith@google.com,82%
https://arxiv.org/pdf/2301.04644.pdf,Does progress on ImageNet transfer to real-world datasets?,Ludwig Schmidt,schmidt@cs.washington.edu,78%
https://arxiv.org/pdf/2301.04644.pdf,Does progress on ImageNet transfer to real-world datasets?,Alex Fang,,0%
https://arxiv.org/pdf/2301.04634.pdf,Street-View Image Generation from a Bird's-Eye View Layout,Bolei Zhou,bolei@cs.ucla.edu,85%
https://arxiv.org/pdf/2301.04634.pdf,Street-View Image Generation from a Bird's-Eye View Layout,Alexander Swerdlow,aswerdlow@ucla.edu,82%
https://arxiv.org/pdf/2301.04634.pdf,Street-View Image Generation from a Bird's-Eye View Layout,Runsheng Xu,,0%
https://arxiv.org/pdf/2301.04631.pdf,Deep Residual Axial Networks,Nazmul Shahadat,nazmul.ruet@gmail.com,85%
https://arxiv.org/pdf/2301.04631.pdf,Deep Residual Axial Networks,Anthony S. Maida,maida@louisiana.edu,78%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,John Elliott,john.o.elliott@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Masahiro Ono,masahiro.ono@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Abhishek Cauligi,abhishek.s.cauligi@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,R. Michael Swan,robert.m.swan@jpl.nasa.gov,82%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Deegan Atha,deegan.j.atha@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Shreyansh Daftry,shreyansh.daftry@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.04630.pdf,ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation,Larry Matthies,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Ayşegül Dündar,adundar@cs.bilkent.edu.tr,82%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Yusuf Dalva,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Hamza Pehlivan,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Cansu Moran,,0%
https://arxiv.org/pdf/2301.04628.pdf,Image-to-Image Translation with Disentangled Latent Vectors for Face Editing,Öykü Irmak Hatipoğlu,,0%
https://arxiv.org/pdf/2301.04626.pdf,Deep Axial Hypercomplex Networks,Nazmul Shahadat,nazmul.ruet@gmail.com,85%
https://arxiv.org/pdf/2301.04626.pdf,Deep Axial Hypercomplex Networks,Anthony S. Maida,maida@louisiana.edu,78%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Kanran Zhou,kanran.zhou@student.kit.edu,95%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Linxi Qiu,linxi.qiu@student.kit.edu,95%
https://arxiv.org/pdf/2301.05027.pdf,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2301.04623.pdf,Enhancing ResNet Image Classification Performance by using Parameterized Hypercomplex Multiplication,Nazmul Shahadat,nazmul.ruet@gmail.com,85%
https://arxiv.org/pdf/2301.04623.pdf,Enhancing ResNet Image Classification Performance by using Parameterized Hypercomplex Multiplication,Anthony S. Maida,maida@louisiana.edu,78%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Feiyan Hu,feiyan.hu@dcu.ie,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Kevin Mcguinness,kevin.mcguinness@dcu.ie,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Simone Palazzo,simone.palazzo@unict.it,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Concetto Spampinato,concetto.spampinato@unict.it,95%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Federica Proietto Salanitri,,0%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Giovanni Bellitto,,0%
https://arxiv.org/pdf/2301.04619.pdf,TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation,Morteza Moradi,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Chengzhi Wu,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Julius Pfrommer,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Jürgen Beyerer,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Kangning Li,,0%
https://arxiv.org/pdf/2301.04613.pdf,Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding,Boris Neubert,,0%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Mingyuan Zhou,mingyuan.zhou@student.kit.edu,95%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2301.04612.pdf,Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.04608.pdf,Padding Module: Learning the Padding in Deep Neural Networks,Pei-chi Huang,phuang@unomaha.edu,82%
https://arxiv.org/pdf/2301.04608.pdf,Padding Module: Learning the Padding in Deep Neural Networks,Xin Zhong,xzhong@unomaha.edu,82%
https://arxiv.org/pdf/2301.04608.pdf,Padding Module: Learning the Padding in Deep Neural Networks,Fahad Alrasheedi,falrasheedi@unomaha.edu,82%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Kanran Zhou,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Jan-philipp Kaiser,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Norbert Mitschke,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Jan-felix Klein,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Julius Pfrommer,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Jürgen Beyerer,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Gisela Lanza,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Michael Heizmann,,0%
https://arxiv.org/pdf/2301.05028.pdf,MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors,Kai Furmans,,0%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Yaoyao Liu,yaoyao.liu@mpi-inf.mpg.de,95%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Yingying Li,yingli2@caltech.edu,82%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Bernt Schiele,schiele@mpi-inf.mpg.de,78%
https://arxiv.org/pdf/2301.05032.pdf,Online Hyperparameter Optimization for Class-Incremental Learning,Qianru Sun,qianrusun@smu.edu.sg,95%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Jiapeng Zhu,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Ceyuan Yang,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Yujun Shen,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Zifan Shi,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Bo Dai,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Deli Zhao,,0%
https://arxiv.org/pdf/2301.04604.pdf,LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis,Qifeng Chen,,0%
https://arxiv.org/pdf/2301.04584.pdf,Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning,Andrey Zhmoginov,azhmogin@google.com,90%
https://arxiv.org/pdf/2301.04584.pdf,Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning,Mark Sandler,sandler@google.com,78%
https://arxiv.org/pdf/2301.04584.pdf,Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning,Max Vladymyrov,,0%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Kaiqiang Chen,chenkaiqiang14@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Kun Fu,kunfuiecas@gmail.com,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Deke Tang,tangdk@geovis.com.cn,78%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Wenjie Liu,liuwenjie18@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Yongqiang Mao,maoyongqiang19@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Wei Chen,chenwei@geovis.com.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Xian Sun,sunxian@aircas.ac.cn,95%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Liangjin Zhao,,0%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Zhirui Wang,,0%
https://arxiv.org/pdf/2301.04581.pdf,Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery,Wenhui Diao,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Ozan Oktay,ozan.oktay@microsoft.com,95%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Shruthi Bannur,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Stephanie Hyland,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Qianchu Liu,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Fernando Pérez-garcía,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Maximilian Ilse,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Daniel C. Castro,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Benedikt Boecking,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Harshita Sharma,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Kenza Bouzid,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Anja Thieme,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Anton Schwaighofer,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Maria Wetscherek,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Matthew P. Lungren,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Aditya Nori,,0%
https://arxiv.org/pdf/2301.04558.pdf,Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing,Javier Alvarez-valle,,0%
https://arxiv.org/pdf/2301.04554.pdf,Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,Wei Guo,wei.guo.cn@outlook.com,95%
https://arxiv.org/pdf/2301.04554.pdf,Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,Benedetta Tondi,,0%
https://arxiv.org/pdf/2301.04554.pdf,Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,Mauro Barni,,0%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Jiwen Lu,wen@tsinghua.edu.cn,90%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Jie Zhou,jzhou@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Xumin Yu,yuxm20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Ziyi Wang,wziyi22@mails.tsinghua.edu.cn,85%
https://arxiv.org/pdf/2301.04545.pdf,AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers,Yongming Rao,ongmimg95@gmail.com,60%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Robbie Holland,robert.holland15@ic.ac.uk,82%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Oliver Leingang,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Christopher Holmes,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Philipp Anders,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Rebecca Kaye,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Sophie Riedl,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Johannes C. Paetzold,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Ivan Ezhov,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Hrvoje Bogunović,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Ursula Schmidt-erfurth,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Lars Fritsche,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Hendrik P. N. Scholl,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Sobha Sivaprasad,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Andrew J. Lotery,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Daniel Rueckert,,0%
https://arxiv.org/pdf/2301.04525.pdf,Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration,Martin J. Menten,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Cesar Henrique Comin,comin@ufscar.br,82%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Matheus Viana Da Silva,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Natália De Carvalho Santos,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Julie Ouellette,,0%
https://arxiv.org/pdf/2301.04517.pdf,A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts,Baptiste Lacoste,,0%
https://arxiv.org/pdf/2304.00001.pdf,Determination of cutting positions of honeycomb blocks using computer vision,Alexander Razumovsky,,0%
https://arxiv.org/pdf/2304.00001.pdf,Determination of cutting positions of honeycomb blocks using computer vision,Yakov Pikalov,,0%
https://arxiv.org/pdf/2304.00001.pdf,Determination of cutting positions of honeycomb blocks using computer vision,Mikhail Saramud,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Seyyed Amirhossein Ameli Kalkhoran,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Mohammadamin Banayeeanzade,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Mahdi Samiei,,0%
https://arxiv.org/pdf/2301.04506.pdf,A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning,Mahdieh Soleymani Baghshah,,0%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Xiaoliang Dai,xiaoliangdai@meta.com,95%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Graham Cormode,gcormode@meta.com,82%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Peter Vajda,vajdap@meta.com,78%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Sayan Ghosh,sayanghosh@meta.com,95%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Karthik Prasad,,0%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Peizhao Zhang,,0%
https://arxiv.org/pdf/2301.04502.pdf,Pruning Compact ConvNets for Efficient Inference,Bichen Wu,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Jingchao Peng,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Haitao Zhao,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Kaijie Zhao,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Zhongze Wang,,0%
https://arxiv.org/pdf/2301.04497.pdf,Dynamic Background Reconstruction via MAE for Infrared Small Target Detection,Lujian Yao,,0%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Indel Pal Singh,inder.singh@uni.lu,82%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Enjie Ghorbel,,0%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Oyebade Oyedotun,,0%
https://arxiv.org/pdf/2301.04494.pdf,Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains,Djamila Aouada,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Peng Cao,caopengneu@gmail.com,95%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Zhiqiang Shen,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Hua Yang,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Xiaoli Liu,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Jinzhu Yang,,0%
https://arxiv.org/pdf/2301.04465.pdf,Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation,Osmar R. Zaiane,,0%
https://arxiv.org/pdf/2301.04460.pdf,Fast spline detection in high density microscopy data,Albert Alonso,,0%
https://arxiv.org/pdf/2301.04460.pdf,Fast spline detection in high density microscopy data,Julius B. Kirkegaard,,0%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Anne Spalanzani,Name.LastName@inria.fr,60%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Rabbia Asghar,,0%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Lukas Rummelhard,,0%
https://arxiv.org/pdf/2301.04454.pdf,Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,Christian Laugier,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Ahmet Karagoz,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Mustafa Ege Seker,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Mert Yergin,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Tarkan Atak Kan,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Mustafa Said Kartal,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Ercan Karaarslan,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Deniz Alis,,0%
https://arxiv.org/pdf/2301.09673.pdf,Prostate Lesion Estimation using Prostate Masks from Biparametric MRI,Ilkay Oksuz,,0%
https://arxiv.org/pdf/2301.04451.pdf,Heterogeneous Tri-stream Clustering Network,Chang-dong Wang,changdongwang@hotmail.com,95%
https://arxiv.org/pdf/2301.04451.pdf,Heterogeneous Tri-stream Clustering Network,Xiaozhi Deng,dengxiaozhi45@gmail.com,95%
https://arxiv.org/pdf/2301.04451.pdf,Heterogeneous Tri-stream Clustering Network,Dong Huang,huangdonghere@gmail.com,95%
https://arxiv.org/pdf/2301.04447.pdf,VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection,Mridula Verma,vmridula@idrbt.ac.in,85%
https://arxiv.org/pdf/2301.04447.pdf,VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection,Ramalingaswamy Cheruku,rmlswamy@nitw.ac.in,60%
https://arxiv.org/pdf/2301.04447.pdf,VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection,Hemraj Singh,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Frauke Wilm,frauke.wilm@fau.de,95%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Marco Fragoso,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Christof A. Bertram,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Nikolas Stathonikos,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Mathias Öttl,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Jingna Qiu,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Robert Klopfleisch,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Andreas Maier,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Katharina Breininger,,0%
https://arxiv.org/pdf/2301.04423.pdf,Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset,Marc Aubreville,,0%
https://arxiv.org/pdf/2301.04422.pdf,"Optical Flow for Autonomous Driving: Applications, Challenges and Improvements",Shihao Shen,,0%
https://arxiv.org/pdf/2301.04422.pdf,"Optical Flow for Autonomous Driving: Applications, Challenges and Improvements",Louis Kerofsky,,0%
https://arxiv.org/pdf/2301.04422.pdf,"Optical Flow for Autonomous Driving: Applications, Challenges and Improvements",Senthil Yogamani,,0%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Hong Wang,hong_wang@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Liang Peng,peng-l20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Wenbo Shao,,0%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Yanchao Xu,,0%
https://arxiv.org/pdf/2301.04421.pdf,Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective,Jun Li,,0%
https://arxiv.org/pdf/2301.04416.pdf,pyssam -- a Python library for statistical modelling of biomedical shape and appearance,Josh Williams,,0%
https://arxiv.org/pdf/2301.04416.pdf,pyssam -- a Python library for statistical modelling of biomedical shape and appearance,Ali Ozel,,0%
https://arxiv.org/pdf/2301.04416.pdf,pyssam -- a Python library for statistical modelling of biomedical shape and appearance,Uwe Wolfram,,0%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Hong Wang,hong_wang@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Chen Lv,lyuchen@ntu.edu.sg,85%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Jun Li,lijun1958@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Weida Wang,wangwd0430@163.com,82%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Wenbo Shao,,0%
https://arxiv.org/pdf/2301.04414.pdf,How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?,Yanchao Xu,,0%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Hong-yu Zhou,whuzhouhongyu@gmail.com,95%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Liansheng Wang,lswang@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Chixiang Lu,luchixiang@gmail.com,95%
https://arxiv.org/pdf/2301.04410.pdf,GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Fernando Alonso-fernandez,Fernando.alonso@uam.es,85%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Julian Fierrez-aguilar,,0%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Javier Ortega-garcia,,0%
https://arxiv.org/pdf/2301.04402.pdf,Secure access system using signature verification over tablet PC,Joaquin Gonzalez-rodriguez,,0%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Changzhen Qiu,qiuchzh@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Yuhan Xie,,0%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Zhiyong Zhang,,0%
https://arxiv.org/pdf/2301.04401.pdf,An atrium segmentation network with location guidance and siamese adjustment,Shaolong Chen,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Chiara Romanengo,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Andrea Raffo,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Silvia Biasotti,,0%
https://arxiv.org/pdf/2301.04371.pdf,Recognising geometric primitives in 3D point clouds of mechanical CAD objects,Bianca Falcidieno,,0%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Zongkai Wu,wuzongkai@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Ting Wang,wangting@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Feiyu Yao,feiyu.yao@columbia.edu,95%
https://arxiv.org/pdf/2301.04352.pdf,Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments,Donglin Wang,wangdonglin@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Soo-hyung Kim,shkim@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Guee-sang Lee,gslee@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Van Thong Huynh,vthuynh@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04288.pdf,Generic Event Boundary Detection in Video with Pyramid Features,Hyung-jeong Yang,hjyang@jnu.ac.kr,82%
https://arxiv.org/pdf/2301.04275.pdf,LENet: Lightweight And Efficient LiDAR Semantic Segmentation Using Multi-Scale Convolution Attention,Ben Ding,,0%
https://arxiv.org/pdf/2301.04272.pdf,Data Distillation: A Survey,Noveen Sachdeva,nosachde@ucsd.edu,75%
https://arxiv.org/pdf/2301.04272.pdf,Data Distillation: A Survey,Julian Mcauley,jmcauley@ucsd.edu,82%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Xiu Li,li.xiu@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Guangyi Chen,guangyichen1994@gmail.com,95%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Kai Li,li.gml.kai@gmail.com,95%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Qiaosong Chu,,0%
https://arxiv.org/pdf/2301.04265.pdf,Adversarial Alignment for Source Free Object Detection,Shuyan Li,,0%
https://arxiv.org/pdf/2301.04261.pdf,Towards Microstructural State Variables in Materials Systems,Megna N. Shah,megna.shah.1@us.af.mil,95%
https://arxiv.org/pdf/2301.04261.pdf,Towards Microstructural State Variables in Materials Systems,Veera Sundararaghavan,veeras@umich.edu,85%
https://arxiv.org/pdf/2301.04261.pdf,Towards Microstructural State Variables in Materials Systems,Jeff P. Simmons,jeff.simmons.3@afrl.af.mil,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Ye Huang,edward.ye.huang@qq.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Xiangjian He,xiangjian.he@gmail.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Xuefei Zhe,zhexuefei@outlook.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Lixin Duan,lxduan@gmail.com,82%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Di Kang,di.kang@outlook.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Linchao Bao,linchaobao@gmail.com,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Wenjing Jia,Wenjing.Jia@uts.edu.au,95%
https://arxiv.org/pdf/2301.04258.pdf,CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder,Liang Chen,liang.chen@outlook.com,95%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,Hengyuan Zhang,,0%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,Jing-yan Liao,,0%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,David Paz,,0%
https://arxiv.org/pdf/2301.04243.pdf,Robust Human Identity Anonymization using Pose Estimation,Henrik I. Christensen,,0%
https://arxiv.org/pdf/2301.04233.pdf,Adapting to Skew: Imputing Spatiotemporal Urban Data with 3D Partial Convolutions and Biased Masking,Bill Howe,billhowe@uw.edu,95%
https://arxiv.org/pdf/2301.04233.pdf,Adapting to Skew: Imputing Spatiotemporal Urban Data with 3D Partial Convolutions and Biased Masking,Bin Han,,0%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Deva Ramanan,deva@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Xindi Wu,xindiw@princeton.edu,85%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Kwunfung Lau,kwun.fung.lau@intel.com,82%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Francesco Ferroni,fferroni@nvidia.com,82%
https://arxiv.org/pdf/2301.04224.pdf,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,Aljoša Ošep,aosep@andrew.cmu.edu,82%
https://arxiv.org/pdf/2301.04221.pdf,Explaining Deep Models through Forgettable Learning Dynamics,Ghassan Alregib,alregib@gatech.edu,78%
https://arxiv.org/pdf/2301.04221.pdf,Explaining Deep Models through Forgettable Learning Dynamics,Ryan Benkert,rbenkert3@gatech.edu,82%
https://arxiv.org/pdf/2301.04221.pdf,Explaining Deep Models through Forgettable Learning Dynamics,Oluwaseun Joseph Aribido,,0%
https://arxiv.org/pdf/2301.04218.pdf,Leveraging Diffusion For Strong and High Quality Face Morphing Attacks,Chen Liu,cliu@clarkson.edu,82%
https://arxiv.org/pdf/2301.04218.pdf,Leveraging Diffusion For Strong and High Quality Face Morphing Attacks,Zander W. Blasingame,,0%
https://arxiv.org/pdf/2301.04212.pdf,Deep Learning based Multi-Label Image Classification of Protest Activities,Kosaku Sato,Ksato@vt.edu,82%
https://arxiv.org/pdf/2301.04212.pdf,Deep Learning based Multi-Label Image Classification of Protest Activities,Jialu Wang,jialu@gwu.edu,85%
https://arxiv.org/pdf/2301.04212.pdf,Deep Learning based Multi-Label Image Classification of Protest Activities,Yingzhou Lu,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Alexandre Adam,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Laurence Perreault-levasseur,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Yashar Hezaveh,,0%
https://arxiv.org/pdf/2301.04168.pdf,Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines,Max Welling,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Shiyi Lan,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Xitong Yang,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Zhiding Yu,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Zuxuan Wu,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Jose M. Alvarez,,0%
https://arxiv.org/pdf/2301.03992.pdf,Vision Transformers Are Good Mask Auto-Labelers,Anima Anandkumar,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Matthew Wallingford,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Aditya Kusupati,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Alex Fang,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Vivek Ramanujan,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Aniruddha Kembhavi,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Roozbeh Mottaghi,,0%
https://arxiv.org/pdf/2301.04101.pdf,Neural Radiance Field Codebooks,Ali Farhadi,,0%
https://arxiv.org/pdf/2301.04467.pdf,FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection,Zhaoxiang Zhang,zhaoxiang.zhang@ia.ac.cn,95%
https://arxiv.org/pdf/2301.04467.pdf,FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection,Yuqi Wang,wangyuqi2020@ia.ac.cn,95%
https://arxiv.org/pdf/2301.04467.pdf,FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection,Yuntao Chen,chenyuntao08@gmail.com,95%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Chen Wang,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Angtian Wang,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Junbo Li,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Alan Yuille,,0%
https://arxiv.org/pdf/2301.04075.pdf,Benchmarking Robustness in Neural Radiance Fields,Cihang Xie,,0%
https://arxiv.org/pdf/2301.04058.pdf,Rethinking Voxelization and Classification for 3D Object Detection,Dmitry Yudin,yudin.da@mipt.ru,78%
https://arxiv.org/pdf/2301.04058.pdf,Rethinking Voxelization and Classification for 3D Object Detection,Alexander Golodkov,golodkov.ao@phystech.edu,78%
https://arxiv.org/pdf/2301.04058.pdf,Rethinking Voxelization and Classification for 3D Object Detection,Youshaa Murhij,yosha.morheg@phystech.edu,60%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Youcef Mezouar,youcef.mezouar@sigma-clermont.fr,95%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Adrien Bartoli,adrien.bartoli@gmail.com,95%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Mohammadreza Shetab-bushehri,m.r.shetab@gmail.com,65%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Miguel Aranda,miguel.aranda@unizar.es,95%
https://arxiv.org/pdf/2301.04037.pdf,"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library",Erol Ozgur,erolozgur@gmail.com,95%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Sivaramakrishnan Rajaraman,sivaramakrishnan.rajaraman@nih.gov,95%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Feng Yang,,0%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Ghada Zamzmi,,0%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Zhiyun Xue,,0%
https://arxiv.org/pdf/2301.04032.pdf,Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?,Sameer Antani,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Jian Yan,tianqj@nju.edu.cn,75%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Xuming Zhang,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Jia Tian,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Wei Li,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Xingfa Gu,,0%
https://arxiv.org/pdf/2302.05297.pdf,Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification,Qingjiu Tian,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Chengbin Peng,pengchengbin@nbu.edu.cn,95%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Hao Xu,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Hui Xiao,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Huazheng Hao,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Li Dong,,0%
https://arxiv.org/pdf/2301.03976.pdf,Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification,Xiaojie Qiu,,0%
https://arxiv.org/pdf/2301.03966.pdf,AdvBiom: Adversarial Attacks on Biometric Matchers,Vishesh Mistry,vishesh.mistry@tech5-sa.com,95%
https://arxiv.org/pdf/2301.03966.pdf,AdvBiom: Adversarial Attacks on Biometric Matchers,Debayan Deb,debayan.deb@tech5-sa.com,95%
https://arxiv.org/pdf/2301.03966.pdf,AdvBiom: Adversarial Attacks on Biometric Matchers,Rahul Parthe,rahul.parthe@tech5-sa.com,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Nicu Sebe,niculae.sebe@unitn.it,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Mengyi Zhao,zhaomengyi@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Shuling Dai,sldai@buaa.edu.cn,82%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Bin Ren,bin.ren@unitn.it,95%
https://arxiv.org/pdf/2301.03949.pdf,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,Mengyuan Liu,liumy85@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Muhammad Arshad Khan,MuKhan@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Amir Ghalamzan E.,aghalamzanesfahani@lincoln.ac.uk,75%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Bappaditya Debnath,b.debnath2017@gmail.com,82%
https://arxiv.org/pdf/2301.03947.pdf,Autonomous Strawberry Picking Robotic System (Robofruit),Soran Parsa,soran.parsa@gmail.com,95%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Ryosuke Hyodo,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Susumu Saito,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Teppei Nakano,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Makoto Akabane,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Ryoichi Kasuga,,0%
https://arxiv.org/pdf/2301.03926.pdf,Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle,Tetsuji Ogawa,,0%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Dan Bigioi,jordanhu@tcd.ie,85%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Maciej Zięba,maciej.zieba@pwr.edu.pl,95%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Shubhajit Basak,s.basak1@nuigalway.ie,82%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Michał Stypułkowski,michal.stypulkowski@cs.uni.wroc.pl,85%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Peter Corcoran,peter.corcoran@universityofgalway.ie,95%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Rachel Mcdonnell,ramcdonn@tcd.ie,65%
https://arxiv.org/pdf/2301.04474.pdf,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,Hugh Jordan,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Thomas Walter,Thomas.Walter@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Thomas Bonte,Thomas.Bonte@minesparis.psl.eu,95%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Maxence Philbert,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Emeline Coleno,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Edouard Bertrand,,0%
https://arxiv.org/pdf/2301.03914.pdf,Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation,Arthur Imbert,,0%
https://arxiv.org/pdf/2301.03867.pdf,Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction,Laslo Dinges,laslo.dinges@ovgu.de,95%
https://arxiv.org/pdf/2301.03867.pdf,Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction,Ayoub Al-hamadi,ayoub.al-hamadi@ovgu.de,95%
https://arxiv.org/pdf/2301.03867.pdf,Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction,Thorsten Hempel,thorsten.hempel@ovgu.de,95%
https://arxiv.org/pdf/2302.08493.pdf,Deep Multi-stream Network for Video-based Calving Sign Detection,Teppei Nakano,teppei@pcl.cs.waseda.ac.jp,85%
https://arxiv.org/pdf/2302.08493.pdf,Deep Multi-stream Network for Video-based Calving Sign Detection,Ryosuke Hyodo,hyodo@pcl.cs.waseda.ac.jp,78%
https://arxiv.org/pdf/2302.08493.pdf,Deep Multi-stream Network for Video-based Calving Sign Detection,Tetsuji Ogawa,ogawa@pcl.cs.waseda.ac.jp,78%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Liming Chen,liming.chen@ec-lyon.fr,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Thomas Duboudin,thomas.duboudin@ec-lyon.fr,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Corentin Abgrall,corentin.abgrall@fr.thalesgroup.com,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Emmanuel Dellandréa,emmanuel.dellandrea@ec-lyon.fr,95%
https://arxiv.org/pdf/2301.03844.pdf,Look Beyond Bias with Entropic Adversarial Data Augmentation,Gilles Hénaff,gilles.henaff@fr.thalesgroup.com,95%
https://arxiv.org/pdf/2301.03843.pdf,A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models,Rei Aso,,0%
https://arxiv.org/pdf/2301.03843.pdf,A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models,Tatsuya Chuman,,0%
https://arxiv.org/pdf/2301.03843.pdf,A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models,Hitoshi Kiya,,0%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Juyeb Shin,juyebshin@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Dongsuk Kum,dskum@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Hyeonjun Jeong,hyeonjun.jeong@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.04470.pdf,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,Francois Rameau,francois.rameau@sunykorea.ac.kr,95%
https://arxiv.org/pdf/2301.03832.pdf,Video Semantic Segmentation with Inter-Frame Feature Fusion and Inner-Frame Feature Refinement,Zilei Wang,zlwang@ustc.edu.cn,82%
https://arxiv.org/pdf/2301.03832.pdf,Video Semantic Segmentation with Inter-Frame Feature Fusion and Inner-Frame Feature Refinement,Jiafan Zhuang,jfzhuang@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2301.03832.pdf,Video Semantic Segmentation with Inter-Frame Feature Fusion and Inner-Frame Feature Refinement,Junjie Li,,0%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Songyang Zhang,sy.zhangbuaa@gmail.com,82%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Hongbin Sun,hsun@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Songtao Liu,liusongtao@megvii.com,95%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Zeming Li,lizeming@megvii.com,95%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Jian Sun,sunjian@megvii.com,95%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Xuming He,hexm@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Nanning Zheng,nnzheng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2301.03831.pdf,Dynamic Grained Encoder for Vision Transformers,Lin Song,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Kaiping Zheng,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Thao Nguyen,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Jesslyn Hwei Sing Chong,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Charlene Enhui Goh,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Melanie Herschel,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Hee Hoon Lee,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Changshuo Liu,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Beng Chin Ooi,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,Wei Wang,,0%
https://arxiv.org/pdf/2301.03829.pdf,From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore,James Yip,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Nishant Yadav,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Mahbubul Alam,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Ahmed Farahat,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Dipanjan Ghosh,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Chetan Gupta,,0%
https://arxiv.org/pdf/2301.03826.pdf,CDA: Contrastive-adversarial Domain Adaptation,Auroop R. Ganguly,,0%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Payman Moallem,moallem@eng.ui.ac.ir,78%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Saed Moradi,,0%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Alireza Memarmoghadam,,0%
https://arxiv.org/pdf/2301.03796.pdf,Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios,Mohamad Farzan Sabahi,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Shuai Shen,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Wenliang Zhao,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Zibin Meng,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Wanhua Li,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Zheng Zhu,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Jie Zhou,,0%
https://arxiv.org/pdf/2301.03786.pdf,DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation,Jiwen Lu,,0%
https://arxiv.org/pdf/2301.03769.pdf,Learning from What is Already Out There: Few-shot Sign Language Recognition with Online Dictionaries,Matyáš Boháček,,0%
https://arxiv.org/pdf/2301.03769.pdf,Learning from What is Already Out There: Few-shot Sign Language Recognition with Online Dictionaries,Marek Hrúz,,0%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Seonguk Seo,seonguk@meta.com,95%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Sara Cao,xuefeicao01@meta.com,78%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Ser-nam Lim,sernam@ucf.edu,85%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Bohyung Han,bhhan@snu.ac.kr,82%
https://arxiv.org/pdf/2301.03767.pdf,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,Mustafa Gokhan Uzunbas,,0%
https://arxiv.org/pdf/2301.03730.pdf,Learning to Perceive in Deep Model-Free Reinforcement Learning,Gonçalo Querido,goncalo.querido@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2301.03730.pdf,Learning to Perceive in Deep Model-Free Reinforcement Learning,Alberto Sardinha,jose.alberto.sardinha@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2301.03730.pdf,Learning to Perceive in Deep Model-Free Reinforcement Learning,Francisco S. Melo,fmelo@inesc-id.pt,82%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Chris Matthews,chriscrick@cs.okstate.edu,85%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Md Zobaer Islam,zobaer.islam@okstate.edu,78%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Sabit Ekin,sabitekin@tamu.edu,95%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Dane C. Johnson,dane.johnson@okstate.edu,95%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Russ Messenger,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Matthew Whitlock,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Erik Spong,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Nate Morton,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Layne Claggett,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Jordan Fox,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Leland Palmer,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,John F. O'hara,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Christopher J. Crick,,0%
https://arxiv.org/pdf/2302.01923.pdf,Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video,Jamey D. Jacob,,0%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Max H. Siegel,maxs@mit.edu,85%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Ilker Yildirim,ilker.yildirim@yale.edu,95%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Amir A. Soltani,,0%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Shraman Ray Chaudhari,,0%
https://arxiv.org/pdf/2301.03711.pdf,3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis,Joshua B. Tenenbaum,,0%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Alberto Díaz-álvarez,alberto.diaz@upm.es,85%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Guillermo Iglesias,guillermo.iglesias@upm.es,95%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Edgar Talavera,e.talavera@upm.es,82%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Jesús Troya Garcìa,,0%
https://arxiv.org/pdf/2301.03701.pdf,Artificial Intelligence Model for Tumoral Clinical Decision Support Systems,Miguel Gracía-remesal,,0%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Keyu Tian,keyutian@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Qishuai Diao,diaoqishuai@bytedance.com,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Chen Lin,chen.lin@eng.ox.ac.uk,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Liwei Wang,wanglw@pku.edu.cn,78%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Zehuan Yuan,yuanzehuan@bytedance.com,95%
https://arxiv.org/pdf/2301.03580.pdf,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,Yi Jiang,jiangyi.enjoy@bytedance.com,95%
https://arxiv.org/pdf/2301.02667.pdf,Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,Hanbyul Joo,hbjoo@snu.ac.kr,82%
https://arxiv.org/pdf/2301.02667.pdf,Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,Jiye Lee,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Bowen Lei,bowenlei@stat.tamu.edu,95%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Dongkuan Xu,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Ruqi Zhang,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Shuren He,,0%
https://arxiv.org/pdf/2301.03573.pdf,Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction,Bani K. Mallick,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Nikolaos Tsakas,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Maria Lymperaiou,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Giorgos Filandrianos,,0%
https://arxiv.org/pdf/2301.03563.pdf,An Impartial Transformer for Story Visualization,Giorgos Stamou,,0%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Christopher Neff,cneff1@uncc.edu,82%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Hamed Tabkhi,htabkhiv@uncc.edu,82%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Ghazal Alinezhad Noghre,galinezh@uncc.edu,60%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Mohammadreza Baharani,mbaharan@uncc.edu,90%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Shanle Yao,adaneshp@uncc.edu,60%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Armin Danesh Pazho,,0%
https://arxiv.org/pdf/2301.03561.pdf,Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things,Babak Rahimi Ardabili,,0%
https://arxiv.org/pdf/2301.03553.pdf,FedDebug: Systematic Debugging for Federated Learning Applications,Waris Gill,waris@vt.edu,85%
https://arxiv.org/pdf/2301.03553.pdf,FedDebug: Systematic Debugging for Federated Learning Applications,Muhammad Ali Gulzar,gulzar@cs.vt.edu,78%
https://arxiv.org/pdf/2301.03553.pdf,FedDebug: Systematic Debugging for Federated Learning Applications,Ali Anwar,aanwar@umn.edu,82%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Julian Jordan,julian.jordan@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Steffen Staab,steffen.staab@ipvs.uni-stuttgart.de,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Julian Schmidt,julian.sj.schmidt@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Jan Rupprecht,jan.rupprecht@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Daniel Frank,daniel.frank@ipvs.uni-stuttgart.de,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Thomas Monninger,thomas.monninger@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,David Raba,david.raba@mercedes-benz.com,95%
https://arxiv.org/pdf/2301.03512.pdf,SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks,Klaus Dietmayer,klaus.dietmayer@uni-ulm.de,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Fenggang Liu,liufenggang@senseauto.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Yangguang Li,liyangguang@sensetime.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Changxin Gao,cgao@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Nong Sang,nsang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Huan Peng,penghuan@senseauto.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Jing Shao,shaojing@senseauto.com,95%
https://arxiv.org/pdf/2301.03510.pdf,Parallel Reasoning Network for Human-Object Interaction Detection,Bin Huang,huangbin1@senseauto.com,95%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Dorit Merhof,dorit.merhof@ur.de,95%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Reza Azad,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Amirhossein Kazerouni,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Moein Heidari,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Ehsan Khodapanah Aghdam,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Amirali Molaei,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Yiwei Jia,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Abin Jose,,0%
https://arxiv.org/pdf/2301.03505.pdf,Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review,Rijo Roy,,0%
https://arxiv.org/pdf/2301.03495.pdf,On the challenges to learn from Natural Data Streams,Davide Maltoni,davide.maltoni@unibo.it,95%
https://arxiv.org/pdf/2301.03495.pdf,On the challenges to learn from Natural Data Streams,Guido Borghi,guido.borghi@unibo.it,95%
https://arxiv.org/pdf/2301.03495.pdf,On the challenges to learn from Natural Data Streams,Gabriele Graffieti,,0%
https://arxiv.org/pdf/2301.03461.pdf,DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction,Yibo Yang,ibo@pku.edu.cn,90%
https://arxiv.org/pdf/2301.03461.pdf,DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction,Lefei Zhang,zhanglefei@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03461.pdf,DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction,Yangyang Xu,yangyangxu@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03439.pdf,Generalized adaptive smoothing based neural network architecture for traffic state estimation,Chuhan Yang,,0%
https://arxiv.org/pdf/2301.03439.pdf,Generalized adaptive smoothing based neural network architecture for traffic state estimation,Sai Venkata Ramana Ambadipudi,,0%
https://arxiv.org/pdf/2301.03439.pdf,Generalized adaptive smoothing based neural network architecture for traffic state estimation,Saif Eddin Jabari,,0%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Yilei Shi,yilei.shi@tum.de,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Xiao Xiang Zhu,xiaoxiang.zhu@tum.de,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Wen Yang,yangwen@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Fang Xu,xufang@whu.edu.cn,95%
https://arxiv.org/pdf/2301.03432.pdf,Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark,Patrick Ebel,patrick.ebel@tum.de,95%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Marc Hanheide,mhanheide@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Sergi Molina,smolinamellado@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Riccardo Polvara,rpolvara@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Ibrahim Hroob,ihroob@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03426.pdf,LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects,Grzegorz Cielniak,gcielniak@lincoln.ac.uk,82%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Robert Jewsbury,rob.jewsbury@warwick.ac.uk,82%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Nasir Rajpoot,n.m.rajpoot@warwick.ac.uk,82%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Quoc Dang Vu,quoc-dang.vu@warwick.ac.uk,95%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Simon Graham,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Mostafa Jahanifar,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Shan E Ahmed Raza,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Fayyaz Minhas,,0%
https://arxiv.org/pdf/2301.03418.pdf,Nuclear Segmentation and Classification: On Color & Compression Generalization,Abhir Bhalerao,,0%
https://arxiv.org/pdf/2301.03407.pdf,On Advantages of Mask-level Recognition for Outlier-aware Segmentation,Matej Grcić,,0%
https://arxiv.org/pdf/2301.03407.pdf,On Advantages of Mask-level Recognition for Outlier-aware Segmentation,Josip Šarić,,0%
https://arxiv.org/pdf/2301.03407.pdf,On Advantages of Mask-level Recognition for Outlier-aware Segmentation,Siniša Šegvić,,0%
https://arxiv.org/pdf/2301.03362.pdf,Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --,Michael Elad,elad@cs.technion.ac.il,78%
https://arxiv.org/pdf/2301.03362.pdf,Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --,Bahjat Kawar,bahjat.kawar@cs.technion.ac.il,95%
https://arxiv.org/pdf/2301.03362.pdf,Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --,Gregory Vaksman,,0%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Hai Zhao,hai@cs.sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Kehai Chen,chenkehai@hit.edu.cn,95%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Rui Wang,wangrui.nlp@gmail.com,95%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Zhuosheng Zhang,zhangzs@sjtu.edu.cn,82%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Masao Utiyama,mutiyama@nict.go.jp,82%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Eiichiro Sumita,eiichiro.sumita@nict.go.jp,95%
https://arxiv.org/pdf/2301.03344.pdf,Universal Multimodal Representation for Language Understanding,Zuchao Li,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Meng Wang,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Feng Gao,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Junyu Dong,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Heng-chao Li,,0%
https://arxiv.org/pdf/2301.03335.pdf,Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification,Qian Du,,0%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Chen Dong,dongchen@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Haotai Liang,tai@bupt.edu.cn,90%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Geng Liu,liugeng@sgchip.sgcc.com.cn,95%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Senran Fan,,0%
https://arxiv.org/pdf/2301.03331.pdf,A Specific Task-oriented Semantic Image Communication System for substation patrol inspection,Xiaodong Xu,,0%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Xiang Wang,wxiang@hust.edu.cn,85%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Changxin Gao,cgao@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Nong Sang,nsang@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Zhengrong Zuo,zhrzuo@hust.edu.cn,82%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Rong Jin,rongjinemail@gmail.com,95%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Shiwei Zhang,,0%
https://arxiv.org/pdf/2301.03330.pdf,HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition,Zhiwu Qing,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Giacomo Zara,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Victor Guilherme Turrisi Da Costa,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Subhankar Roy,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Paolo Rota,,0%
https://arxiv.org/pdf/2301.03322.pdf,Simplifying Open-Set Video Domain Adaptation with Contrastive Learning,Elisa Ricci,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xiangyu Li,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Gongning Luo,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Kuanquan Wang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Hongyu Wang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jun Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xinjie Liang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jie Jiang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Zhenghao Song,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Chunyue Zheng,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Haokai Chi,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Mingwang Xu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Yingte He,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xinghua Ma,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jingwen Guo,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Yifan Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Chuanpu Li,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Zeli Chen,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Md Mahfuzur Rahman Siddiquee,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Andriy Myronenko,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Antoine P. Sanner,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Anirban Mukhopadhyay,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Ahmed E. Othman,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xingyu Zhao,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Weiping Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Jinhuang Zhang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xiangyuan Ma,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Qinghui Liu,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Bradley J. Macintosh,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Wei Liang,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Moona Mazher,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Abdul Qayyum,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Valeriia Abramova,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Xavier Lladó,,0%
https://arxiv.org/pdf/2301.03281.pdf,The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge,Shuo Li,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Zhongling Huang,huangzhongling@nwpu.edu.cn,95%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Mihai Datcu,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Andrei Anghel,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Juanping Zhao,,0%
https://arxiv.org/pdf/2301.03589.pdf,"Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar",Remus Cacoveanu,,0%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Hao Tang,haotang@meta.com,95%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Weiyao Wang,weiyaowang@meta.com,95%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Kevin Liang,kevinjliang@meta.com,95%
https://arxiv.org/pdf/2301.03213.pdf,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,Matt Feiszli,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Ling Zhu,zhul1757@2m9h.net,78%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Ya Zhang,ya_zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Chaoyi Wu,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Feng Chang,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Xiao Su,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Zhihan Wu,,0%
https://arxiv.org/pdf/2301.03202.pdf,Integrating features from lymph node stations for metastatic lymph node detection,Yanfeng Wang,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,A. T. Gifford,alessandro.gifford@gmail.com,82%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,R. M. Cichy,rmcichy@zedat.fu-berlin.de,82%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,B. Lahner,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,S. Saba-sadiya,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,M. G. Vilas,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,A. Lascelles,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,A. Oliva,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,K. Kay,,0%
https://arxiv.org/pdf/2301.03198.pdf,The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,G. Roig,,0%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Yang Gao,gaoy@nju.edu.cn,78%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Wenzhe Yin,w.yin@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Haochen Wang,h.wang3@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Jie Liu,j.liu5@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Yanqi Bao,yanqibao1997@gmail.com,95%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Efstratios Gavves,egavves@uva.nl,82%
https://arxiv.org/pdf/2301.03194.pdf,Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network,Jan-jakob Sonke,j.sonke@nki.nl,82%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Qing Guo,ingqguo@ieee.org,78%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Ivor W. Tsang,tsang@cfar.a-star.edu.sg,78%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Lan Fu,lan.fu@innopeaktech.com,95%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Wei Feng,wfeng@ieee.org,82%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Rynson W. H. Lau,Rynson.Lau@cityu.edu.hk,95%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Yuhao Liu,yuhaoliu7456@outlook.com,95%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Ke Xu,zhanghake2-c@my.cityu.edu.hk,85%
https://arxiv.org/pdf/2301.03182.pdf,Structure-Informed Shadow Removal Networks,Zhanghan Ke,,0%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Haoqian Liang,lianghq@bupt.edu.cn,78%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Ya Yang,yangya@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.03178.pdf,Deep Planar Parallax for Monocular Depth Estimation,Zhichao Li,,0%
https://arxiv.org/pdf/2301.03169.pdf,A Study on the Generality of Neural Network Structures for Monocular Depth Estimation,Sunghoon Im,sunghoonim@dgist.ac.kr,95%
https://arxiv.org/pdf/2301.03169.pdf,A Study on the Generality of Neural Network Structures for Monocular Depth Estimation,Jinwoo Bae,woobae@hyundai.com,78%
https://arxiv.org/pdf/2301.03169.pdf,A Study on the Generality of Neural Network Structures for Monocular Depth Estimation,Kyumin Hwang,kyumin@dgist.ac.kr,85%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Jinwon Lee,djwlee@gwnu.ac.kr,78%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Seungeun Lim,aseungeunlim@korea.ac.kr,95%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Duhwan Mun,edhmun@korea.ac.kr,78%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Fazhi He,cfzhe@whu.edu.cn,78%
https://arxiv.org/pdf/2301.03167.pdf,Machining feature recognition using descriptors with range constraints for mechanical 3D models,Changmo Yeo,,0%
https://arxiv.org/pdf/2301.03164.pdf,Cursive Caption Text Detection in Videos,Imran Siddiqi,imran.siddiqi@bahria.edu.pk,95%
https://arxiv.org/pdf/2301.03164.pdf,Cursive Caption Text Detection in Videos,Ali Mirza,ali.mirza@mantalus.com,95%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Hanlong Chen,hanlong@g.ucla.edu,85%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Luzhe Huang,lzhuang0324@g.ucla.edu,82%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Tairan Liu,liutr@g.ucla.edu,78%
https://arxiv.org/pdf/2301.03162.pdf,eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging,Aydogan Ozcan,ozcan@ucla.edu,78%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Haowei Wang,wanghaowei@stu.xmu.edu.cn,95%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Yongjian Wu,littlekenwu@tencent.com,78%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Xiaoshuai Sun,xssun@xmu.edu.cn,82%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Yiyi Zhou,zhouyiyi@xmu.edu.cn,95%
https://arxiv.org/pdf/2301.03160.pdf,Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network,Jiayi Ji,,0%
https://arxiv.org/pdf/2301.03155.pdf,Instance Segmentation Based Graph Extraction for Handwritten Circuit Diagram Images,Johannes Bayer,johannes.bayer@dfki.de,95%
https://arxiv.org/pdf/2301.03155.pdf,Instance Segmentation Based Graph Extraction for Handwritten Circuit Diagram Images,Amit Kumar Roy,amit.roy@dfki.de,95%
https://arxiv.org/pdf/2301.03155.pdf,Instance Segmentation Based Graph Extraction for Handwritten Circuit Diagram Images,Andreas Dengel,andreas.dengel@dfki.de,95%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Mohammadreza Naderi,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Mohammadhossein Givkashi,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Nader Karimi,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Shahram Shirani,,0%
https://arxiv.org/pdf/2301.03130.pdf,SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions,Shadrokh Samavi,,0%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Stylianos Oikonomou,stylianos@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Anil Bandhakavi,anil@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Adelize Van Eeden,adelize.ve@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Pim Jordi Verschuuren,Pim.jv@logically.co.uk,85%
https://arxiv.org/pdf/2301.03127.pdf,Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,Jie Gao,jie@logically.co.uk,85%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Cory Cornelius,cory.cornelius@intel.com,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Shengyun Peng,speng65@gatech.edu,82%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Jason Martin,jason.martin@intel.com,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Weilin Xu,weilin.xu@intel.com,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Kevin Li,kevin.li@gatech.edu,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Rahul Duggal,rahulduggal@gatech.edu,95%
https://arxiv.org/pdf/2301.03110.pdf,RobArch: Designing Robust Architectures against Adversarial Attacks,Duen Horng Chau,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Rui Zheng,zhengrui@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Man Chen,maggiech1221@126.com,75%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Jiawen Li,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Yunqian Huang,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Sheng Song,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Hongbo Chen,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Junni Shi,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Duo Xu,,0%
https://arxiv.org/pdf/2301.03081.pdf,Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System,Haibin Zhang,,0%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Yisroel Mirsky,yisroel@bgu.ac.il,85%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Lior Yasur,lioryasu@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Guy Frankovits,guyfrank@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.03064.pdf,Deepfake CAPTCHA: A Method for Preventing Fake Calls,Fred M. Grabovski,freddie@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Liheng Bian,bian@bit.edu.cn,78%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Daoyu Li,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Hanwen Xu,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Miao Cao,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,Xin Yuan,,0%
https://arxiv.org/pdf/2301.03047.pdf,Large-scale Global Low-rank Optimization for Computational Compressed Imaging,David J. Brady,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Ming Li,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Xiangyu Xu,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Hehe Fan,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Pan Zhou,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jun Liu,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jia-wei Liu,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jiahe Li,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Jussi Keppo,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Mike Zheng Shou,,0%
https://arxiv.org/pdf/2301.03046.pdf,STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition,Shuicheng Yan,,0%
https://arxiv.org/pdf/2301.03045.pdf,Seamless Multimodal Biometrics for Continuous Personalised Wellbeing Monitoring,João Ribeiro Pinto,,0%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,James Tin-yau Kwok,jamesk@cse.ust.hk,85%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Ming Lin,minglamz@amazon.com,85%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Jie Gui,guijie@seu.edu.cn,95%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Lanting Fang,lanting@seu.edu.cn,85%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Liguo Huang,lghuang@smu.edu,82%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Bin Luo,luobin@nju.edu.cn,95%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Jidong Ge,,0%
https://arxiv.org/pdf/2301.03041.pdf,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,Yuxiang Liu,,0%
https://arxiv.org/pdf/2301.03039.pdf,Equivalence of Two Expressions of Principal Line,Cheng-yen Hsu,,0%
https://arxiv.org/pdf/2301.03039.pdf,Equivalence of Two Expressions of Principal Line,Hsin-yi Chen,,0%
https://arxiv.org/pdf/2301.03039.pdf,Equivalence of Two Expressions of Principal Line,Jen-hui Chuang,,0%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Zhengyi Liu,liuzywen@ahu.edu.cn,78%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Bin Tang,pubs-permissions@ieee.org,60%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Yacheng Tan,,0%
https://arxiv.org/pdf/2301.03036.pdf,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,Qian He,,0%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Wei Wu,liuzywen@ahu.edu.cn,85%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Zhengyi Liu,,0%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Yacheng Tan,,0%
https://arxiv.org/pdf/2301.03033.pdf,RGB-T Multi-Modal Crowd Counting Based on Transformer,Guanghui Zhang,,0%
https://arxiv.org/pdf/2301.03027.pdf,Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.03027.pdf,Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction,Jeong Eun Lee,leeje290@gmail.com,78%
https://arxiv.org/pdf/2301.03027.pdf,Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction,Gyutaek Oh,,0%
https://arxiv.org/pdf/2302.05294.pdf,MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope,Jingwei Zhang,jwzhang22@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2302.05294.pdf,MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope,Farzan Farnia,farnia@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Chong Wang,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Yuyuan Liu,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Yuanhong Chen,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Fengbei Liu,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Yu Tian,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Davis J. Mccarthy,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Helen Frazer,,0%
https://arxiv.org/pdf/2301.04011.pdf,Learning Support and Trivial Prototypes for Interpretable Image Classification,Gustavo Carneiro,,0%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Lijun Zhao,zhaolj@hit.edu.cn,78%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Ke Wang,wangke@hit.edu.cn,95%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Tao Xie,xietao1997@hit.edu.cn,95%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Kun Dai,,0%
https://arxiv.org/pdf/2301.02993.pdf,DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching,Ruifeng Li,,0%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Yiyu Shi,yshi4@nd.edu,82%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Tsung-yi Ho,tyho@cs.nthu.edu.tw,82%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Yu-jen Chen,yujenchen@gapp.nthu.edu.tw,95%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Ching-hao Chiu,,0%
https://arxiv.org/pdf/2301.02989.pdf,Fair Multi-Exit Framework for Facial Attribute Classification,Hao-wei Chung,,0%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Yuyin Sun,yuyinsun@amazon.com,95%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Jiajia Luo,lujiajia@amazon.com,85%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Lu Xia,luxial@amazon.com,95%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Zhongyu Jiang,zyjiang@uw.edu,82%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Nan Qiao,qiaonan@amazon.com,95%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Ke Zhang,kezha@amazon.com,85%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Jenq-neng Hwang,hwang@uw.edu,78%
https://arxiv.org/pdf/2301.02979.pdf,CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations,Cheng-yen Yang,cycyang@uw.edu,82%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Ying Wei,weiying@ise.neu.edu.cn,95%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Yuefeng Wang,wangyuefeng0203@gmail.com,95%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Shuailei Ma,,0%
https://arxiv.org/pdf/2301.04019.pdf,FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection,Shanze Wang,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Jie Li,jielixjtu@xjtu.edu.cn,95%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Fengping Wang,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Chun Qi,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Lin Wang,,0%
https://arxiv.org/pdf/2301.02969.pdf,Multi-scale multi-modal micro-expression recognition algorithm based on transformer,Pan Wang,,0%
https://arxiv.org/pdf/2301.02934.pdf,Advancing 3D finger knuckle recognition via deep feature learning,Xu Cheng,xcheng@nuist.edu.cn,82%
https://arxiv.org/pdf/2301.02934.pdf,Advancing 3D finger knuckle recognition via deep feature learning,Guoying Zhao,guoying.zhao@oulu.fi,95%
https://arxiv.org/pdf/2301.02934.pdf,Advancing 3D finger knuckle recognition via deep feature learning,Kevin H. M. Cheng,homan.cheng@oulu.fi,78%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Pushpak Pati,pus@zurich.ibm.com,90%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Guillaume Jaume,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Zeineb Ayadi,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Kevin Thandiackal,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Behzad Bozorgtabar,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Maria Gabrani,,0%
https://arxiv.org/pdf/2301.02933.pdf,Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer,Orcun Goksel,,0%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Han Hui Lin,hanhlin@gene.com,95%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Hai Ngu,hain@gene.com,85%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Mohsen Hejrati,hejratis@gene.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Sarah Chu,chus18@gene.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Baris Bingol,barisb@gene.com,85%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Hosein Barzekar,barzekar.h@gmail.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Soumitra Ghosh,ghoshs29@gene.com,78%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Steven Ray Valdespino,,0%
https://arxiv.org/pdf/2301.02925.pdf,Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease,Somaye Hashemifar,,0%
https://arxiv.org/pdf/2301.02911.pdf,Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants,Marwa Mahmoud,marwa.mahmoud@glasgow.ac.uk,95%
https://arxiv.org/pdf/2301.02911.pdf,Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants,Bruno Tafur,,0%
https://arxiv.org/pdf/2301.02911.pdf,Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants,Staci Weiss,,0%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Byoungjip Kim,bjkim@lgresearch.ai,82%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Moontae Lee,moontae.lee@lgresearch.ai,95%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Dasol Hwang,dasol.hwang@lgresearch.ai,95%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Honglak Lee,honglak@lgresearch.ai,85%
https://arxiv.org/pdf/2301.02903.pdf,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,Sungik Choi,sungik.choi@lgresearch.ai,95%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Jiangying Qin,jy_qin@whu.edu.cn,82%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Hanqi Zhang,hqzhang@whu.edu.cn,82%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Ming Li,lisouming@whu.edu.cn,95%
https://arxiv.org/pdf/2301.02869.pdf,Deep Learning-Based UAV Aerial Triangulation without Image Control Points,Jiageng Zhong,zhongjiageng@whu.edu.cn,95%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Hui Yuan,huiyuan@sdu.edu.cn,95%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Pan Gao,Pan.Gao@nuaa.edu.cn,95%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Zihao Li,,0%
https://arxiv.org/pdf/2301.02836.pdf,Dynamic Local Feature Aggregation for Learning on Point Clouds,Ran Wei,,0%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Rob Brennan,rob.brennan@adaptcentre.ie,95%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Alessandra Mileo,alessandra.mileo@dcu.ie,95%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Teerath Kumar,teerath.menghwar2@mail.dcu.ie,85%
https://arxiv.org/pdf/2301.02830.pdf,Image Data Augmentation Approaches: A Comprehensive Survey and Future directions,Malika Bendechache,malika.bendechache@universityofgalway.ie,95%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Zongmin Liu,liu_zm@ctbu.edu.cn,78%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Jirui Wang,,0%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Jie Li,,0%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Pengda Liu,,0%
https://arxiv.org/pdf/2302.05293.pdf,A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes,Kai Ren,,0%
https://arxiv.org/pdf/2301.02789.pdf,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,Gangwei Xu,gwxu@hust.edu.cn,82%
https://arxiv.org/pdf/2301.02789.pdf,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,Xin Yang,xinyang2014@hust.edu.cn,95%
https://arxiv.org/pdf/2301.02789.pdf,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,Huan Zhou,huanzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Xinpeng Zhang,xzhang@shu.edu.cn,82%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Weisi Lin,wslin@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Zhi Liu,liuzhisjtu@163.com,95%
https://arxiv.org/pdf/2301.02778.pdf,Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment,Gongyang Li,ligongyang@shu.edu.cn,95%
https://arxiv.org/pdf/2301.02761.pdf,Active Learning Guided by Efficient Surrogate Learners,Kwang In Kim,2kimkin@postech.ac.kr,78%
https://arxiv.org/pdf/2301.02761.pdf,Active Learning Guided by Efficient Surrogate Learners,Yunpyo An,anyunpyo@unist.ac.kr,95%
https://arxiv.org/pdf/2301.02761.pdf,Active Learning Guided by Efficient Surrogate Learners,Suyeong Park,suyeong@unist.ac.kr,85%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,John S. Schreck,schreck@ucar.edu,78%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,Matthew Hayman,mhayman@ucar.edu,82%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,Gabrielle Gantos,,0%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,Aaron Bansemer,,0%
https://arxiv.org/pdf/2301.02757.pdf,Mimicking non-ideal instrument behavior for hologram processing using neural style translation,David John Gagne,,0%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Amirreza Babaahmadi,babaahmadi.amir@ut.ac.ir,78%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Sahar Khalafi,,0%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Masoud Shariatpanahi,,0%
https://arxiv.org/pdf/2301.02735.pdf,Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach,Moosa Ayati,,0%
https://arxiv.org/pdf/2301.02726.pdf,Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation,Hilmil Pradana,hilmi@nict.go.jp,90%
https://arxiv.org/pdf/2301.02726.pdf,Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation,Koji Zettsu,zettsu@nict.go.jp,78%
https://arxiv.org/pdf/2301.02726.pdf,Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation,Minh-son Dao,dao@nict.go.jp,78%
https://arxiv.org/pdf/2301.02703.pdf,RUPNet: Residual upsampling network for real-time polyp segmentation,Ulas Bagci,ulas.bagci@northwestern.edu,95%
https://arxiv.org/pdf/2301.02703.pdf,RUPNet: Residual upsampling network for real-time polyp segmentation,Debesh Jha,debesh.jha@northwestern.edu,95%
https://arxiv.org/pdf/2301.02703.pdf,RUPNet: Residual upsampling network for real-time polyp segmentation,Nikhil Kumar Tomar,nikhilroxtomar@gmail.com,95%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Rameen Abdal,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Hsin-ying Lee,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Peihao Zhu,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Menglei Chai,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Aliaksandr Siarohin,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Peter Wonka,,0%
https://arxiv.org/pdf/2301.02700.pdf,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,Sergey Tulyakov,,0%
https://arxiv.org/pdf/2301.02693.pdf,Design of Arabic Sign Language Recognition Model,Muhammad Al-barham,,0%
https://arxiv.org/pdf/2301.02693.pdf,Design of Arabic Sign Language Recognition Model,Ahmad Jamal,,0%
https://arxiv.org/pdf/2301.02693.pdf,Design of Arabic Sign Language Recognition Model,Musa Al-yaman,,0%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Bastian Leibe,leibe@vision.rwth-aachen.de,78%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Jonathon Luiten,luiten@vision.rwth-aachen.de,78%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Alexander Hermans,hermans@vision.rwth-aachen.de,78%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Deva Ramanan,deva@cs.cmu.edu,85%
https://arxiv.org/pdf/2301.02657.pdf,TarViS: A Unified Approach for Target-based Video Segmentation,Ali Athar,athar@vision.rwth-aachen.de,82%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Roberto Martín-martín,robertomm@cs.utexas.edu,85%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Le Xue,jniebles@cs.stanford.edu,85%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Manli Shu,manli.shu@salesforce.com,95%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Caiming Xiong,cxiong@salesforce.com,82%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Tom Goldstein,tomg@umd.edu,85%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Ran Xu,ran.xu@salesforce.com,95%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Ning Yu,ning.yu@salesforce.com,95%
https://arxiv.org/pdf/2301.02650.pdf,Hierarchical Point Attention for Indoor 3D Object Detection,Juan Carlos Niebles,,0%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Hjalmar Kühl,hjalmar.kuehl@idiv.de,85%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Tilo Burghardt,tilo@cs.bris.ac.uk,85%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Otto Brookes,otto.brookes@bristol.ac.uk,95%
https://arxiv.org/pdf/2301.02642.pdf,Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,Majid Mirmehdi,majid@cs.bris.ac.uk,85%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Pedro C. Neto,2pedro.d.carneiro@inesctec.pt,85%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Sara P. Oliveira,4s.oliveira@nki.nl,78%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Diana Montezuma,3diana.felizardo@impdiagnostics.com,85%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Domingos Oliveira,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,João Fraga,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Ana Monteiro,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,João Monteiro,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Liliana Ribeiro,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Sofia Gonçalves,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Stefan Reinhard,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Inti Zlobec,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Isabel M. Pinto,,0%
https://arxiv.org/pdf/2301.02608.pdf,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,Jaime S. Cardoso,,0%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Xudong Lin,xudong.lin@columbia.edu,95%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Andrew Lu,,0%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Yulei Niu,,0%
https://arxiv.org/pdf/2301.03410.pdf,In Defense of Structural Symbolic Representation for Video Event-Relation Prediction,Shih-fu Chang,,0%
https://arxiv.org/pdf/2301.02524.pdf,Tackling Data Bias in Painting Classification with Style Transfer,Frederick W. B. Li,frederick.li@durham.ac.uk,95%
https://arxiv.org/pdf/2301.02524.pdf,Tackling Data Bias in Painting Classification with Style Transfer,Hubert P. H. Shum,hubert.shum@durham.ac.uk,95%
https://arxiv.org/pdf/2301.02524.pdf,Tackling Data Bias in Painting Classification with Style Transfer,Mridula Vijendran,mridula.vijendran@durham.ac.uk,95%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Konstantinos Vougioukas,k.vougioukas@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Maciej Zięba,maciej.zieba@pwr.edu.pl,95%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Michał Stypułkowski,michal.stypulkowski@cs.uni.wroc.pl,85%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Sen He,senhe752@gmail.com,95%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Maja Pantic,m.pantic@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.03396.pdf,Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,Stavros Petridis,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Sijin Chen,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Hongyuan Zhu,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Xin Chen,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Yinjie Lei,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Tao Chen,,0%
https://arxiv.org/pdf/2301.02508.pdf,End-to-End 3D Dense Captioning with Vote2Cap-DETR,Gang Yu,,0%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Xiaopeng Yang,xiaopengyang@bit.edu.cn,95%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Xiaodong Qu,xdqu@bit.edu.cn,82%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Tian Lan,tlan@bit.edu.cn,82%
https://arxiv.org/pdf/2301.02488.pdf,TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition,Weicheng Gao,,0%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Guangqi Jiang,jiang@cczu.edu.cn,78%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Huibing Wang,huibing.wang@dlmu.edu.cn,95%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Zetian Mi,tian@dlmu.edu.cn,90%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Mingze Yao,,0%
https://arxiv.org/pdf/2301.02484.pdf,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,Xianping Fu,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Benbakreti Samir,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Said Mwanahija,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Benbakreti Soumia,,0%
https://arxiv.org/pdf/2301.02468.pdf,Deep Learning For Classification Of Chest X-Ray Images (Covid 19),Umut Özkaya,,0%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Gabriele Graffieti,gabriele.graffieti@unibo.it,95%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Vincenzo Lomonaco,vincenzo.lomonaco@unipi.it,95%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Lorenzo Pellegrini,l.pellegrini@unibo.it,82%
https://arxiv.org/pdf/2301.02464.pdf,"Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning",Davide Maltoni,davide.maltoni@unibo.it,95%
https://arxiv.org/pdf/2301.02440.pdf,An Image captioning algorithm based on the Hybrid Deep Learning Technique (CNN+GRU),Hina Sattar,hinasattar987@gmail.com,95%
https://arxiv.org/pdf/2301.02440.pdf,An Image captioning algorithm based on the Hybrid Deep Learning Technique (CNN+GRU),Muhammad Azhar,Muhammad.azhar@chosun.ac.kr,95%
https://arxiv.org/pdf/2301.02440.pdf,An Image captioning algorithm based on the Hybrid Deep Learning Technique (CNN+GRU),Rana Adnan Ahmad,rana22293@gmail.com,85%
https://arxiv.org/pdf/2301.02437.pdf,Valid P-Value for Deep Learning-Driven Salient Region,Daiki Miwa,miwa.daiki.mllab.nit@gmail.com,95%
https://arxiv.org/pdf/2301.02437.pdf,Valid P-Value for Deep Learning-Driven Salient Region,Ichiro Takeuchi,ichiro.takeuchi@mae.nagoya-u.ac.jp,95%
https://arxiv.org/pdf/2301.02437.pdf,Valid P-Value for Deep Learning-Driven Salient Region,Vo Nguyen Le Duy,duy.mllab.nit@gmail.com,78%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Yabiao Wang,caseywang@tencent.com,78%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Xiangyang Xue,xiangyangxue@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Siqian Yang,seasonsyang@tencent.com,82%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Chengming Xu,cmxu18@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.02419.pdf,Exploring Efficient Few-shot Adaptation for Vision Transformers,Zhanxiong Wang,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Liu Liu,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Yukai Lin,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Xiao Liang,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Qichao Xu,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Miao Jia,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Yangdong Liu,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Yuxiang Wen,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Wei Luo,,0%
https://arxiv.org/pdf/2301.02403.pdf,CyberLoc: Towards Accurate Long-term Visual Localization,Jiangwei Li,,0%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Gangming Zhao,ming@bupt.edu.cn,90%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Xianpeng Wu,wxpzju123@163.com,65%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Chengwei Pan,pancw@buaa.edu.cn,78%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Fandong Zhang,zhangfandong@deepwise.com,95%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Kongming Liang,,0%
https://arxiv.org/pdf/2301.02393.pdf,Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation,Xinyang Hu,,0%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Tomoyuki Hiroyasu,tomo@is.doshisha.ac.jp,90%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Kyoka Yoshiok,,0%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Kensuke Tanioka,,0%
https://arxiv.org/pdf/2301.02390.pdf,Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset,Satoru Hiwa,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Tomoyuki Hiroyasu,tomo@is.doshisha.ac.jp,90%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Yusuke Nagira,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Yuzuha Hara,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Satoru Hiwa,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Naoki Okumura,,0%
https://arxiv.org/pdf/2301.02388.pdf,Generating corneal panoramic images from contact specular microscope images,Noriko Koizumi,,0%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Menghan Xia,menghanxyz@gmail.com,85%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Yuechen Zhang,yczhang21@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Tien-tsin Wong,ttwong@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Jinbo Xing,jbxing@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Xiaodong Cun,,0%
https://arxiv.org/pdf/2301.02379.pdf,CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,Jue Wang,,0%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Zehao Huang,zehaohuang18@gmail.com,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Si Liu,liusi@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Zi-han Ding,zihanding819@gmail.com,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Zhenwei Shen,shenzhenwei@outlook.com,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Jiao Dai,daijiao@iie.ac.cn,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Jizhong Han,hanjizhong@iie.ac.cn,95%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.02371.pdf,Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection,Shaofei Huang,nowherespyfly@gmail.com,60%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Zehao Huang,zehaohuang18@gmail.com,95%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Si Liu,liusi@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Jiahui Fu,jiahuifu@buaa.edu.cn,95%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Zitian Wang,wangzt.kghl@gmail.com,78%
https://arxiv.org/pdf/2301.02364.pdf,Object as Query: Lifting any 2D Object Detector to 3D Detection,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Hongteng Xu,hongtengxu@ruc.edu.cn,95%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Chuhao Jin,jinchuhao@ruc.edu.cn,95%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Ruihua Song,rsong@ruc.edu.cn,82%
https://arxiv.org/pdf/2301.02363.pdf,Text2Poster: Laying out Stylized Texts on Retrieved Images,Zhiwu Lu,,0%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Yifei Lou,yifei.lou@utdallas.edu,95%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Jack Xin,jxin@math.uci.edu,82%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Kevin Bui,kevinb3@uci.edu,85%
https://arxiv.org/pdf/2301.03393.pdf,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,Fredrick Park,fpark@whittier.edu,82%
https://arxiv.org/pdf/2301.02341.pdf,A survey on Organoid Image Analysis Platforms,Alireza Ranjbaran,Ranjbarana@cardiff.ac.uk,78%
https://arxiv.org/pdf/2301.02341.pdf,A survey on Organoid Image Analysis Platforms,Azadeh Nazemi,azadeh1972@gmail.com,85%
https://arxiv.org/pdf/2301.02317.pdf,Convolutional XGBoost (C-XGBOOST) Model for Brain Tumor Detection,Muyiwa Babayomi,,0%
https://arxiv.org/pdf/2301.02317.pdf,Convolutional XGBoost (C-XGBOOST) Model for Brain Tumor Detection,Oluwatosin Atinuke Olagbaju,,0%
https://arxiv.org/pdf/2301.02317.pdf,Convolutional XGBoost (C-XGBOOST) Model for Brain Tumor Detection,Abdulrasheed Adedolapo Kadiri,,0%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Sabine Süsstrunk,sabine.susstrunk@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Tong Zhang,tong.zhang@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Mathieu Salzmann,mathieu.salzmann@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Bahar Aydemir,bahar.aydemir@epfl.ch,95%
https://arxiv.org/pdf/2301.02315.pdf,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,Ludo Hoffstetter,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Kumar Ashutosh,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Rohit Girdhar,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Lorenzo Torresani,,0%
https://arxiv.org/pdf/2301.02311.pdf,HierVL: Learning Hierarchical Video-Language Embeddings,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Patrick Grady,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Jeremy A. Collins,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Chengcheng Tang,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Christopher D. Twigg,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Kunal Aneja,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,James Hays,,0%
https://arxiv.org/pdf/2301.02310.pdf,PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,Charles C. Kemp,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Kumar Ashutosh,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Rohit Girdhar,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Lorenzo Torresani,,0%
https://arxiv.org/pdf/2301.02307.pdf,What You Say Is What You Show: Visual Narration Detection in Instructional Videos,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Vishwanath Saragadam,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Daniel Lejeune,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Jasper Tan,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Guha Balakrishnan,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Ashok Veeraraghavan,,0%
https://arxiv.org/pdf/2301.05187.pdf,WIRE: Wavelet Implicit Neural Representations,Richard G. Baraniuk,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Filip Radenovic,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Abhimanyu Dubey,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Abhishek Kadian,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Todor Mihaylov,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Simon Vandenhende,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Yash Patel,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Yi Wen,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Vignesh Ramanathan,,0%
https://arxiv.org/pdf/2301.02280.pdf,"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",Dhruv Mahajan,,0%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Ivan Fung,work.ivanfung@gmail.com,95%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Meihua Zhou,mhzhou@wnmc.edu.cn,82%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Nan Wan,wannan@wnmc.edu.cn,95%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Li Yang,,0%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Keke Di,,0%
https://arxiv.org/pdf/2301.02277.pdf,LostNet: A smart way for lost and find,Tingting Wang,,0%
https://arxiv.org/pdf/2301.02268.pdf,Restarts subject to approximate sharpness: A parameter-free and optimal scheme for first-order methods,Matthew J. Colbrook,m.colbrook@damtp.cam.ac.uk,82%
https://arxiv.org/pdf/2301.02268.pdf,Restarts subject to approximate sharpness: A parameter-free and optimal scheme for first-order methods,Ben Adcock,,0%
https://arxiv.org/pdf/2301.02268.pdf,Restarts subject to approximate sharpness: A parameter-free and optimal scheme for first-order methods,Maksym Neyra-nesterenko,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Hu Xu,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Saining Xie,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Po-yao Huang,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Licheng Yu,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Russell Howes,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Gargi Ghosh,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Luke Zettlemoyer,,0%
https://arxiv.org/pdf/2301.02241.pdf,CiT: Curation in Training for Effective Vision-Language Data,Christoph Feichtenhofer,,0%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Amir Ghodrati,ghodrati@qti.qualcomm.com,78%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Shashanka Venkataramanan,shashanka.venkataramanan@inria.fr,95%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Yuki M. Asano,,0%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Fatih Porikli,,0%
https://arxiv.org/pdf/2301.02240.pdf,Skip-Attention: Improving Vision Transformers by Paying Less Attention,Amirhossein Habibian,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Yu-lun Liu,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Chen Gao,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Andreas Meuleman,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Hung-yu Tseng,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Ayush Saraf,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Changil Kim,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Yung-yu Chuang,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Johannes Kopf,,0%
https://arxiv.org/pdf/2301.02239.pdf,Robust Dynamic Radiance Fields,Jia-bin Huang,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Benjamin Attal,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Jia-bin Huang,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Christian Richardt,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Michael Zollhoefer,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Johannes Kopf,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Matthew O'toole,,0%
https://arxiv.org/pdf/2301.02238.pdf,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,Changil Kim,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Jasmine Collins,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Anqi Liang,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Jitendra Malik,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Hao Zhang,,0%
https://arxiv.org/pdf/2301.02232.pdf,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,Frédéric Devernay,,0%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Han Hu,hanhu@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Qi Dai,qid@microsoft.com,85%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Chen Li,t-chenli1@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Jia Ning,t-jianing@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Zigang Geng,t-ziganggeng@microsoft.com,95%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Zheng Zhang,zhez@microsoft.com,75%
https://arxiv.org/pdf/2301.02229.pdf,All in Tokens: Unifying Output Space of Visual Tasks via Soft Token,Kun He,,0%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Ya Zhang,ya zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Yanfeng Wang,wangyanfeng@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Weidi Xie,weidi@sjtu.edu.cn,85%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Chaoyi Wu,,0%
https://arxiv.org/pdf/2301.02228.pdf,MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,Xiaoman Zhang,,0%
https://arxiv.org/pdf/2301.02217.pdf,EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding,Shuhan Tan,shuhan@cs.utexas.edu,85%
https://arxiv.org/pdf/2301.02217.pdf,EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding,Kristen Grauman,grauman@cs.utexas.edu,78%
https://arxiv.org/pdf/2301.02217.pdf,EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding,Tushar Nagarajan,tushar.nagarajan@cs.utexas.edu,95%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Elijah Cole,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Suzanne Stathatos,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Björn Lütjens,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Tarun Sharma,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Justin Kay,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Jason Parham,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Benjamin Kellenberger,,0%
https://arxiv.org/pdf/2301.02211.pdf,Teaching Computer Vision for Ecology,Sara Beery,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Vikram V. Ramaswamy,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Sing Yu Lin,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Dora Zhao,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Aaron B. Adcock,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Laurens Van Der Maaten,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Deepti Ghadiyaram,,0%
https://arxiv.org/pdf/2301.02560.pdf,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,Olga Russakovsky,,0%
https://arxiv.org/pdf/2301.02160.pdf,ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions,Dongwon Lee,dongwon@psu.edu,85%
https://arxiv.org/pdf/2301.02160.pdf,ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions,Aashish Anantha Ramakrishnan,,0%
https://arxiv.org/pdf/2301.02160.pdf,ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions,Sharon X. Huang,,0%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Yuxue Yang,yangyuxue2023@ia.ac.cn,95%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Lue Fan,fanlue2019@ia.ac.cn,95%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Feng Wang,feng.wff@gmail.com,85%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Naiyan Wang,winsty@gmail.com,60%
https://arxiv.org/pdf/2301.02562.pdf,Super Sparse 3D Object Detection,Zhaoxiang Zhang,zhaoxiang.zhang@ia.ac.cn,95%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Usman Muhammad,usman.muhammad@aalto.fi,95%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Jorma Laaksonen,,0%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Djamila Romaissa Beddiar,,0%
https://arxiv.org/pdf/2301.02145.pdf,Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection,Mourad Oussalah,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Carsten T. Lüth,carsten.lueth@dkfz-heidelberg.de,85%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,David Zimmerer,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Gregor Koehler,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Paul F. Jaeger,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Fabian Isensee,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Jens Petersen,,0%
https://arxiv.org/pdf/2301.02126.pdf,CRADL: Contrastive Representations for Unsupervised Anomaly Detection and Localization,Klaus H. Maier-hein,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Martin Pernuš,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Clinton Fookes,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Vitomir Štruc,,0%
https://arxiv.org/pdf/2301.02110.pdf,FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion,Simon Dobrišek,,0%
https://arxiv.org/pdf/2301.02615.pdf,"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack",Tzvi Lederer,tzvil@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.02615.pdf,"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack",Gallil Maimon,gallil.maimon@mail.huji.ac.il,95%
https://arxiv.org/pdf/2301.02615.pdf,"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack",Lior Rokach,liorrk@post.bgu.ac.il,85%
https://arxiv.org/pdf/2301.02092.pdf,DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax,Sadra Safadoust,ssafadoust20@ku.edu.tr,82%
https://arxiv.org/pdf/2301.02092.pdf,DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax,Fatma Güney,fguney@ku.edu.tr,82%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Leonard Bruns,leonardb@kth.se,85%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Patric Jensfelt,patric@kth.se,85%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Fereidoon Zangeneh,,0%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Amit Dekel,,0%
https://arxiv.org/pdf/2301.02086.pdf,A Probabilistic Framework for Visual Localization in Ambiguous Scenes,Alessandro Pieropan,,0%
https://arxiv.org/pdf/2301.02074.pdf,Test of Time: Instilling Video-Language Models with a Sense of Time,Piyush Bagad,,0%
https://arxiv.org/pdf/2301.02074.pdf,Test of Time: Instilling Video-Language Models with a Sense of Time,Makarand Tapaswi,,0%
https://arxiv.org/pdf/2301.02074.pdf,Test of Time: Instilling Video-Language Models with a Sense of Time,Cees G. M. Snoek,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,Shixing Cao,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,Nicholas Konz,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,James Duncan,,0%
https://arxiv.org/pdf/2301.02069.pdf,Deep Learning for Breast MRI Style Transfer with Limited Training Data,Maciej A. Mazurowski,,0%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Ik-jae Lee,ikjae412@yuhs.ac,85%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Jun Won Kim,junwon@yuhs.ac,85%
https://arxiv.org/pdf/2301.02064.pdf,Single-round Self-supervised Distributed Learning using Vision Transformer,Sangjoon Park,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Ivan Bilić,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Filip Marić,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Ivan Marković,,0%
https://arxiv.org/pdf/2301.02051.pdf,A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image,Ivan Petrović,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Xiang Li,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Jinshan Pan,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Jinhui Tang,,0%
https://arxiv.org/pdf/2301.02031.pdf,DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution,Jiangxin Dong,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Nina Shvetsova,shvetsov@uni-frankfurt.de,90%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Felix Petersen,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Anna Kukleva,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Bernt Schiele,,0%
https://arxiv.org/pdf/2301.02009.pdf,Learning by Sorting: Self-supervised Learning with Group Ordering Constraints,Hilde Kuehne,,0%
https://arxiv.org/pdf/2301.02008.pdf,Expressive Speech-driven Facial Animation with controllable emotions,Yutong Chen,chenyt19@tsinghua.org.cn,78%
https://arxiv.org/pdf/2301.02008.pdf,Expressive Speech-driven Facial Animation with controllable emotions,Junhong Zhao,junhong.jennifer@gmail.com,85%
https://arxiv.org/pdf/2301.02008.pdf,Expressive Speech-driven Facial Animation with controllable emotions,Wei-qiang Zhang,wqzhang@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Shuailei Ma,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Yuefeng Wang,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Jiaqi Fan,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Ying Wei,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Thomas H. Li,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Hongli Liu,,0%
https://arxiv.org/pdf/2301.01970.pdf,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,Fanbing Lv,,0%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Ming Yang,myang@njnu.edu.cn,82%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Lei Yu,yulei@njnu.edu.cn,95%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Shengqi Huang,huangshengqi@njnu.edu.cn,95%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Wanqi Yang,yangwq@njnu.edu.cn,78%
https://arxiv.org/pdf/2301.01956.pdf,High-level semantic feature matters few-shot unsupervised domain adaptation,Lei Wang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Yu Zhang,yu@seu.edu.cn,85%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Zihua Wang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Xu Yang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Hanwang Zhang,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Haiyang Xu,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Ming Yan,,0%
https://arxiv.org/pdf/2301.01955.pdf,Adaptively Clustering Neighbor Elements for Image-Text Generation,Fei Huang,,0%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Yu Zhang,zhang yu@seu.edu.cn,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Hanwang Zhang,hanwangzhang@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Fei Huang,f.huang@alibaba-inc.com,82%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Songfang Huang,songfang.hsf@alibaba-inc.com,85%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Qinghao Ye,yeqinghao.yqh@alibaba-inc.com,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Xu Yang,xuyang palm@seu.edu.cn,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Zhangzikang Li,lizhangzikang@gmail.com,95%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Haiyang Xu,,0%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Chenliang Li,,0%
https://arxiv.org/pdf/2301.01953.pdf,Learning Trajectory-Word Alignments for Video-Language Tasks,Ming Yan,,0%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Caixia Yuan,yuancx@bupt.edu.cn,78%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Fulong Ye,fulong ye@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Zhuoxin Han,hanzhuoxin@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Xiaojie Wang,xjwang@bupt.edu.cn,82%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Yuxing Long,longyuxing@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Binyuan Hui,,0%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Yanyang Li,,0%
https://arxiv.org/pdf/2301.01949.pdf,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,Yongbin Li,,0%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,Brad Mcdanel,bmcdanel@fandm.edu,82%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,Surat Teerapittayanon,surat.tee@nanotec.or.th,85%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,Marcus Comiter,marcuscomiter@post.harvard.edu,95%
https://arxiv.org/pdf/2301.01947.pdf,StitchNet: Composing Neural Networks from Pre-Trained Fragments,H. T. Kung,kung@harvard.edu,78%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Li Liu,lliu@ee.cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Ang Li,,0%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Jiayi Han,,0%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Yongjian Zhao,,0%
https://arxiv.org/pdf/2301.01940.pdf,Enabling Augmented Segmentation and Registration in Ultrasound-Guided Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume,Keyu Li,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Dongdong Chen,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Yuxing Dai,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Lichi Zhang,,0%
https://arxiv.org/pdf/2301.01932.pdf,PA-GM: Position-Aware Learning of Embedding Networks for Deep Graph Matching,Zhihong Zhang,,0%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Hasti Shabani,ha_shabani@sbu.ac.ir,82%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Bahareh Morovati,,0%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Reza Lashgari,,0%
https://arxiv.org/pdf/2301.01931.pdf,Reduced Deep Convolutional Activation Features (R-DeCAF) in Histopathology Images to Improve the Classification Performance for Breast Cancer Diagnosis,Mojtaba Hajihasani,,0%
https://arxiv.org/pdf/2301.01928.pdf,Event Camera Data Pre-training,Liyuan Pan,liyuan.pan@bit.edu.cn,95%
https://arxiv.org/pdf/2301.01928.pdf,Event Camera Data Pre-training,Yan Yang,Yan.Yang@anu.edu.au,95%
https://arxiv.org/pdf/2301.01928.pdf,Event Camera Data Pre-training,Liu Liu,liuliu33@huawei.com,95%
https://arxiv.org/pdf/2301.01922.pdf,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,Andrew Beng Jin Teoh,bjteoh@yonsei.ac.kr,78%
https://arxiv.org/pdf/2301.01922.pdf,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,Jaewoo Park,julypraise@gmail.com,65%
https://arxiv.org/pdf/2301.01922.pdf,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,Hojin Park,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Ziwei Sun,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Zexi Hua,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Hengcao Li,,0%
https://arxiv.org/pdf/2301.01917.pdf,Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information,Haiyan Zhong,,0%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Michael Cahyadi,michael.cahyadi001@binus.ac.id,95%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,William Shan,william.sitanggang@binus.ac.id,85%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Jurike Moniaga,jurike@binus.edu,85%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Muhammad Rafi,muhammad.rafi007@binus.ac.id,95%
https://arxiv.org/pdf/2301.01914.pdf,Accuracy and Fidelity Comparison of Luna and DALL-E 2 Diffusion-Based Image Generation Systems,Henry Lucky,henry.lucky@binus.ac.id,95%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Yuqian Chen,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Fan Zhang,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Leo R. Zekelman,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Tengfei Xue,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Chaoyi Zhang,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Yang Song,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Nikos Makris,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Yogesh Rathi,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Weidong Cai,,0%
https://arxiv.org/pdf/2301.01911.pdf,TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography,Lauren J. O'donnell,,0%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Michael Johnston,mjohnstn@amazon.com,65%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Govind Thattai,thattg@amazon.com,76%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Da Yin,da.yin@cs.ucla.edu,95%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Feng Gao,fenggo@amazon.com,85%
https://arxiv.org/pdf/2301.01893.pdf,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,Kai-wei Chang,kwchang@cs.ucla.edu,82%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Haoyang Zhang,haoyang.zhang@horizon.ai,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Jian Jia,jiajian2018@ia.ac.cn,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Fei He,hefei2018@ia.ac.cn,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Kaiqi Huang,kaiqi.huang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Yanhu Shan,yanhu.shan@horizon.ai,95%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Xin Zhao,xzhao@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2301.01882.pdf,InsPro: Propagating Instance Query and Proposal for Online Video Instance Segmentation,Naiyu Gao,naiyu01.gao@horizon.ai,95%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Jian Liang,liangjian92@gmail.com,95%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Zhenan Sun,znsun@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Lingxiao He,helingxiao3@jd.com,95%
https://arxiv.org/pdf/2301.01879.pdf,Learning Feature Recovery Transformer for Occluded Person Re-identification,Boqiang Xu,boqiang.xu@cripac.ia.ac.cn,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Weining Lu,luwn@tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Yu Cheng,yu.cheng@microsoft.com,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Xiang Fang,xfang9508@gmail.com,82%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Xing Di,xing.di@protagolabs.com,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Pan Zhou,panzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2301.01871.pdf,Hypotheses Tree Building for One-Shot Temporal Sentence Localization,Daizong Liu,dzliu@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Zhecheng Wang,zhecheng@stanford.edu,85%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Timothy Dai,timdai@stanford.edu,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Andrew Y. Ng,ang@cs.stanford.edu,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Ram Rajagopal,ramr@stanford.edu,85%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Hesu Yoon,hyoon28@stanford.edu,82%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Tianyuan Huang,tianyuah@stanford.edu,75%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Hao Sheng,haosheng@stanford.edu,95%
https://arxiv.org/pdf/2301.01842.pdf,Detecting Neighborhood Gentrification at Scale via Street-level Visual Data,Jackelyn Hwang,jihwang@stanford.edu,82%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Abubakar Sani-mohammed,abubakar.sanimohammed@connect.polyu.hk,85%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Marco Heurich,marco.heurich@npv-bw.bayern.de,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Tsz Chung Wong,tszchung.wong@connect.polyu.hk,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Jinhong Wang,jinhong.wang@connect.polyu.hk,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Wei Yao,wei.hn.yao@polyu.edu.hk,95%
https://arxiv.org/pdf/2301.01841.pdf,Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery,Puzuo Wang,puzuo.wang@connect.polyu.hk,95%
https://arxiv.org/pdf/2301.01814.pdf,Living Images: A Recursive Approach to Computing the Structural Beauty of Images or the Livingness of Space,Chris De Rijke,chris.de.rijke@hig.se,95%
https://arxiv.org/pdf/2301.01814.pdf,Living Images: A Recursive Approach to Computing the Structural Beauty of Images or the Livingness of Space,Bin Jiang,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Tianjiao Ding,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Shengbang Tong,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Kwan Ho Ryan Chan,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Xili Dai,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Yi Ma,,0%
https://arxiv.org/pdf/2301.01805.pdf,Unsupervised Manifold Linearizing and Clustering,Benjamin D. Haeffele,,0%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Lingting Ge,lingting.ge@tusimple.ai,95%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Minghan Zhu,minghanz@umich.edu,85%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Panqu Wang,panqu.wang@tusimple.ai,95%
https://arxiv.org/pdf/2301.01802.pdf,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,Huei Peng,hpeng@umich.edu,82%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Vignesh Ramanathan,vigneshr@meta.com,85%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Anmol Kalia,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Vladan Petrovic,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Yi Wen,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Baixue Zheng,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Baishan Guo,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Rui Wang,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Aaron Marquez,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Rama Kovvuri,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Abhishek Kadian,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Amir Mousavi,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Yiwen Song,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Abhimanyu Dubey,,0%
https://arxiv.org/pdf/2301.01795.pdf,PACO: Parts and Attributes of Common Objects,Dhruv Mahajan,,0%
https://arxiv.org/pdf/2301.01791.pdf,Fully Automated Artery-Vein ratio and vascular tortuosity measurement in retinal fundus images,Rolando Estrada,restrada1@student.gsu.edu,82%
https://arxiv.org/pdf/2301.01791.pdf,Fully Automated Artery-Vein ratio and vascular tortuosity measurement in retinal fundus images,Aashis Khanal,akhanal1@student.gsu.edu,82%
https://arxiv.org/pdf/2301.01767.pdf,Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,Chao Feng,,0%
https://arxiv.org/pdf/2301.01767.pdf,Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,Ziyang Chen,,0%
https://arxiv.org/pdf/2301.01767.pdf,Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,Andrew Owens,,0%
https://arxiv.org/pdf/2301.01758.pdf,An Ensemble Mobile-Cloud Computing Method for Affordable and Accurate Glucometer Readout,Navidreza Asadi,navidreza.asadi@tum.de,95%
https://arxiv.org/pdf/2301.01758.pdf,An Ensemble Mobile-Cloud Computing Method for Affordable and Accurate Glucometer Readout,Maziar Goudarzi,goudarzi@sharif.edu,78%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Sagnik Majumder,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Hao Jiang,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Pierre Moulon,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Ethan Henderson,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Paul Calamia,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.02184.pdf,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,Vamsi Krishna Ithapu,,0%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Zhihan Lv,lvzhihan@gmail.com,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Jinman Kim,man.kim@sydney.edu.au,78%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Tao Huang,tao.huang1@jcu.edu.au,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Zhengmin Kong,zmkong@whu.edu.cn,82%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Yusheng Zhou,yushengzhou@whu.edu.cn,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Euijoon Ahn,euijoon.ahn@jcu.edu.au,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,David Dagan Feng,dagan.feng@sydney.edu.au,82%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Jianan Liu,jianan.liu@vitalent.se,95%
https://arxiv.org/pdf/2301.01732.pdf,Explicit Abnormality Extraction for Unsupervised Motion Artifact Reduction in Magnetic Resonance Imaging,Hao Li,hao.li@med.uni,95%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Ashkan Ebadi,ashkan.ebadi@nrc-cnrc.gc.ca,95%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Jessy Song,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Adrian Florea,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Pengcheng Xi,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Stéphane Tremblay,,0%
https://arxiv.org/pdf/2301.01679.pdf,COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images,Alexander Wong,,0%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Yao Zhao,yzhao@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Lang Nie,nielang@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Kang Liao,kang liao@bjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Chunyu Lin,cylin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2301.01661.pdf,RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning,Zishuo Zheng,,0%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Yuliang Liu,ylliu@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Chunhua Shen,chhshen@gmail.com,82%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Lianwen Jin,eelwjin@scut.edu.cn,78%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Mingxin Huang,huangmingxin21@foxmail.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Dezhi Peng,pengdzscut@foxmail.com,78%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Xinyu Wang,xinyu.wang02@adelaide.edu.au,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Jiaxin Zhang,zhangjiaxin.zjx1995@bytedance.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Xiang Bai,xbai@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Can Huang,can.huang@bytedance.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Jingqun Tang,tangjingqun@bytedance.com,95%
https://arxiv.org/pdf/2301.01635.pdf,SPTS v2: Single-Point Scene Text Spotting,Dahua Lin,dhlin@ie.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Errui Ding,dingerrui@baidu.com,95%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Xiang Bai,xbai@hust.edu.cn,82%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Xiao Tan,tanxchong@gmail.com,78%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Zhe Liu,zheliu1994@hust.edu.cn,95%
https://arxiv.org/pdf/2301.01615.pdf,StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection,Xiaoqing Ye,,0%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Cécile Favre,cecile.favre@univ-lyon2.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Camille Noûs,camille.nous@cogitamus.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Nouria Harbi,nouria.harbi@univ-lyon2.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Jérôme Darmont,jerome.darmont@univ-lyon2.fr,95%
https://arxiv.org/pdf/2302.05289.pdf,Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning,Abderrazek Azri,a.azri@univ-lyon2.fr,82%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Marcel Koch,marcel.koch@eah-jena.de,95%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Sören Laue,laue@cs.uni-kl.de,78%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Joachim Giesen,joachim.giesen@uni-jena.de,95%
https://arxiv.org/pdf/2301.01583.pdf,Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption,Matthias Mitterreiter,matthias.mitterreiter@uni-jena.de,95%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Yutaro Shigeto,shigeto@stair.center,78%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Masashi Shimbo,shimbo@stair.center,78%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Yuya Yoshikawa,yoshikawa@stair.center,82%
https://arxiv.org/pdf/2301.01569.pdf,Learning Decorrelated Representations Efficiently Using Fast Fourier Transform,Akikazu Takeuchi,takeuchi@stair.center,78%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Binod Bhattarai,b.bhattarai@ucl.ac.uk,82%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Tae-kyun Kim,tk.kim@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Danail Stoyanov,danail.stoyanov@ucl.ac.uk,95%
https://arxiv.org/pdf/2301.01531.pdf,MoBYv2AL: Self-supervised Active Learning for Image Classification,Razvan Caramalau,r.caramalau18@imperial.ac.uk,82%
https://arxiv.org/pdf/2301.01520.pdf,Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy,Diego Marcos,diego.marcos@inria.fr,95%
https://arxiv.org/pdf/2301.01520.pdf,Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy,Dino Ienco,dino.ienco@inrae.fr,95%
https://arxiv.org/pdf/2301.01520.pdf,Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy,Cassio F. Dantas,,0%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Wojciech Niewolski,wojciech.niewolski@orange.com,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Jaroslaw Legierski,jaroslaw.legierski@orange.com,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Maria Ganzha,maria.ganzha@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Przemyslaw Ratuszek,przemyslaw.ratuszek@orange.com,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Piotr Sowinski,piotr.sowinski@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Marcin Paprzycki,marcin.paprzycki@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Kajetan Rachwal,kajetan.rachwal@ibspan.waw.pl,95%
https://arxiv.org/pdf/2301.01501.pdf,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,Zbigniew Kopertowski,zbigniew.kopertowski@orange.com,95%
https://arxiv.org/pdf/2301.01495.pdf,Beckman Defense,A. V. Subramanyam,subramanyam@iiitd.ac.in,78%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Ralf Dörner,ralf.doerner@hs-rm.de,85%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Christian Geiger,geiger@hs-duesseldorf.de,78%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Rene Ebertowski,rene.ebertowski@hs-duesseldorf.de,95%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Alexander Pech,alexander.pech@hs-duesseldorf.de,95%
https://arxiv.org/pdf/2301.01490.pdf,Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering,Philipp Ladwig,philipp.ladwig@hs-duesseldorf.de,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Jian Cao,caojian_heu@163.com,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Ye Li,liye@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Wei Huo,weihuo@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Zhuoyan Liu,liuzhuoyan@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Yunfeng Li,liyunfeng@hrbeu.edu.cn,95%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Bo Wang,,0%
https://arxiv.org/pdf/2301.01482.pdf,Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles,Yueming Li,,0%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Qi Dou,qdou@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Yuan Zhong,yzhong22@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Wenlong Deng,dwenlong@ece.ubc.ca,85%
https://arxiv.org/pdf/2301.01481.pdf,On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations,Xiaoxiao Li,xiaoxiao.li@ece.ubc.ca,95%
https://arxiv.org/pdf/2301.11813.pdf,Biomedical Image Reconstruction: A Survey,Samuel Cahyawijaya,scahyawijaya@connect.ust.hk,82%
https://arxiv.org/pdf/2301.01456.pdf,Audio-Visual Efficient Conformer for Robust Speech Recognition,Maxime Burchi,maxime.burchi@uni-wuerzburg.de,95%
https://arxiv.org/pdf/2301.01456.pdf,Audio-Visual Efficient Conformer for Robust Speech Recognition,Radu Timofte,radu.timofte@uni-wuerzburg.de,95%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Viktor Prasanna,prasanna@usc.edu,78%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Bingyi Zhang,bingyizh@usc.edu,85%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Rajgopal Kannan,rajgopal.kannan.civ@army.mil,95%
https://arxiv.org/pdf/2301.01454.pdf,"Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA",Carl Busart,carl.e.busart.civ@army.mil,95%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Marshall Burke,mburke@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Stefano Ermon,ermon@cs.stanford.edu,78%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Eun Jee Sung,ejsung@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Chenlin Meng,chenlin@cs.stanford.edu,85%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Sihang Chen,schen22@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Matthew Kolodner,mkolod@stanford.edu,90%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,David Lobell,dlobell@stanford.edu,82%
https://arxiv.org/pdf/2301.01449.pdf,Building Coverage Estimation with Low-resolution Remote Sensing Imagery,Enci Liu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Zhilin Zheng,zhilin.zheng95@gmail.com,95%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Chengwei Shao,cwshao@sina.com,82%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Yun Bian,bianyun2012@foxmail.com,95%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Xu Fang,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Jiawen Yao,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Mengmeng Zhu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Le Lu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Lingyun Huang,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Jing Xiao,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Yu Shi,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Hong Lu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Jianping Lu,,0%
https://arxiv.org/pdf/2301.01448.pdf,A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging,Ling Zhang,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Weiwei Wan,wan@sys.es.osaka-u.ac.jp,82%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Hao Chen,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Masaki Matsushita,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Takeyuki Kotaka,,0%
https://arxiv.org/pdf/2301.01441.pdf,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,Kensuke Harada,,0%
https://arxiv.org/pdf/2301.01431.pdf,Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers,Haojie Yu,yuhaojie02@meituan.com,95%
https://arxiv.org/pdf/2301.01431.pdf,Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers,Xiaoming Xu,xuxiaoming04@meituan.com,95%
https://arxiv.org/pdf/2301.01431.pdf,Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers,Kang Zhao,zhaokang@meituan.com,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Dennis Park,dennis.park@tri.global,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Jiaman Li,jiamanli@stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Huazhe Xu,huazhexu@stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Jiajun Wu,jiajunwu@cs.stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Sifan Ye,sifan.ye@cs.stanford.edu,95%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,Yixing Wang,,0%
https://arxiv.org/pdf/2301.01424.pdf,Scene Synthesis from Human Motion,C. Karen Liu,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Yuren Cong,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Martin Renqiang Min,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Li Erran Li,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Bodo Rosenhahn,,0%
https://arxiv.org/pdf/2301.01413.pdf,Attribute-Centric Compositional Text-to-Image Generation,Michael Ying Yang,,0%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Tianxu Qin,tianxuq@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Kaihui Zheng,kaihuiz@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Yuqing Ren,yuqingr@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.10295.pdf,Object Segmentation with Audio Context,Zixin Shen,zixins@andrew.cmu.edu,85%
https://arxiv.org/pdf/2301.01380.pdf,Ego-Only: Egocentric Action Detection without Exocentric Transferring,Huiyu Wang,,0%
https://arxiv.org/pdf/2301.01380.pdf,Ego-Only: Egocentric Action Detection without Exocentric Transferring,Mitesh Kumar Singh,,0%
https://arxiv.org/pdf/2301.01380.pdf,Ego-Only: Egocentric Action Detection without Exocentric Transferring,Lorenzo Torresani,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Pew-thian Yap,ptyap@med.unc.edu,82%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Xiaoyang Chen,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Jinjian Wu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Wenjiao Lyu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Yicheng Zou,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Kim-han Thung,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Siyuan Liu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Ye Wu,,0%
https://arxiv.org/pdf/2301.01369.pdf,Brain Tissue Segmentation Across the Human Lifespan via Supervised Contrastive Learning,Sahar Ahmad,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Daniel H. Pak,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Xiao Chen,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Eric Z. Chen,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Yikang Liu,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Terrence Chen,,0%
https://arxiv.org/pdf/2301.01355.pdf,Holistic Multi-Slice Framework for Dynamic Simultaneous Multi-Slice MRI Reconstruction,Shanhui Sun,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Firas Laakom,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Jenni Raitoharju,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Alexandros Iosifidis,,0%
https://arxiv.org/pdf/2301.01352.pdf,WLD-Reg: A Data-dependent Within-layer Diversity Regularizer,Moncef Gabbouj,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Shreyansh Daftry,Shreyansh.Daftry@jpl.nasa.gov,95%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Brian Coltin,brian.coltin@nasa.gov,95%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Zhanlin Chen,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Yang Cheng,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Scott Tepsuporn,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Ussama Naam,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Lanssie Mingyue Ma,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Shehryar Khattak,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Matthew Deans,,0%
https://arxiv.org/pdf/2301.01350.pdf,LunarNav: Crater-based Localization for Long-range Autonomous Lunar Rover Navigation,Larry Matthies,,0%
https://arxiv.org/pdf/2301.01343.pdf,Explainability and Robustness of Deep Visual Classification Models,Jindong Gu,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Sucheng Ren,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Fangyun Wei,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Zheng Zhang,,0%
https://arxiv.org/pdf/2301.01296.pdf,TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models,Han Hu,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Junjie Yan,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Yingfei Liu,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Jianjian Sun,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Fan Jia,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Shuailin Li,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Tiancai Wang,,0%
https://arxiv.org/pdf/2301.01283.pdf,Cross Modal Transformer: Towards Fast and Robust 3D Object Detection,Xiangyu Zhang,,0%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Junayed Mahmud,jmahmud@gmu.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,George Purnell,gwpurnell@email.wm.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Ali Yachnes,ayachnes@email.wm.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Zach H'doubler,pzhdoubler@email.wm.edu,65%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Carlos Bernal-cárdenas,carlosbe@microsoft.com,85%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Michele Tufano,michele.tufano@microsoft.com,95%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Kevin Moran,kpmoran@gmu.edu,82%
https://arxiv.org/pdf/2301.01224.pdf,An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation,Denys Poshyvanyk,denys@cs.wm.edu,85%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Boqi Chen,,0%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Kevin Thandiackal,,0%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Pushpak Pati,,0%
https://arxiv.org/pdf/2301.01211.pdf,Generative appearance replay for continual unsupervised domain adaptation,Orcun Goksel,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Qingyi Pan,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Ning Guo,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Letu Qingge,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Jingyi Zhang,,0%
https://arxiv.org/pdf/2301.01182.pdf,PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment,Pei Yang,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Charlie Hewitt,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Tadas Baltrušaitis,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Erroll Wood,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Lohit Petikam,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Louis Florentin,,0%
https://arxiv.org/pdf/2301.01161.pdf,Procedural Humans for Computer Vision,Hanz Cuevas Velasquez,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Yue Han,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Jiangning Zhang,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Yabiao Wang,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Yong Liu,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Lu Qi,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Xiangtai Li,,0%
https://arxiv.org/pdf/2301.01156.pdf,Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.01149.pdf,I2F: A Unified Image-to-Feature Approach for Domain Adaptive Semantic Segmentation,Xiangru Lin,xrlin2@cs.hku.hk,82%
https://arxiv.org/pdf/2301.01149.pdf,I2F: A Unified Image-to-Feature Approach for Domain Adaptive Semantic Segmentation,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.01149.pdf,I2F: A Unified Image-to-Feature Approach for Domain Adaptive Semantic Segmentation,Haoyu Ma,mahaoyu@connect.hku.hk,95%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Jiangning Zhang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Xiangtai Li,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Jian Li,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Liang Liu,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Zhucun Xue,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Boshen Zhang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Zhengkai Jiang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Tianxin Huang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Yabiao Wang,,0%
https://arxiv.org/pdf/2301.01146.pdf,Rethinking Mobile Block for Efficient Attention-based Models,Chengjie Wang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Shuhao Shi,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Kai Qiao,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Jian Chen,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Shuai Yang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Jie Yang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Baojie Song,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Linyuan Wang,,0%
https://arxiv.org/pdf/2301.01123.pdf,MGTAB: A Multi-Relational Graph-Based Twitter Account Detection Benchmark,Bin Yan,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Zhisheng Zhong,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Jiequan Cui,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Yibo Yang,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Xiaoyang Wu,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Xiaojuan Qi,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Xiangyu Zhang,,0%
https://arxiv.org/pdf/2301.01100.pdf,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,Jiaya Jia,,0%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Jianlong Zhou,Jianlong.Zhou@uts.edu.au,95%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Boyuan Zheng,Boyuan.Zheng-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Chunjie Liu,,0%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Yiqiao Li,,0%
https://arxiv.org/pdf/2301.01088.pdf,Explaining Imitation Learning through Frames,Fang Chen,,0%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Clément Jambon,clement.jambon@polytechnique.edu,95%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,George Drettakis,george.drettakis@inria.fr,95%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Gilles Rainer,gilles.rainer.enst@gmail.com,95%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Thomas Leimkühler,thomas.leimkuehler@mpi-inf.mpg.de,85%
https://arxiv.org/pdf/2301.01087.pdf,Neural Point Catacaustics for Novel-View Synthesis of Reflections,Georgios Kopanas,kopanas@inria.fr,78%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Changjie Fan,fanchangjie@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Zhipeng Hu,zphu@corp.netease.com,82%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Xin Yu,xin.yu@uts.edu.au,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Tangjie Lv,hzlvtangjie@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Yifeng Ma,mayf18@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Suzhen Wang,wangsuzhen@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Yu Ding,dingyu01@corp.netease.com,95%
https://arxiv.org/pdf/2301.01081.pdf,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,Zhidong Deng,,0%
https://arxiv.org/pdf/2301.01079.pdf,Fine-Grained Hard Negative Mining: Generalizing Mitosis Detection with a Fifth of the MIDOG 2022 Dataset,Maxime W. Lafarge,,0%
https://arxiv.org/pdf/2301.01079.pdf,Fine-Grained Hard Negative Mining: Generalizing Mitosis Detection with a Fifth of the MIDOG 2022 Dataset,Viktor H. Koelzer,,0%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Liqun Lin,lin liqun@fzu.edu.cn,95%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Chengdong Lan,lancd@fzu.edu.cn,78%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Tiesong Zhao,t.zhao@fzu.edu.cn,82%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Weiling Chen,weiling.chen@fzu.edu.cn,95%
https://arxiv.org/pdf/2301.01069.pdf,Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment,Yang Zheng,,0%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Yawei Luo,yaweiluo@zju.edu.cn,95%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Jun Xiao,junx@cs.zju.edu.cn,85%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Fei Gao,feig@zjut.edu.cn,85%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Feifei Shao,,0%
https://arxiv.org/pdf/2301.01060.pdf,Knowledge-guided Causal Intervention for Weakly-supervised Object Localization,Yi Yang,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Janne Mustaniemi,janne.mustaniemi@oulu.fi,95%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Juho Kannala,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Esa Rahtu,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Li Liu,,0%
https://arxiv.org/pdf/2301.01057.pdf,BS3D: Building-scale 3D Reconstruction from RGB-D Images,Janne Heikkilä,,0%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Tabea-clara Bucher,tabea.bucher@dkfz-heidelberg.de,95%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Titus J. Brinker,titus.brinker@nct-heidelberg.de,95%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Hendrik A. Mehrtens,,0%
https://arxiv.org/pdf/2301.01054.pdf,Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise,Alexander Kurz,,0%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Ligang Liu,lgliu@ustc.edu.cn,82%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Jue Wang,maxjwang@tencent.com,78%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Boyu Zhang,,0%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Hongliang Yuan,,0%
https://arxiv.org/pdf/2301.01036.pdf,High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction,Mingyan Zhu,,0%
https://arxiv.org/pdf/2301.01033.pdf,Dissecting Continual Learning a Structural and Data Analysis,Francesco Pelosin,,0%
https://arxiv.org/pdf/2302.05283.pdf,Deep Learning from Parametrically Generated Virtual Buildings for Real-World Object Recognition,Mohammad Alawadhi,,0%
https://arxiv.org/pdf/2302.05283.pdf,Deep Learning from Parametrically Generated Virtual Buildings for Real-World Object Recognition,Wei Yan,,0%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Sinan Kalkan,skalkan@metu.edu.tr,82%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Emre Akbas,eakbas@metu.edu.tr,82%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Kemal Oksuz,kemal.oksuz@metu.edu.tr,95%
https://arxiv.org/pdf/2301.01019.pdf,Correlation Loss: Enforcing Correlation between Classification and Localization,Fehmi Kahraman,fehmi.kahraman 01@metu.edu.tr,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Hui Wan,hwan@us.ibm.com,82%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Marina Danilevsky,mdanile@us.ibm.com,90%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Tejas Indulal Dhamecha,tdhamecha@microsoft.com,82%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Riyaz Bhat,riyaz.bhat@ibm.com,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Rudra Murthy,rmurthyv@in.ibm.com,82%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Siva Sankalp Patel,siva.sankalp.patel@ibm.com,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Danish Contractor,danish.contractor@ibm.com,95%
https://arxiv.org/pdf/2301.01015.pdf,Semi-Structured Object Sequence Encoders,Chulaka Gunasekara,chulaka.gunasekara@ibm.com,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Penghao Wu,wupenghao@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Li Chen,lichen@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Junchi Yan,yanjunchi@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Xiaosong Jia,jiaxiaosong@sjtu.edu.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Hongyang Li,lihongyang@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.01006.pdf,Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling,Yu Qiao,qiaoyu@pjlab.org.cn,95%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Yu-gang Jiang,ygj@fudan.edu.cn,90%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Meng Wang,eric.wangmeng@gmail.com,95%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Hanze Dong,hzdong15@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Leonid Sigal,lsigal@cs.ubc.ca,82%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Xiangyang Xue,xyxue@fudan.edu.cn,82%
https://arxiv.org/pdf/2301.00998.pdf,Vocabulary-informed Zero-shot and Open-set Learning,Xiaomei Wang,,0%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Yawen Huang,yawenhuang@tencent.com,95%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Yuexiang Li,vicyxli@tencent.com,78%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Nanjun He,nanjunhe@tencent.com,95%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Yefeng Zheng,yefengzheng@tencent.com,95%
https://arxiv.org/pdf/2301.00989.pdf,A New Perspective to Boost Vision Transformer for Medical Image Classification,Kai Ma,kylekma@tencent.com,82%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Bernard Ghanem,bernard.ghanem@kaust.edu.sa,95%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Fahad Albalawi,falbalawi@sdaia.gov.sa,82%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Hasan Abed Al Kader Hammoud,,0%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Shuming Liu,,0%
https://arxiv.org/pdf/2301.00986.pdf,"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",Mohammed Alkhrashi,,0%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Sirui Zhao,sirui@mail.ustc.edu.cn,85%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Xinglong Mao,maoxl@mail.ustc.edu.cn,78%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Enhong Chen,cheneh@ustc.edu.cn,78%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Hao Wang,wanghao3@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Yiming Zhang,ymzhang21@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Tong Xu,tongxu@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Huaying Tang,,0%
https://arxiv.org/pdf/2301.00985.pdf,DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition,Shifeng Liu,,0%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Hao Fang,fanghao2021@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Sergio Escalera,sergio@maia.ub.es,85%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Jun Wan,jun.wan@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Stan Z. Li,stan.zq.li@westlake.edu.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Ajian Liu,ajian.liu@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Chenxu Zhao,zhaochenxu@sailyond.com,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Xu Zhang,xuzhang0908@mail.bnu.edu.cn,95%
https://arxiv.org/pdf/2301.00975.pdf,Surveillance Face Anti-spoofing,Zhen Lei,zhen.lei@ia.ac.cn,95%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Tejas Karkera,tejkar10@gmail.com,75%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Muhammad Saqib,saqib.uet1@gmail.com,78%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Soumi Chattopadhyay,soumi61@gmail.com,85%
https://arxiv.org/pdf/2301.00973.pdf,Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review,Chandranath Adak,adak32@gmail.com,78%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Xu Yan,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Chaoda Zheng,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Ying Xue,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Zhen Li,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Shuguang Cui,,0%
https://arxiv.org/pdf/2301.00970.pdf,Benchmarking the Robustness of LiDAR Semantic Segmentation Models,Dengxin Dai,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Zhijing Yang,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Junyang Chen,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Yukai Shi,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Hao Li,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Tianshui Chen,,0%
https://arxiv.org/pdf/2301.00965.pdf,OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided Mixup,Liang Lin,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Xiangtai Li,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Shilin Xu,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Yibo Yang,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Haobo Yuan,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Guangliang Cheng,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Yunhai Tong,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Zhouchen Lin,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.00954.pdf,PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation,Dacheng Tao,,0%
https://arxiv.org/pdf/2301.00950.pdf,Class-Continuous Conditional Generative Neural Radiance Field,Minhyeok Lee,mlee@cau.ac.kr,82%
https://arxiv.org/pdf/2301.00950.pdf,Class-Continuous Conditional Generative Neural Radiance Field,Jiwook Kim,,0%
https://arxiv.org/pdf/2301.10293.pdf,A Fast Feature Point Matching Algorithm Based on IMU Sensor,Lu Cao,andrewcao95@pku.edu.cn,78%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Yang Li,yangli@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Yicong Li,,0%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Yang Tan,,0%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Jingyun Yang,,0%
https://arxiv.org/pdf/2301.00934.pdf,Finding the Most Transferable Tasks for Brain Image Segmentation,Xiao-ping Zhang,,0%
https://arxiv.org/pdf/2301.00896.pdf,Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos,Wei Xingxing,xxwei@buaa.edu.cn,85%
https://arxiv.org/pdf/2301.00896.pdf,Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos,Wang Songping,,0%
https://arxiv.org/pdf/2301.00896.pdf,Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos,Yan Huanqian,,0%
https://arxiv.org/pdf/2301.00897.pdf,Game of Intelligent Life,Chaytan Inman,chaytan@uw.edu,85%
https://arxiv.org/pdf/2301.00897.pdf,Game of Intelligent Life,Shaun Lee,shauncl8@uw.edu,85%
https://arxiv.org/pdf/2301.00897.pdf,Game of Intelligent Life,Marlene Grieskamp,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Sanghyun Woo,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Shoubhik Debnath,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Ronghang Hu,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Xinlei Chen,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Zhuang Liu,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,In So Kweon,,0%
https://arxiv.org/pdf/2301.00808.pdf,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,Saining Xie,,0%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Chen Change Loy,ccloy@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Xiangtai Li,xiangtai.li@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Jianzong Wu,jzwu@stu.pku.edu.cn,82%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Henghui Ding,henghui.ding@ntu.edu.sg,95%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Xia Li,,0%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Guangliang Cheng,,0%
https://arxiv.org/pdf/2301.00805.pdf,Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation,Yunhai Tong,,0%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Harpreet Sawhney,harpreet.sawhney@microsoft.com,95%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Rama Chellappa,rchella4@jhu.edu,65%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Benjamin Lundell,benjamin.lundell@microsoft.com,95%
https://arxiv.org/pdf/2301.00794.pdf,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,Anshul Shah,ashah95@jhu.edu,82%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Zongwei Zhou,zzhou82@jh.edu,82%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yucheng Tang,yuchengt@nvidia.com,85%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Jie Liu,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yixiao Zhang,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Jie-neng Chen,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Junfei Xiao,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yongyi Lu,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Bennett A. Landman,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Yixuan Yuan,,0%
https://arxiv.org/pdf/2301.00785.pdf,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,Alan Yuille,,0%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Chaoqi Chen,cqchen1994@gmail.com,82%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Sibei Yang,yangsb@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Hong-yu Zhou,whuzhouhongyu@gmail.com,95%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Chixiang Lu,luchixiang@gmail.com,95%
https://arxiv.org/pdf/2301.00772.pdf,PCRLv2: A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis,Yizhou Yu,yizhouy@acm.org,85%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Seol Ah Park,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Tamara Sipka,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Zuzana Kriva,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,George Lutfalla,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Mai Nguyen-chi,,0%
https://arxiv.org/pdf/2301.00765.pdf,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,Karol Mikula,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Takayuki Nishio,nishio@ict.e.titech.ac.jp,78%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Shoki Ohta,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Riichi Kudo,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Kahoko Takahashi,,0%
https://arxiv.org/pdf/2301.00752.pdf,Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications,Hisashi Nagata,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Luca Casini,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Valentina Orrù,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Andrea Montanucci,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Nicolò Marchetti,,0%
https://arxiv.org/pdf/2302.05286.pdf,Archaeological Sites Detection with a Human-AI Collaboration Workflow,Marco Roccetti,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Sumit Shekhar,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Max Reimann,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Moritz Hilscher,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Amir Semmo,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Jürgen Döllner,,0%
https://arxiv.org/pdf/2301.00750.pdf,Interactive Control over Temporal Consistency while Stylizing Video Streams,Matthias Trapp,,0%
https://arxiv.org/pdf/2301.00746.pdf,NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,Santhosh Kumar Ramakrishnan,,0%
https://arxiv.org/pdf/2301.00746.pdf,NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,Ziad Al-halah,,0%
https://arxiv.org/pdf/2301.00746.pdf,NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,Kristen Grauman,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Shuangmei Wang,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Rui Ma,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Tieru Wu,,0%
https://arxiv.org/pdf/2301.00740.pdf,P3DC-Shot: Prior-Driven Discrete Data Calibration for Nearest-Neighbor Few-Shot Classification,Yang Cao,,0%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Hao Chen,hao.chen@inria.fr,95%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Benoit Lagadec,benoit.lagadec@esifrance.net,95%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Francois Bremond,cois.bremond@inria.fr,78%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Yaohui Wang,yaohui.wang@inria.fr,95%
https://arxiv.org/pdf/2301.00725.pdf,Learning Invariance from Generated Variance for Unsupervised Person Re-identification,Antitza Dantcheva,antitza.dantcheva@inria.fr,95%
https://arxiv.org/pdf/2301.00714.pdf,Learning Road Scene-level Representations via Semantic Region Prediction,Alan Yuille,ayuille1@jhu.edu,82%
https://arxiv.org/pdf/2301.00714.pdf,Learning Road Scene-level Representations via Semantic Region Prediction,Zihao Xiao,zxiao10@jhu.edu,82%
https://arxiv.org/pdf/2301.00714.pdf,Learning Road Scene-level Representations via Semantic Region Prediction,Yi-ting Chen,ychen@cs.nycu.edu.tw,82%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Han Zhang,han@google.com,85%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Dilip Krishnan,dilipkay@google.com,85%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Huiwen Chang,huiwenchang@google.com,95%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Jarred Barber,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Aj Maschinot,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Jose Lezama,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Lu Jiang,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Ming-hsuan Yang,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Kevin Murphy,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,William T. Freeman,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Michael Rubinstein,,0%
https://arxiv.org/pdf/2301.00704.pdf,Muse: Text-To-Image Generation via Masked Generative Transformers,Yuanzhen Li,,0%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Dennis Eschweiler,dennis.eschweiler@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Ina Laube,ina.laube@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Johannes Stegmaier,johannes.stegmaier@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Rüveyda Yilmaz,rueveyda.yilmaz@lfb.rwth-aachen.de,82%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Rijo Roy,rijo.roy@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Matisse Baumann,matisse.baumann@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Abin Jose,abin.jose@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2301.10227.pdf,Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets,Daniel Brückner,,0%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Jie Sun,sunjie1979@qut.edu.cn,95%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Lijian Zhou,zhoulijian@qut.edu.cn,95%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Kun Zhao,,0%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Qian Gao,,0%
https://arxiv.org/pdf/2301.00622.pdf,Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images,Siyuan Hao,,0%
https://arxiv.org/pdf/2301.00620.pdf,Dynamically Modular and Sparse General Continual Learning,Elahe Arani,e.arani@tue.nl,82%
https://arxiv.org/pdf/2301.00620.pdf,Dynamically Modular and Sparse General Continual Learning,Arnav Varma,arnav.varma@navinfo.eu,95%
https://arxiv.org/pdf/2301.00620.pdf,Dynamically Modular and Sparse General Continual Learning,Bahram Zonooz,bahram.zonooz@gmail.com,95%
https://arxiv.org/pdf/2301.00618.pdf,An Event-based Algorithm for Simultaneous 6-DOF Camera Pose Tracking and Mapping,Masoud Dayani Najafabadi,,0%
https://arxiv.org/pdf/2301.00618.pdf,An Event-based Algorithm for Simultaneous 6-DOF Camera Pose Tracking and Mapping,Mohammad Reza Ahmadzadeh,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Kristian Muri Knausgård,kristianmk@ieee.org,85%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Ørjan Langøy Olsen,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Tonje Knutsen Sørdalen,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Morten Goodwin,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Ketil Malde,,0%
https://arxiv.org/pdf/2301.00596.pdf,A contrastive learning approach for individual re-identification in a wild fish population,Kim Tallaksen Halvorsen,,0%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Chiyu Zhang,1alienzhang19961005@gmail.com,78%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Jun Yang,yjun@sicnu.edu.cn,85%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Zaiyan Dai,3daizaiyan@stu.sicnu.edu.cn,95%
https://arxiv.org/pdf/2301.00592.pdf,Edge Enhanced Image Style Transfer via Transformers,Peng Cao,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Fan Zhang,cefzhang@ust.hk,78%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Arianna Salazar Miranda,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Fábio Duarte,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Lawrence Vale,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Gary Hack,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Min Chen,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Yu Liu,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Michael Batty,,0%
https://arxiv.org/pdf/2301.00580.pdf,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,Carlo Ratti,,0%
https://arxiv.org/pdf/2301.00555.pdf,Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning,Jisu Shin,,0%
https://arxiv.org/pdf/2301.00555.pdf,Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning,Seunghyun Shin,,0%
https://arxiv.org/pdf/2301.00555.pdf,Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning,Hae-gon Jeon,,0%
https://arxiv.org/pdf/2301.00545.pdf,Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels,Xinwei Sun,sunxinwei@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00545.pdf,Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00545.pdf,Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels,Yikai Wang,yikaiwang19@fudan.edu.cn,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Ruimao Zhang,ruimao.zhang@ieee.org,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Zhanglin Peng,zhanglin.peng@connect.hku.hk,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Ziyi Tang,tangziyi@cuhk.edu.cn,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Liang Lin,linliang@ieee.org,95%
https://arxiv.org/pdf/2301.00531.pdf,Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification,Jinrui Chen,,0%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Sebin Lee,seb.lee@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Woobin Im,iwbn@kaist.ac.kr,60%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Sung-eui Yoon,sungeui@kaist.ac.kr,85%
https://arxiv.org/pdf/2301.00527.pdf,Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data,Jumin Lee,jmlee@kaist.ac.kr,82%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Asma Ahmed Hashmi,asmah17@gmail.com,85%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Aigerim Zhumabayeva,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Nikita Kotelevskii,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Artem Agafonov,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Mohammad Yaqub,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Maxim Panov,,0%
https://arxiv.org/pdf/2301.00524.pdf,Learning Confident Classifiers in the Presence of Label Noise,Martin Takáč,,0%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Zichuan Xu,z.xu@dlut.edu.cn,82%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Wenzheng Xu,wenzheng.xu@scu.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Daizong Liu,dzliu@stu.pku.edu.cn,82%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Song Yang,S.Yang@bit.edu.cn,82%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Yao Wan,wanyao@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Jiahao Zhu,jiahaozhu@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Yu Cheng,yu.cheng@microsoft.com,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Xing Di,xing.di@protagolabs.com,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Zeyu Xiong,zeyuxiong@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Pan Zhou,panzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2301.00514.pdf,Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding,Lichao Sun,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Da Ma,dma@wakehealth.edu,82%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Marinko V. Sarunic,m.sarunic@ucl.ac.uk,82%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Timothy T. Yu,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Jayden Cole,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Myeong Jin Ju,,0%
https://arxiv.org/pdf/2301.00504.pdf,Spectral Bandwidth Recovery of Optical Coherence Tomography Images using Deep Learning,Mirza F. Beg,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Benjamin Wilson,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,William Qi,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Tanmay Agarwal,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,John Lambert,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Jagjeet Singh,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Siddhesh Khandelwal,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Bowen Pan,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Ratnesh Kumar,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Andrew Hartnett,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Jhony Kaesemodel Pontes,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Deva Ramanan,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,Peter Carr,,0%
https://arxiv.org/pdf/2301.00493.pdf,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,James Hays,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Qi Dou,qidou@cuhk.edu.hk,95%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Yonghao Long,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Wang Wei,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Tao Huang,,0%
https://arxiv.org/pdf/2301.00452.pdf,Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning,Yuehao Wang,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,James Batten,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,Matthew Sinclair,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,Ben Glocker,,0%
https://arxiv.org/pdf/2301.00447.pdf,Image To Tree with Recursive Prompting,Michiel Schaap,,0%
https://arxiv.org/pdf/2301.00436.pdf,Hierarchical Explanations for Video Action Recognition,Sadaf Gulshad,s.gulshad@uva.nl,82%
https://arxiv.org/pdf/2301.00436.pdf,Hierarchical Explanations for Video Action Recognition,Teng Long,t.long@uva.nl,82%
https://arxiv.org/pdf/2301.00436.pdf,Hierarchical Explanations for Video Action Recognition,Nanne Van Noord,n.j.e.vannoord@uva.nl,82%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Mingzhe Chen,mingzhe.chen@miami.edu,95%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Dusit Niyato,dniyato@ntu.edu.sg,82%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Wenjing Zhang,zhangwenjing@bupt.edu.cn,95%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Yining Wang,,0%
https://arxiv.org/pdf/2301.00433.pdf,Optimization of Image Transmission in a Cooperative Semantic Communication Networks,Tao Luo,,0%
https://arxiv.org/pdf/2301.00424.pdf,GoogLe2Net: Going Transverse with Convolutions,Yuanpeng He,,0%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Zheng Zhu,zhengzhu@ieee.org,95%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Wenbo Xu,wenbo.xu@phigent.ai,95%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Guan Huang,guan.huang@phigent.ai,95%
https://arxiv.org/pdf/2301.00411.pdf,Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields,Boyu Zhang,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Shizhan Gong,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Cheng Chen,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Yuqi Gong,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Nga Yan Chan,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Wenao Ma,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Calvin Hoi-kwan Mak,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Jill Abrigo,,0%
https://arxiv.org/pdf/2301.00409.pdf,Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification,Qi Dou,,0%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Feihu Xu,feihuxu@ustc.edu.cn,95%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Rui Ding,rding@tju.edu.cn,82%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Qifeng Gao,gaoqifeng 98@tju.edu.cn,95%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Yuping Duan,doveduan@gmail.com,78%
https://arxiv.org/pdf/2301.00406.pdf,Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data,Juntian Ye,,0%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Wenhe Jia,jiawh@bupt.edu.cn,78%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Lu Yang,,0%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Shan Li,,0%
https://arxiv.org/pdf/2301.00394.pdf,Deep Learning Technique for Human Parsing: A Survey and Outlook,Qing Song,,0%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Jun Wen,wen@hms.harvard.edu,78%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Siheng Chen,sihengc@sjut.edu.cn,85%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Linchao Zhu,zhulinchao@zju.edu.cn,95%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Zenan Huang,,0%
https://arxiv.org/pdf/2301.00383.pdf,Discriminative Radial Domain Adaptation,Nenggan Zheng,,0%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Haibin Ling,hling@cs.stonybrook.edu,82%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Heng Fan,heng.fan@unt.edu,95%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Libo Zhang,libo@iscas.ac.cn,85%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Wenzhang Zhou,zhouwenzhang19@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2301.00371.pdf,Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment,Tiejian Luo,tjluo@ucas.ac.cn,82%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Mukesh Prasad,Mukesh.Prasad@uts.edu.au,95%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Kunal Chaturvedi,,0%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Ali Braytee,,0%
https://arxiv.org/pdf/2301.00366.pdf,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,Jun Li,,0%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yanbo Fan,fanyanbo0124@gmail.com,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Fei Yin,yinf20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yujiu Yang,yang.yujiu@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Baoyuan Wu,wubaoyuan@cuhk.edu.cn,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yong Zhang,zhangyong201303@gmail.com,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Jingyi Zhang,jingyi.zhang1995@gmail.com,95%
https://arxiv.org/pdf/2301.00364.pdf,Generalizable Black-Box Adversarial Attack with Meta Learning,Yan Feng,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Zhenong Jin,jinzn@umn.edu,78%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Leikun Yin,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Rahul Ghosh,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Chenxi Lin,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,David Hale,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Christoph Weigl,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,James Obarowski,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Junxiong Zhou,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Jessica Till,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Xiaowei Jia,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Troy Mao,,0%
https://arxiv.org/pdf/2301.00363.pdf,Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin,Vipin Kumar,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Huazhu Fu,hzfu@ieee.org,82%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Xuedong Yuan,yxdongdong@163.com,60%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Ke Zou,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Yidi Chen,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Ling Huang,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Xiaojing Shen,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Meng Wang,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Rick Siow Mong Goh,,0%
https://arxiv.org/pdf/2301.00349.pdf,Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty,Yong Liu,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Eva L. Dyer,evadyer@gatech.edu,95%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Lakshmi Sathidevi,lsathidevi3@gatech.edu,82%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Erik C. Johnson,erik.c.johnson@jhuapl.edu,95%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Jorge Quesada,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Ran Liu,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Nauman Ahad,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Joy M. Jackson,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Mehdi Azabou,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Jingyun Xiao,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Christopher Liding,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Matthew Jin,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,Carolina Urzay,,0%
https://arxiv.org/pdf/2301.00345.pdf,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,William Gray-roncal,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Fengbei Liu,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Yuanhong Chen,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Chong Wang,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Yu Tain,,0%
https://arxiv.org/pdf/2301.01143.pdf,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning,Gustavo Carneiro,,0%
https://arxiv.org/pdf/2301.00330.pdf,Efficient On-device Training via Gradient Filtering,Radu Marculescu,radum@utexas.edu,85%
https://arxiv.org/pdf/2301.00330.pdf,Efficient On-device Training via Gradient Filtering,Yuedong Yang,albertyoung@utexas.edu,75%
https://arxiv.org/pdf/2301.00330.pdf,Efficient On-device Training via Gradient Filtering,Guihong Li,,0%
https://arxiv.org/pdf/2301.00326.pdf,Yuille-Poggio's Flow and Global Minimizer of Polynomials through Convexification by Heat Evolution,Qiao Wang,qiaowang@seu.edu.cn,95%
https://arxiv.org/pdf/2301.00314.pdf,Causal Deep Learning,M. Alex O. Vasilescu,,0%
https://arxiv.org/pdf/2303.00138.pdf,Texture-Based Input Feature Selection for Action Recognition,Yalong Jiang,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Yichen Sheng,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Jianming Zhang,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Julien Philip,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Yannick Hold-geoffroy,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Xin Sun,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,He Zhang,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Lu Ling,,0%
https://arxiv.org/pdf/2303.00137.pdf,PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing,Bedrich Benes,,0%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Zhaolin Chen,zhaolin.chen@monash.edu,95%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Mevan Ekanayake,,0%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Kamlesh Pawar,,0%
https://arxiv.org/pdf/2303.00111.pdf,PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification,Gary Egan,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Kai Hendrik Wöhnert,Sven-Jannik.Woehnert@haw-hamburg.de,85%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Sven-jannik Wöhnert,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Eldar Almamedov,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Carsten Frank,,0%
https://arxiv.org/pdf/2303.00092.pdf,A study on the use of perceptual hashing to detect manipulation of embedded messages in images,Volker Skwarek,,0%
https://arxiv.org/pdf/2303.00086.pdf,Applying Plain Transformers to Real-World Point Clouds,Lanxiao Li,lanxiao.li@kit.edu,95%
https://arxiv.org/pdf/2303.00086.pdf,Applying Plain Transformers to Real-World Point Clouds,Michael Heizmann,michael.heizmann@kit.edu,95%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Decai Chen,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Haofei Lu,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Ingo Feldmann,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Oliver Schreer,,0%
https://arxiv.org/pdf/2303.00050.pdf,Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface,Peter Eisert,,0%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Hailin Jin,hljin@adobe.com,82%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Shaogang Gong,s.gong@qmul.ac.uk,82%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Yang Liu,yangliu@pku.edu.cn,95%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Jiabo Huang,jiabo.huang@qmul.ac.uk,95%
https://arxiv.org/pdf/2303.00040.pdf,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,Dezhao Luo,dezhao.luo@qmul.ac.uk,95%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Lior Yariv,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Peter Hedman,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Christian Reiser,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Dor Verbin,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Pratul P. Srinivasan,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Richard Szeliski,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Jonathan T. Barron,,0%
https://arxiv.org/pdf/2302.14859.pdf,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,Ben Mildenhall,,0%
https://arxiv.org/pdf/2302.14835.pdf,Novel Machine Learning Approach for Predicting Poverty using Temperature and Remote Sensing Data in Ethiopia,Om Shah,,0%
https://arxiv.org/pdf/2302.14835.pdf,Novel Machine Learning Approach for Predicting Poverty using Temperature and Remote Sensing Data in Ethiopia,Krti Tallam,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Meshia Cédric Oveneke,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Rucha Vaishampayan,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Deogratias Lukamba Nsadisa,,0%
https://arxiv.org/pdf/2302.14831.pdf,FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle,Jenny Ambukiyenyi Onya,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,Saurabh Saxena,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,Abhishek Kar,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,Mohammad Norouzi,,0%
https://arxiv.org/pdf/2302.14816.pdf,Monocular Depth Estimation using Diffusion Models,David J. Fleet,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Maryam Viqar,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Violeta Madjarova,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Vipul Baghel,,0%
https://arxiv.org/pdf/2302.14808.pdf,Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography,Elena Stoykova,,0%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Sajid Javed,sajid.javed@ku.ac.ae,95%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Majid Khonji,majid.khonji@ku.ac.ae,95%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Mohamed Nagy,mohamed.nagy@ieee.org,95%
https://arxiv.org/pdf/2302.14807.pdf,DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion,Jorge Dias,jorge.dias@ku.ac.ae,95%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Kit Mills Bransby,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Vincenzo Tufaro,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Murat Cap,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Greg Slabaugh,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Christos Bourantas,,0%
https://arxiv.org/pdf/2302.14795.pdf,3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks,Qianni Zhang,,0%
https://arxiv.org/pdf/2302.14794.pdf,Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,Marcel Worring,m.worring@uva.nl,82%
https://arxiv.org/pdf/2302.14794.pdf,Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,Ivona Najdenkoska,,0%
https://arxiv.org/pdf/2302.14794.pdf,Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,Xiantong Zhen,,0%
https://arxiv.org/pdf/2303.12948.pdf,FTSO: Effective NAS via First Topology Second Operator,Lei Chen,leichen@cse.ust.hk,95%
https://arxiv.org/pdf/2303.12948.pdf,FTSO: Effective NAS via First Topology Second Operator,Likang Wang,lwangcg@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14777.pdf,VQA with Cascade of Self- and Co-Attention Blocks,Ashish Anand,anand.ashish@iitg.ac.in,95%
https://arxiv.org/pdf/2302.14777.pdf,VQA with Cascade of Self- and Co-Attention Blocks,Prithwijit Guha,pguha@iitg.ac.in,82%
https://arxiv.org/pdf/2302.14777.pdf,VQA with Cascade of Self- and Co-Attention Blocks,Aakansha Mishra,,0%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Jilin Mei,meijilin@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Yu Hu,huyu@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Jianchao Tan,jianchaotan@kuaishou.com,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Zihao Sun,sunzihao18z@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Chengru Song,songchengru@kuaishou.com,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Shun Lu,lushun19s@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14772.pdf,PA&DA: Jointly Sampling PAth and DAta for Consistent NAS,Longxing Yang,yanglongxing20b@ict.ac.cn,95%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Wei Huang,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Zhiliang Peng,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Li Dong,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Furu Wei,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Jianbin Jiao,,0%
https://arxiv.org/pdf/2302.14771.pdf,Generic-to-Specific Distillation of Masked Autoencoders,Qixiang Ye,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Voncarlos Marcelo Araújo,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Sébastien Gambs,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Clément Chion,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Robert Michaud,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Léo Schneider,,0%
https://arxiv.org/pdf/2302.14769.pdf,Membership Inference Attack for Beluga Whales Discrimination,Hadrien Lautraite,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Laurence Lamant,sylvain.cussat-blanc@irit.fr,92%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Salvatore Valitutti,salvatore.valitutti@inserm.fr,95%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Kévin Cortacero,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Brienne Mckenzie,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Sabina Müller,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Roxana Khazen,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Fanny Lafouresse,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Gaëlle Corsaut,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Nathalie Van Acker,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,François-xavier Frenois,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Nicolas Meyer,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Béatrice Vergier,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Dennis G. Wilson,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Hervé Luga,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Oskar Staufer,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Michael L. Dustin,,0%
https://arxiv.org/pdf/2302.14762.pdf,Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis,Sylvain Cussat-blanc,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Ji Hou,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Xiaoliang Dai,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Zijian He,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Angela Dai,,0%
https://arxiv.org/pdf/2302.14746.pdf,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,Matthias Nießner,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Yunpeng Bai,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Cairong Wang,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Shuzhao Xie,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Chao Dong,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Chun Yuan,,0%
https://arxiv.org/pdf/2302.14736.pdf,TextIR: A Simple Framework for Text-based Editable Image Restoration,Zhi Wang,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Prasun Roy,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Saumik Bhattacharya,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Subhankar Ghosh,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Umapada Pal,,0%
https://arxiv.org/pdf/2302.14728.pdf,Semantically Consistent Person Image Generation,Michael Blumenstein,,0%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Ni Zhang,zhangni_nlc@nec.cn,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Pengyi Zhang,zhang_pengyi@nec.cn,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Peter Wonka,peter.wonka@kaust.edu.sa,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Jian Shi,jian.shi@kaust.edu.sa,95%
https://arxiv.org/pdf/2302.14696.pdf,Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection,Hakim Ghazzai,hakim.ghazzai@kaust.edu.sa,95%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Samyak Jain,samyakjain.cse18@itbhu.ac.in,95%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Sravanti Addepalli,sravantia@iisc.ac.in,85%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Pawan Sahu,,0%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,Priyam Dey,,0%
https://arxiv.org/pdf/2302.14685.pdf,DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks,R. Venkatesh Babu,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Bo Peng,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Jun Hu,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Jingtao Zhou,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Xuan Gao,,0%
https://arxiv.org/pdf/2302.14683.pdf,IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF,Juyong Zhang,,0%
https://arxiv.org/pdf/2302.14680.pdf,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,Holy Lovenia,hlovenia@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14680.pdf,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,Samuel Cahyawijaya,scahyawijaya@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14680.pdf,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,Pascale Fung,,0%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Shijian Lu,shijian.Lu@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Alex C. Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Wenhan Yang,yangwh@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Yap-peng Tan,eyptan@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Yi Yu,yuyi0010@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.14677.pdf,Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Yufei Wang,yufei001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Julius Pfrommer,julius.pfrommer@iosb.fraunhofer.de,95%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Jürgen Beyerer,juergen.beyerer@iosb.fraunhofer.de,82%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Junwei Zheng,junwei.zheng@kit.edu,95%
https://arxiv.org/pdf/2302.14673.pdf,Attention-based Point Cloud Edge Sampling,Chengzhi Wu,chengzhi.wu@kit.edu,95%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Jing Wu,jingwu6@illinois.edu,95%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Ruoyu Sun,sunruoyu@cuhk.edu.cn,95%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Yite Wang,yitew2@illinois.edu,85%
https://arxiv.org/pdf/2302.14670.pdf,Balanced Training for Sparse GANs,Naira Hovakimyan,nhovakim@illinois.edu,90%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Josua Sassen,,0%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Klaus Hildebrandt,,0%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Martin Rumpf,,0%
https://arxiv.org/pdf/2302.14665.pdf,Parametrizing Product Shape Manifolds by Composite Networks,Benedikt Wirth,,0%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Rahul Mazumder,rahulmaz@mit.edu,85%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Zhe Zhao,zhezhao@google.com,95%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Hussein Hazimeh,hazimeh@google.com,82%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Riade Benbaki,rbenbaki@mit.edu,82%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Xiang Meng,mengx@mit.edu,78%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Wenyu Chen,wenyu@mit.edu,85%
https://arxiv.org/pdf/2302.14623.pdf,Fast as CHITA: Neural Network Pruning with Combinatorial Optimization,Natalia Ponomareva,mareva@google.com,90%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Kailun Yang,kailun.yang@hnu.edu.cn,95%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Junwei Zheng,,0%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Jiaming Zhang,,0%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Kunyu Peng,,0%
https://arxiv.org/pdf/2302.14595.pdf,MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments,Rainer Stiefelhagen,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Shoudong Han,shoudonghan@hust.edu.cn,95%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Hao Ren,haoren2000@hust.edu.cn,95%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Huilin Ding,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Ziwen Zhang,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Hongwei Wang,,0%
https://arxiv.org/pdf/2302.14589.pdf,Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation,Faquan Wang,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Kai Zhai,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Qiang Nie,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Bo Ouyang,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Xiang Li,,0%
https://arxiv.org/pdf/2302.14581.pdf,HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation,Shanlin Yang,,0%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Deyu Meng,dymeng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Yawen Huang,yawenhuang@tencent.com,95%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Yuexiang Li,vicyxli@tencent.com,78%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Minghao Zhou,woshizhouminghao@stu.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Hong Wang,hazelhwang@tencent.com,82%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Qian Zhao,timmy.zhaoqian@mail.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14578.pdf,Interactive Segmentation as Gaussian Process Classification,Yefeng Zheng,yefengzheng@tencent.com,95%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Markus Eisenbach,markus.eisenbach@tu-ilmenau.de,95%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Jannik Lübberstedt,,0%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Dustin Aganian,,0%
https://arxiv.org/pdf/2302.14574.pdf,A Little Bit Attention Is All You Need for Person Re-Identification,Horst-michael Gross,,0%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Yu Zhu,yuzhu@nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Jinqiu Sun,sunjinqiu@nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Pei Wang,wangpei23@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Yanning Zhang,ynzhang@nwpu.edu.cn,82%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Axi Niu,,0%
https://arxiv.org/pdf/2302.14557.pdf,GRAN: Ghost Residual Attention Network for Single Image Super Resolution,Qingsen Yan,,0%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,Om Damani,damani@cse.iitb.ac.in,78%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,Rajiv Kumar,rajiv@cse.iitb.ac.in,85%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,Chintan Tundia,chintan@cse.iitb.ac.in,85%
https://arxiv.org/pdf/2302.14554.pdf,FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection,G. Sivakumar,siva@iitb.ac.in,90%
https://arxiv.org/pdf/2302.14533.pdf,DEff-GAN: Diverse Attribute Transfer for Few-Shot Image Synthesis,Rajiv Kumar,1rajiv@cse.iitb.ac.in,85%
https://arxiv.org/pdf/2302.14533.pdf,DEff-GAN: Diverse Attribute Transfer for Few-Shot Image Synthesis,G. Sivakumar,,0%
https://arxiv.org/pdf/2302.14522.pdf,AdaptiveShape: Solving Shape Variability for 3D Object Detection with Geometry Aware Anchor Distributions,Michael Walter,michael.walter5@zf.com,95%
https://arxiv.org/pdf/2302.14522.pdf,AdaptiveShape: Solving Shape Variability for 3D Object Detection with Geometry Aware Anchor Distributions,Benjamin Sick,benjamin.sick@zf.com,95%
https://arxiv.org/pdf/2302.14522.pdf,AdaptiveShape: Solving Shape Variability for 3D Object Detection with Geometry Aware Anchor Distributions,Jochen Abhau,jochen.abhau@zf.com,95%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Yong Liu,yongliu@iipc.zju.edu.cn,95%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Guowei Wan,wanguowei@baidu.com,95%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Lin Li,,0%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Wendong Ding,,0%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Yongkun Wen,,0%
https://arxiv.org/pdf/2302.14511.pdf,A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation,Yufei Liang,,0%
https://arxiv.org/pdf/2302.14503.pdf,Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?,Dongheui Lee,dongheui.lee@tuwien.ac.at,95%
https://arxiv.org/pdf/2302.14503.pdf,Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?,Esteve Valls Mascaro,esteve.valls.mascaro@tuwien.ac.at,95%
https://arxiv.org/pdf/2302.14503.pdf,Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?,Hyemin Ahn,hyemin.ahn@unist.ac.kr,95%
https://arxiv.org/pdf/2302.14490.pdf,Estimating Head Motion from MR-Images,Clemens Pollak,,0%
https://arxiv.org/pdf/2302.14490.pdf,Estimating Head Motion from MR-Images,David Kügler,,0%
https://arxiv.org/pdf/2302.14490.pdf,Estimating Head Motion from MR-Images,Martin Reuter,,0%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Abhishek Joshi,abhi.joshi@samsung.com,82%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Sathish Chalasani,sathish.c@samsung.com,85%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Kiran Nanjunda Iyer,kiran.iyer@samsung.com,95%
https://arxiv.org/pdf/2302.14487.pdf,Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer,Sudeep Kumar Sahoo,sudeep.sahoo@samsung.com,95%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Gianluca D'amico,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Mauro Marinoni,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Federico Nesti,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Giulio Rossolini,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Giorgio Buttazzo,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Salvatore Sabina,,0%
https://arxiv.org/pdf/2302.14486.pdf,TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation,Gianluigi Lauro,,0%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Shuo Wang,shawnwang.tech@gmail.com,82%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Tian-zhu Xiang,tianzhu.xiang19@gmail.com,95%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Huan Xiong,huan.xiong.math@gmail.com,95%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Jie Qin,qinjiebuaa@gmail.com,95%
https://arxiv.org/pdf/2302.14485.pdf,Memory-aided Contrastive Consensus Learning for Co-salient Object Detection,Peng Zheng,zhengpeng0108@gmail.com,95%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Sangwoo Mo,swmo@kaist.ac.kr,82%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Jong-chyi Su,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Chih-yao Ma,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Mido Assran,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Ishan Misra,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Licheng Yu,,0%
https://arxiv.org/pdf/2302.14483.pdf,RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data,Sean Bell,,0%
https://arxiv.org/pdf/2302.14475.pdf,Benchmarking Deepart Detection,Xiaopeng Hong,hongxiaopeng@ieee.org,95%
https://arxiv.org/pdf/2302.14475.pdf,Benchmarking Deepart Detection,Zhiwu Huang,zhiwu.huang@soton.ac.uk,95%
https://arxiv.org/pdf/2302.14475.pdf,Benchmarking Deepart Detection,Yabin Wang,iamwangyabin@stu.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14470.pdf,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,Aleksandra Franz,franzer@in.tum.de,78%
https://arxiv.org/pdf/2302.14470.pdf,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,Barbara Solenthaler,solenthaler@inf.ethz.ch,78%
https://arxiv.org/pdf/2302.14470.pdf,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,Nils Thuerey,nils.thuerey@tum.de,95%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Ričards Marcinkevičs,ricards.marcinkevics@inf.ethz.ch,95%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Patricia Reis Wolfertstetter,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Ugne Klimiene,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Kieran Chin-cheong,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Alyssia Paschke,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Julia Zerres,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Markus Denzinger,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,David Niederberger,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Sven Wellmann,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Ece Ozkan,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Christian Knorr,,0%
https://arxiv.org/pdf/2302.14460.pdf,Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis,Julia E. Vogt,,0%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Xingyu Zeng,zengxingyu@sensetime.com,95%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Kun Wang,wangkun@sensetime.com,95%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Shaobo Lin,linshaobo@sensetime.com,95%
https://arxiv.org/pdf/2302.14452.pdf,An Effective Crop-Paste Pipeline for Few-shot Object Detection,Rui Zhao,zhaorui@sensetime.com,95%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Lichao Wang,l.wang22@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Jiahao Huang,j.huang21@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14450.pdf,Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation,Xiaodan Xing,x.xing@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.14444.pdf,Learning to Estimate Two Dense Depths from LiDAR and Event Data,Julien Moreau,julien.moreau@hds.utc.fr,95%
https://arxiv.org/pdf/2302.14444.pdf,Learning to Estimate Two Dense Depths from LiDAR and Event Data,Vincent Brebion,vincent.brebion@hds.utc.fr,95%
https://arxiv.org/pdf/2302.14444.pdf,Learning to Estimate Two Dense Depths from LiDAR and Event Data,Franck Davoine,franck.davoine@hds.utc.fr,95%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Mingqiang Wei,mqwei@nuaa.edu.cn,82%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Pan Gao,pan.gao@nuaa.edu.cn,95%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Shanshan Li,markli@nuaa.edu.cn,78%
https://arxiv.org/pdf/2302.14435.pdf,ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer,Xiaoyang Tan,x.tan@nuaa.edu.cn,82%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Jianqiang Ren,jianqiang.rjq@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Mengyang Feng,mengyang.fmy@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Xuansong Xie,xingtong.xxs@taobao.com,60%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Biwen Lei,biwen.lbw@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14434.pdf,A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images,Miaomiao Cui,miaomiao.cmm@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Jinqiao Wang,jqwang@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Wei Li,liwei1@sensetime.com,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Rui Zhao,zhaorui@sensetime.com,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Ming Tang,tangm@nlpr.ia.ac.cn,78%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Zhaowen Li,zhaowen.li@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Zhiyang Chen,zhiyang.chen@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Chaoyang Zhao,chaoyang.zhao@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14431.pdf,Efficient Masked Autoencoders with Self-Consistency,Yousong Zhu,yousong.zhu@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Arindam Basu,arinbasu@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Chuanlin Lan,chuanlin.lan@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Ziyuan Yin,ziyuanyin3-c@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14430.pdf,Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain,Rosa H. M. Chan,rosachan@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Yu Zhang,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Junle Yu,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Xiaolin Huang,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Wenhui Zhou,,0%
https://arxiv.org/pdf/2302.14418.pdf,PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry,Ji Hou,,0%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Yanqing Liu,yanqing liu@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Zheng Zhu,zhengzhu@ieee.org,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Wei Jiang,jiangwei zju@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Jianyang Gu,gu jianyang@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Kai Wang,kai.wang@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2302.14416.pdf,DREAM: Efficient Dataset Distillation by Representative Matching,Yang You,youy@comp.nus.edu.sg,82%
https://arxiv.org/pdf/2302.14415.pdf,Mesh-SORT: Simple and effective location-wise tracker with lost management strategies,Zongtan Li,zongtanli2023@163.com,95%
https://arxiv.org/pdf/2302.14409.pdf,An Adaptive Method for Camera Attribution under Complex Radial Distortion Corrections,Andrea Montibeller,drea.montibeller@unitn.it,78%
https://arxiv.org/pdf/2302.14409.pdf,An Adaptive Method for Camera Attribution under Complex Radial Distortion Corrections,Fernando Pérez-gonzález,fperez@gts.uvigo.es,90%
https://arxiv.org/pdf/2302.14402.pdf,Neural Video Compression with Diverse Contexts,Yan Lu,yanlu@microsoft.com,95%
https://arxiv.org/pdf/2302.14402.pdf,Neural Video Compression with Diverse Contexts,Bin Li,libin@microsoft.com,95%
https://arxiv.org/pdf/2302.14402.pdf,Neural Video Compression with Diverse Contexts,Jiahao Li,li.jiahao@microsoft.com,95%
https://arxiv.org/pdf/2302.14390.pdf,Your time series is worth a binary image: machine vision assisted deep framework for time series forecasting,Xinqi Fan,xinqi.fan@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14390.pdf,Your time series is worth a binary image: machine vision assisted deep framework for time series forecasting,Zijun Zhang,zijzhang@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.14390.pdf,Your time series is worth a binary image: machine vision assisted deep framework for time series forecasting,Luoxiao Yang,luoxiyang2-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Alessandro Achille,aachille@amazon.com,82%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Matthew Trager,mttrager@amazon.com,82%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Pramuditha Perera,pramudi@amazon.com,90%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Parminder Bhatia,parmib@amazon.com,81%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Luca Zancato,zancato@amazon.it,78%
https://arxiv.org/pdf/2302.14383.pdf,Linear Spaces of Meanings: Compositional Structures in Vision-Language Models,Stefano Soatto,soattos@amazon.com,82%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Wonwoong Cho,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Hareesh Ravi,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Midhun Harikumar,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Vinh Khuc,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Krishna Kumar Singh,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Jingwan Lu,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,David I. Inouye,,0%
https://arxiv.org/pdf/2302.14368.pdf,Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods,Ajinkya Kale,,0%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Sicheng Xu,sichengxu@microsoft.com,95%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Chong Li,chol@microsoft.com,75%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Jiaolong Yang,jiaoyan@microsoft.com,65%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Zhiqi Li,zhiqilicg@gmail.com,95%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Baining Guo,bainguo@microsoft.com,82%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Xin Tong,xtong@microsoft.com,82%
https://arxiv.org/pdf/2302.14365.pdf,RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,Yizhong Zhang,yizzhan@microsoft.com,65%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Yi Lin,ylinax@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Jieqi Shi,jshias@connect.ust.hk,82%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Xiaoyang Lyu,xylyu@eee.hku.hk,82%
https://arxiv.org/pdf/2302.14363.pdf,Efficient Implicit Neural Reconstruction Using LiDAR,Dongyu Yan,,0%
https://arxiv.org/pdf/2302.14362.pdf,One-Shot Video Inpainting,Sangjin Lee,,0%
https://arxiv.org/pdf/2302.14362.pdf,One-Shot Video Inpainting,Suhwan Cho,,0%
https://arxiv.org/pdf/2302.14362.pdf,One-Shot Video Inpainting,Sangyoun Lee,,0%
https://arxiv.org/pdf/2302.14354.pdf,Deep Learning for Identifying Iran's Cultural Heritage Buildings in Need of Conservation Using Image Classification and Grad-CAM,Amir Albadvi,albadvi@modares.ac.ir,82%
https://arxiv.org/pdf/2302.14354.pdf,Deep Learning for Identifying Iran's Cultural Heritage Buildings in Need of Conservation Using Image Classification and Grad-CAM,Mahdi Bahrami,mahdi_bahrami@modares.ac.ir,95%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Xianglong Lang,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Zhuming Wang,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Zun Li,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Meng Tian,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Ge Shi,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Lifang Wu,,0%
https://arxiv.org/pdf/2302.14350.pdf,Knowledge Augmented Relation Inference for Group Activity Recognition,Liang Wang,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Jihyun Lee,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Minhyuk Sung,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Honggyu Choi,,0%
https://arxiv.org/pdf/2302.14348.pdf,Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes,Tae-kyun Kim,,0%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Zhihao Liang,eezhihaoliang@mail.scut.edu.cn,95%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Kui Jia,kuijia@scut.edu.cn,95%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Zhangjin Huang,eehuangzhangjin@mail.scut.edu.cn,95%
https://arxiv.org/pdf/2302.14340.pdf,HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization,Changxing Ding,chxding@scut.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Wenwen Yu,wenwenyu@hust.edu.cn,95%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Yuliang Liu,ylliu@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Wei Hua,whua hust@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Xiang Bai,xbai@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Deqiang Jiang,dqiangjiang@tencent.com,82%
https://arxiv.org/pdf/2302.14338.pdf,Turning a CLIP Model into a Scene Text Detector,Bo Ren,timren@tencent.com,78%
https://arxiv.org/pdf/2302.14337.pdf,UniFLG: Unified Facial Landmark Generator from Text or Speech,Kei Sawada,keisawada@rinna.co.jp,95%
https://arxiv.org/pdf/2302.14337.pdf,UniFLG: Unified Facial Landmark Generator from Text or Speech,Yukiya Hono,yuhono@rinna.co.jp,82%
https://arxiv.org/pdf/2302.14337.pdf,UniFLG: Unified Facial Landmark Generator from Text or Speech,Kentaro Mitsui,kemits@rinna.co.jp,65%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Jianan Zhao,zhaojianan.zjn@antgroup.com,95%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Furong Xu,booyoungxu.xfr@antgroup.com,78%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Yuan Cheng,yuan@fudan.edu.cn,85%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Wei Chu,weichu.cw@antgroup.com,95%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Ruobing Zheng,zhengruobing.zrb@antgroup.com,95%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Wen Li,yinian.lw@antgroup.com,75%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Cheng Zou,wuyou.zc@antgroup.com,65%
https://arxiv.org/pdf/2302.14335.pdf,DC-Former: Diverse and Compact Transformer for Person Re-Identification,Meng Wang,darren.wm@antgroup.com,75%
https://arxiv.org/pdf/2302.14332.pdf,Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer,Florian Richter,frichter@ucsd.edu,82%
https://arxiv.org/pdf/2302.14332.pdf,Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer,Michael C. Yip,yip@ucsd.edu,78%
https://arxiv.org/pdf/2302.14332.pdf,Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer,Jingpei Lu,,0%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Xuan Sun,sunxuan@boe.com.cn,95%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Ziyue Zhang,,0%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Guannan Chen,,0%
https://arxiv.org/pdf/2303.00520.pdf,Valid Information Guidance Network for Compressed Video Quality Enhancement,Dan Zhu,,0%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Huiliang Shen,shenhl@zju.edu.cn,78%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Yongzhi Fan,tony fan@zju.edu.cn,78%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Siyuan Cao,cao siyuan@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Beinan Yu,yubeinan@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Yixuan Li,yixuanli@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Lun Luo,luolun@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14325.pdf,BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images,Shuhang Zheng,zhengsh@zju.edu.cn,78%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Shaohui Liu,shliu@hit.edu.cn,82%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Yan Shu,,0%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Honglei Xu,,0%
https://arxiv.org/pdf/2302.14323.pdf,Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm,Feng Jiang,,0%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Alex C. Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Haoliang Li,haoliang.li@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Yap-peng Tan,eyptan@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Siyuan Yang,siyuan005@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Yufei Wang,yufei001@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2302.14309.pdf,Temporal Coherent Test-Time Optimization for Robust Video Classification,Chenyu Yi,yich0003@e.ntu.edu.sg,78%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Xiang Li,xiangli@dase.ecnu.edu.cn,95%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Ming Gao,mgao@dase.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Yunshi Lan,yslan@dase.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.14307.pdf,GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting,Kangyang Luo,,0%
https://arxiv.org/pdf/2302.14306.pdf,CLR-GAM: Contrastive Point Cloud Learning with Guided Augmentation and Feature Mapping,Srikanth Malla,srikanth.m@samsung.com,85%
https://arxiv.org/pdf/2302.14306.pdf,CLR-GAM: Contrastive Point Cloud Learning with Guided Augmentation and Feature Mapping,Yi-ting Chen,ychen@cs.nycu.edu.tw,82%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Hang Su,suhangss@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Chang Liu,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Wenzhao Xiang,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Yuan He,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Hui Xue,,0%
https://arxiv.org/pdf/2302.14302.pdf,Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain,Shibao Zheng,,0%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Hui Xue,hui.xueh@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Hang Su,suhangss@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Wenzhao Xiang,xiangwenzhao22@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Yinpeng Dong,dongyinpeng@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Yuan He,heyuan.hy@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Xiao Yang,yangxiao19@mails.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Yuefeng Chen,yuefeng.chenyf@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Chang Liu,,0%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Jun Zhu,,0%
https://arxiv.org/pdf/2302.14301.pdf,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,Shibao Zheng,,0%
https://arxiv.org/pdf/2302.14290.pdf,Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation,Gaurav Patel,gpatel10@purdue.edu,82%
https://arxiv.org/pdf/2302.14290.pdf,Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation,Konda Reddy Mopuri,krmopuri@ai.iith.ac.in,82%
https://arxiv.org/pdf/2302.14290.pdf,Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation,Qiang Qiu,qqiu@purdue.edu,82%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Zhengzhuo Xu,,0%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Shuo Yang,,0%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Xingjun Wang,,0%
https://arxiv.org/pdf/2302.14284.pdf,Rethink Long-tailed Recognition with Vision Transformers,Chun Yuan,,0%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Ting Ma,tma@hit.edu.cn,82%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Jiesi Hu,,0%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Yanwu Yang,,0%
https://arxiv.org/pdf/2302.14277.pdf,DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features,Xutao Guo,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Xueyi Liu,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Ji Zhang,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Ruizhen Hu,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Haibin Huang,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,He Wang,,0%
https://arxiv.org/pdf/2302.14268.pdf,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,Li Yi,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Jiyuan Liu,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Bingyi Lu,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Mingkang Xiong,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Tao Zhang,,0%
https://arxiv.org/pdf/2302.14267.pdf,Adversarial Attack with Raindrops,Huilin Xiong,,0%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Di Huang,dhuang@buaa.edu.cn,82%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Ran Qin,,0%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Haoxiang Ma,,0%
https://arxiv.org/pdf/2302.14264.pdf,RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention,Boyang Gao,,0%
https://arxiv.org/pdf/2302.14256.pdf,Remote Sensing Scene Classification with Masked Image Modeling (MIM),Liya Wang,,0%
https://arxiv.org/pdf/2302.14256.pdf,Remote Sensing Scene Classification with Masked Image Modeling (MIM),Alex Tien,,0%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Chaohui Yu,huakun.ych@alibaba-inc.com,65%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Zhibin Wang,zhibin.waz@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Qiang Zhou,jianchong.zq@alibaba-inc.com,75%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Jingliang Li,lijingliang20@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Fan Wang,fan.w@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.14250.pdf,Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation,Jianlong Yuan,,0%
https://arxiv.org/pdf/2302.14239.pdf,"Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images",Li Zhang,yxliu@casm.ac.cn,85%
https://arxiv.org/pdf/2302.14239.pdf,"Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images",Zhongli Fan,fanzhongli@whu.edu.cn,95%
https://arxiv.org/pdf/2302.14239.pdf,"Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images",Yuxuan Liu,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Kay Hutchinson,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Zongyu Li,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Ian Reyes,,0%
https://arxiv.org/pdf/2302.14237.pdf,Towards Surgical Context Inference and Translation to Gestures,Homa Alemzadeh,,0%
https://arxiv.org/pdf/2302.14217.pdf,Global Proxy-based Hard Mining for Visual Place Recognition,Philippe Giguère,philippe.giguere@ift.ulaval.ca,95%
https://arxiv.org/pdf/2302.14217.pdf,Global Proxy-based Hard Mining for Visual Place Recognition,Brahim Chaib-draa,brahim.chaib-draa@ift.ulaval.ca,95%
https://arxiv.org/pdf/2302.14217.pdf,Global Proxy-based Hard Mining for Visual Place Recognition,Amar Ali-bey,amar.ali-bey.1@ulaval.ca,95%
https://arxiv.org/pdf/2303.12700.pdf,ReorientDiff: Diffusion Model based Reorientation for Object Manipulation,Yongxin Chen,yongchen@gatech.edu,82%
https://arxiv.org/pdf/2303.12700.pdf,ReorientDiff: Diffusion Model based Reorientation for Object Manipulation,Utkarsh A. Mishra,umishra31@gatech.edu,82%
https://arxiv.org/pdf/2302.14197.pdf,Image-Based Virtual Try-on System With Clothing-Size Adjustment,Minoru Kuribayashi,,0%
https://arxiv.org/pdf/2302.14197.pdf,Image-Based Virtual Try-on System With Clothing-Size Adjustment,Koki Nakai,,0%
https://arxiv.org/pdf/2302.14197.pdf,Image-Based Virtual Try-on System With Clothing-Size Adjustment,Nobuo Funabiki,,0%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,Shan Liu,shanl@tencent.com,85%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,Jiahao Gu,jiahaogu@usc.edu,95%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,Pranav Kadam,pranavka@usc.edu,85%
https://arxiv.org/pdf/2302.14193.pdf,PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,C. -c. Jay Kuo,cckuo@sipi.usc.edu,82%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Xi Peng,pengx.gm@gmail.com,78%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Jianping Fan,jfan1@Lenovo.com,82%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Kui Ren,kuiren@zju.edu.cn,95%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Jun Yu,yujun@hdu.edu.cn,95%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Buyu Liu,buyu@nec-labs.com,85%
https://arxiv.org/pdf/2302.14166.pdf,GLOW: Global Layout Aware Attacks on Object Detection,Baojun,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Prashant Pandey,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Mustafa Chasmai,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Monish Natarajan,,0%
https://arxiv.org/pdf/2302.14163.pdf,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,Brejesh Lall,,0%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Xiyang Dai,xidai@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Dongdong Chen,dochen@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Ziyu Jiang,jiangziyu@tamu.edu,95%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Lu Yuan,luyuan@microsoft.com,95%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Yinpeng Chen,yiche@microsoft.com,65%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Zicheng Liu,zliu@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Mengchen Liu,mengcliu@microsoft.com,82%
https://arxiv.org/pdf/2302.14138.pdf,Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,Zhangyang Wang,atlaswang@utexas.edu,78%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Eun Som Jeon,,0%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Hongjun Choi,,0%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Ankita Shukla,,0%
https://arxiv.org/pdf/2302.14130.pdf,Leveraging Angular Distributions for Improved Knowledge Distillation,Pavan Turaga,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Tonmoy Hossain,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Zoraiz Qureshi,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Nivetha Jayakumar,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Thomas Eluvathingal Muttikkal,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Sohil Patel,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,David Schiff,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Miaomiao Zhang,,0%
https://arxiv.org/pdf/2302.14124.pdf,Multimodal Deep Learning to Differentiate Tumor Recurrence from Treatment Effect in Human Glioblastoma,Bijoy Kundu,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Antoine Yang,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Arsha Nagrani,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Paul Hongsuck Seo,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Antoine Miech,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Jordi Pont-tuset,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Ivan Laptev,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Josef Sivic,,0%
https://arxiv.org/pdf/2302.14115.pdf,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,Cordelia Schmid,,0%
https://arxiv.org/pdf/2302.14098.pdf,An Embedded and Real-Time Pupil Detection Pipeline,Ankur Raj,,0%
https://arxiv.org/pdf/2302.14098.pdf,An Embedded and Real-Time Pupil Detection Pipeline,Diwas Bhattarai,,0%
https://arxiv.org/pdf/2302.14098.pdf,An Embedded and Real-Time Pupil Detection Pipeline,Kristof Van Laerhoven,,0%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Hao Zhao,zhaohao@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Guyue Zhou,zhouguyue@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Yongliang Shi,Shiyongliang@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Ya-qin Zhang,zhangyaqin@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Jirui Yuan,yuanjirui@air.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Pengfei Li,li-pf22@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.14052.pdf,LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR,Ruowen Zhao,,0%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Alexander C. Li,derli@cmu.edu,78%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Ellis Brown,ellisbrown@cmu.edu,95%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Alexei A. Efros,,0%
https://arxiv.org/pdf/2302.14051.pdf,Internet Explorer: Targeted Representation Learning on the Open Web,Deepak Pathak,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Shaohan Huang,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Li Dong,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Wenhui Wang,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Yaru Hao,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Saksham Singhal,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Shuming Ma,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Tengchao Lv,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Lei Cui,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Owais Khan Mohammed,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Barun Patra,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Qiang Liu,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Kriti Aggarwal,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Zewen Chi,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Johan Bjorck,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Vishrav Chaudhary,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Subhojit Som,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Xia Song,,0%
https://arxiv.org/pdf/2302.14045.pdf,Language Is Not All You Need: Aligning Perception with Language Models,Furu Wei,,0%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Yanfeng Wang,wangyanfeng@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Weidi Xie,weidi@sjtu.edu.cn,85%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Ya Zhang,ya_zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Xiaoman Zhang,,0%
https://arxiv.org/pdf/2302.14042.pdf,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,Chaoyi Wu,,0%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Cedric Girerd,cedric.girerd@lirmm.fr,95%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Michael C. Yip,yip@ucsd.edu,78%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Fei Liu,f4liu@ucsd.edu,82%
https://arxiv.org/pdf/2302.14039.pdf,"Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",Jingpei Lu,,0%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Anthony Hoogs,anthony.hoogs@kitware.com,95%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Chen Zhao,chen.zhao@kitware.com,95%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Christopher Funk,christopher.funk@kitware.com,95%
https://arxiv.org/pdf/2303.12698.pdf,Open Set Action Recognition via Multi-Label Evidential Learning,Dawei Du,dawei.du@kitware.com,95%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Pheng-ann Heng,pheng@cse.cuhk.edu.hk,95%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Xianzhi Li,xzli@hust.edu.cn,82%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Ziyu Guo,zyguo@cse.cuhk.edu.hk,82%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Renrui Zhang,,0%
https://arxiv.org/pdf/2302.14007.pdf,Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training,Longtian Qiu,,0%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Wenwei Zhang,wenwei001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Wentao Liu,liuwentao@sensetime.com,95%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Size Wu,size001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Sheng Jin,jinsheng@sensetime.com,95%
https://arxiv.org/pdf/2302.13996.pdf,Aligning Bag of Regions for Open-Vocabulary Object Detection,Chen Change Loy,ccloy@ntu.edu.sg,82%
https://arxiv.org/pdf/2302.13991.pdf,Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays,Taufiq Hasan,taufiq@bme.buet.ac.bd,85%
https://arxiv.org/pdf/2302.13991.pdf,Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays,Md. Aynal Haque,aynal@eee.buet.ac.bd,90%
https://arxiv.org/pdf/2302.13991.pdf,Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays,Mohammad Zunaed,rafizunaed@gmail.com,78%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Zhenwei Zhu,garyzhu1996@gmail.com,78%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Liying Yang,yyliang@must.edu.mo,85%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Ning Li,,0%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Chaohao Jiang,,0%
https://arxiv.org/pdf/2302.13987.pdf,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,Yanyan Liang,,0%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Roberto Alcover-couso,roberto.alcover@uam.es,85%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Juan C. Sanmiguel,los.sanmiguel@uam.es,78%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Jose M. Martinez,josem.martinez@uam.es,95%
https://arxiv.org/pdf/2302.13961.pdf,Soft labelling for semantic segmentation: Bringing coherence to label down-sampling,Marcos Escudero-vinolo,marcos.escudero@uam.es,85%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Hao Cheng,h.cheng-2@utwente.nl,82%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Mengmeng Liu,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Lin Chen,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Hellward Broszio,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Jiangtao Li,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Runjiang Zhao,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Monika Sester,,0%
https://arxiv.org/pdf/2302.13933.pdf,LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints,Michael Ying Yang,,0%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,Robert Platt,r.platt@northeastern.edu,82%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,Ondrej Biza,biza.o@northeastern.edu,78%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,David M. Klee,klee.d@northeastern.edu,78%
https://arxiv.org/pdf/2302.13926.pdf,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,Robin Walters,r.walters@northeastern.edu,82%
https://arxiv.org/pdf/2302.13902.pdf,Language identification as improvement for lip-based biometric visual systems,Lucia Cascone,,0%
https://arxiv.org/pdf/2302.13902.pdf,Language identification as improvement for lip-based biometric visual systems,Michele Nappi,,0%
https://arxiv.org/pdf/2302.13902.pdf,Language identification as improvement for lip-based biometric visual systems,Fabio Narducci,,0%
https://arxiv.org/pdf/2302.13891.pdf,Supervised Virtual-to-Real Domain Adaptation for Object Detection Task using YOLO,Bayu Rahayudi,ubay1@ub.ac.id,60%
https://arxiv.org/pdf/2302.13891.pdf,Supervised Virtual-to-Real Domain Adaptation for Object Detection Task using YOLO,Akbar Satya Nugraha,personal.akbarsn@gmail.com,85%
https://arxiv.org/pdf/2302.13891.pdf,Supervised Virtual-to-Real Domain Adaptation for Object Detection Task using YOLO,Yudistira Novanto,yudistira@ub.ac.id,85%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Jiangang Chen,jgchen@cee.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Menghan Hu,mhhu@ce.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Yiman Liu,LiuyimanSCMC@163.com,95%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Qingli Li,qlli@cs.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Yuqi Zhang,changyuqi@hotmail.com,85%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Bin Dong,dongbin@scmc.com.cn,95%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Tongtong Liang,zhimaliang@163.com,78%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Xiaoxiang Han,,0%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Jiajun Yuan,,0%
https://arxiv.org/pdf/2302.13869.pdf,EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography,Qiaohong Liu,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Leonard Berrada,lberrada | bballe@deepmind.com,82%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Sahra Ghalebikesabi,sahra.ghalebikesabi@univ.ox.ac.uk,95%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Borja Balle,bballe@deepmind.com,82%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Sven Gowal,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Ira Ktena,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Robert Stanforth,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Jamie Hayes,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Soham De,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Samuel L. Smith,,0%
https://arxiv.org/pdf/2302.13861.pdf,Differentially Private Diffusion Models Generate Useful Synthetic Images,Olivia Wiles,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Yuxiang Wei,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Yabo Zhang,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Zhilong Ji,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Jinfeng Bai,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Lei Zhang,,0%
https://arxiv.org/pdf/2302.13848.pdf,ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation,Wangmeng Zuo,,0%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Canlong Zhang,clzhang@gxnu.edu.cn,82%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Kaijie He,hekaijie123@outlook.com,95%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Sheng Xie,,0%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Zhixin Li,,0%
https://arxiv.org/pdf/2302.13840.pdf,Target-Aware Tracking with Long-term Context Attention,Zhiwen Wang,,0%
https://arxiv.org/pdf/2302.13838.pdf,Cross-modal Face- and Voice-style Transfer,Naoya Takahashi,Naoya.Takahashi@sony.com,95%
https://arxiv.org/pdf/2302.13838.pdf,Cross-modal Face- and Voice-style Transfer,Yuki Mitsufuji,Yuhki.Mitsufuji@sony.com,82%
https://arxiv.org/pdf/2302.13838.pdf,Cross-modal Face- and Voice-style Transfer,Mayank K. Singh,Mayank.A.Singh@sony.com,95%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Shuang Li,shuangli@bit.edu.cn,95%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Chi Harold Liu,liuchi02@gmail.com,95%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Mixue Xie,mxxie@bit.edu.cn,82%
https://arxiv.org/pdf/2302.13824.pdf,Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,Rui Zhang,zhangrui20@bit.edu.cn,95%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Long Sun,,0%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Jiangxin Dong,,0%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Jinhui Tang,,0%
https://arxiv.org/pdf/2302.13800.pdf,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,Jinshan Pan,,0%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Marco Monforte,marco.monforte@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Arren Glover,arren.glover@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Massimiliano Iacono,massimiliano.iacono@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Chiara Bartolozzi,chiara.bartolozzi@iit.it,95%
https://arxiv.org/pdf/2302.13796.pdf,Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control,Luna Gava,luna.gava@iit.it,95%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Pengxiang Xiao,,0%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Shuai He,,0%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Limin Liu,,0%
https://arxiv.org/pdf/2302.13770.pdf,Mask Reference Image Quality Assessment,Anlong Ming,,0%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Bishan Wang,wangbs@whu.edu.cn,78%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Jianzhuang Liu,liu.jianzhuang@huawei.com,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Haijian Zhang,haijian.zhang@whu.edu.cn,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Wen Yang,yangwen@whu.edu.cn,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Xiang Zhang,xiangz@whu.edu.cn,85%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Gui-song Xia,guisong.xia@whu.edu.cn,95%
https://arxiv.org/pdf/2302.13766.pdf,Learning to Super-Resolve Blurry Images with Events,Lei Yu,,0%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Shibiao Xu,shibiaoxu@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Weiliang Meng,weiliang.meng@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Rongtao Xu,xurongtao2019@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Jiaxi Sun,sunjiaxi2020@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Xiaopeng Zhang,xiaopeng.zhang@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13765.pdf,Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation,Changwei Wang,wangchangwei2019@ia.ac.cn,95%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Xinyang Jiang,xinyangjiang@microsoft.com,95%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Jiaqi Gao,,0%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Yuqing Yang,,0%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Dongsheng Li,,0%
https://arxiv.org/pdf/2302.13748.pdf,Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism,Lili Qiu,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Maheshi Lokumarambage,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Vishnu Gowrisetty,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Hossein Rezaei,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Thushan Sivalingam,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Nandana Rajatheva,,0%
https://arxiv.org/pdf/2302.13721.pdf,Wireless End-to-End Image Transmission System using Semantic Communications,Anil Fernando,,0%
https://arxiv.org/pdf/2302.13700.pdf,Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech,Jiyoung Lee,,0%
https://arxiv.org/pdf/2302.13700.pdf,Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech,Joon Son Chung,,0%
https://arxiv.org/pdf/2302.13700.pdf,Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech,Soo-whan Chung,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Zhenghua Xu,zhenghua.xu@hebut.edu.cn,95%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Xiangtao Wang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Ruizhi Wang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Biao Tian,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Jiaojiao Zhang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Shuo Zhang,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Junyang Chen,,0%
https://arxiv.org/pdf/2302.13699.pdf,MPS-AMS: Masked Patches Selection and Adaptive Masking Strategy Based Self-Supervised Medical Image Segmentation,Thomas Lukasiewicz,,0%
https://arxiv.org/pdf/2302.13694.pdf,DLOFTBs -- Fast Tracking of Deformable Linear Objects with B-splines,Amadeusz Szymko,name.surname@put.poznan.pl,60%
https://arxiv.org/pdf/2302.13694.pdf,DLOFTBs -- Fast Tracking of Deformable Linear Objects with B-splines,Piotr Kicki,,0%
https://arxiv.org/pdf/2302.13694.pdf,DLOFTBs -- Fast Tracking of Deformable Linear Objects with B-splines,Krzysztof Walas,,0%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Shuicheng Yan,yansc@sea.com,78%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Angela Yao,ayao@comp.nus.edu.sg,82%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Tat-seng Chua,chuats@comp.nus.edu.sg,78%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Pan Zhou,zhoupan@sea.com,95%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Richang Hong,hongrc.hfut@gmail.com,78%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Junbin Xiao,junbin@comp.nus.edu.sg,85%
https://arxiv.org/pdf/2302.13668.pdf,Contrastive Video Question Answering via Video Graph Transformer,Yicong Li,liyicong@u.nus.edu,95%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Nikhil J. Dhinagar,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Conor Owens-walton,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Emily Laltoo,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Christina P. Boyle,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Yao-liang Chen,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Philip Cook,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Corey Mcmillan,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Chih-chien Tsai,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,J-j Wang,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Yih-ru Wu,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Ysbrand Van Der Werf,,0%
https://arxiv.org/pdf/2302.13631.pdf,Curriculum Based Multi-Task Learning for Parkinson's Disease Detection,Paul M. Thompson,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Rahim Entezari,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Mitchell Wortsman,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Olga Saukh,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,M. Moein Shariatnia,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Hanie Sedghi,,0%
https://arxiv.org/pdf/2302.13602.pdf,The Role of Pre-training Data in Transfer Learning,Ludwig Schmidt,,0%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Jianqi Ma,jianqi.ma@connect.polyu.hk,95%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Hongwei Yong,hongwei.yong@polyu.edu.hk,95%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Shi Guo,shiguo.guo@connect.polyu.hk,95%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Xindong Zhang,cslzhang@comp.polyu.edu.hk,78%
https://arxiv.org/pdf/2302.13598.pdf,Spatial-Frequency Attention for Image Denoising,Lei Zhang,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Wei Wang,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Xuejing Lei,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Yueru Chen,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,Ming-sui Lee,,0%
https://arxiv.org/pdf/2302.13596.pdf,LSR: A Light-Weight Super-Resolution Method,C. -c. Jay Kuo,,0%
https://arxiv.org/pdf/2302.13594.pdf,Leveraging Video Coding Knowledge for Deep Video Enhancement,Van-quang Nguyen,quang@vision.is.tohoku.ac.jp,85%
https://arxiv.org/pdf/2302.13594.pdf,Leveraging Video Coding Knowledge for Deep Video Enhancement,Thuong Nguyen Canh,ngcthuong@ids.osaka-u.ac.jp,85%
https://arxiv.org/pdf/2302.13594.pdf,Leveraging Video Coding Knowledge for Deep Video Enhancement,Thong Bach,thongbtqm@gmail.com,85%
https://arxiv.org/pdf/2302.13578.pdf,Online Black-Box Confidence Estimation of Deep Neural Networks,Georg Schneider,firstname.surname@zf.com,70%
https://arxiv.org/pdf/2302.13578.pdf,Online Black-Box Confidence Estimation of Deep Neural Networks,Fabian Woitschek,,0%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Xian Wei,xian.wei@tum.de,95%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Arafat Al-jawari,arafatmmj@gmail.com,85%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Xihao Wang,xihaowang2016@gmail.com,95%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Hai Lan,lanhai09@fjirsm.ac.cn,95%
https://arxiv.org/pdf/2302.13577.pdf,DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving,Jiaming Lei,,0%
https://arxiv.org/pdf/2302.13570.pdf,Physical Adversarial Attacks on Deep Neural Networks for Traffic Sign Recognition: A Feasibility Study,Georg Schneider,firstname.surname@zf.com,70%
https://arxiv.org/pdf/2302.13570.pdf,Physical Adversarial Attacks on Deep Neural Networks for Traffic Sign Recognition: A Feasibility Study,Fabian Woitschek,,0%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Jeong Eun Lee,leeje290@gmail.com,78%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Jaeyoung Huh,,0%
https://arxiv.org/pdf/2303.00091.pdf,Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model,Sangjoon Park,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Fabian Woitschek,fabian.woitschek@zf.com,95%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Arndt Von Twickel,arndt.twickel@bsi.bund.de,95%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Devi Padmavathi Alagarswamy,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Christian Berghoff,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Vasilios Danos,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Fabian Langer,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Thora Markert,,0%
https://arxiv.org/pdf/2302.13567.pdf,Towards Audit Requirements for AI-based Systems in Mobility Applications,Georg Schneider,,0%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Kibo Ote,kibou@crl.hpk.co.jp,85%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Fumio Hashimoto,fumio.hashimoto@crl.hpk.co.jp,95%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Masanobu Ibaraki,iba@akita-noken.jp,90%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Keisuke Matsubara,matsubara@akita-pu.ac.jp,78%
https://arxiv.org/pdf/2302.13546.pdf,Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image Denoising,Yuya Onishi,yuya.onishi@hpk.co.jp,95%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Sameera Ramasinghe,,0%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Violetta Shevchenko,,0%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Gil Avraham,,0%
https://arxiv.org/pdf/2302.13543.pdf,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,Anton Van Den Hengel,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Ruihang Miao,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Weizhou Liu,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Mingrui Chen,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Zheng Gong,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Weixin Xu,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Chen Hu,,0%
https://arxiv.org/pdf/2302.13540.pdf,OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion,Shuchang Zhou,,0%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Xiaofei Wang,wangxiaofei2022@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Shaohui Mei,meish@nwpu.edu.cn,78%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Jiawei Lian,lianjiawei@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Mingyang Ma,mamingyang@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.13519.pdf,CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World,Yuru Su,,0%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Chaohui Yu,huakun.ych@alibaba-inc.com,65%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Zhibin Wang,zhibin.waz@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Qiang Zhou,jianchong.zq@alibaba-inc.com,75%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Jingliang Li,lijingliang20@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Fan Wang,fan.w@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.13495.pdf,LMSeg: Language-guided Multi-dataset Segmentation,Yuang Liu,frankliu624@gmail.com,78%
https://arxiv.org/pdf/2303.08670.pdf,Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video,Yong Man Ro,ymro@kaist.ac.kr,82%
https://arxiv.org/pdf/2303.08670.pdf,Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video,Chae Won Kim,chaewonkim@kaist.ac.kr,95%
https://arxiv.org/pdf/2303.08670.pdf,Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video,Minsu Kim,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Shaohui Mei,meish@nwpu.edu.cn,78%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Jiawei Lian,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Xiaofei Wang,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Yuru Su,,0%
https://arxiv.org/pdf/2302.13487.pdf,Contextual adversarial attack against aerial detection in the physical world,Mingyang Ma,,0%
https://arxiv.org/pdf/2302.13434.pdf,Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition,Yifan Jiang,yfjiang@korea.ac.kr,82%
https://arxiv.org/pdf/2302.13434.pdf,Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition,Han Chen,jessicachan@korea.ac.kr,85%
https://arxiv.org/pdf/2302.13434.pdf,Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition,Hanseok Ko,hsko@korea.ac.kr,82%
https://arxiv.org/pdf/2302.13408.pdf,Generative Models for 3D Point Clouds,Lingjie Kong,ljkong@stanford.edu,82%
https://arxiv.org/pdf/2302.13408.pdf,Generative Models for 3D Point Clouds,Siamak Shakeri,siamaks@stanford.edu,85%
https://arxiv.org/pdf/2302.13408.pdf,Generative Models for 3D Point Clouds,Pankaj Rajak,prajak7@stanford.edu,82%
https://arxiv.org/pdf/2302.13392.pdf,NSANet: Noise Seeking Attention Network,Maryam Jameela,,0%
https://arxiv.org/pdf/2302.13392.pdf,NSANet: Noise Seeking Attention Network,Gunho Sohn,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Catarina Moreira,catarina.pintomoreira@data61.csiro.au,95%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Chihcheng Hsieh,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Isabel Blanco Nobre,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Sandra Costa Sousa,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Chun Ouyang,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Margot Brereton,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Jacinto C. Nascimento,,0%
https://arxiv.org/pdf/2302.13390.pdf,MDF-Net for abnormality detection by fusing X-rays with clinical data,Joaquim Jorge,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Linghao Chen,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Yunzhou Song,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Hujun Bao,,0%
https://arxiv.org/pdf/2302.13375.pdf,Perceiving Unseen 3D Objects by Poking the Objects,Xiaowei Zhou,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Wayner Barrios,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Mattia Soldan,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Alberto Mario Ceballos-arroyo,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Fabian Caba Heilbron,,0%
https://arxiv.org/pdf/2302.13372.pdf,Localizing Moments in Long Video Via Multimodal Guidance,Bernard Ghanem,,0%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Jorge Vila-tomás,jorge.vila-tomas@uv.es,95%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Pablo Hernández-cámara,pablo.hernandez-camara@uv.es,95%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Valero Laparra,valero.laparra@uv.es,95%
https://arxiv.org/pdf/2302.13345.pdf,Analysis of Deep Image Quality Models,Jesús Malo,jesus.malo@uv.es,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Didier Hans,didier.hans@chuv.ch,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Zhe Wang,zwang78@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Fang Chen,fangyxy@hactcm.edu.cn,85%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Rachid Jennane,rachid.jennane@univ-orleans.fr,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Fabian Bauer,fabian.bauer@dkfz-heidelberg.de,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Yung Hsin Chen,ychen4@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Yuhua Ru,ruyuhua@163.com,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Mohamed Jarraya,mjarraya@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Aladine Chetouani,aladine.chetouani@univ-paris13.fr,95%
https://arxiv.org/pdf/2302.13336.pdf,Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early Knee Osteoarthritis Detection,Liping Zhang,lzhang90@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Haoyu Luo,luohaoyu@stu.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Yihong Gong,ygong@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Xing Wei,weixing@mail.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Yuhang He,chengjie8@huawei.com,78%
https://arxiv.org/pdf/2302.13334.pdf,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,Songlin Dong,,0%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Hyunsu Kim,hyunsu1125.kim@navercorp.com,95%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Yoonjeon Kim,yoonkim313@kaist.ac.kr,82%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Yunjey Choi,yunjey.choi@navercorp.com,95%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Junho Kim,jhkim.ai@navercorp.com,82%
https://arxiv.org/pdf/2302.13331.pdf,Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance,Eunho Yang,eunhoy@kaist.ac.kr,85%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Hung Cao,hcao3@unb.ca,82%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Atah Nuh Mih,,0%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Joshua Pickard,,0%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Monica Wachowicz,,0%
https://arxiv.org/pdf/2302.13317.pdf,TransferD2: Automated Defect Detection Approach in Smart Manufacturing using Transfer Learning Techniques,Rickey Dubay,,0%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Shoaib Ehsan,sehsan@essex.ac.uk,82%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Mihnea-alexandru Tomita,matomi@essex.ac.uk,90%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Michael Milford,michael.milford@qut.edu.au,95%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Bruno Ferrarini,bferra@essex.ac.uk,90%
https://arxiv.org/pdf/2302.13314.pdf,Data-Efficient Sequence-Based Visual Place Recognition with Highly Compressed JPEG Images,Klaus Mcdonald-maier,,0%
https://arxiv.org/pdf/2302.13301.pdf,Pillar R-CNN for Point Cloud 3D Object Detection,Chao Ma,chaoma@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13301.pdf,Pillar R-CNN for Point Cloud 3D Object Detection,Guangsheng Shi,sgsadvance@163.com,60%
https://arxiv.org/pdf/2302.13301.pdf,Pillar R-CNN for Point Cloud 3D Object Detection,Ruifeng Li,,0%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Luisa Verdoliva,verdoliv@unina.it,90%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Marius Pedersen,marius.pedersen@ntnu.no,95%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Ying Xu,,0%
https://arxiv.org/pdf/2302.13288.pdf,Learning Pairwise Interaction for Generalizable DeepFake Detection,Kiran Raja,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Hatef Otroshi Shahreza,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Pietro Melzi,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Dailé Osorio-roig,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Christian Rathgeb,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Christoph Busch,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Sébastien Marcel,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Ruben Tolosana,,0%
https://arxiv.org/pdf/2302.13286.pdf,Benchmarking of Cancelable Biometrics for Deep Templates,Ruben Vera-rodriguez,,0%
https://arxiv.org/pdf/2302.13279.pdf,Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition,Xingchao Yang,,0%
https://arxiv.org/pdf/2302.13279.pdf,Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition,Takafumi Taketomi,,0%
https://arxiv.org/pdf/2302.13279.pdf,Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition,Yoshihiro Kanamori,,0%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Hongxun Yao,h.yao@hit.edu.cn,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Kuiyuan Yang,kuyang@microsoft.com,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Wei Yu,w.yu@hit.edu.cn,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Yalong Bai,ylbai@mtlab.hit.edu.cn,82%
https://arxiv.org/pdf/2302.13275.pdf,Learning cross space mapping via DNN using large scale click-through logs,Yong Rui,grui@microsoft.com,78%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Haoning Wu,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Liang Liao,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Jingwen Hou,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Chaofeng Chen,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Erli Zhang,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Annan Wang,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Wenxiu Sun,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Qiong Yan,,0%
https://arxiv.org/pdf/2302.13269.pdf,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,Weisi Lin,,0%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Ming Wu,wuming@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Shenwei Xie,xieshenwei@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Junli Yang,yangjunli@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Chuang Zhang,zhangchuang@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Wanfeng Zheng,zhengwanfeng@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.13263.pdf,PaRK-Detect: Towards Efficient Multi-Task Satellite Imagery Road Extraction via Patch-Wise Keypoints Detection,Zhenglin Xian,,0%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Zhenzhong Chen,zzchen@ieee.org,82%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Yuantong Zhang,,0%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Daiqin Yang,,0%
https://arxiv.org/pdf/2302.13256.pdf,Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information,Wenpeng Ding,,0%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Haoliang Li,haoliang.li@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Jie Liu,wanpeoplejie@gmail.com,85%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Varut Vardhanabhuti,varv@hku.hk,75%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Hong Yan,h.yan@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Victor Ho-fun Lee,vhflee@hku.hk,82%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Kecheng Chen,,0%
https://arxiv.org/pdf/2302.13251.pdf,Unsupervised Domain Adaptation for Low-dose CT Reconstruction via Bayesian Uncertainty Alignment,Renjie Wan,,0%
https://arxiv.org/pdf/2302.13207.pdf,Stereo X-ray Tomography,Zhenduo Shang,,0%
https://arxiv.org/pdf/2302.13207.pdf,Stereo X-ray Tomography,Thomas Blumensath,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Nchongmaje Ndipenoch,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Alina Miron,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Zidong Wang,,0%
https://arxiv.org/pdf/2302.13195.pdf,"nnUNet RASPP for Retinal OCT Fluid Detection, Segmentation and Generalisation over Variations of Data Sources",Yongmin Li,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Shaoyan Pan,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Shao-yuan Lo,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Min Huang,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Chaoqiong Ma,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Jacob Wynne,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Tonghe Wang,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Tian Liu,,0%
https://arxiv.org/pdf/2302.13172.pdf,Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation,Xiaofeng Yang,,0%
https://arxiv.org/pdf/2302.13170.pdf,Partial Label Learning for Emotion Recognition from EEG,Ali Etemad,ali.etemad@queensu.ca,95%
https://arxiv.org/pdf/2302.13170.pdf,Partial Label Learning for Emotion Recognition from EEG,Guangyi Zhang,guangyi.zhang@queensu.ca,95%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,Avisek Lahiri,avisek@google.com,85%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,Wan-duo Kurt Ma,mawand@ecs.vuw.ac.nz,95%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,J. P. Lewis,bastiaan.kleijn@vuw.ac.nz,75%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,Thomas Leung,leungt@google.com,78%
https://arxiv.org/pdf/2302.13153.pdf,Directed Diffusion: Direct Control of Object Placement through Attention Guidance,W. Bastiaan Kleijn,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,Tarasha Khurana,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,Peiyun Hu,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,David Held,,0%
https://arxiv.org/pdf/2302.13130.pdf,Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting,Deva Ramanan,,0%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Galit Fuhrmann Alpert,fuhrmann@bgu.ac.il,90%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Dima Kagan,kagandi@post.bgu.ac.il,78%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Shmuel Horowitz,shmuelho@post.bgu.ac.il,85%
https://arxiv.org/pdf/2303.02052.pdf,Interruptions detection in video conferences,Michael Fire,,0%
https://arxiv.org/pdf/2302.13125.pdf,Non-Intrusive Driver Behavior Characterization From Road-Side Cameras,Pavana Pradeep Kumar,pavana.pradeep@temple.edu,85%
https://arxiv.org/pdf/2302.13125.pdf,Non-Intrusive Driver Behavior Characterization From Road-Side Cameras,Krishna Kant,kkant@temple.edu,82%
https://arxiv.org/pdf/2302.13125.pdf,Non-Intrusive Driver Behavior Characterization From Road-Side Cameras,Amitangshu Pal,amitangshu@cse.iitk.ac.in,85%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Qihan Ren,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Huiqi Deng,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Yunuo Chen,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Siyu Lou,,0%
https://arxiv.org/pdf/2302.13095.pdf,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,Quanshi Zhang,,0%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Jingtao Ding,dingjt15@tsinghua.org.cn,78%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Yu Liu,liuyu2419@126.com,95%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Yanxin Xi,yanxin.xi@helsinki.fi,95%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Yong Li,liyong07@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.13094.pdf,Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction,Xin Zhang,zhangxin4087@163.com,95%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Feng Ding,,0%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Jian Jin,,0%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Lili Meng,,0%
https://arxiv.org/pdf/2302.13092.pdf,JND-Based Perceptual Optimization For Learned Image Compression,Weisi Lin,,0%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Huilin Zhou,zhouhuilin116@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Shih-han Chan,s2chan@ucsd.edu,95%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Huiqi Deng,denghq7@sjtu.edu.cn,78%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Dongrui Liu,drliu96@sjtu.edu.cn,82%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Wen Shen,wen shen@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Hao Zhang,,0%
https://arxiv.org/pdf/2302.13091.pdf,Explaining Generalization Power of a DNN Using Interactive Concepts,Quanshi Zhang,,0%
https://arxiv.org/pdf/2302.13084.pdf,RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information,Dong-gyu Lee,dglee@knu.ac.kr,82%
https://arxiv.org/pdf/2302.13084.pdf,RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information,Satyawant Kumar,,0%
https://arxiv.org/pdf/2302.13084.pdf,RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information,Abhishek Kumar,,0%
https://arxiv.org/pdf/2302.13080.pdf,Does a Neural Network Really Encode Symbolic Concepts?,Mingjie Li,,0%
https://arxiv.org/pdf/2302.13080.pdf,Does a Neural Network Really Encode Symbolic Concepts?,Quanshi Zhang,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Martin Sundermeyer,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Tomas Hodan,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Yann Labbe,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Gu Wang,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Eric Brachmann,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Bertram Drost,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Carsten Rother,,0%
https://arxiv.org/pdf/2302.13075.pdf,"BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",Jiri Matas,,0%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Errui Ding,dingerrui@baidu.com,95%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Jian Wang,wangjian33@baidu.com,95%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Desen Zhou,zhouds@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Zhichao Liu,liuzhch@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Rui Fan,fanrui@shanghaitech.edu.cn,95%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Leshan Wang,wanglsh@shanghaitech.edu.cn,78%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Songyang Zhang,,0%
https://arxiv.org/pdf/2302.13074.pdf,Temporal Segment Transformer for Action Segmentation,Yang Bai,,0%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Yiqin Yu,yuyiqin@cn.ibm.com,95%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Jing Mei,meijing@cn.ibm.com,95%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Yuan Zhou,zhxyuan@cn.ibm.com,85%
https://arxiv.org/pdf/2302.13069.pdf,Medical visual question answering using joint self-supervised learning,Tanveer Syeda-mahmood,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Lemuel Puglisi,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Frederik Barkhof,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Daniel C. Alexander,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Geoffrey Jm Parker,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Arman Eshaghi,,0%
https://arxiv.org/pdf/2302.13057.pdf,DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification,Daniele Ravì,,0%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Xiaowei Xu,xuxw525@ouc.edu.cn,82%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Leon Bevan Bullock,leonbevanbullock@ouc.edu.cn,95%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Huasong Zhou,zhouhuasong@stu.ouc.edu.cn,95%
https://arxiv.org/pdf/2302.13056.pdf,SATBA: An Invisible Backdoor Attack Based On Spatial Attention,Xiaodong Wang,wangxiaodong@ouc.edu.cn,95%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Jawad Muhammad,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Yunlong Wang,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Junxing Hu,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Kunbo Zhang,,0%
https://arxiv.org/pdf/2302.13049.pdf,CASIA-Iris-Africa: A Large-scale African Iris Image Database,Zhenan Sun,,0%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Muhammad Haroon Yousaf,haroon.yousaf@uettaxila.edu.pk,78%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Muhammad Saad Saeed,saad.saeed@uettaxila.edu.pk,78%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Shah Nawaz,shah.nawaz@desy.de,95%
https://arxiv.org/pdf/2302.13033.pdf,Speaker Recognition in Realistic Scenario Using Multimodal Data,Saqlain Hussain Shah,saqlain.hussain@uettaxila.edu.pk,85%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Lam Pham,lam.pham@ait.ac.at,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Anh Nguyen,AnhNTN34@fsoft.com.vn,85%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Cam Le,cam.levt123@hcmut.edu.vn,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Alexander Schindler,Alexander.Schindler@ait.ac.at,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Jasmin Lampert,Jasmin.Lampert@ait.ac.at,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Ian Mcloughlin,ian.mcloughlin@singaporetech.edu.sg,95%
https://arxiv.org/pdf/2302.13028.pdf,A Light-weight Deep Learning Model for Remote Sensing Image Classification,Dat Ngo,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Zhengyu Ma,mazhy@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Yonghong Tian,yhtian@pku.edu.cn,82%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Yanqi Chen,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Wei Fang,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Xiawu Zheng,,0%
https://arxiv.org/pdf/2302.13019.pdf,A Unified Framework for Soft Threshold Pruning,Zhaofei Yu,,0%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Binbin Lv,lv-bin-bin@outlook.com,78%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Yaqi Liu,liuyaqi@besti.edu.cn,95%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Xin Jin,jinxin@besti.edu.cn,95%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Xiaoyu Chen,,0%
https://arxiv.org/pdf/2302.13004.pdf,TBFormer: Two-Branch Transformer for Image Forgery Localization,Xiaokun Zhang,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Hao Zhang,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Hongyang Li,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Ailing Zeng,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Feng Li,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Shilong Liu,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Xingyu Liao,,0%
https://arxiv.org/pdf/2302.13002.pdf,Introducing Depth into Transformer-based 3D Object Detection,Lei Zhang,,0%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Bihan Wen,bihan.wen@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Alex Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Lap-pui Chau,lap-pui.chau@polyu.edu.hk,95%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Lanqing Guo,lanqing001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Wenhan Yang,yangwh@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Yi Yu,yuyi0010@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.12995.pdf,Raw Image Reconstruction with Learned Compact Metadata,Yufei Wang,yufei001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Zhen Lei,zlei@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Benzhi Wang,wangbenzhi2021@ia.ac.cn,95%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Jinlin Wu,jinlin.wu@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Guo-jun Qi,guojunq@gmail.com,85%
https://arxiv.org/pdf/2302.12986.pdf,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,Yang Yang,yang.yang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Donald Stewart,dolstewa@ucsc.edu,75%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Alex Pang,pang@soe.ucsc.edu,78%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Gregory Dusek,gregory.dusek@noaa.gov,95%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Akila De Silva,audesilv@ucsc.edu,75%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Mona Zhao,yzhao172@ucsc.edu,78%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,Fahim Hasan Khan,fkhan4@ucsc.edu,82%
https://arxiv.org/pdf/2302.12983.pdf,RipViz: Finding Rip Currents by Learning Pathline Behavior,James Davis,davis@cs.ucsc.edu,78%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Daniel Racoceanu,firstname.lastname@icm-institute.org,70%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Guanghui Fu,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Gabriel Jimenez,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Sophie Loizillon,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Lydia Chougar,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Didier Dormont,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Romain Valabregue,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Ninon Burgos,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Stéphane Lehéricy,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,Olivier Colliot,,0%
https://arxiv.org/pdf/2302.12980.pdf,Frequency Disentangled Learning for Segmentation of Midbrain Structures from Quantitative Susceptibility Mapping Data,The Iceberg Study Group,,0%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Yongqiang Ma,musayq@xjtu.edu.cn,65%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Guibo Zhu,gbzhu@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Nanning Zheng,nnzheng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Yulong Liu,,0%
https://arxiv.org/pdf/2302.12971.pdf,BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding,Wei Zhou,,0%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Beiying Yang,beiying.yang@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Lu Zhou,lu.zhou@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Jinqiao Wang,jqwang@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Guibo Zhu,gbzhu@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Jinzhao Luo,luojinzhao2020@ia.ac.cn,95%
https://arxiv.org/pdf/2302.12967.pdf,Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition,Guojing Ge,guojing.ge@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Jiawei Hou,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Qi Chen,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Yurong Cheng,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Guang Chen,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Xiangyang Xue,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Taiping Zeng,,0%
https://arxiv.org/pdf/2302.12966.pdf,SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving,Jian Pu,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Otilia Stretcu,otiliastr@google.com,85%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Ariel Fuxman,afuxman@google.com,82%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Edward Vendrow,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Kenji Hata,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Krishnamurthy Viswanathan,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Vittorio Ferrari,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Sasan Tavakkol,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Wenlei Zhou,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Aditya Avinash,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Enming Luo,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Neil Gordon Alldrin,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Mohammadhossein Bateni,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Gabriel Berger,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Andrew Bunner,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Chun-ta Lu,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Javier A Rey,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Giulia Desalvo,,0%
https://arxiv.org/pdf/2302.12948.pdf,Agile Modeling: From Concept to Classifier in Minutes,Ranjay Krishna,,0%
https://arxiv.org/pdf/2302.12935.pdf,Visual Privacy: Current and Emerging Regulations Around Unconsented Video Analytics in Retail,Scott Pletcher,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Peyman Tahghighi,ptahghig@uoguelph.ca,90%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Nicole Norena,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Eran Ukwatta,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Ryan B Appleby,,0%
https://arxiv.org/pdf/2302.12923.pdf,Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs,Amin Komeili,,0%
https://arxiv.org/pdf/2302.12883.pdf,3D Surface Reconstruction in the Wild by Deforming Shape Priors from Synthetic Data,Jun-jee Chao,chao0107@umn.edu,78%
https://arxiv.org/pdf/2302.12883.pdf,3D Surface Reconstruction in the Wild by Deforming Shape Priors from Synthetic Data,Volkan Isler,isler@umn.edu,78%
https://arxiv.org/pdf/2302.12883.pdf,3D Surface Reconstruction in the Wild by Deforming Shape Priors from Synthetic Data,Nicolai Häni,haeni001@umn.edu,65%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Randall Balestriero,rbalestriero@fb.com,82%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Ahmed Imtiaz Humayun,imtiaz@rice.edu,90%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Richard Baraniuk,richb@rice.edu,78%
https://arxiv.org/pdf/2302.12828.pdf,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,Guha Balakrishnan,guha@rice.edu,85%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Vickie Ye,,0%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Georgios Pavlakos,,0%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Jitendra Malik,,0%
https://arxiv.org/pdf/2302.12827.pdf,Decoupling Human and Camera Motion from Videos in the Wild,Angjoo Kanazawa,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Simone Foti,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Bongjin Koo,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Danail Stoyanov,,0%
https://arxiv.org/pdf/2302.12798.pdf,3D Generative Model Latent Disentanglement via Local Eigenprojection,Matthew J. Clarkson,,0%
https://arxiv.org/pdf/2302.12772.pdf,FLSea: Underwater Visual-Inertial and Stereo-Vision Forward-Looking Datasets,Yelena Randall,y4randall@gmail.com,82%
https://arxiv.org/pdf/2302.12772.pdf,FLSea: Underwater Visual-Inertial and Stereo-Vision Forward-Looking Datasets,Tali Treibitz,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Siddharth Karamcheti,skaramcheti@cs.stanford.edu,82%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Suraj Nair,surajn@cs.stanford.edu,85%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Annie S. Chen,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Thomas Kollar,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Chelsea Finn,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Dorsa Sadigh,,0%
https://arxiv.org/pdf/2302.12766.pdf,Language-Driven Representation Learning for Robotics,Percy Liang,,0%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Zhifei Zhang,zzhang@adobe.com,82%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,James Hays,hays@gatech.edu,78%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Krishna Kumar Singh,krishsin@adobe.com,65%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Cusuh Ham,cusuh@gatech.edu,85%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Jingwan Lu,jlu@adobe.com,82%
https://arxiv.org/pdf/2302.12764.pdf,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,Tobias Hinz,thinz@adobe.com,82%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Ruchika Chavhan,R.Chavhan@sms.ed.ac.uk,82%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Timothy Hospedales,t.hospedales@ed.ac.uk,82%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Henry Gouk,,0%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Jan Stuehmer,,0%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Calum Heggan,,0%
https://arxiv.org/pdf/2302.12712.pdf,Amortised Invariance Learning for Contrastive Self-Supervision,Mehrdad Yaghoobi,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Haoyi Xiong,haoyi.xiong.fr@ieee.org,95%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Yuxuan Zhang,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Qingzhong Wang,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Jiang Bian,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Yi Liu,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Yanwu Xu,,0%
https://arxiv.org/pdf/2302.12688.pdf,Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks,Dejing Dou,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Tianpeng Deng,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Yanqi Huang,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Guoqiang Han,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Zhenwei Shi,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Jiatai Lin,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Qi Dou,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Zaiyi Liu,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Xiao-jing Guo,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,C. L. Philip Chen,,0%
https://arxiv.org/pdf/2302.12662.pdf,FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification,Chu Han,,0%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Hans Wernher Van De Venn,vhns@zhaw.ch,60%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Charith Munasinghe,,0%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Fatemeh Mohammadi Amin,,0%
https://arxiv.org/pdf/2302.12656.pdf,"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation",Davide Scaramuzza,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Torsten Schlett,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Sebastian Schachner,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Christian Rathgeb,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Juan Tapia,,0%
https://arxiv.org/pdf/2302.12593.pdf,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,Christoph Busch,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Vivien Zahs,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Katharina Anders,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Julia Kohns,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Alexander Stark,,0%
https://arxiv.org/pdf/2302.12591.pdf,Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data,Bernhard Höfle,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Arindam Das,firstname.lastname@ul.ie,70%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Sudip Das,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Ganesh Sistu,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Jonathan Horgan,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Ujjwal Bhattacharya,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Edward Jones,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Martin Glavin,,0%
https://arxiv.org/pdf/2302.12589.pdf,Revisiting Modality Imbalance In Multimodal Pedestrian Detection,Ciarán Eising,,0%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Yueyang Teng,tengyy@bime.neu.edu.cn,78%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Hengzhi Xue,,0%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Qingqing Fang,,0%
https://arxiv.org/pdf/2302.12571.pdf,3D PETCT Tumor Lesion Segmentation via GCN Refinement,Yudong Yao,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Sungho Suh,sungho.suh@dfki.de,95%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Jwalin Bhatt,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Yaroslav Zharov,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Tilo Baumbach,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Vincent Heuveline,,0%
https://arxiv.org/pdf/2302.12562.pdf,A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image,Paul Lukowicz,,0%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Yanming Guo,guoyanming@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Wei Chen,weichen@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Yu Liu,liuyu8824@dlut.edu.cn,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Cunjuan Zhu,Zhucunjuan@163.com,95%
https://arxiv.org/pdf/2302.12552.pdf,Deep Learning for Video-Text Retrieval: a Review,Qi Jia,jiaqi@dlut.edu.cn,95%
https://arxiv.org/pdf/2303.12697.pdf,Visual motion analysis of the player's finger,Marco Costanzo,marco.costanzo@mail.polimi.it,95%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Bo Li,libonchu@outlook.com,95%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Bin Liu,nyliubin@nchu.edu.cn,95%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Xiaolin Wei,,0%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Junjie Cao,,0%
https://arxiv.org/pdf/2302.12532.pdf,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,Yu-kun Lai,,0%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Simone Saitta,simone.saitta@polimi.it,95%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Marcello Carioni,m.c.carioni@utwente.nl,82%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Subhadip Mukherjee,,0%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Carola-bibiane Schönlieb,,0%
https://arxiv.org/pdf/2302.12835.pdf,Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI,Alberto Redaelli,,0%
https://arxiv.org/pdf/2302.12505.pdf,Spatial Bias for Attention-free Non-local Neural Networks,Junhyung Go,,0%
https://arxiv.org/pdf/2302.12505.pdf,Spatial Bias for Attention-free Non-local Neural Networks,Jongbin Ryu,,0%
https://arxiv.org/pdf/2302.12495.pdf,Data fusion of satellite imagery for generation of daily cloud free images at high resolution level,Petro Martyniuk,petro.martyniyk@eosda.com,85%
https://arxiv.org/pdf/2302.12495.pdf,Data fusion of satellite imagery for generation of daily cloud free images at high resolution level,Natalya Ivanchuk,natalya.ivanchuk@eosda.com,95%
https://arxiv.org/pdf/2302.12495.pdf,Data fusion of satellite imagery for generation of daily cloud free images at high resolution level,Peter Kogut,peter.kogut@eosda.com,95%
https://arxiv.org/pdf/2302.12491.pdf,Joint Learning of Blind Super-Resolution and Crack Segmentation for Realistic Degraded Images,Yuki Kondo,kondo@toyota-ti.ac.jp,78%
https://arxiv.org/pdf/2302.12491.pdf,Joint Learning of Blind Super-Resolution and Crack Segmentation for Realistic Degraded Images,Norimichi Ukita,ukita@toyota-ti.ac.jp,78%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Shumpei Takezaki,,0%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Kiyohito Tanaka,,0%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Seiichi Uchida,,0%
https://arxiv.org/pdf/2302.12482.pdf,Disease Severity Regression with Continuous Data Augmentation,Takeaki Kadota,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Liangqi Zhang,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Yihao Luo,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Xiang Cao,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Haibo Shen,,0%
https://arxiv.org/pdf/2302.12477.pdf,Frequency and Scale Perspectives of Feature Extraction,Tianjiang Wang,,0%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Junghyo Jo,jojunghyo@snu.ac.kr,95%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Youngjung Uh,yj.uh@yonsei.ac.kr,82%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Mingi Kwon,kwonmingi@yonsei.ac.kr,95%
https://arxiv.org/pdf/2302.12469.pdf,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,Yong-hyun Park,,0%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Shancong Mou,shancong.mou@gatech.edu,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Ping Huang,huang ping@apple.com,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Jiulong Shan,jlshan@apple.com,82%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Haoping Bai,haoping bai@apple.com,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Xiaoyi Gu,xiaoyigu@gatech.edu,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Jianjun Shi,jianjun.shi@isye.gatech.edu,95%
https://arxiv.org/pdf/2302.12464.pdf,RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection,Meng Cao,mengcao@apple.com,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Wei Li,liwei089@ieee.org,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Junchuan Yu,yujunchuan@mail.cgs.gov.cn,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Zili Lu,luzili0705@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Yuexing Peng,yxpeng@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Daqing Ge,gedaqing@mail.cgs.gov.cn,95%
https://arxiv.org/pdf/2302.12420.pdf,An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,Wei Xiang,w.xiang@latrobe.edu.au,82%
https://arxiv.org/pdf/2302.12416.pdf,A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data,Hayat Rajani,hayat.rajani@udg.edu,95%
https://arxiv.org/pdf/2302.12416.pdf,A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data,Nuno Gracias,ngracias@silver.udg.edu,82%
https://arxiv.org/pdf/2302.12416.pdf,A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data,Rafael Garcia,rafael.garcia@udg.edu,95%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Yaofo Chen,sensc@mail.scut.edu.cn,55%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Mingkui Tan,mingkuitan@scut.edu.cn,95%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Shuaicheng Niu,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Jiaxiang Wu,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Yifan Zhang,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Zhiquan Wen,,0%
https://arxiv.org/pdf/2302.12400.pdf,Towards Stable Test-Time Adaptation in Dynamic Wild World,Peilin Zhao,,0%
https://arxiv.org/pdf/2302.12393.pdf,Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics,Zhou Wang,zhou.wang@uwaterloo.ca,95%
https://arxiv.org/pdf/2302.12393.pdf,Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics,Wei Zhou,wei.zhou@uwaterloo.ca,95%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Debasmit Das,debadas@qti.qualcomm.com,82%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Hyojin Park,hyojinp@qti.qualcomm.com,85%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Fatih Porikli,fporikli@qti.qualcomm.com,82%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Risheek Garrepalli,rgarrepa@qti.qualcomm.com,90%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Shubhankar Borse,sborse@qti.qualcomm.com,82%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Kambiz Azarian,kambiza@qti.qualcomm.com,85%
https://arxiv.org/pdf/2302.14611.pdf,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,Hong Cai,hongcai@qti.qualcomm.com,95%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Junhuan Yang,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Yi Sheng,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Yuzhou Zhang,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Weiwen Jiang,,0%
https://arxiv.org/pdf/2303.12753.pdf,On-Device Unsupervised Image Segmentation,Lei Yang,,0%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Ryan Goldhahn,goldhahn1@llnl.gov,78%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Bhavya Kailkhura,kailkhura1@llnl.gov,78%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Yize Li,li.yize@northeastern.edu,95%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Xue Lin,xue.lin@northeastern.edu,95%
https://arxiv.org/pdf/2302.12366.pdf,Less is More: Data Pruning for Faster Adversarial Training,Pu Zhao,p.zhao@northeastern.edu,82%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Martin Claus,mclaus@geomar.de,82%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Willi Rath,wrath@geomar.de,82%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Peer Kröger,pkr@informatik.uni-kiel.de,90%
https://arxiv.org/pdf/2302.12317.pdf,Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures,Marco Landt-hayen,mlandt-hayen@geomar.de,82%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Avinash C. Kak,kak@purdue.edu,78%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Rahul Deshmukh,deshmuk5@purdue.edu,55%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Constantine J. Roros,croros@purdue.edu,82%
https://arxiv.org/pdf/2302.12301.pdf,An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research,Amith Kashyap,kashyap9@purdue.edu,78%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Shariq Farooq Bhat,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Reiner Birkl,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Diana Wofk,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Peter Wonka,,0%
https://arxiv.org/pdf/2302.12288.pdf,ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth,Matthias Müller,,0%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Yuzhe Yang,yuzhe@mit.edu,85%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Haoran Zhang,,0%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Dina Katabi,,0%
https://arxiv.org/pdf/2302.12254.pdf,Change is Hard: A Closer Look at Subpopulation Shift,Marzyeh Ghassemi,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Zhixiang Wang,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Yu-lun Liu,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Jia-bin Huang,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Shin'ichi Satoh,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Sizhuo Ma,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Gurunandan Krishnan,,0%
https://arxiv.org/pdf/2302.12253.pdf,DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs,Jian Wang,,0%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Muzammal Naseer,muzammal.naseer@alumni.anu.edu.au,95%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Ahmad Mahmood,,0%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Salman Khan,,0%
https://arxiv.org/pdf/2302.12252.pdf,Boosting Adversarial Transferability using Dynamic Cues,Fahad Khan,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Zhiding Yu,zhidingy@nvidia.com,85%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Yiming Li,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Christopher Choy,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Chaowei Xiao,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Jose M. Alvarez,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Sanja Fidler,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Chen Feng,,0%
https://arxiv.org/pdf/2302.12251.pdf,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,Anima Anandkumar,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Christian Reiser,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Richard Szeliski,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Dor Verbin,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Pratul P. Srinivasan,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Ben Mildenhall,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Andreas Geiger,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Jonathan T. Barron,,0%
https://arxiv.org/pdf/2302.12249.pdf,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,Peter Hedman,,0%
https://arxiv.org/pdf/2302.12248.pdf,Learning Visual Representations via Language-Guided Sampling,Justin Johnson,justincj@umich.edu,85%
https://arxiv.org/pdf/2302.12248.pdf,Learning Visual Representations via Language-Guided Sampling,Mohamed El Banani,mbanani@umich.edu,82%
https://arxiv.org/pdf/2302.12248.pdf,Learning Visual Representations via Language-Guided Sampling,Karan Desai,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Paul Pu Liang,pliang@cs.cmu.edu,82%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Yun Cheng,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Xiang Fan,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Chun Kai Ling,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Suzanne Nie,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Richard Chen,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Zihao Deng,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Nicholas Allen,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Randy Auerbach,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Faisal Mahmood,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Ruslan Salakhutdinov,,0%
https://arxiv.org/pdf/2302.12247.pdf,Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework,Louis-philippe Morency,,0%
https://arxiv.org/pdf/2302.12245.pdf,Set Features for Fine-grained Anomaly Detection,Niv Cohen,nivc@cs.huji.ac.il,85%
https://arxiv.org/pdf/2302.12245.pdf,Set Features for Fine-grained Anomaly Detection,Issar Tzachor,,0%
https://arxiv.org/pdf/2302.12245.pdf,Set Features for Fine-grained Anomaly Detection,Yedid Hoshen,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Mengde Xu,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Zheng Zhang,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Fangyun Wei,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Han Hu,,0%
https://arxiv.org/pdf/2302.12242.pdf,Side Adapter Network for Open-Vocabulary Semantic Segmentation,Xiang Bai,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Chen Geng,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Sida Peng,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Zhen Xu,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Hujun Bao,,0%
https://arxiv.org/pdf/2302.12237.pdf,Learning Neural Volumetric Representations of Dynamic Humans in Minutes,Xiaowei Zhou,,0%
https://arxiv.org/pdf/2302.12231.pdf,DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models,Jamie Wynn,,0%
https://arxiv.org/pdf/2302.12231.pdf,DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models,Daniyar Turmukhambetov,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Rinon Gal,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Moab Arar,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Yuval Atzmon,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Amit H. Bermano,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Gal Chechik,,0%
https://arxiv.org/pdf/2302.12228.pdf,Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Kimin Lee,kiminl@google.com,85%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Hao Liu,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Moonkyung Ryu,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Olivia Watkins,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Yuqing Du,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Craig Boutilier,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Pieter Abbeel,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Mohammad Ghavamzadeh,,0%
https://arxiv.org/pdf/2302.12192.pdf,Aligning Text-to-Image Models using Human Feedback,Shixiang Shane Gu,,0%
https://arxiv.org/pdf/2302.12189.pdf,"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales",Michele Cafagna,michele.cafagna@um.edu.mt,95%
https://arxiv.org/pdf/2302.12189.pdf,"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales",Albert Gatt,a.gatt@uu.nl,82%
https://arxiv.org/pdf/2302.12189.pdf,"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales",Kees Van Deemter,c.j.vandeemter@uu.nl,78%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Erkang Chen,ekchen@jmu.edu.cn,82%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Jingxia Jiang,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Jinbin Bai,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Yun Liu,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Junjie Yin,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Sixiang Chen,,0%
https://arxiv.org/pdf/2302.12186.pdf,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,Tian Ye,,0%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Leonard Sunwoo,leonard.sunwoo@gmail.com,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Jin-hwa Kim,j1nhwa.kim@navercorp.com,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Edward Choi,edwardchoi@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Da Young Lee,dyan.lee717@gmail.com,82%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Wonjae Kim,wonjae.kim@navercorp.com,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Tackeun Kim,tackeun.kim@snu.ac.kr,95%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Hyungyung Lee,,0%
https://arxiv.org/pdf/2302.12172.pdf,Vision-Language Generative Model for View-Specific Chest X-ray Generation,Jihang Kim,,0%
https://arxiv.org/pdf/2302.12084.pdf,Dermatological Diagnosis Explainability Benchmark for Convolutional Neural Networks,Raluca Jalaboi,,0%
https://arxiv.org/pdf/2302.12084.pdf,Dermatological Diagnosis Explainability Benchmark for Convolutional Neural Networks,Ole Winther,,0%
https://arxiv.org/pdf/2302.12084.pdf,Dermatological Diagnosis Explainability Benchmark for Convolutional Neural Networks,Alfiia Galimzianova,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Roni Paiss,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Ariel Ephrat,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Omer Tov,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Shiran Zada,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Inbar Mosseri,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Michal Irani,,0%
https://arxiv.org/pdf/2302.12066.pdf,Teaching CLIP to Count to Ten,Tali Dekel,,0%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Liguo Zhou,liguo.zhou@tum.de,95%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Hanzhen Zhang,,0%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Ruining Wang,,0%
https://arxiv.org/pdf/2302.12052.pdf,Attention Mechanism for Contrastive Learning in GAN-based Image-to-Image Translation,Alois Knoll,,0%
https://arxiv.org/pdf/2302.12047.pdf,Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach,Da Li,dali.academic@gmail.com,95%
https://arxiv.org/pdf/2302.12047.pdf,Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach,Minyoung Kim,mikim21@gmail.com,82%
https://arxiv.org/pdf/2302.12047.pdf,Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach,Timothy Hospedales,,0%
https://arxiv.org/pdf/2302.11984.pdf,Unsupervised Domain Adaptation via Distilled Discriminative Clustering,Yaowei Wang,yaoweiwang@pku.edu.cn,95%
https://arxiv.org/pdf/2302.11984.pdf,Unsupervised Domain Adaptation via Distilled Discriminative Clustering,Kui Jia,kuijia@scut.edu.cn,95%
https://arxiv.org/pdf/2302.11984.pdf,Unsupervised Domain Adaptation via Distilled Discriminative Clustering,Hui Tang,eehuitang@mail.scut.edu.cn,95%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Zhenyu Wu,wuzhenyu@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Ziwei Wang,wang-zw18@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Jiwen Lu,lujiwen@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.11983.pdf,Category-level Shape Estimation for Densely Cluttered Objects,Haibin Yan,eyanhaibin@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Md Awsafur Rahman,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Bishmoy Paul,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Najibul Haque Sarker,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Zaber Ibn Abdul Hakim,,0%
https://arxiv.org/pdf/2302.11970.pdf,ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection,Shaikh Anowarul Fattah,,0%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Tao Li,li.tao@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Xiaolin Huang,xiaolinhuang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Sizhe Chen,sizhe.chen@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11963.pdf,Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective,Zhengbao He,,0%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Chunqi Tian,tianchunqi@163.com,95%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Yaoru Sun,yaoru@tongji.edu.cn,85%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Maoyu Mao,maomy@tongji.edu.cn,82%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Jun Yang,junyang@tongji.edu.cn,95%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Lizhi Bai,bailizhi@tongji.edu.cn,95%
https://arxiv.org/pdf/2302.11951.pdf,Pixel Difference Convolutional Network for RGB-D Semantic Segmentation,Guorun Wang,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Ling Li,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Bandara Dissanayake,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Tatsuya Omotezako,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Yunjie Zhong,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Qing Zhang,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Rizhao Cai,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Qian Zheng,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Dennis Sng,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Weisi Lin,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Yufei Wang,,0%
https://arxiv.org/pdf/2302.11950.pdf,Evaluating the Efficacy of Skincare Product: A Realistic Short-Term Facial Pore Simulation,Alex C Kot,,0%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Mohammad Al-sa'd,mohammad.al-sad@tuni.fi,85%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Tuomas Jalonen,tuomas.jalonen@tuni.fi,95%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Serkan Kiranyaz,mkiranyaz@qu.edu.qa,78%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Roope Mellanen,roope.mellanen@konecranes.com,95%
https://arxiv.org/pdf/2302.11947.pdf,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,Moncef Gabbouj,cef.gabbouj@tuni.fi,78%
https://arxiv.org/pdf/2302.11929.pdf,A metric to compare the anatomy variation between image time series,Alphin J Thottupattu,alphinj.thottupattu@research.iiit.ac.in,95%
https://arxiv.org/pdf/2302.11929.pdf,A metric to compare the anatomy variation between image time series,Jayanthi Sivaswamy,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Mark Eastwood,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Heba Sailem,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Silviu Tudor,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Xiaohong Gao,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Judith Offman,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Emmanouil Karteris,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Angeles Montero Fernandez,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Danny Jonigk,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,William Cookson,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Miriam Moffatt,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Sanjay Popat,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Fayyaz Minhas,,0%
https://arxiv.org/pdf/2302.12653.pdf,MesoGraph: Automatic Profiling of Malignant Mesothelioma Subtypes from Histological Images,Jan Lukas Robertus,,0%
https://arxiv.org/pdf/2302.11924.pdf,Crossing Points Detection in Plain Weave for Old Paintings with Deep Learning,J. J. Murillo-fuentes,murillo@us.es,90%
https://arxiv.org/pdf/2302.11924.pdf,Crossing Points Detection in Plain Weave for Old Paintings with Deep Learning,A. Delgado,,0%
https://arxiv.org/pdf/2302.11924.pdf,Crossing Points Detection in Plain Weave for Old Paintings with Deep Learning,L. Alba-carcelén,,0%
https://arxiv.org/pdf/2302.11893.pdf,A framework for benchmarking class-out-of-distribution detection and its application to ImageNet,Ran El-yaniv,rani@cs.technion.ac.il,85%
https://arxiv.org/pdf/2302.11893.pdf,A framework for benchmarking class-out-of-distribution detection and its application to ImageNet,Mohammed Dabbah,m.m.dabbah@gmail.com,82%
https://arxiv.org/pdf/2302.11893.pdf,A framework for benchmarking class-out-of-distribution detection and its application to ImageNet,Ido Galil,idogalil.ig@gmail.com,95%
https://arxiv.org/pdf/2302.11874.pdf,What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers,Ran El-yaniv,rani@cs.technion.ac.il,85%
https://arxiv.org/pdf/2302.11874.pdf,What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers,Mohammed Dabbah,m.m.dabbah@gmail.com,82%
https://arxiv.org/pdf/2302.11874.pdf,What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers,Ido Galil,idogalil.ig@gmail.com,95%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Lin Zhan,,0%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Jiayuan Fan,,0%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Peng Ye,,0%
https://arxiv.org/pdf/2302.11868.pdf,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,Jianjian Cao,,0%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Subha Fernando,subhaf@uom.lk,85%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Amirthalingam Ramanan,a.ramanan@univ.jfn.ac.lk,82%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Janani Thangavel,jananitha@univ.jfn.ac.lk,85%
https://arxiv.org/pdf/2302.11867.pdf,Transformers in Single Object Tracking: An Experimental Survey,Thanikasalam Kokul,kokul@univ.jfn.ac.lk,78%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Irena Gao,irena@cs.stanford.edu,85%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Shiori Sagawa,ssagawa@cs.stanford.edu,82%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Pang Wei Koh,,0%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Tatsunori Hashimoto,,0%
https://arxiv.org/pdf/2302.11861.pdf,Out-of-Domain Robustness via Targeted Augmentations,Percy Liang,,0%
https://arxiv.org/pdf/2302.11850.pdf,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,Angel Villar-corrales,,0%
https://arxiv.org/pdf/2302.11850.pdf,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,Ismail Wahdan,,0%
https://arxiv.org/pdf/2302.11850.pdf,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,Sven Behnke,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Lucas Wannenmacher,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Michael Fitzke,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Diane Wilson,,0%
https://arxiv.org/pdf/2302.11840.pdf,StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images,Andre Dourson,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Chongyi Li,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Chun-le Guo,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Man Zhou,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Zhexin Liang,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Shangchen Zhou,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Ruicheng Feng,,0%
https://arxiv.org/pdf/2302.11831.pdf,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,Chen Change Loy,,0%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Jifeng Shen,shenjifeng@ujs.edu.cn,95%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Jianhua Xu,xujianhua@njnu.edu.cn,95%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Wankou Yang,wkyang@seu.edu.cn,82%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Jun Li,lijuncst@njnu.edu.cn,95%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Guangtao Wang,,0%
https://arxiv.org/pdf/2302.11816.pdf,EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection,Zhijian Wu,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Gerard Maggiolino,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Adnan Ahmad,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Jinkun Cao,,0%
https://arxiv.org/pdf/2302.11813.pdf,Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification,Kris Kitani,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Kun Yang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Jing Liu,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Dingkang Yang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Hanqi Wang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Peng Sun,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Yanni Zhang,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Yan Liu,,0%
https://arxiv.org/pdf/2302.11810.pdf,A novel efficient Multi-view traffic-related object detection framework,Liang Song,,0%
https://arxiv.org/pdf/2302.11806.pdf,PLU-Net: Extraction of multi-scale feature fusion,Weihu Song,weihusong@buaa.edu.cn,95%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Zhiqi Yu,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Jingjing Li,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Zhekai Du,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Lei Zhu,,0%
https://arxiv.org/pdf/2302.11803.pdf,A Comprehensive Survey on Source-free Domain Adaptation,Heng Tao Shen,,0%
https://arxiv.org/pdf/2302.11802.pdf,Patch Network for medical image Segmentation,Weihu Song,weihusong@buaa.edu.cn,95%
https://arxiv.org/pdf/2302.11802.pdf,Patch Network for medical image Segmentation,Heng Yu,,0%
https://arxiv.org/pdf/2302.11802.pdf,Patch Network for medical image Segmentation,Jianhua Wu,,0%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Nisha Huang,huangnisha2021@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Fan Tang,tfan.108@gmail.com,85%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Weiming Dong,weiming.dong@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Changsheng Xu,csxu@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.11797.pdf,Region-Aware Diffusion for Zero-shot Text-driven Image Editing,Tong-yee Lee,tonylee@mail.ncku.edu.tw,82%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Huazhu Fu,hzfu@ieee.org,82%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Erjian Guo,eguo9622@uni.sydney.edu.au,82%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Dong Xu,dongxu@hku.hk,95%
https://arxiv.org/pdf/2302.11795.pdf,Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework,Luping Zhou,luping.zhou@sydney.edu.au,95%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Nadeem Atif,atif176102103@iitg.ac.in,78%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Debajit Sarma,s.debajit@iitg.ac.in,85%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Saquib Mazhar,saquibmazhar@iitg.ac.in,95%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,Shaik Rafi Ahamed,raﬁahamed@iitg.ac.in,78%
https://arxiv.org/pdf/2302.11785.pdf,Efficient Context Integration through Factorized Pyramidal Learning for Ultra-Lightweight Semantic Segmentation,M. K. Bhuyan,,0%
https://arxiv.org/pdf/2302.11767.pdf,Adaptive Approximate Implicitization of Planar Parametric Curves via Weak Gradient Constraints,Minghao Guo,mhguo@ccut.edu.cn,82%
https://arxiv.org/pdf/2302.11767.pdf,Adaptive Approximate Implicitization of Planar Parametric Curves via Weak Gradient Constraints,Yan Gao,,0%
https://arxiv.org/pdf/2302.11767.pdf,Adaptive Approximate Implicitization of Planar Parametric Curves via Weak Gradient Constraints,Zheng Pan,,0%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Liyan Ma,liyanma@shu.edu.cn,95%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Zhenglin Li,zhenglin li@shu.edu.cn,95%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Jinan Yu,,0%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Yan Peng,,0%
https://arxiv.org/pdf/2302.11757.pdf,Open-World Object Detection via Discriminative Class Prototype Learning,Shaorong Xie,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Huaqi Tao,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Bingxi Liu,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Jinqiang Cui,,0%
https://arxiv.org/pdf/2302.11728.pdf,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,Hong Zhang,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Yang Chen,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Hexiang Hu,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Yi Luan,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Haitian Sun,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Soravit Changpinyo,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Alan Ritter,,0%
https://arxiv.org/pdf/2302.11713.pdf,Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?,Ming-wei Chang,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Pranav Aggarwal,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Hareesh Ravi,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Naveen Marri,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Sachin Kelkar,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Fengbin Chen,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Vinh Khuc,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Midhun Harikumar,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Ritiz Tambi,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Sudharshan Reddy Kakumanu,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Purvak Lapsiya,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Alvin Ghouas,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Sarah Saber,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Malavika Ramprasad,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Baldo Faieta,,0%
https://arxiv.org/pdf/2302.11710.pdf,Controlled and Conditional Text to Image Generation with Diffusion Prior,Ajinkya Kale,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Sihan Xu,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Zelong Jiang,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Ruisi Liu,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Kaikai Yang,,0%
https://arxiv.org/pdf/2302.11705.pdf,ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder,Zhijie Huang,,0%
https://arxiv.org/pdf/2302.11703.pdf,fAIlureNotes: Supporting Designers in Understanding the Limits of AI Models for Computer Vision Tasks,Steven Moore,steven.moore@tum.de,95%
https://arxiv.org/pdf/2302.11703.pdf,fAIlureNotes: Supporting Designers in Understanding the Limits of AI Models for Computer Vision Tasks,Q. Vera Liao,veraliao@microsoft.com,78%
https://arxiv.org/pdf/2302.11703.pdf,fAIlureNotes: Supporting Designers in Understanding the Limits of AI Models for Computer Vision Tasks,Hariharan Subramonyam,harihars@stanford.edu,60%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Yi Ru Wang,yiruwang@cs.washington.edu,95%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Yuchi Zhao,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Haoping Xu,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Saggi Eppel,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Alan Aspuru-guzik,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Florian Shkurti,,0%
https://arxiv.org/pdf/2302.11683.pdf,MVTrans: Multi-View Perception of Transparent Objects,Animesh Garg,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Chen Guo,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Tianjian Jiang,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Xu Chen,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Jie Song,,0%
https://arxiv.org/pdf/2302.11566.pdf,Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,Otmar Hilliges,,0%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Adriano Koshiyama,adriano.koshiyama@holisticai.com,95%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Sara Zannone,sara.zannone@holisticai.com,95%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Cristian Muñoz,cristian.munoz@holisticai.com,95%
https://arxiv.org/pdf/2302.11562.pdf,Uncovering Bias in Face Generation Models,Umar Mohammed,,0%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Ibrahim Elwarfalli,ieelwarfalli@mix.wvu.edu,82%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Md Shamimul Islam,shamimul435@gmail.com,75%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,A. J. M. Akhtarujjaman Joha,ajmjohamiu@gmail.com,82%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Sohaib Abdullah,sohaib.abdullah2010@gmail.com,95%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Md Nur Hossain,,0%
https://arxiv.org/pdf/2302.11559.pdf,Word level Bangla Sign Language Dataset for Continuous BSL Recognition,Md Mahedi Hasan,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Chaoyi Wu,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Xiaoman Zhang,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Yanfeng Wang,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Ya Zhang,,0%
https://arxiv.org/pdf/2302.11557.pdf,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,Weidi Xie,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Yilun Du,yilundu@mit.edu,95%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Conor Durkan,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Robin Strudel,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Joshua B. Tenenbaum,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Sander Dieleman,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Rob Fergus,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Jascha Sohl-dickstein,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Arnaud Doucet,,0%
https://arxiv.org/pdf/2302.11552.pdf,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Will Grathwohl,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Fei Xia,xiafei@google.com,95%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Tianhe Yu,tianheyu@google.com,95%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Ted Xiao,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Austin Stone,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Jonathan Tompson,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Anthony Brohan,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Su Wang,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Jaspiar Singh,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Clayton Tan,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Dee M,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Jodilyn Peralta,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Brian Ichter,,0%
https://arxiv.org/pdf/2302.11550.pdf,Scaling Robot Learning with Semantically Imagined Experience,Karol Hausman,,0%
https://arxiv.org/pdf/2302.11527.pdf,A study on the invariance in security whatever the dimension of images for the steganalysis by deep-learning,Kévin Planolles,,0%
https://arxiv.org/pdf/2302.11527.pdf,A study on the invariance in security whatever the dimension of images for the steganalysis by deep-learning,Marc Chaumont,,0%
https://arxiv.org/pdf/2302.11527.pdf,A study on the invariance in security whatever the dimension of images for the steganalysis by deep-learning,Frédéric Comby,,0%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Subir Kumar Saha,saha@mech.iitd.ac.in,82%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Deepak Raina,deepak.raina@mech.iitd.ac.in,95%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Kashish Verma,vermakashish888@gmail.com,95%
https://arxiv.org/pdf/2302.11524.pdf,Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation,Sh Chandrashekhara,,0%
https://arxiv.org/pdf/2302.11522.pdf,Evaluation of Extra Pixel Interpolation with Mask Processing for Medical Image Segmentation with Deep Learning,Olivier Rukundo,,0%
https://arxiv.org/pdf/2302.11517.pdf,A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection,Wei Tang,,0%
https://arxiv.org/pdf/2302.11517.pdf,A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection,Kangning Cui,,0%
https://arxiv.org/pdf/2302.11517.pdf,A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection,Raymond H. Chan,,0%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Vishwa S. Parekh,vparekh@som.umaryland.edu,82%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Samson Zhou,samsonzhou@gmail.com,95%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Michael A. Jacobs,Michael.A.Jacobs@uth.tmc.edu,95%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Guangyao Zheng,,0%
https://arxiv.org/pdf/2302.11510.pdf,Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging,Vladimir Braverman,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Pranav Kadam,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Hardik Prajapati,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Min Zhang,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Jintang Xue,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,Shan Liu,,0%
https://arxiv.org/pdf/2302.11506.pdf,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,C. -c. Jay Kuo,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Ngoc Long Nguyen,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Jérémy Anger,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Lara Raad,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Bruno Galerne,,0%
https://arxiv.org/pdf/2302.11494.pdf,On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution,Gabriele Facciolo,,0%
https://arxiv.org/pdf/2302.11488.pdf,"Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers",Pranav Jeevan,,0%
https://arxiv.org/pdf/2302.11488.pdf,"Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers",Nikhil Cherian Kurian,,0%
https://arxiv.org/pdf/2302.11488.pdf,"Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers",Amit Sethi,,0%
https://arxiv.org/pdf/2302.11481.pdf,Transformer-Based Sensor Fusion for Autonomous Driving: A Survey,Apoorv Singh,,0%
https://arxiv.org/pdf/2302.11472.pdf,Distilling Calibrated Student from an Uncalibrated Teacher,Ishan Mishra,mishra.10@iitj.ac.in,78%
https://arxiv.org/pdf/2302.11472.pdf,Distilling Calibrated Student from an Uncalibrated Teacher,Deepak Mishra,dmishra@iitj.ac.in,82%
https://arxiv.org/pdf/2302.11472.pdf,Distilling Calibrated Student from an Uncalibrated Teacher,Sethu Vamsi Krishna,,0%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Linqi Song,linqi.song@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Shiqi Wang,shiqwang@cityu.edu.hk,82%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Hanwei Zhu,hanwei.zhu@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Lingyu Zhu,lingyzhu-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Wenhan Yang,yangwh@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.11464.pdf,Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,Baoliang Chen,blchen6-c@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Wanli Ouyang,wanli.ouyang@sydney.edu.au,95%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Haiyang Yang,hyyang@smail.nju.edu.cn,82%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Donglian Qi,qidl@zju.edu.cn,78%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Lei Bai,baisanshi@gmail.com,78%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Rui Zhao,zhaorui@sensetime.com,95%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Shixiang Tang,stan3906@uni.sydney.edu.au,65%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Feng Zhu,zhufeng@sensetime.com,95%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Meilin Chen,merlinis@zju.edu.cn,60%
https://arxiv.org/pdf/2302.11461.pdf,Saliency Guided Contrastive Learning on Scene Images,Yizhou Wang,yizhouwang@zju.edu.cn,95%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Dongheui Lee,dongheui.lee@tuwien.ac.at,95%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Manuel Stoiber,firstname.lastname@dlr.de,78%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Mariam Elsayed,,0%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Anne E. Reichert,,0%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Florian Steidle,,0%
https://arxiv.org/pdf/2302.11458.pdf,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,Rudolph Triebel,,0%
https://arxiv.org/pdf/2302.11446.pdf,Singular value decomposition based matrix surgery,Sabah Jassim,sabah.jassim@buckingham.ac.uk,95%
https://arxiv.org/pdf/2302.11446.pdf,Singular value decomposition based matrix surgery,Jehan Ghafuri,,0%
https://arxiv.org/pdf/2302.11427.pdf,Enhanced Face Authentication With Separate Loss Functions,Anh-kiet Duong,,0%
https://arxiv.org/pdf/2302.11427.pdf,Enhanced Face Authentication With Separate Loss Functions,Hoang-lan Nguyen,,0%
https://arxiv.org/pdf/2302.11427.pdf,Enhanced Face Authentication With Separate Loss Functions,Toan-thinh Truong,,0%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Guanbin Li,bin@mail.sysu.edu.cn,90%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Xiang Wan,wanxiang@sribd.cn,95%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Wei Lou,weilou@link.cuhk.edu.cn,95%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Feng Gao,gaof57@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Chenghang Li,cli136@connect.hkust-gz.edu.cn,82%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Xiaoying Lou,,0%
https://arxiv.org/pdf/2302.11416.pdf,Structure Embedded Nucleus Classification for Histopathology Images,Haofeng Li,,0%
https://arxiv.org/pdf/2302.11413.pdf,Gradient Adjusting Networks for Domain Inversion,Erez Sheffi,,0%
https://arxiv.org/pdf/2302.11413.pdf,Gradient Adjusting Networks for Domain Inversion,Michael Rotman,,0%
https://arxiv.org/pdf/2302.11413.pdf,Gradient Adjusting Networks for Domain Inversion,Lior Wolf,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Minzhou Pan,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Yi Zeng,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Lingjuan Lyu,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Xue Lin,,0%
https://arxiv.org/pdf/2302.11408.pdf,ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Ruoxi Jia,,0%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Zhenguo Li,li.zhenguo@huawei.com,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Guansong Lu,luguansong@huawei.com,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Yikai Wang,yikaiwang19@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Jianan Wang,jawang19@fudan.edu.cn,82%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Hang Xu,xu.hang@huawei.com,95%
https://arxiv.org/pdf/2302.11383.pdf,Entity-Level Text-Guided Image Manipulation,Wei Zhang,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Paolo Panicucci,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Jérémy Lebreton,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Roland Brochard,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Emmanuel Zenou,,0%
https://arxiv.org/pdf/2302.11364.pdf,Vision-Based Estimation of Small Body Rotational State during the Approach Phase,Michel Delpech,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Ahmed Khan,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Minoru Kuribayashi,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Koksheik Wong,,0%
https://arxiv.org/pdf/2302.11361.pdf,HDR image watermarking using saliency detection and quantization index modulation,Vishnu Monn Baskaran,,0%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Wei Yi,kussoyi@gmail.com,78%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Haiyi Mao,maohaiyi@std.uestc.edu.cn,95%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Jinping Tang,jptang@std.uestc.edu.cn,82%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Cong Peng,,0%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Yue Liu,,0%
https://arxiv.org/pdf/2302.11356.pdf,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,Hua Peng,,0%
https://arxiv.org/pdf/2302.11352.pdf,X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation,Marcel Worring,m.worring@uva.nl,82%
https://arxiv.org/pdf/2302.11352.pdf,X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation,Tom Van Sonsbeek,t.j.vansonsbeek@uva.nl,82%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Sangnie Bhardwaj,sangnie@google.com,85%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Willie Mcclinton,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Tongzhou Wang,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Guillaume Lajoie,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Chen Sun,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Phillip Isola,,0%
https://arxiv.org/pdf/2302.11349.pdf,Steerable Equivariant Representation Learning,Dilip Krishnan,,0%
https://arxiv.org/pdf/2302.11327.pdf,A Gradient Boosting Approach for Training Convolutional and Deep Neural Networks,Gonzalo Martínez-muñoz,gonzalo.martinez@uam.es,85%
https://arxiv.org/pdf/2302.11327.pdf,A Gradient Boosting Approach for Training Convolutional and Deep Neural Networks,Seyedsaman Emami,emami.seyedsaman@uam.es,95%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Chengxi Zeng,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Xinyu Yang,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,David Smithard,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Majid Mirmehdi,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Alberto M Gambaruto,,0%
https://arxiv.org/pdf/2302.11325.pdf,Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation,Tilo Burghardt,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Yibing Song,yibingsong.cv@gmail.com,95%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Hongyu Liu,hliudq@cse.ust.hk,82%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Xintong Han,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Chengbin Jin,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Lihui Qian,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Huawei Wei,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Zhe Lin,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Faqiang Wang,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Haoye Dong,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Jia Xu,,0%
https://arxiv.org/pdf/2302.11306.pdf,Human MotionFormer: Transferring Human Motions with Vision Transformers,Qifeng Chen,,0%
https://arxiv.org/pdf/2302.11301.pdf,View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation,Xu Zhao,zhaoxu@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.11301.pdf,View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation,Xiaoyue Wan,,0%
https://arxiv.org/pdf/2302.11301.pdf,View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation,Zhuo Chen,,0%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Gen Luo,luogen@stu.xmu.edu.cn,95%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Xiaoshuai Sun,xssun@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Yiyi Zhou,zhouyiyi@xmu.edu.cn,95%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Rongrong Ji,rrji@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.11299.pdf,Towards End-to-end Semi-supervised Learning for One-stage Object Detection,Lei Jin,,0%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Jingxiang Qu,qujx@whut.edu.cn,78%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Yisheng Lv,yisheng.lv@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Fenghua Zhu,fenghua.zhu@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Yuxu Lu,yuxulu@whut.edu.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Yu Guo,yuguo@whut.edu.cn,95%
https://arxiv.org/pdf/2302.11283.pdf,Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways,Ryan Wen Liu,wenliu@whut.edu.cn,78%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Meng Liu,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Kong Aik Lee,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Longbiao Wang,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Hanyi Zhang,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Chang Zeng,,0%
https://arxiv.org/pdf/2302.11254.pdf,Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification,Jianwu Dang,,0%
https://arxiv.org/pdf/2302.11252.pdf,Focusing On Targets For Improving Weakly Supervised Visual Grounding,Viet-quoc Pham,,0%
https://arxiv.org/pdf/2302.11252.pdf,Focusing On Targets For Improving Weakly Supervised Visual Grounding,Nao Mishima,,0%
https://arxiv.org/pdf/2302.11244.pdf,Considering Layerwise Importance in the Lottery Ticket Hypothesis,Benjamin Vandersmissen,benjamin.vandersmissen@uantwerpen.be,95%
https://arxiv.org/pdf/2302.11244.pdf,Considering Layerwise Importance in the Lottery Ticket Hypothesis,Jose Oramas,,0%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Soravit Changpinyo,schangpi@google.com,90%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Vittorio Ferrari,vittoferrari@google.com,82%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Jordi Pont-tuset,jponttuset@google.com,65%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Radu Soricut,rsoricut@google.com,82%
https://arxiv.org/pdf/2302.11217.pdf,Connecting Vision and Language with Video Localized Narratives,Paul Voigtlaender,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Shannan Guan,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Xin Yu,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Wei Huang,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Gengfa Fang,,0%
https://arxiv.org/pdf/2302.12007.pdf,DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition,Haiyan Lu,,0%
https://arxiv.org/pdf/2302.11208.pdf,KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer,Norimichi Ukita,ukita@toyota-ti.ac.jp,78%
https://arxiv.org/pdf/2302.11208.pdf,KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer,Kaikai Zhao,zhaokaikai@toyota-ti.ac.jp,95%
https://arxiv.org/pdf/2302.11200.pdf,Semi-Supervised Segmentation of Multi-vendor and Multi-center Cardiac MRI using Histogram Matching,Ilkay Oksuz,oksuzilkay@itu.edu.tr,95%
https://arxiv.org/pdf/2302.11200.pdf,Semi-Supervised Segmentation of Multi-vendor and Multi-center Cardiac MRI using Histogram Matching,Mahyar Bolhassani,bolhassani19@itu.edu.tr,78%
https://arxiv.org/pdf/2302.11184.pdf,A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning,Guang Yang,g.yang@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.11184.pdf,A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning,Jin Zhu,zhujin1121@gmail.com,95%
https://arxiv.org/pdf/2302.11184.pdf,A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning,Pietro Lio,,0%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Dejan Vucinic,dejan.vucinic@wdc.com,95%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Jaco Hofmann,jaco.hofmann@wdc.com,95%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Minghai Qin,minghai.qin@wdc.com,95%
https://arxiv.org/pdf/2302.11180.pdf,DISCO: Distributed Inference with Sparse Communications,Chao Sun,chao.sun@wdc.com,95%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Hexiang Hu,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Yi Luan,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Yang Chen,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Urvashi Khandelwal,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Mandar Joshi,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Kenton Lee,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Kristina Toutanova,,0%
https://arxiv.org/pdf/2302.11154.pdf,Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,Ming-wei Chang,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Zhenghua Xu,zhenghua.xu@hebut.edu.cn,95%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Hexiang Zhang,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Dan Yao,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Shuo Zhang,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Junyang Chen,,0%
https://arxiv.org/pdf/2302.11106.pdf,Multi-Head Feature Pyramid Networks for Breast Mass Detection,Thomas Lukasiewicz,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Haiyu Wu,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Grace Bezold,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Aman Bhatta,,0%
https://arxiv.org/pdf/2302.11102.pdf,Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning,Kevin W. Bowyer,,0%
https://arxiv.org/pdf/2302.11097.pdf,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,Fei Yin,fyin@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.11097.pdf,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,Ming-liang Zhang,zhangmingliang2018@ia.ac.cn,95%
https://arxiv.org/pdf/2302.11097.pdf,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,Cheng-lin Liu,liucl@nlpr.ia.ac.cn,78%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Quanjin Liu,liuquanjin@aqnu.edu.cn,95%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Yu Ren,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Guoli Wang,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Pingping Wang,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Kunmeng Liu,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Hongfu Sun,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Xiang Li,,0%
https://arxiv.org/pdf/2302.11095.pdf,MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network,Benzheng Wei,,0%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Yifei Zhou,zhou@berkeley.edu,78%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Ser-nam Lim,sernam@ucf.edu,85%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Juntao Ren,,0%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Fengyu Li,,0%
https://arxiv.org/pdf/2302.11084.pdf,Test-Time Distribution Normalization for Contrastively Learned Vision-language Models,Ramin Zabih,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Guoli Wang,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Pingping Wang,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Jinyu Cong,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Kunmeng Liu,,0%
https://arxiv.org/pdf/2302.11082.pdf,BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition,Benzheng Wei,,0%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Rob Timpe,rob@xwing.com,85%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Maxime Gariel,maxime@xwing.com,85%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Evan Wilson,evan@xwing.com,85%
https://arxiv.org/pdf/2302.11049.pdf,Framework for Certification of AI-Based Systems,Brian Shimanuki,brian@xwing.com,85%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Neil Chulpongsatorn,thobthai.chulpongsat@ucalgary.ca,75%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Aman Parnami,aman@iiitd.ac.in,85%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Ryo Suzuki,ryo.suzuki@ucalgary.ca,95%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Kyzyl Monteiro,kyzyl17296@iiitd.ac.in,85%
https://arxiv.org/pdf/2302.11046.pdf,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,Ritik Vatsal,ritik19321@iiitd.ac.in,85%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Labib Ahmed Siddique,1labib.ahmed.siddique@g.bracu.ac.bd,95%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Tanvir Rahman,5rtanvir@udel.edu,85%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Rabita Junhai,2rabita.junhai@g.bracu.ac.bd,95%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Tanzim Reza,3tanzim.reza@bracu.ac.bd,95%
https://arxiv.org/pdf/2302.11027.pdf,"Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms",Salman Sayeed Khan,4salman.sayeed@bracu.ac.bd,85%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Ramneet Kaur,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Xiayan Ji,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Souradeep Dutta,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Michele Caprio,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Yahan Yang,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Elena Bernardis,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Oleg Sokolsky,,0%
https://arxiv.org/pdf/2302.11019.pdf,Using Semantic Information for Defining and Detecting OOD Inputs,Insup Lee,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Uddeshya Upadhyay,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Jae Myung Kim,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Cordelia Schmidt,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Bernhard Schölkopf,,0%
https://arxiv.org/pdf/2302.11012.pdf,Likelihood Annealing: Fast Calibrated Uncertainty for Regression,Zeynep Akata,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Nikita Morozov,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Denis Rakitin,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Oleg Desheulin,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Dmitry Vetrov,,0%
https://arxiv.org/pdf/2302.10970.pdf,Differentiable Rendering with Reparameterized Volume Sampling,Kirill Struminsky,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Mahdi Ghafourian,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Bilgesu Sumer,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Ruben Vera-rodriguez,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Julian Fierrez,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Ruben Tolosana,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Aythami Moralez,,0%
https://arxiv.org/pdf/2302.10883.pdf,"Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions",Els Kindt,,0%
https://arxiv.org/pdf/2302.10873.pdf,Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,Ioannis Karamouzas,ioannis@clemson.edu,85%
https://arxiv.org/pdf/2302.10873.pdf,Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,Jean-bernard Hayet,jbhayet@cimat.mx,82%
https://arxiv.org/pdf/2302.10873.pdf,Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,Pei Xu,peix@clemson.edu,85%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Rafsanjany Kushol,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Collin C. Luk,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Avyarthana Dey,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Michael Benatar,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Hannah Briemberg,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Annie Dionne,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Nicolas Dupré,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Richard Frayne,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Angela Genge,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Summer Gibson,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Simon J. Graham,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Lawrence Korngut,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Peter Seres,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Robert C. Welsh,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Alan Wilman,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Lorne Zinman,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Sanjay Kalra,,0%
https://arxiv.org/pdf/2302.10859.pdf,SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,Yee-hong Yang,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Peichao Li,peichao.2.li@kcl.ac.uk,95%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Muhammad Asad,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Conor Horgan,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Oscar Maccormac,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Jonathan Shapey,,0%
https://arxiv.org/pdf/2302.10927.pdf,Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging,Tom Vercauteren,,0%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Paolo Fiorini,paolo.ﬁorini@univr.it,95%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Nicolas Padoy,npadoy@unistra.fr,82%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Pietro Mascagni,p.mascagni@unistra.fr,82%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Tong Yu,tyu@unistra.fr,82%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Diego Dall'alba,diego.dallalba@univr.it,85%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Sanat Ramesh,sanat.ramesh@univr.it,95%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Cristians Gonzalez,,0%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Didier Mutter,,0%
https://arxiv.org/pdf/2302.10834.pdf,Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition,Jacques Marescaux,,0%
https://arxiv.org/pdf/2302.10820.pdf,Device Tuning for Multi-Task Large Model,Penghao Jiang,,0%
https://arxiv.org/pdf/2302.10820.pdf,Device Tuning for Multi-Task Large Model,Xuanchen Hou,,0%
https://arxiv.org/pdf/2302.10820.pdf,Device Tuning for Multi-Task Large Model,Yinsi Zhou,,0%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Jiahao Zhu,jiahaozhu@hust.edu.cn,95%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Zeyu Xiong,zeyuxiong@hust.edu.cn,95%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Daizong Liu,dzliu@stu.pku.edu.cn,82%
https://arxiv.org/pdf/2302.10813.pdf,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,Pan Zhou,panzhou@hust.edu.cn,95%
https://arxiv.org/pdf/2302.10808.pdf,Bokeh Rendering Based on Adaptive Depth Calibration Network,Yuhan Dong,dongyuhan@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.10808.pdf,Bokeh Rendering Based on Adaptive Depth Calibration Network,Lei Zhou,leizhou.astro@gmail.com,95%
https://arxiv.org/pdf/2302.10808.pdf,Bokeh Rendering Based on Adaptive Depth Calibration Network,Lu Liu,l-liu20@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Zhengyuan Yang,zhengyang@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Nan Duan,nanduan@microsoft.com,95%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Lijuan Wang,lijuanw@microsoft.com,85%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Minheng Ni,t-mni@microsoft.com,78%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Linjie Li,Lindsey.Li@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Shengming Yin,v-sheyin@microsoft.com,78%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Zicheng Liu,zliu@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Jianfeng Wang,jianfw@microsoft.com,81%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Chenfei Wu,chewu@microsoft.com,82%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Fan Yang,fanyang@microsoft.com,95%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Xiaodong Wang,,0%
https://arxiv.org/pdf/2302.10781.pdf,Learning 3D Photography Videos via Self-supervised Diffusion on Single Images,Yuejian Fang,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Timilehin B. Aderinola,timi.aderinola@insight-centre.org,82%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Hananeh Younesian,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Darragh Whelan,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Brian Caulfield,,0%
https://arxiv.org/pdf/2302.10749.pdf,Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone,Georgiana Ifrim,,0%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Víctor M. Brea,victor.brea@usc.es,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Saqib Nazir,saqib.nazir@upb.ro,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Manuel Mucientes,manuel.mucientes@usc.es,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Lorenzo Vaquero,lorenzo.vaquero.otal@usc.es,95%
https://arxiv.org/pdf/2302.10730.pdf,Depth Estimation and Image Restoration by Deep Learning from Defocused Images,Daniela Coltuc,daniela.coltuc@upb.ro,95%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Leonardo Rossi,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Vittorio Bernuzzi,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Tomaso Fontanini,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Massimo Bertozzi,,0%
https://arxiv.org/pdf/2302.10719.pdf,Memory-augmented Online Video Anomaly Detection,Andrea Prati,,0%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Jingxing Zhou,jingxing.zhou@porsche-engineering.de,95%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Niket Ahuja,niket.ahuja@porsche-engineering.de,95%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Tobias Kalb,tobias.kalb@porsche-engineering.de,95%
https://arxiv.org/pdf/2302.10718.pdf,Effects of Architectures on Continual Semantic Segmentation,Jürgen Beyerer,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Huaping Liu,hpliu@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Yuhong Deng,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Xiaofeng Guo,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Yixuan Wei,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Kai Lu,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Bin Fang,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Di Guo,,0%
https://arxiv.org/pdf/2302.10717.pdf,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,Fuchun Sun,,0%
https://arxiv.org/pdf/2302.10698.pdf,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,Orcun Goksel,orcun.goksel@it.uu.se,95%
https://arxiv.org/pdf/2302.10698.pdf,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,Lin Zhang,,0%
https://arxiv.org/pdf/2302.10698.pdf,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,Tiziano Portenier,,0%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Peng Chen,chenpeng@zjut.edu.cn,95%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Binwei Xu,xubinwei@zjut.edu.cn,95%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Weihua Gong,whgong@zjut.edu.cn,82%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Ronghua Liang,rhliang@zjut.edu.cn,82%
https://arxiv.org/pdf/2302.10697.pdf,A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection,Haoran Liang,haoran@zjut.edu.cn,85%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Zhijie Deng,zhijied@sjtu.edu.cn,85%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Tianyu Pang,tianyupang@sea.com,95%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Shuicheng Yan,yansc@sea.com,78%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Chao Du,duchao@sea.com,95%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Cheng Lu,lucheng.lc15@gmail.com,95%
https://arxiv.org/pdf/2302.10688.pdf,On Calibrating Diffusion Probabilistic Models,Min Lin,linmin@sea.com,95%
https://arxiv.org/pdf/2302.10685.pdf,Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes,Zhaofei Yu,yuzf12@pku.edu.cn,78%
https://arxiv.org/pdf/2302.10685.pdf,Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes,Zecheng Hao,,0%
https://arxiv.org/pdf/2302.10685.pdf,Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes,Jianhao Ding,,0%
https://arxiv.org/pdf/2302.10685.pdf,Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes,Tong Bu,,0%
https://arxiv.org/pdf/2302.10685.pdf,Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes,Tiejun Huang,,0%
https://arxiv.org/pdf/2302.10679.pdf,Evaluating the effect of data augmentation and BALD heuristics on distillation of Semantic-KITTI dataset,Alexandre Almin,.lastname@nsvya.tech,65%
https://arxiv.org/pdf/2302.10679.pdf,Evaluating the effect of data augmentation and BALD heuristics on distillation of Semantic-KITTI dataset,Anh Duong,,0%
https://arxiv.org/pdf/2302.10679.pdf,Evaluating the effect of data augmentation and BALD heuristics on distillation of Semantic-KITTI dataset,Léo Lemarié,,0%
https://arxiv.org/pdf/2302.10679.pdf,Evaluating the effect of data augmentation and BALD heuristics on distillation of Semantic-KITTI dataset,B Ravi Kiran,,0%
https://arxiv.org/pdf/2302.10668.pdf,$PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction,Andrea Vedaldi,vedaldi@robots.ox.ac.uk,78%
https://arxiv.org/pdf/2302.10668.pdf,$PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction,Luke Melas-kyriazi,lukemk@robots.ox.ac.uk,85%
https://arxiv.org/pdf/2302.10668.pdf,$PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction,Christian Rupprecht,chrisr@robots.ox.ac.uk,81%
https://arxiv.org/pdf/2302.10663.pdf,RealFusion: 360° Reconstruction of Any Object from a Single Image,Christian Rupprecht,chrisr@robots.ox.ac.uk,81%
https://arxiv.org/pdf/2302.10663.pdf,RealFusion: 360° Reconstruction of Any Object from a Single Image,Andrea Vedaldi,vedaldi@robots.ox.ac.uk,78%
https://arxiv.org/pdf/2302.10663.pdf,RealFusion: 360° Reconstruction of Any Object from a Single Image,Luke Melas-kyriazi,lukemk@robots.ox.ac.uk,85%
https://arxiv.org/pdf/2302.10663.pdf,RealFusion: 360° Reconstruction of Any Object from a Single Image,Iro Laina,iro@robots.ox.ac.uk,85%
https://arxiv.org/pdf/2302.10661.pdf,Clinically Acceptable Segmentation of Organs at Risk in Cervical Cancer Radiation Treatment from Clinically Available Annotations,Dustin Van Weersel,dustin@xomnia.com,85%
https://arxiv.org/pdf/2302.10661.pdf,Clinically Acceptable Segmentation of Organs at Risk in Cervical Cancer Radiation Treatment from Clinically Available Annotations,Henrike Westerveld,g.westerveld@erasmusmc.nl,78%
https://arxiv.org/pdf/2302.10661.pdf,Clinically Acceptable Segmentation of Organs at Risk in Cervical Cancer Radiation Treatment from Clinically Available Annotations,Monika Grewal,monika.grewal@cwi.nl,95%
https://arxiv.org/pdf/2302.10661.pdf,Clinically Acceptable Segmentation of Organs at Risk in Cervical Cancer Radiation Treatment from Clinically Available Annotations,Peter A. N. Bosman,peter.bosman@cwi.nl,95%
https://arxiv.org/pdf/2302.10661.pdf,Clinically Acceptable Segmentation of Organs at Risk in Cervical Cancer Radiation Treatment from Clinically Available Annotations,Tanja Alderliesten,,0%
https://arxiv.org/pdf/2302.10645.pdf,BrackishMOT: The Brackish Multi-Object Tracking Dataset,Malte Pedersen,,0%
https://arxiv.org/pdf/2302.10645.pdf,BrackishMOT: The Brackish Multi-Object Tracking Dataset,Daniel Lehotský,,0%
https://arxiv.org/pdf/2302.10645.pdf,BrackishMOT: The Brackish Multi-Object Tracking Dataset,Ivan Nikolov,,0%
https://arxiv.org/pdf/2302.10645.pdf,BrackishMOT: The Brackish Multi-Object Tracking Dataset,Thomas B. Moeslund,,0%
https://arxiv.org/pdf/2302.10641.pdf,A3S: Adversarial learning of semantic representations for Scene-Text Spotting,Masato Fujitake,fujitake@fastaccounting.co.jp,78%
https://arxiv.org/pdf/2302.10635.pdf,Semantic Segmentation of Urban Textured Meshes Through Point Sampling,Grégoire Grzeczkowicz,Gregoire.Grzeczkowicz@ign.fr,95%
https://arxiv.org/pdf/2302.10635.pdf,Semantic Segmentation of Urban Textured Meshes Through Point Sampling,Bruno Vallet,,0%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Riccardo Munafò,riccardo.munafo@polimi.it,95%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Giacomo Ingallina,ingallina.giacomo@hsr.it,95%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Francesco Maisano,francesco.maisano@hsr.it,95%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Simone Saitta,simone.saitta@polimi.it,95%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Paolo Denti,paolo.denti@hsr.it,95%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Emiliano Votta,emiliano.votta@polimi.it,95%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Alberto Redaelli,alberto.redaelli@polimi.it,95%
https://arxiv.org/pdf/2302.10634.pdf,A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis From 3D Echocardiography,Eustachio Agricola,eustachio.agricola@hsr.it,95%
https://arxiv.org/pdf/2302.10630.pdf,LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring,Hongming Shan,hmshan@fudan.edu.cn,82%
https://arxiv.org/pdf/2302.10630.pdf,LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring,Qi Gao,qgao21@m.fudan.edu.cn,82%
https://arxiv.org/pdf/2302.10630.pdf,LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring,Chuang Niu,niuc@rpi.edu,78%
https://arxiv.org/pdf/2302.10630.pdf,LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring,Ge Wang,wangg6@rpi.edu,78%
https://arxiv.org/pdf/2302.10630.pdf,LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring,Zhihao Chen,zhihaochen21@m.fudan.edu.cn,95%
https://arxiv.org/pdf/2302.10624.pdf,Self-improving object detection via disagreement reconciliation,Lorenzo Natale,name.surname@iit.it,70%
https://arxiv.org/pdf/2302.10624.pdf,Self-improving object detection via disagreement reconciliation,Gianluca Scarpellini,,0%
https://arxiv.org/pdf/2302.10624.pdf,Self-improving object detection via disagreement reconciliation,Stefano Rosa,,0%
https://arxiv.org/pdf/2302.10624.pdf,Self-improving object detection via disagreement reconciliation,Pietro Morerio,,0%
https://arxiv.org/pdf/2302.10624.pdf,Self-improving object detection via disagreement reconciliation,Alessio Del Bue,,0%
https://arxiv.org/pdf/2302.10586.pdf,Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels,Jiacheng Sun,sunjiacheng1@huawei.com,95%
https://arxiv.org/pdf/2302.10586.pdf,Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels,Yong Zhong,yongzhong@ruc.edu.cn,95%
https://arxiv.org/pdf/2302.10586.pdf,Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels,Zebin You,zebin@ruc.edu.cn,85%
https://arxiv.org/pdf/2302.10586.pdf,Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels,Chongxuan Li,chongxuanli@ruc.edu.cn,95%
https://arxiv.org/pdf/2302.10586.pdf,Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels,Fan Bao,,0%
https://arxiv.org/pdf/2302.10586.pdf,Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels,Jun Zhu,,0%
https://arxiv.org/pdf/2302.10574.pdf,MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis,Weiqin Zhao,wqzhao98@connect.hku.hk,82%
https://arxiv.org/pdf/2302.10574.pdf,MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis,Lequan Yu,lqyu@hku.hk,82%
https://arxiv.org/pdf/2302.10574.pdf,MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis,Tianye Niu,niuty@szbl.ac.cn,78%
https://arxiv.org/pdf/2302.10574.pdf,MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis,Maximus Yeung,mcfyeung@pathology.hku.hk,82%
https://arxiv.org/pdf/2302.10574.pdf,MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis,Shujun Wang,,0%
https://arxiv.org/pdf/2302.10549.pdf,MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,Zizhang Wu,,0%
https://arxiv.org/pdf/2302.10549.pdf,MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,Yuanzhu Gan,,0%
https://arxiv.org/pdf/2302.10549.pdf,MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,Lei Wang,,0%
https://arxiv.org/pdf/2302.10549.pdf,MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,Guilian Chen,,0%
https://arxiv.org/pdf/2302.10549.pdf,MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,Jian Pu,,0%
https://arxiv.org/pdf/2302.10544.pdf,EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images,Chong Bao,chongbao@zju.edu.cn,95%
https://arxiv.org/pdf/2302.10544.pdf,EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images,Hujun Bao,baohujun@zju.edu.cn,95%
https://arxiv.org/pdf/2302.10544.pdf,EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images,Guofeng Zhang,zhangguofeng@zju.edu.cn,95%
https://arxiv.org/pdf/2302.10544.pdf,EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images,Haomin Liu,liuhaomin@sensetime.com,95%
https://arxiv.org/pdf/2302.10544.pdf,EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images,Zhichao Ye,,0%
https://arxiv.org/pdf/2302.10544.pdf,EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images,Xin Zhou,,0%
https://arxiv.org/pdf/2302.10523.pdf,I2V: Towards Texture-Aware Self-Supervised Blind Denoising using Self-Residual Learning for Real-World Images,Kanggeun Lee,leekanggeun@gmail.com,95%
https://arxiv.org/pdf/2302.10523.pdf,I2V: Towards Texture-Aware Self-Supervised Blind Denoising using Self-Residual Learning for Real-World Images,Won-ki Jeong,wkjeong@korea.ac.kr,82%
https://arxiv.org/pdf/2302.10523.pdf,I2V: Towards Texture-Aware Self-Supervised Blind Denoising using Self-Residual Learning for Real-World Images,Kyungryun Lee,,0%
https://arxiv.org/pdf/2302.10923.pdf,PointFISH -- learning point cloud representations for RNA localization patterns,Thomas Walter,Thomas.Walter@minesparis.psl.eu,95%
https://arxiv.org/pdf/2302.10923.pdf,PointFISH -- learning point cloud representations for RNA localization patterns,Arthur Imbert,Arthur.Imbert@minesparis.psl.eu,95%
https://arxiv.org/pdf/2302.10923.pdf,PointFISH -- learning point cloud representations for RNA localization patterns,Florian Mueller,,0%
https://arxiv.org/pdf/2302.10518.pdf,USR: Unsupervised Separated 3D Garment and Human Reconstruction via Geometry and Semantic Consistency,Jingyi Chai,chaijingyi@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.10518.pdf,USR: Unsupervised Separated 3D Garment and Human Reconstruction via Geometry and Semantic Consistency,Yuxuan Xiong,xiongyx@sjtu.edu.cn,78%
https://arxiv.org/pdf/2302.10518.pdf,USR: Unsupervised Separated 3D Garment and Human Reconstruction via Geometry and Semantic Consistency,Bingbing Ni,nibingbing@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.10518.pdf,USR: Unsupervised Separated 3D Garment and Human Reconstruction via Geometry and Semantic Consistency,Yue Shi,shiyue001@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.10518.pdf,USR: Unsupervised Separated 3D Garment and Human Reconstruction via Geometry and Semantic Consistency,Wenjun Zhang,zhangwenjun@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.10511.pdf,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,Zizhang Wu,,0%
https://arxiv.org/pdf/2302.10511.pdf,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,Guilian Chen,,0%
https://arxiv.org/pdf/2302.10511.pdf,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,Yuanzhu Gan,,0%
https://arxiv.org/pdf/2302.10511.pdf,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,Lei Wang,,0%
https://arxiv.org/pdf/2302.10511.pdf,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,Jian Pu,,0%
https://arxiv.org/pdf/2302.10502.pdf,Learning Gradually Non-convex Image Priors Using Score Matching,Erich Kobler,erich.kobler@ukbonn.de,95%
https://arxiv.org/pdf/2302.10502.pdf,Learning Gradually Non-convex Image Priors Using Score Matching,Thomas Pock,,0%
https://arxiv.org/pdf/2302.10501.pdf,Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention,Haiyue Zhu,haiyue@simtech.a-star.edu.sg,85%
https://arxiv.org/pdf/2302.10501.pdf,Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention,Jiahui Wang,,0%
https://arxiv.org/pdf/2302.10501.pdf,Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention,Haoren Guo,,0%
https://arxiv.org/pdf/2302.10501.pdf,Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention,Abdullah Al Mamun,,0%
https://arxiv.org/pdf/2302.10501.pdf,Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention,Cheng Xiang,,0%
https://arxiv.org/pdf/2302.10501.pdf,Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention,Tong Heng Lee,,0%
https://arxiv.org/pdf/2302.10494.pdf,The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers,Jaeho Lee,jaeho.lee@postech.ac.kr,95%
https://arxiv.org/pdf/2302.10494.pdf,The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers,Namhoon Lee,namhoonlee@postech.ac.kr,95%
https://arxiv.org/pdf/2302.10494.pdf,The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers,Seungwoo Son,swson@postech.ac.kr,82%
https://arxiv.org/pdf/2302.10494.pdf,The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers,Jegwang Ryu,jegwang.ryu@postech.ac.kr,95%
https://arxiv.org/pdf/2302.10484.pdf,Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN,Juncheng Li,cvjunchengli@gmail.com,95%
https://arxiv.org/pdf/2302.10484.pdf,Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN,Dong Yue,medongy@vip.163.com,85%
https://arxiv.org/pdf/2302.10484.pdf,Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN,Jian Yang,csjyang@njust.edu.cn,78%
https://arxiv.org/pdf/2302.10484.pdf,Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN,Guangwei Gao,csggao@gmail.com,78%
https://arxiv.org/pdf/2302.10484.pdf,Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN,Huimin Lu,dr.huimin.lu@ieee.org,95%
https://arxiv.org/pdf/2302.10484.pdf,Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN,Guoan Xu,,0%
https://arxiv.org/pdf/2302.10481.pdf,LMPDNet: TOF-PET list-mode image reconstruction using model-based deep learning method,Chenxu Li,,0%
https://arxiv.org/pdf/2302.10481.pdf,LMPDNet: TOF-PET list-mode image reconstruction using model-based deep learning method,Rui Hu,,0%
https://arxiv.org/pdf/2302.10481.pdf,LMPDNet: TOF-PET list-mode image reconstruction using model-based deep learning method,Jianan Cui,,0%
https://arxiv.org/pdf/2302.10481.pdf,LMPDNet: TOF-PET list-mode image reconstruction using model-based deep learning method,Huafeng Liu,,0%
https://arxiv.org/pdf/2302.10480.pdf,Climate Model Driven Seasonal Forecasting Approach with Deep Learning,Alper Unal,alper.unal@itu.edu.tr,95%
https://arxiv.org/pdf/2302.10480.pdf,Climate Model Driven Seasonal Forecasting Approach with Deep Learning,Busra Asan,,0%
https://arxiv.org/pdf/2302.10480.pdf,Climate Model Driven Seasonal Forecasting Approach with Deep Learning,Ismail Sezen,,0%
https://arxiv.org/pdf/2302.10480.pdf,Climate Model Driven Seasonal Forecasting Approach with Deep Learning,Bugra Yesilkaynak,,0%
https://arxiv.org/pdf/2302.10480.pdf,Climate Model Driven Seasonal Forecasting Approach with Deep Learning,Yusuf Aydin,,0%
https://arxiv.org/pdf/2302.10480.pdf,Climate Model Driven Seasonal Forecasting Approach with Deep Learning,Mehmet Ilicak,,0%
https://arxiv.org/pdf/2302.10480.pdf,Climate Model Driven Seasonal Forecasting Approach with Deep Learning,Gozde Unal,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Kun Wang,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Zi Wang,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Zhang Li,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Ang Su,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Xichao Teng,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Erting Pan,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Minhao Liu,,0%
https://arxiv.org/pdf/2302.10473.pdf,Oriented object detection in optical remote sensing images using deep learning: a survey,Qifeng Yu,,0%
https://arxiv.org/pdf/2302.10465.pdf,A Flexible Multi-view Multi-modal Imaging System for Outdoor Scenes,Meng Zhang,zhangm20@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.10465.pdf,A Flexible Multi-view Multi-modal Imaging System for Outdoor Scenes,Jianjiang Feng,jfeng@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.10465.pdf,A Flexible Multi-view Multi-modal Imaging System for Outdoor Scenes,Jie Zhou,jzhou@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.10465.pdf,A Flexible Multi-view Multi-modal Imaging System for Outdoor Scenes,Yifan Chen,chenyf21@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.10465.pdf,A Flexible Multi-view Multi-modal Imaging System for Outdoor Scenes,Wenxuan Guo,,0%
https://arxiv.org/pdf/2302.10465.pdf,A Flexible Multi-view Multi-modal Imaging System for Outdoor Scenes,Bohao Fan,,0%
https://arxiv.org/pdf/2302.10463.pdf,Vision-based Multi-future Trajectory Prediction: A Survey,Flora Salim,flora.salim@unsw.edu.au,95%
https://arxiv.org/pdf/2302.10463.pdf,Vision-based Multi-future Trajectory Prediction: A Survey,Hao Xue,hao.xue@unsw.edu.au,95%
https://arxiv.org/pdf/2302.10463.pdf,Vision-based Multi-future Trajectory Prediction: A Survey,Renhao Huang,renhao.huang@unsw.edu.au,95%
https://arxiv.org/pdf/2302.10463.pdf,Vision-based Multi-future Trajectory Prediction: A Survey,Yang Song,song.yang1@unsw.edu.au,95%
https://arxiv.org/pdf/2302.10463.pdf,Vision-based Multi-future Trajectory Prediction: A Survey,Maurice Pagnucco,,0%
https://arxiv.org/pdf/2302.10450.pdf,Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information,Haris Vikalo,hvikalo@ece.utexas.edu,82%
https://arxiv.org/pdf/2302.10450.pdf,Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information,Marius Arvinte,arvinte@utexas.edu,78%
https://arxiv.org/pdf/2302.10450.pdf,Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information,Madhumitha Sakthi,,0%
https://arxiv.org/pdf/2302.10450.pdf,Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information,Ahmed Tewfik,,0%
https://arxiv.org/pdf/2302.10445.pdf,Graph-Transporter: A Graph-based Learning Method for Goal-Conditioned Deformable Object Rearranging Task,Xueqian Wang,wang.xq@sz.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.10445.pdf,Graph-Transporter: A Graph-based Learning Method for Goal-Conditioned Deformable Object Rearranging Task,Lipeng Chen,lipengchen@tencent.com,95%
https://arxiv.org/pdf/2302.10445.pdf,Graph-Transporter: A Graph-based Learning Method for Goal-Conditioned Deformable Object Rearranging Task,Yuhong Deng,francisdeng@tencent.com,78%
https://arxiv.org/pdf/2302.10445.pdf,Graph-Transporter: A Graph-based Learning Method for Goal-Conditioned Deformable Object Rearranging Task,Chongkun Xia,xiachongkun@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.10437.pdf,Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection,Surya Nepal,surya.nepal@data61.csiro.au,95%
https://arxiv.org/pdf/2302.10437.pdf,Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection,Chang Xu,c.xu@sydney.edu.au,82%
https://arxiv.org/pdf/2302.10437.pdf,Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection,Siqi Ma,siqi.ma@adfa.edu.au,95%
https://arxiv.org/pdf/2302.10437.pdf,Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection,Chuyang Zhou,,0%
https://arxiv.org/pdf/2302.10437.pdf,Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection,Jiajun Huang,,0%
https://arxiv.org/pdf/2302.10437.pdf,Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection,Daochang Liu,,0%
https://arxiv.org/pdf/2302.10437.pdf,Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection,Chengbin Du,,0%
https://arxiv.org/pdf/2302.10430.pdf,Interval Type-2 Fuzzy Neural Networks for Multi-Label Classification,Dayong Tian,,0%
https://arxiv.org/pdf/2302.10430.pdf,Interval Type-2 Fuzzy Neural Networks for Multi-Label Classification,Feifei Li,,0%
https://arxiv.org/pdf/2302.10430.pdf,Interval Type-2 Fuzzy Neural Networks for Multi-Label Classification,Yiwen Wei,,0%
https://arxiv.org/pdf/2302.10425.pdf,Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows,Jianqin Yin,jqyin@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.10425.pdf,Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows,Chao Qi,qichao@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.10425.pdf,Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows,Pengxiang Ding,dingpx2015@bupt.edu.cn,78%
https://arxiv.org/pdf/2302.10425.pdf,Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows,Jinghang Xu,,0%
https://arxiv.org/pdf/2302.10420.pdf,HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection,Chengxi Han,,0%
https://arxiv.org/pdf/2302.10420.pdf,HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection,Chen Wu,,0%
https://arxiv.org/pdf/2302.10420.pdf,HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection,Bo Du,,0%
https://arxiv.org/pdf/2302.10920.pdf,'The Taurus': Cattle Breeds & Diseases Identification Mobile Application using Machine Learning,Erandika Lakmali,surekhamaduhansi@gmail.com,75%
https://arxiv.org/pdf/2302.10920.pdf,'The Taurus': Cattle Breeds & Diseases Identification Mobile Application using Machine Learning,R. M. D. S. M. Chandrarathna,,0%
https://arxiv.org/pdf/2302.10920.pdf,'The Taurus': Cattle Breeds & Diseases Identification Mobile Application using Machine Learning,T. W. M. S. A. Weerasinghe,,0%
https://arxiv.org/pdf/2302.10920.pdf,'The Taurus': Cattle Breeds & Diseases Identification Mobile Application using Machine Learning,N. S. Madhuranga,,0%
https://arxiv.org/pdf/2302.10920.pdf,'The Taurus': Cattle Breeds & Diseases Identification Mobile Application using Machine Learning,T. M. L. S. Thennakoon,,0%
https://arxiv.org/pdf/2302.10920.pdf,'The Taurus': Cattle Breeds & Diseases Identification Mobile Application using Machine Learning,Anjalie Gamage,,0%
https://arxiv.org/pdf/2302.10414.pdf,Improving Scene Text Image Super-resolution via Dual Prior Modulation Network,Shipeng Zhu,shipengzhu@seu.edu.cn,95%
https://arxiv.org/pdf/2302.10414.pdf,Improving Scene Text Image Super-resolution via Dual Prior Modulation Network,Pengfei Fang,fangpengfei@seu.edu.cn,95%
https://arxiv.org/pdf/2302.10414.pdf,Improving Scene Text Image Super-resolution via Dual Prior Modulation Network,Hui Xue,hxue@seu.edu.cn,82%
https://arxiv.org/pdf/2302.10414.pdf,Improving Scene Text Image Super-resolution via Dual Prior Modulation Network,Zuoyan Zhao,zuoyanzhao@seu.edu.cn,95%
https://arxiv.org/pdf/2302.10413.pdf,CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization,Phi Le Nguyen,hieu.ph@vinuni.edu.vn,60%
https://arxiv.org/pdf/2302.10413.pdf,CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization,Truong Thao Nguyen,nguyen.truong@aist.go.jp,95%
https://arxiv.org/pdf/2302.10413.pdf,CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization,Nang Hung Nguyen,,0%
https://arxiv.org/pdf/2302.10413.pdf,CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization,Duc Long Nguyen,,0%
https://arxiv.org/pdf/2302.10413.pdf,CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization,Trong Bang Nguyen,,0%
https://arxiv.org/pdf/2302.10413.pdf,CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization,Thanh-hung Nguyen,,0%
https://arxiv.org/pdf/2302.10413.pdf,CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization,Huy Hieu Pham,,0%
https://arxiv.org/pdf/2302.10412.pdf,Non-pooling Network for medical image segmentation,Weihu Song,weihusong@buaa.edu.cn,95%
https://arxiv.org/pdf/2302.10412.pdf,Non-pooling Network for medical image segmentation,Heng Yu,,0%
https://arxiv.org/pdf/2302.10406.pdf,Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines,Xu Steven Xu,sxu@genmab.com,95%
https://arxiv.org/pdf/2302.10406.pdf,Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines,Hong Zhang,zhangh@ustc.edu.cn,78%
https://arxiv.org/pdf/2302.10406.pdf,Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines,Jitendra Jonnagaddala,jitendra.jonnagaddala@unsw.edu.au,95%
https://arxiv.org/pdf/2302.10406.pdf,Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines,Min Cen,,0%
https://arxiv.org/pdf/2302.10406.pdf,Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines,Xingyu Li,,0%
https://arxiv.org/pdf/2302.10406.pdf,Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines,Bangwei Guo,,0%
https://arxiv.org/pdf/2302.10396.pdf,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,Anh-dzung Doan,dzung.doan@adelaide.edu.au,95%
https://arxiv.org/pdf/2302.10396.pdf,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,Bach Long Nguyen,,0%
https://arxiv.org/pdf/2302.10396.pdf,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,Surabhi Gupta,,0%
https://arxiv.org/pdf/2302.10396.pdf,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,Ian Reid,,0%
https://arxiv.org/pdf/2302.10396.pdf,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,Markus Wagner,,0%
https://arxiv.org/pdf/2302.10396.pdf,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,Tat-jun Chin,,0%
https://arxiv.org/pdf/2302.10390.pdf,DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images,Ke Yu,yu.ke@pitt.edu,95%
https://arxiv.org/pdf/2302.10390.pdf,DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images,Li Sun,lisun@bu.edu,95%
https://arxiv.org/pdf/2302.10390.pdf,DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images,Junxiang Chen,,0%
https://arxiv.org/pdf/2302.10390.pdf,DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images,Max Reynolds,,0%
https://arxiv.org/pdf/2302.10390.pdf,DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images,Tigmanshu Chaudhary,,0%
https://arxiv.org/pdf/2302.10390.pdf,DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images,Kayhan Batmanghelich,,0%
https://arxiv.org/pdf/2302.10383.pdf,"On Interpretable Approaches to Cluster, Classify and Represent Multi-Subspace Data via Minimum Lossy Coding Length based on Rate-Distortion Theory",Kai-liang Lu,lukailiang@163.com,95%
https://arxiv.org/pdf/2302.10383.pdf,"On Interpretable Approaches to Cluster, Classify and Represent Multi-Subspace Data via Minimum Lossy Coding Length based on Rate-Distortion Theory",Avraham Chapman,avraham.chapman@adelaide.edu.au,95%
https://arxiv.org/pdf/2302.10343.pdf,Non-rigid Medical Image Registration using Physics-informed Neural Networks,Dean C. Barratt,d.barratt@ucl.ac.uk,82%
https://arxiv.org/pdf/2302.10343.pdf,Non-rigid Medical Image Registration using Physics-informed Neural Networks,Mark Emberton,m.emberton@ucl.ac.uk,82%
https://arxiv.org/pdf/2302.10343.pdf,Non-rigid Medical Image Registration using Physics-informed Neural Networks,Zeike A. Taylor,z.taylor@leeds.ac.uk,82%
https://arxiv.org/pdf/2302.10343.pdf,Non-rigid Medical Image Registration using Physics-informed Neural Networks,Shaheer U. Saeed,shaheer.saeed.17@ucl.ac.uk,95%
https://arxiv.org/pdf/2302.10343.pdf,Non-rigid Medical Image Registration using Physics-informed Neural Networks,Zachary M. C. Baum,zachary.baum.19@ucl.ac.uk,95%
https://arxiv.org/pdf/2302.10343.pdf,Non-rigid Medical Image Registration using Physics-informed Neural Networks,Zhe Min,z.min@ucl.ac.uk,82%
https://arxiv.org/pdf/2302.10343.pdf,Non-rigid Medical Image Registration using Physics-informed Neural Networks,Yipeng Hu,yipeng.hu@ucl.ac.uk,95%
https://arxiv.org/pdf/2302.10341.pdf,DC4L: Distribution Shift Recovery via Data-Driven Control for Deep Learning Models,Insup Lee,LEE@SEAS.UPENN.EDU,78%
https://arxiv.org/pdf/2302.10341.pdf,DC4L: Distribution Shift Recovery via Data-Driven Control for Deep Learning Models,Vivian Lin,VILIN@SEAS.UPENN.EDU,82%
https://arxiv.org/pdf/2302.10341.pdf,DC4L: Distribution Shift Recovery via Data-Driven Control for Deep Learning Models,Souradeep Dutta,DUTTASO@SEAS.UPENN.EDU,78%
https://arxiv.org/pdf/2302.10341.pdf,DC4L: Distribution Shift Recovery via Data-Driven Control for Deep Learning Models,Michele Caprio,CAPRIO@SEAS.UPENN.EDU,78%
https://arxiv.org/pdf/2302.10341.pdf,DC4L: Distribution Shift Recovery via Data-Driven Control for Deep Learning Models,Kuk Jin Jang,JANGKJ@SEAS.UPENN.EDU,78%
https://arxiv.org/pdf/2302.10341.pdf,DC4L: Distribution Shift Recovery via Data-Driven Control for Deep Learning Models,Oleg Sokolsky,SOKOLSKY@SEAS.UPENN.EDU,78%
https://arxiv.org/pdf/2302.10326.pdf,Unsupervised Out-of-Distribution Detection with Diffusion Inpainting,Zhenzhen Liu,,0%
https://arxiv.org/pdf/2302.10326.pdf,Unsupervised Out-of-Distribution Detection with Diffusion Inpainting,Jin Peng Zhou,,0%
https://arxiv.org/pdf/2302.10326.pdf,Unsupervised Out-of-Distribution Detection with Diffusion Inpainting,Yufan Wang,,0%
https://arxiv.org/pdf/2302.10326.pdf,Unsupervised Out-of-Distribution Detection with Diffusion Inpainting,Kilian Q. Weinberger,,0%
https://arxiv.org/pdf/2302.10318.pdf,Hadamard Layer to Improve Semantic Segmentation,Mariano Rivera,mrivera@cimat.mx,82%
https://arxiv.org/pdf/2302.10318.pdf,Hadamard Layer to Improve Semantic Segmentation,Angello Hoyos,angello.hoyos@cimat.mx,95%
https://arxiv.org/pdf/2302.10289.pdf,Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models,Shantanu Ghosh,,0%
https://arxiv.org/pdf/2302.10289.pdf,Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models,Ke Yu,,0%
https://arxiv.org/pdf/2302.10289.pdf,Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models,Forough Arabshahi,,0%
https://arxiv.org/pdf/2302.10289.pdf,Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models,Kayhan Batmanghelich,,0%
https://arxiv.org/pdf/2302.10279.pdf,Image Reconstruction via Deep Image Prior Subspaces,Riccardo Barbano,riccardo.barbano.19@ucl.ac.uk,95%
https://arxiv.org/pdf/2302.10279.pdf,Image Reconstruction via Deep Image Prior Subspaces,Javier Antorán,,0%
https://arxiv.org/pdf/2302.10279.pdf,Image Reconstruction via Deep Image Prior Subspaces,Johannes Leuschner,,0%
https://arxiv.org/pdf/2302.10279.pdf,Image Reconstruction via Deep Image Prior Subspaces,José Miguel Hernández-lobato,,0%
https://arxiv.org/pdf/2302.10279.pdf,Image Reconstruction via Deep Image Prior Subspaces,Bangti Jin,,0%
https://arxiv.org/pdf/2302.10279.pdf,Image Reconstruction via Deep Image Prior Subspaces,Željko Kereta,,0%
https://arxiv.org/pdf/2302.10266.pdf,Kernel function impact on convolutional neural networks,M. Amine Mahmoudi,mohamed.mahmoudi@univ-mascara.dz,82%
https://arxiv.org/pdf/2302.10266.pdf,Kernel function impact on convolutional neural networks,Aladine Chetouani,,0%
https://arxiv.org/pdf/2302.10266.pdf,Kernel function impact on convolutional neural networks,Fatma Boufera,,0%
https://arxiv.org/pdf/2302.10266.pdf,Kernel function impact on convolutional neural networks,Hedi Tabia,,0%
https://arxiv.org/pdf/2302.10260.pdf,"Unsupervised Learning on a DIET: Datum IndEx as Target Free of Self-Supervision, Reconstruction, Projector Head",Randall Balestriero,rbalestriero@meta.com,82%
https://arxiv.org/pdf/2302.10174.pdf,Towards Universal Fake Image Detectors that Generalize Across Generative Models,Utkarsh Ojha,,0%
https://arxiv.org/pdf/2302.10174.pdf,Towards Universal Fake Image Detectors that Generalize Across Generative Models,Yuheng Li,,0%
https://arxiv.org/pdf/2302.10174.pdf,Towards Universal Fake Image Detectors that Generalize Across Generative Models,Yong Jae Lee,,0%
https://arxiv.org/pdf/2302.10167.pdf,Cross-domain Compositing with Pretrained Diffusion Models,Roy Hachnochi,,0%
https://arxiv.org/pdf/2302.10167.pdf,Cross-domain Compositing with Pretrained Diffusion Models,Mingrui Zhao,,0%
https://arxiv.org/pdf/2302.10167.pdf,Cross-domain Compositing with Pretrained Diffusion Models,Nadav Orzech,,0%
https://arxiv.org/pdf/2302.10167.pdf,Cross-domain Compositing with Pretrained Diffusion Models,Rinon Gal,,0%
https://arxiv.org/pdf/2302.10167.pdf,Cross-domain Compositing with Pretrained Diffusion Models,Ali Mahdavi-amiri,,0%
https://arxiv.org/pdf/2302.10167.pdf,Cross-domain Compositing with Pretrained Diffusion Models,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2302.10167.pdf,Cross-domain Compositing with Pretrained Diffusion Models,Amit Haim Bermano,,0%
https://arxiv.org/pdf/2302.10164.pdf,Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts,Francesco Croce,,0%
https://arxiv.org/pdf/2302.10164.pdf,Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts,Sylvestre-alvise Rebuffi,,0%
https://arxiv.org/pdf/2302.10164.pdf,Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts,Evan Shelhamer,,0%
https://arxiv.org/pdf/2302.10164.pdf,Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts,Sven Gowal,,0%
https://arxiv.org/pdf/2302.10126.pdf,iQPP: A Benchmark for Image Query Performance Prediction,Radu Tudor Ionescu,raducu.ionescu@gmail.com,95%
https://arxiv.org/pdf/2302.10126.pdf,iQPP: A Benchmark for Image Query Performance Prediction,Josiane Mothe,josiane.mothe@irit.fr,95%
https://arxiv.org/pdf/2302.10126.pdf,iQPP: A Benchmark for Image Query Performance Prediction,Eduard Poesina,eduardgabriel.poe@gmail.com,85%
https://arxiv.org/pdf/2302.10109.pdf,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,Jiatao Gu,jiatao@apple.com,85%
https://arxiv.org/pdf/2302.10109.pdf,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,Lingjie Liu,lingjie.liu@seas.upenn.edu,95%
https://arxiv.org/pdf/2302.10109.pdf,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,Alex Trevithick,,0%
https://arxiv.org/pdf/2302.10109.pdf,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,Kai-en Lin,,0%
https://arxiv.org/pdf/2302.10109.pdf,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,Josh Susskind,,0%
https://arxiv.org/pdf/2302.10109.pdf,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,Christian Theobalt,,0%
https://arxiv.org/pdf/2302.10109.pdf,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,Ravi Ramamoorthi,,0%
https://arxiv.org/pdf/2302.10082.pdf,UAVStereo: A Multiple Resolution Dataset for Stereo Matching in UAV Scenarios,Cao Xuefeng,Feng@163.com,90%
https://arxiv.org/pdf/2302.10082.pdf,UAVStereo: A Multiple Resolution Dataset for Stereo Matching in UAV Scenarios,Zhang Xiaoyi,,0%
https://arxiv.org/pdf/2302.10082.pdf,UAVStereo: A Multiple Resolution Dataset for Stereo Matching in UAV Scenarios,Yu Anzhu,,0%
https://arxiv.org/pdf/2302.10082.pdf,UAVStereo: A Multiple Resolution Dataset for Stereo Matching in UAV Scenarios,Yu Wenshuai,,0%
https://arxiv.org/pdf/2302.10082.pdf,UAVStereo: A Multiple Resolution Dataset for Stereo Matching in UAV Scenarios,Li Zhenqi,,0%
https://arxiv.org/pdf/2302.10082.pdf,UAVStereo: A Multiple Resolution Dataset for Stereo Matching in UAV Scenarios,Quan Yujun,,0%
https://arxiv.org/pdf/2302.10040.pdf,Ontology-aware Network for Zero-shot Sketch-based Image Retrieval,Haoxiang Zhang,,0%
https://arxiv.org/pdf/2302.10040.pdf,Ontology-aware Network for Zero-shot Sketch-based Image Retrieval,He Jiang,,0%
https://arxiv.org/pdf/2302.10040.pdf,Ontology-aware Network for Zero-shot Sketch-based Image Retrieval,Ziqiang Wang,,0%
https://arxiv.org/pdf/2302.10040.pdf,Ontology-aware Network for Zero-shot Sketch-based Image Retrieval,Deqiang Cheng,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Xiao Wang,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Guangyao Chen,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Guangwu Qian,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Pengcheng Gao,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Xiao-yong Wei,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Yaowei Wang,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Yonghong Tian,,0%
https://arxiv.org/pdf/2302.10035.pdf,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,Wen Gao,,0%
https://arxiv.org/pdf/2302.10021.pdf,Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective,Nikolaos Kegkeroglou,nkegkeroglou@gmail.com,82%
https://arxiv.org/pdf/2302.10021.pdf,Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective,Petros Maragos,maragos@cs.ntua.gr,78%
https://arxiv.org/pdf/2302.10021.pdf,Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective,Panagiotis P. Filntisis,,0%
https://arxiv.org/pdf/2302.10007.pdf,On the Metrics for Evaluating Monocular Depth Estimation,Akhil Gurram,akhilgurram.ai@gmail.com,95%
https://arxiv.org/pdf/2302.10007.pdf,On the Metrics for Evaluating Monocular Depth Estimation,Antonio M. Lopez,antonio@cvc.uab.es,85%
https://arxiv.org/pdf/2302.10004.pdf,Simulating analogue film damage to analyse and improve artefact restoration on high-resolution scans,Daniela Ivanova,,0%
https://arxiv.org/pdf/2302.10004.pdf,Simulating analogue film damage to analyse and improve artefact restoration on high-resolution scans,John Williamson,,0%
https://arxiv.org/pdf/2302.10004.pdf,Simulating analogue film damage to analyse and improve artefact restoration on high-resolution scans,Paul Henderson,,0%
https://arxiv.org/pdf/2302.10001.pdf,STB-VMM: Swin Transformer Based Video Motion Magnification,Marco A. Pérez,marcoantonio.perez@iqs.url.edu,95%
https://arxiv.org/pdf/2302.10001.pdf,STB-VMM: Swin Transformer Based Video Motion Magnification,Ricard Lado-roigé,,0%
https://arxiv.org/pdf/2302.09998.pdf,Gesture Recognition with Keypoint and Radar Stream Fusion for Automated Vehicles,Adrian Holzbock,,0%
https://arxiv.org/pdf/2302.09998.pdf,Gesture Recognition with Keypoint and Radar Stream Fusion for Automated Vehicles,Nicolai Kern,,0%
https://arxiv.org/pdf/2302.09998.pdf,Gesture Recognition with Keypoint and Radar Stream Fusion for Automated Vehicles,Christian Waldschmidt,,0%
https://arxiv.org/pdf/2302.09998.pdf,Gesture Recognition with Keypoint and Radar Stream Fusion for Automated Vehicles,Klaus Dietmayer,,0%
https://arxiv.org/pdf/2302.09998.pdf,Gesture Recognition with Keypoint and Radar Stream Fusion for Automated Vehicles,Vasileios Belagiannis,,0%
https://arxiv.org/pdf/2302.09997.pdf,A Large Scale Homography Benchmark,Daniel Barath,,0%
https://arxiv.org/pdf/2302.09997.pdf,A Large Scale Homography Benchmark,Dmytro Mishkin,,0%
https://arxiv.org/pdf/2302.09997.pdf,A Large Scale Homography Benchmark,Michal Polic,,0%
https://arxiv.org/pdf/2302.09997.pdf,A Large Scale Homography Benchmark,Wolfgang Förstner,,0%
https://arxiv.org/pdf/2302.09997.pdf,A Large Scale Homography Benchmark,Jiri Matas,,0%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",Xiaolong Xu,xlxu@ieee.org,82%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",Muhammad Bilal,m.bilal@ieee.org,82%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",Xiaodong Liu,x.liu@napier.ac.uk,82%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",Zhiyun Yang,zhiyunyang@nuist.edu.cn,95%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",Qi Liu,qi.liu@nuist.edu.cn,95%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",Ru Ji,,0%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",Yonghong Zhang,,0%
https://arxiv.org/pdf/2302.09990.pdf,"Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges",S Vimal,,0%
https://arxiv.org/pdf/2302.09976.pdf,Discouraging posterior collapse in hierarchical Variational Autoencoders using context,Anna Kuzina,,0%
https://arxiv.org/pdf/2302.09976.pdf,Discouraging posterior collapse in hierarchical Variational Autoencoders using context,Jakub M. Tomczak,,0%
https://arxiv.org/pdf/2302.09973.pdf,Advanced Image Quality Assessment for Hand- and Fingervein Biometrics,Simon Kirchgasser,skirch@cs.sbg.ac.at,90%
https://arxiv.org/pdf/2302.09973.pdf,Advanced Image Quality Assessment for Hand- and Fingervein Biometrics,Andreas Uhl,uhl@cs.sbg.ac.at,78%
https://arxiv.org/pdf/2302.09973.pdf,Advanced Image Quality Assessment for Hand- and Fingervein Biometrics,Georg Wimmer,gwimmer@cs.sbg.ac.at,82%
https://arxiv.org/pdf/2302.09973.pdf,Advanced Image Quality Assessment for Hand- and Fingervein Biometrics,Christof Kauba,ckauba@cs.sbg.ac.at,82%
https://arxiv.org/pdf/2302.09956.pdf,"Because Every Sensor Is Unique, so Is Every Pair: Handling Dynamicity in Traffic Forecasting",Arian Prabowo,,0%
https://arxiv.org/pdf/2302.09956.pdf,"Because Every Sensor Is Unique, so Is Every Pair: Handling Dynamicity in Traffic Forecasting",Wei Shao,,0%
https://arxiv.org/pdf/2302.09956.pdf,"Because Every Sensor Is Unique, so Is Every Pair: Handling Dynamicity in Traffic Forecasting",Hao Xue,,0%
https://arxiv.org/pdf/2302.09956.pdf,"Because Every Sensor Is Unique, so Is Every Pair: Handling Dynamicity in Traffic Forecasting",Piotr Koniusz,,0%
https://arxiv.org/pdf/2302.09956.pdf,"Because Every Sensor Is Unique, so Is Every Pair: Handling Dynamicity in Traffic Forecasting",Flora D. Salim,,0%
https://arxiv.org/pdf/2302.09949.pdf,SpecXAI -- Spectral interpretability of Deep Learning Models,Stefan Druc,,0%
https://arxiv.org/pdf/2302.09949.pdf,SpecXAI -- Spectral interpretability of Deep Learning Models,Peter Wooldridge,,0%
https://arxiv.org/pdf/2302.09949.pdf,SpecXAI -- Spectral interpretability of Deep Learning Models,Adarsh Krishnamurthy,,0%
https://arxiv.org/pdf/2302.09949.pdf,SpecXAI -- Spectral interpretability of Deep Learning Models,Soumik Sarkar,,0%
https://arxiv.org/pdf/2302.09949.pdf,SpecXAI -- Spectral interpretability of Deep Learning Models,Aditya Balu,,0%
https://arxiv.org/pdf/2302.09945.pdf,Generalization capabilities of conditional GAN for turbulent flow under changes of geometry,Hanno Gottschalk,hanno.gottschalk@uni-wuppertal.de,95%
https://arxiv.org/pdf/2302.09945.pdf,Generalization capabilities of conditional GAN for turbulent flow under changes of geometry,Francesca Di Mare,francesca.dimare@ruhr-uni-bochum.de,95%
https://arxiv.org/pdf/2302.09945.pdf,Generalization capabilities of conditional GAN for turbulent flow under changes of geometry,Claudia Drygala,drygala@uni-wuppertal.de,78%
https://arxiv.org/pdf/2302.09922.pdf,Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision,Yao Zhao,yzhao@bjtu.edu.cn,82%
https://arxiv.org/pdf/2302.09922.pdf,Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision,Chunyu Lin,cylin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2302.09922.pdf,Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision,Lang Nie,nielang@bjtu.edu.cn,95%
https://arxiv.org/pdf/2302.09922.pdf,Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision,Zisong Chen,zschen@bjtu.edu.cn,82%
https://arxiv.org/pdf/2302.09922.pdf,Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision,Kang Liao,kang liao@bjtu.edu.cn,95%
https://arxiv.org/pdf/2302.09919.pdf,Interactive Face Video Coding: A Generative Compression Framework,Binzhe Li,binzheli2-c@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.09919.pdf,Interactive Face Video Coding: A Generative Compression Framework,Shurun Wang,shurun.wsr@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.09919.pdf,Interactive Face Video Coding: A Generative Compression Framework,Zhao Wang,zhaowang@pku.edu.cn,95%
https://arxiv.org/pdf/2302.09919.pdf,Interactive Face Video Coding: A Generative Compression Framework,Bolin Chen,c@my.cityu.edu.hk,90%
https://arxiv.org/pdf/2302.09919.pdf,Interactive Face Video Coding: A Generative Compression Framework,Yan Ye,yan.ye@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.09919.pdf,Interactive Face Video Coding: A Generative Compression Framework,Shiqi Wang,,0%
https://arxiv.org/pdf/2302.09907.pdf,General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment,Liang Xie,,0%
https://arxiv.org/pdf/2302.09907.pdf,General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment,Yibo Yang,,0%
https://arxiv.org/pdf/2302.09907.pdf,General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment,Wenxiao Wang,,0%
https://arxiv.org/pdf/2302.09907.pdf,General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment,Binbin Lin,,0%
https://arxiv.org/pdf/2302.09907.pdf,General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment,Deng Cai,,0%
https://arxiv.org/pdf/2302.09907.pdf,General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment,Xiaofei He,,0%
https://arxiv.org/pdf/2302.09907.pdf,General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment,Ronghua Liang,,0%
https://arxiv.org/pdf/2302.09899.pdf,A Survey on Semi-Supervised Semantic Segmentation,Julián Luengo,julianlm@decsai.ugr.es,85%
https://arxiv.org/pdf/2302.09899.pdf,A Survey on Semi-Supervised Semantic Segmentation,Adrian Peláez-vegas,adrianpelaez@ugr.es,85%
https://arxiv.org/pdf/2302.09899.pdf,A Survey on Semi-Supervised Semantic Segmentation,Pablo Mesejo,pmesejo@decsai.ugr.es,82%
https://arxiv.org/pdf/2302.09886.pdf,InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation,Yang Cong,congyang81@gmail.com,95%
https://arxiv.org/pdf/2302.09886.pdf,InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation,Jiahua Dong,dongjiahua1995@gmail.com,95%
https://arxiv.org/pdf/2302.09886.pdf,InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation,Lixu Wang,lixuwang2025@u.northwestern.edu,95%
https://arxiv.org/pdf/2302.09886.pdf,InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation,Jun Li,junli@njust.edu.cn,95%
https://arxiv.org/pdf/2302.09886.pdf,InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation,Gan Sun,sungan1412@gmail.com,95%
https://arxiv.org/pdf/2302.09886.pdf,InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation,Lingjuan Lyu,Lingjuan.Lv@sony.com,85%
https://arxiv.org/pdf/2302.09886.pdf,InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation,Ender Konukoglu,ender.konukoglu@vision.ee.ethz.ch,95%
https://arxiv.org/pdf/2302.09884.pdf,GlocalFuse-Depth: Fusing Transformers and CNNs for All-day Self-supervised Monocular Depth Estimation,Kenneth K. Y. Wong,kywong@eee.hku.hk,82%
https://arxiv.org/pdf/2302.09884.pdf,GlocalFuse-Depth: Fusing Transformers and CNNs for All-day Self-supervised Monocular Depth Estimation,Zezheng Zhang,zezhengz@connect.hku.hk,85%
https://arxiv.org/pdf/2302.09884.pdf,GlocalFuse-Depth: Fusing Transformers and CNNs for All-day Self-supervised Monocular Depth Estimation,Ryan K. Y. Chan,,0%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Peisen Zhao,pszhao93@gmail.com,82%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Jianlong Chang,jianlong.chang@huawei.com,95%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Ya Zhang,ya zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Chen Ju,ju chen@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Jinxiang Liu,jinxliu@sjtu.edu.cn,82%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Chaofan Ma,chaofanma@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Qi Tian,tian.qi1@huawei.com,95%
https://arxiv.org/pdf/2302.09850.pdf,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,Haicheng Wang,,0%
https://arxiv.org/pdf/2302.09838.pdf,JNDMix: JND-Based Data Augmentation for No-reference Image Quality Assessment,Jiamu Sheng,,0%
https://arxiv.org/pdf/2302.09838.pdf,JNDMix: JND-Based Data Augmentation for No-reference Image Quality Assessment,Jiayuan Fan,,0%
https://arxiv.org/pdf/2302.09838.pdf,JNDMix: JND-Based Data Augmentation for No-reference Image Quality Assessment,Peng Ye,,0%
https://arxiv.org/pdf/2302.09838.pdf,JNDMix: JND-Based Data Augmentation for No-reference Image Quality Assessment,Jianjian Cao,,0%
https://arxiv.org/pdf/2302.09835.pdf,Simple U-net Based Synthetic Polyp Image Generation: Polyp to Negative and Negative to Polyp,Younghak Shin,shinyh0919@gmail.com,78%
https://arxiv.org/pdf/2302.09835.pdf,Simple U-net Based Synthetic Polyp Image Generation: Polyp to Negative and Negative to Polyp,Hemin Ali Qadir,,0%
https://arxiv.org/pdf/2302.09835.pdf,Simple U-net Based Synthetic Polyp Image Generation: Polyp to Negative and Negative to Polyp,Ilangko Balasingham,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Soham Rohit Chitnis,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Sidong Liu,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Tirtharaj Dash,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Tanmay Tulsidas Verlekar,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Antonio Di Ieva,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Shlomo Berkovsky,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Lovekesh Vig,,0%
https://arxiv.org/pdf/2302.09833.pdf,Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification,Ashwin Srinivasan,,0%
https://arxiv.org/pdf/2302.09825.pdf,TBPos: Dataset for Large-Scale Precision Visual Localization,Masud Fahim,,0%
https://arxiv.org/pdf/2302.09825.pdf,TBPos: Dataset for Large-Scale Precision Visual Localization,Ilona Söchting,,0%
https://arxiv.org/pdf/2302.09825.pdf,TBPos: Dataset for Large-Scale Precision Visual Localization,Luca Ferranti,,0%
https://arxiv.org/pdf/2302.09825.pdf,TBPos: Dataset for Large-Scale Precision Visual Localization,Juho Kannala,,0%
https://arxiv.org/pdf/2302.09825.pdf,TBPos: Dataset for Large-Scale Precision Visual Localization,Jani Boutellier,,0%
https://arxiv.org/pdf/2302.09817.pdf,Explainable Human-centered Traits from Head Motion and Facial Expression Dynamics,Surbhi Madan,,0%
https://arxiv.org/pdf/2302.09817.pdf,Explainable Human-centered Traits from Head Motion and Facial Expression Dynamics,Monika Gahalawat,,0%
https://arxiv.org/pdf/2302.09817.pdf,Explainable Human-centered Traits from Head Motion and Facial Expression Dynamics,Tanaya Guha,,0%
https://arxiv.org/pdf/2302.09817.pdf,Explainable Human-centered Traits from Head Motion and Facial Expression Dynamics,Roland Goecke,,0%
https://arxiv.org/pdf/2302.09817.pdf,Explainable Human-centered Traits from Head Motion and Facial Expression Dynamics,Ramanathan Subramanian,,0%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Xin Gao,xin.gao@kaust.edu.sa,95%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Juexiao Zhou,,0%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Longxi Zhou,,0%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Di Wang,,0%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Xiaopeng Xu,,0%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Haoyang Li,,0%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Yuetan Chu,,0%
https://arxiv.org/pdf/2302.11571.pdf,Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI,Wenkai Han,,0%
https://arxiv.org/pdf/2302.09814.pdf,Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network,Jie Zhang,jiezhangsp@gmail.com,95%
https://arxiv.org/pdf/2302.09814.pdf,Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network,Xiaojian Yuan,xjyuan@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2302.09814.pdf,Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network,Kejiang Chen,chenkj@ustc.edu.cn,78%
https://arxiv.org/pdf/2302.09814.pdf,Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network,Weiming Zhang,,0%
https://arxiv.org/pdf/2302.09814.pdf,Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network,Nenghai Yu,,0%
https://arxiv.org/pdf/2302.09814.pdf,Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network,Yang Zhang,,0%
https://arxiv.org/pdf/2302.09807.pdf,A Novel Collaborative Self-Supervised Learning Method for Radiomic Data,Lili He,lili.he@cchmc.org,95%
https://arxiv.org/pdf/2302.09807.pdf,A Novel Collaborative Self-Supervised Learning Method for Radiomic Data,Jonathan R. Dillman,jonathan.dillman@cchmc.org,95%
https://arxiv.org/pdf/2302.09807.pdf,A Novel Collaborative Self-Supervised Learning Method for Radiomic Data,Hailong Li,hailong.li@cchmc.org,95%
https://arxiv.org/pdf/2302.09807.pdf,A Novel Collaborative Self-Supervised Learning Method for Radiomic Data,Zhiyuan Li,li3z3@mail.uc.edu,78%
https://arxiv.org/pdf/2302.09807.pdf,A Novel Collaborative Self-Supervised Learning Method for Radiomic Data,Nehal A. Parikh,nehal.parikh@cchmc.org,95%
https://arxiv.org/pdf/2302.09807.pdf,A Novel Collaborative Self-Supervised Learning Method for Radiomic Data,Anca L. Ralescu,ralescal@ucmail.uc.edu,65%
https://arxiv.org/pdf/2302.09795.pdf,Simple Disentanglement of Style and Content in Visual Representations,Subha Maity,smaity@umich.edu,82%
https://arxiv.org/pdf/2302.09795.pdf,Simple Disentanglement of Style and Content in Visual Representations,Mikhail Yurochkin,mikhail.yurochkin@ibm.com,95%
https://arxiv.org/pdf/2302.09795.pdf,Simple Disentanglement of Style and Content in Visual Representations,Lilian Ngweta,ngwetl@rpi.edu,76%
https://arxiv.org/pdf/2302.09795.pdf,Simple Disentanglement of Style and Content in Visual Representations,Alex Gittens,,0%
https://arxiv.org/pdf/2302.09795.pdf,Simple Disentanglement of Style and Content in Visual Representations,Yuekai Sun,,0%
https://arxiv.org/pdf/2302.09794.pdf,Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection,Suhwan Cho,chosuhwan@yonsei.ac.kr,95%
https://arxiv.org/pdf/2302.09794.pdf,Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection,Chaewon Park,chaewon28@yonsei.ac.kr,85%
https://arxiv.org/pdf/2302.09794.pdf,Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection,Donghyeong Kim,hydragon516@yonsei.ac.kr,60%
https://arxiv.org/pdf/2302.09794.pdf,Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection,Sangyoun Lee,syleee@yonsei.ac.kr,82%
https://arxiv.org/pdf/2302.09794.pdf,Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection,Minhyeok Lee,,0%
https://arxiv.org/pdf/2302.09790.pdf,HTNet: Human Topology Aware Network for 3D Human Pose Estimation,Runwei Ding,dingrunwei@pku.edu.cn,95%
https://arxiv.org/pdf/2302.09790.pdf,HTNet: Human Topology Aware Network for 3D Human Pose Estimation,Miaoju Ban,miaoju.ban@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2302.09790.pdf,HTNet: Human Topology Aware Network for 3D Human Pose Estimation,Wenhao Li,wenhaoli@pku.edu.cn,95%
https://arxiv.org/pdf/2302.09790.pdf,HTNet: Human Topology Aware Network for 3D Human Pose Estimation,Hong Liu,hongliu@pku.edu.cn,95%
https://arxiv.org/pdf/2302.09790.pdf,HTNet: Human Topology Aware Network for 3D Human Pose Estimation,Jialun Cai,,0%
https://arxiv.org/pdf/2302.09790.pdf,HTNet: Human Topology Aware Network for 3D Human Pose Estimation,Jianbing Wu,,0%
https://arxiv.org/pdf/2302.09789.pdf,Self-Supervised Monocular Depth Estimation with Self-Reference Distillation and Disparity Offset Refinement,Xingming Wu,wxmbuaa@163.com,65%
https://arxiv.org/pdf/2302.09789.pdf,Self-Supervised Monocular Depth Estimation with Self-Reference Distillation and Disparity Offset Refinement,Weihai Chen,whchen@buaa.edu.cn,82%
https://arxiv.org/pdf/2302.09789.pdf,Self-Supervised Monocular Depth Estimation with Self-Reference Distillation and Disparity Offset Refinement,Shuwei Shao,swshao@buaa.edu.cn,82%
https://arxiv.org/pdf/2302.09789.pdf,Self-Supervised Monocular Depth Estimation with Self-Reference Distillation and Disparity Offset Refinement,Zhong Liu,liuzhong@buaa.edu.cn,95%
https://arxiv.org/pdf/2302.09789.pdf,Self-Supervised Monocular Depth Estimation with Self-Reference Distillation and Disparity Offset Refinement,Ran Li,,0%
https://arxiv.org/pdf/2302.09785.pdf,Towards Simultaneous Segmentation of Liver Tumors and Intrahepatic Vessels via Cross-attention Mechanism,Haopeng Kuang,,0%
https://arxiv.org/pdf/2302.09785.pdf,Towards Simultaneous Segmentation of Liver Tumors and Intrahepatic Vessels via Cross-attention Mechanism,Dingkang Yang,,0%
https://arxiv.org/pdf/2302.09785.pdf,Towards Simultaneous Segmentation of Liver Tumors and Intrahepatic Vessels via Cross-attention Mechanism,Shunli Wang,,0%
https://arxiv.org/pdf/2302.09785.pdf,Towards Simultaneous Segmentation of Liver Tumors and Intrahepatic Vessels via Cross-attention Mechanism,Xiaoying Wang,,0%
https://arxiv.org/pdf/2302.09785.pdf,Towards Simultaneous Segmentation of Liver Tumors and Intrahepatic Vessels via Cross-attention Mechanism,Lihua Zhang,,0%
https://arxiv.org/pdf/2302.09779.pdf,Incremental Few-Shot Object Detection via Simple Fine-Tuning Approach,Jong-hwan Kim,johkim@rit.kaist.ac.kr,82%
https://arxiv.org/pdf/2302.09779.pdf,Incremental Few-Shot Object Detection via Simple Fine-Tuning Approach,Tae-min Choi,tmchoi@rit.kaist.ac.kr,82%
https://arxiv.org/pdf/2302.09778.pdf,Composer: Creative and Controllable Image Synthesis with Composable Conditions,Deli Zhao,zhaodeli@gmail.com,95%
https://arxiv.org/pdf/2302.09778.pdf,Composer: Creative and Controllable Image Synthesis with Composable Conditions,Yujun Shen,shenyujun0302@gmail.com,95%
https://arxiv.org/pdf/2302.09778.pdf,Composer: Creative and Controllable Image Synthesis with Composable Conditions,Lianghua Huang,,0%
https://arxiv.org/pdf/2302.09778.pdf,Composer: Creative and Controllable Image Synthesis with Composable Conditions,Di Chen,,0%
https://arxiv.org/pdf/2302.09778.pdf,Composer: Creative and Controllable Image Synthesis with Composable Conditions,Yu Liu,,0%
https://arxiv.org/pdf/2302.09778.pdf,Composer: Creative and Controllable Image Synthesis with Composable Conditions,Jingren Zhou,,0%
https://arxiv.org/pdf/2302.09765.pdf,ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,Tae-hyun Oh,taehyun@postech.ac.kr,85%
https://arxiv.org/pdf/2302.09765.pdf,ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,Moon Ye-bin,ybmoon@postech.ac.kr,85%
https://arxiv.org/pdf/2302.09765.pdf,ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,Dongmin Choi,,0%
https://arxiv.org/pdf/2302.09765.pdf,ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,Yongjin Kwon,,0%
https://arxiv.org/pdf/2302.09765.pdf,ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,Junsik Kim,,0%
https://arxiv.org/pdf/2302.09762.pdf,Metropolis Theorem and Its Applications in Single Image Detail Enhancement,He Jiang,jianghe@cumt.edu.cn,95%
https://arxiv.org/pdf/2302.09762.pdf,Metropolis Theorem and Its Applications in Single Image Detail Enhancement,Mujtaba Asad,,0%
https://arxiv.org/pdf/2302.09762.pdf,Metropolis Theorem and Its Applications in Single Image Detail Enhancement,Jingjing Liu,,0%
https://arxiv.org/pdf/2302.09762.pdf,Metropolis Theorem and Its Applications in Single Image Detail Enhancement,Haoxiang Zhang,,0%
https://arxiv.org/pdf/2302.09762.pdf,Metropolis Theorem and Its Applications in Single Image Detail Enhancement,Deqiang Cheng,,0%
https://arxiv.org/pdf/2302.09736.pdf,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,Xuan Luo,ttssxuanluo@tencent.com,95%
https://arxiv.org/pdf/2302.09736.pdf,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,Xiaocheng Feng,xcfeng@ir.hit.edu.cn,82%
https://arxiv.org/pdf/2302.09736.pdf,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,Heng Gong,moonzheng@tencent.com,85%
https://arxiv.org/pdf/2302.09736.pdf,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,Bing Qin,qinb@ir.hit.edu.cn,78%
https://arxiv.org/pdf/2302.09736.pdf,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,Duyu Tang,tangduyu.nlp@hotmail.com,95%
https://arxiv.org/pdf/2302.09736.pdf,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,Weihong Zhong,whzhong@ir.hit.edu.cn,82%
https://arxiv.org/pdf/2302.09736.pdf,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,Mao Zheng,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Henry Williams,henry.williams@auckland.ac.nz,95%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Ans Qureshi,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Neville Loh,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Young Min Kwon,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,David Smith,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Trevor Gee,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Oliver Bachelor,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Josh Mcculloch,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Mahla Nejati,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Jongyoon Lim,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Richard Green,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Ho Seok Ahn,,0%
https://arxiv.org/pdf/2302.09716.pdf,Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning,Bruce Macdonald,,0%
https://arxiv.org/pdf/2302.09696.pdf,An Efficient and Robust Method for Chest X-Ray Rib Suppression that Improves Pulmonary Abnormality Diagnosis,Di Xu,DiXu@mednet.ucla.edu,95%
https://arxiv.org/pdf/2302.09696.pdf,An Efficient and Robust Method for Chest X-Ray Rib Suppression that Improves Pulmonary Abnormality Diagnosis,Qifan Xu,QifanXu@mednet.ucla.edu,95%
https://arxiv.org/pdf/2302.09696.pdf,An Efficient and Robust Method for Chest X-Ray Rib Suppression that Improves Pulmonary Abnormality Diagnosis,Dan Ruan,DanRaun@mednet.ucla.edu,85%
https://arxiv.org/pdf/2302.09696.pdf,An Efficient and Robust Method for Chest X-Ray Rib Suppression that Improves Pulmonary Abnormality Diagnosis,Ke Sheng,KSheng@mednet.ucla.edu,82%
https://arxiv.org/pdf/2302.09696.pdf,An Efficient and Robust Method for Chest X-Ray Rib Suppression that Improves Pulmonary Abnormality Diagnosis,Kevin Nhieu,kknhieu7@g.ucla.edu,82%
https://arxiv.org/pdf/2302.09682.pdf,Dual Attention Model with Reinforcement Learning for Classification of Histology Whole-Slide Images,Talha Qaiser,talha.qaiser@warwick.ac.uk,95%
https://arxiv.org/pdf/2302.09682.pdf,Dual Attention Model with Reinforcement Learning for Classification of Histology Whole-Slide Images,Manahil Raza,manahil.raza@warwick.ac.uk,95%
https://arxiv.org/pdf/2302.09682.pdf,Dual Attention Model with Reinforcement Learning for Classification of Histology Whole-Slide Images,Raja Muhammad Saad Bashir,saad.bashir@warwick.ac.uk,78%
https://arxiv.org/pdf/2302.09682.pdf,Dual Attention Model with Reinforcement Learning for Classification of Histology Whole-Slide Images,Ruqayya Awan,ruqayya.awan@warwick.ac.uk,95%
https://arxiv.org/pdf/2302.09682.pdf,Dual Attention Model with Reinforcement Learning for Classification of Histology Whole-Slide Images,Nasir M. Rajpoot,n.m.rajpoot@warwick.ac.uk,82%
https://arxiv.org/pdf/2302.09657.pdf,Table Tennis Stroke Detection and Recognition Using Ball Trajectory Data,Sucheth Shenoy,sucheth17@gmail.com,85%
https://arxiv.org/pdf/2302.09657.pdf,Table Tennis Stroke Detection and Recognition Using Ball Trajectory Data,Rohan S Jamadagni,rohanjamadagni@gmail.com,95%
https://arxiv.org/pdf/2302.09657.pdf,Table Tennis Stroke Detection and Recognition Using Ball Trajectory Data,Kaustubh Milind Kulkarni,kmkapril15@gmail.com,65%
https://arxiv.org/pdf/2302.09657.pdf,Table Tennis Stroke Detection and Recognition Using Ball Trajectory Data,Jeffrey Aaron Paul,jeffrey.paul2000@gmail.com,95%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Xinyue Hu,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Lin Gu,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Kazuma Kobayashi,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Qiyuan An,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Qingyu Chen,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Zhiyong Lu,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Chang Su,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Tatsuya Harada,,0%
https://arxiv.org/pdf/2302.09636.pdf,Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning,Yingying Zhu,,0%
https://arxiv.org/pdf/2302.09629.pdf,BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image,Shankarachary Ragi,hafizur.raj@gmail.com,55%
https://arxiv.org/pdf/2302.09629.pdf,BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image,Md Hafizur Rahman,,0%
https://arxiv.org/pdf/2302.09629.pdf,BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image,Md Ali Azam,,0%
https://arxiv.org/pdf/2302.09629.pdf,BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image,Md Abir Hossen,,0%
https://arxiv.org/pdf/2302.09629.pdf,BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image,Venkataramana Gadhamshetty,,0%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Shuiwang Ji,sji@tamu.edu,82%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Bin Li,binli@ustc.edu.cn,95%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Qi Zhou,zhouqida@mail.ustc.edu.cn,95%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Mingxuan Ye,anye@mail.ustc.edu.cn,78%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Feng Wu,fengwu@ustc.edu.cn,95%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Jie Wang,jiewangx@ustc.edu.cn,95%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Rui Yang,,0%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Zijie Geng,,0%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Zhihao Shi,,0%
https://arxiv.org/pdf/2302.09601.pdf,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,Yongdong Zhang,,0%
https://arxiv.org/pdf/2302.09598.pdf,Guided Depth Map Super-resolution: A Survey,Debin Zhao,dbzhao@hit.edu.cn,82%
https://arxiv.org/pdf/2302.09598.pdf,Guided Depth Map Super-resolution: A Survey,Xiangyang Ji,xyji@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.09598.pdf,Guided Depth Map Super-resolution: A Survey,Zhiwei Zhong,zhwzhong@hit.edu.cn,82%
https://arxiv.org/pdf/2302.09598.pdf,Guided Depth Map Super-resolution: A Survey,Junjun Jiang,jiangjunjun@hit.edu.cn,95%
https://arxiv.org/pdf/2302.09598.pdf,Guided Depth Map Super-resolution: A Survey,Xianming Liu,,0%
https://arxiv.org/pdf/2302.09590.pdf,Accelerated Video Annotation driven by Deep Detector and Tracker,Aamir Ahmad,aamir.ahmad@ifr.uni-stuttgart.de,95%
https://arxiv.org/pdf/2302.09590.pdf,Accelerated Video Annotation driven by Deep Detector and Tracker,Eric Price,eric.price@ifr.uni-stuttgart.de,95%
https://arxiv.org/pdf/2302.09585.pdf,StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,Yining Shi,,0%
https://arxiv.org/pdf/2302.09585.pdf,StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,Kun Jiang,,0%
https://arxiv.org/pdf/2302.09585.pdf,StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,Ke Wang,,0%
https://arxiv.org/pdf/2302.09585.pdf,StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,Jiusi Li,,0%
https://arxiv.org/pdf/2302.09585.pdf,StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,Yunlong Wang,,0%
https://arxiv.org/pdf/2302.09585.pdf,StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,Mengmeng Yang,,0%
https://arxiv.org/pdf/2302.09585.pdf,StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,Diange Yang,,0%
https://arxiv.org/pdf/2302.09584.pdf,DGP-Net: Dense Graph Prototype Network for Few-Shot SAR Target Recognition,Xiangyu Zhou,,0%
https://arxiv.org/pdf/2302.09584.pdf,DGP-Net: Dense Graph Prototype Network for Few-Shot SAR Target Recognition,Qianru Wei,,0%
https://arxiv.org/pdf/2302.09584.pdf,DGP-Net: Dense Graph Prototype Network for Few-Shot SAR Target Recognition,Yuhui Zhang,,0%
https://arxiv.org/pdf/2302.09579.pdf,Evaluating Representations with Readout Model Switching,Jorg Bornschein,bornschein@deepmind.com,78%
https://arxiv.org/pdf/2302.09579.pdf,Evaluating Representations with Readout Model Switching,Marcus Hutter,mhutter@deepmind.com,82%
https://arxiv.org/pdf/2302.09579.pdf,Evaluating Representations with Readout Model Switching,Yazhe Li,yazhe@deepmind.com,85%
https://arxiv.org/pdf/2302.09572.pdf,Rethinking Data-Free Quantization as a Zero-Sum Game,Yang Wang,yangwang@hfut.edu.cn,95%
https://arxiv.org/pdf/2302.09572.pdf,Rethinking Data-Free Quantization as a Zero-Sum Game,Biao Qian,hfutqian@gmail.com,78%
https://arxiv.org/pdf/2302.09572.pdf,Rethinking Data-Free Quantization as a Zero-Sum Game,Richang Hong,hongrc.hfut@gmail.com,78%
https://arxiv.org/pdf/2302.09572.pdf,Rethinking Data-Free Quantization as a Zero-Sum Game,Meng Wang,eric.mengwang@gmail.com,95%
https://arxiv.org/pdf/2302.09569.pdf,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,Minjin Hwang,,0%
https://arxiv.org/pdf/2302.09569.pdf,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,Bappaditya Dey,,0%
https://arxiv.org/pdf/2302.09569.pdf,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,Enrique Dehaerne,,0%
https://arxiv.org/pdf/2302.09569.pdf,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,Sandip Halder,,0%
https://arxiv.org/pdf/2302.09569.pdf,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,Young-han Shin,,0%
https://arxiv.org/pdf/2302.09565.pdf,Optimizing YOLOv7 for Semiconductor Defect Detection,Enrique Dehaerne,enrique.dehaerne@imec.be,95%
https://arxiv.org/pdf/2302.09565.pdf,Optimizing YOLOv7 for Semiconductor Defect Detection,Bappaditya Dey,,0%
https://arxiv.org/pdf/2302.09565.pdf,Optimizing YOLOv7 for Semiconductor Defect Detection,Sandip Halder,,0%
https://arxiv.org/pdf/2302.09565.pdf,Optimizing YOLOv7 for Semiconductor Defect Detection,Stefan De Gendt,,0%
https://arxiv.org/pdf/2302.09561.pdf,TAX: Tendency-and-Assignment Explainer for Semantic Segmentation with Multi-Annotators,Yu-chiang Frank Wang,ycwang@ntu.edu.tw,82%
https://arxiv.org/pdf/2302.09561.pdf,TAX: Tendency-and-Assignment Explainer for Semantic Segmentation with Multi-Annotators,Yuan-chia Cheng,,0%
https://arxiv.org/pdf/2302.09561.pdf,TAX: Tendency-and-Assignment Explainer for Semantic Segmentation with Multi-Annotators,Zu-yun Shiau,,0%
https://arxiv.org/pdf/2302.09561.pdf,TAX: Tendency-and-Assignment Explainer for Semantic Segmentation with Multi-Annotators,Fu-en Yang,,0%
https://arxiv.org/pdf/2302.09560.pdf,Deep Selector-JPEG: Adaptive JPEG Image Compression for Computer Vision in Image classification with Human Vision Criteria,Hossam Amer,,0%
https://arxiv.org/pdf/2302.09560.pdf,Deep Selector-JPEG: Adaptive JPEG Image Compression for Computer Vision in Image classification with Human Vision Criteria,Sepideh Shaterian,,0%
https://arxiv.org/pdf/2302.09560.pdf,Deep Selector-JPEG: Adaptive JPEG Image Compression for Computer Vision in Image classification with Human Vision Criteria,En-hui Yang,,0%
https://arxiv.org/pdf/2302.09556.pdf,Supervised Contrastive Learning and Feature Fusion for Improved Kinship Verification,Nazim Bendib,bendib@esi.dz,78%
https://arxiv.org/pdf/2302.09554.pdf,Mixed Hierarchy Network for Image Restoration,Hu Gao,h@mail.bnu.edu.cn,90%
https://arxiv.org/pdf/2302.09554.pdf,Mixed Hierarchy Network for Image Restoration,Depeng Dang,ddepeng@bnu.edu.cn,85%
https://arxiv.org/pdf/2302.09528.pdf,A Comprehensive Evaluation Study on Risk Level Classification of Melanoma by Computer Vision on ISIC 2016-2020 Datasets,Chengdong Yao,chengdong.yao-1@student.uts.edu.au,95%
https://arxiv.org/pdf/2302.09522.pdf,Interactive Video Corpus Moment Retrieval using Reinforcement Learning,Zhixin Ma,zxma.2020@phdcs.smu.edu.sg,82%
https://arxiv.org/pdf/2302.09522.pdf,Interactive Video Corpus Moment Retrieval using Reinforcement Learning,Chong-wah Ngo,cwngo@smu.edu.sg,82%
https://arxiv.org/pdf/2302.09516.pdf,A Bibliography of Multiple Sclerosis Lesions Detection Methods using Brain MRIs,Atif Shah,atifshahcs@gmail.com,95%
https://arxiv.org/pdf/2302.09516.pdf,A Bibliography of Multiple Sclerosis Lesions Detection Methods using Brain MRIs,Maged S. Al-shaibani,,0%
https://arxiv.org/pdf/2302.09516.pdf,A Bibliography of Multiple Sclerosis Lesions Detection Methods using Brain MRIs,Moataz Ahmad,,0%
https://arxiv.org/pdf/2302.09516.pdf,A Bibliography of Multiple Sclerosis Lesions Detection Methods using Brain MRIs,Reem Bunyan,,0%
https://arxiv.org/pdf/2303.08660.pdf,Fashion-model pose recommendation and generation using Machine Learning,Joy Larnyoh,joylarnyoh@gmail.com,95%
https://arxiv.org/pdf/2303.08660.pdf,Fashion-model pose recommendation and generation using Machine Learning,Santhosh Kannan S P,mail2santhoshkannan@gmail.com,85%
https://arxiv.org/pdf/2303.08660.pdf,Fashion-model pose recommendation and generation using Machine Learning,Raja Csp Raman,raja.csp@gmail.com,85%
https://arxiv.org/pdf/2303.08660.pdf,Fashion-model pose recommendation and generation using Machine Learning,Rohith Mahadevan,rohithmahadev30@gmail.com,85%
https://arxiv.org/pdf/2303.08660.pdf,Fashion-model pose recommendation and generation using Machine Learning,Krithiga Shankar,skrithi05@gmail.com,60%
https://arxiv.org/pdf/2303.08660.pdf,Fashion-model pose recommendation and generation using Machine Learning,Vijitha Kannumuru,,0%
https://arxiv.org/pdf/2302.09502.pdf,Self-supervised Cloth Reconstruction via Action-conditioned Cloth Tracking,David Held,dheld@andrew.cmu.edu,82%
https://arxiv.org/pdf/2302.09502.pdf,Self-supervised Cloth Reconstruction via Action-conditioned Cloth Tracking,Zixuan Huang,zixuanhu@andrew.cmu.edu,85%
https://arxiv.org/pdf/2302.09502.pdf,Self-supervised Cloth Reconstruction via Action-conditioned Cloth Tracking,Xingyu Lin,xlin3@andrew.cmu.edu,82%
https://arxiv.org/pdf/2302.09498.pdf,Mutual Exclusive Modulator for Long-Tailed Recognition,Zongtai Luo,luozongtai@hotmail.com,95%
https://arxiv.org/pdf/2302.09498.pdf,Mutual Exclusive Modulator for Long-Tailed Recognition,Xiaolin Zhang,solli.zhang@gmail.com,78%
https://arxiv.org/pdf/2302.09498.pdf,Mutual Exclusive Modulator for Long-Tailed Recognition,Jianbo Liu,liujianbo@link.cuhk.edu.hk,95%
https://arxiv.org/pdf/2302.09498.pdf,Mutual Exclusive Modulator for Long-Tailed Recognition,Haixu Long,hxlong@mail.ustc.edu.cn,82%
https://arxiv.org/pdf/2302.09498.pdf,Mutual Exclusive Modulator for Long-Tailed Recognition,Yanbin Liu,csyanbin@gmail.com,85%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Aishan Liu,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Jun Guo,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Jiakai Wang,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Siyuan Liang,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Renshuai Tao,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Wenbo Zhou,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Cong Liu,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Xianglong Liu,,0%
https://arxiv.org/pdf/2302.09491.pdf,X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection,Dacheng Tao,,0%
https://arxiv.org/pdf/2302.09488.pdf,A Picture May Be Worth a Thousand Lives: An Interpretable Artificial Intelligence Strategy for Predictions of Suicide Risk from Social Media Images,Yaakov Ophir,yaakovophir@gmail.com,95%
https://arxiv.org/pdf/2302.09488.pdf,A Picture May Be Worth a Thousand Lives: An Interpretable Artificial Intelligence Strategy for Predictions of Suicide Risk from Social Media Images,Yael Badian,,0%
https://arxiv.org/pdf/2302.09488.pdf,A Picture May Be Worth a Thousand Lives: An Interpretable Artificial Intelligence Strategy for Predictions of Suicide Risk from Social Media Images,Refael Tikochinski,,0%
https://arxiv.org/pdf/2302.09488.pdf,A Picture May Be Worth a Thousand Lives: An Interpretable Artificial Intelligence Strategy for Predictions of Suicide Risk from Social Media Images,Nitay Calderon,,0%
https://arxiv.org/pdf/2302.09488.pdf,A Picture May Be Worth a Thousand Lives: An Interpretable Artificial Intelligence Strategy for Predictions of Suicide Risk from Social Media Images,Anat Brunstein Klomek,,0%
https://arxiv.org/pdf/2302.09488.pdf,A Picture May Be Worth a Thousand Lives: An Interpretable Artificial Intelligence Strategy for Predictions of Suicide Risk from Social Media Images,Roi Reichart,,0%
https://arxiv.org/pdf/2302.09486.pdf,LC-NeRF: Local Controllable Face Generation in Neural Randiance Field,Wenyang Zhou,,0%
https://arxiv.org/pdf/2302.09486.pdf,LC-NeRF: Local Controllable Face Generation in Neural Randiance Field,Lu Yuan,,0%
https://arxiv.org/pdf/2302.09486.pdf,LC-NeRF: Local Controllable Face Generation in Neural Randiance Field,Shuyu Chen,,0%
https://arxiv.org/pdf/2302.09486.pdf,LC-NeRF: Local Controllable Face Generation in Neural Randiance Field,Lin Gao,,0%
https://arxiv.org/pdf/2302.09486.pdf,LC-NeRF: Local Controllable Face Generation in Neural Randiance Field,Shimin Hu,,0%
https://arxiv.org/pdf/2302.09473.pdf,Video-Text Retrieval by Supervised Sparse Multi-Grained Learning,Peng Shi,peng.shi@uwaterloo.ca,95%
https://arxiv.org/pdf/2302.09473.pdf,Video-Text Retrieval by Supervised Sparse Multi-Grained Learning,Yimu Wang,yimu.wang@uwaterloo.ca,95%
https://arxiv.org/pdf/2303.12489.pdf,Few-shot Multimodal Multitask Multilingual Learning,Aman Chadha,amanc@stanford.edu,85%
https://arxiv.org/pdf/2303.12489.pdf,Few-shot Multimodal Multitask Multilingual Learning,Vinija Jain,vinija@stanford.edu,85%
https://arxiv.org/pdf/2302.09467.pdf,Designing a 3D-Aware StyleNeRF Encoder for Face Editing,Songlin Yang,yangsonglin2021@ia.ac.cn,95%
https://arxiv.org/pdf/2302.09467.pdf,Designing a 3D-Aware StyleNeRF Encoder for Face Editing,Wei Wang,wwang@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.09467.pdf,Designing a 3D-Aware StyleNeRF Encoder for Face Editing,Jing Dong,jdong@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.09467.pdf,Designing a 3D-Aware StyleNeRF Encoder for Face Editing,Bo Peng,bo.peng@nlpr.ia.ac.cn,95%
https://arxiv.org/pdf/2302.09462.pdf,MedViT: A Robust Vision Transformer for Generalized Medical Image Classification,Omid Nejati Manzari,omid_nejaty@alumni.iust.ac.ir,85%
https://arxiv.org/pdf/2302.09462.pdf,MedViT: A Robust Vision Transformer for Generalized Medical Image Classification,Hamid Ahmadabadi,,0%
https://arxiv.org/pdf/2302.09462.pdf,MedViT: A Robust Vision Transformer for Generalized Medical Image Classification,Hossein Kashiani,,0%
https://arxiv.org/pdf/2302.09462.pdf,MedViT: A Robust Vision Transformer for Generalized Medical Image Classification,Shahriar B. Shokouhi,,0%
https://arxiv.org/pdf/2302.09462.pdf,MedViT: A Robust Vision Transformer for Generalized Medical Image Classification,Ahmad Ayatollahi,,0%
https://arxiv.org/pdf/2302.09461.pdf,Liveness score-based regression neural networks for face anti-spoofing,Changick Kim,changick@kaist.ac.kr,85%
https://arxiv.org/pdf/2302.09461.pdf,Liveness score-based regression neural networks for face anti-spoofing,Youngjun Kwak,,0%
https://arxiv.org/pdf/2302.09461.pdf,Liveness score-based regression neural networks for face anti-spoofing,Minyoung Jung,,0%
https://arxiv.org/pdf/2302.09461.pdf,Liveness score-based regression neural networks for face anti-spoofing,Hunjae Yoo,,0%
https://arxiv.org/pdf/2302.09461.pdf,Liveness score-based regression neural networks for face anti-spoofing,Jinho Shin,,0%
https://arxiv.org/pdf/2302.09429.pdf,NU-AIR -- A Neuromorphic Urban Aerial Dataset for Detection and Localization of Pedestrians and Vehicles,Craig Iaboni,,0%
https://arxiv.org/pdf/2302.09429.pdf,NU-AIR -- A Neuromorphic Urban Aerial Dataset for Detection and Localization of Pedestrians and Vehicles,Thomas Kelly,,0%
https://arxiv.org/pdf/2302.09429.pdf,NU-AIR -- A Neuromorphic Urban Aerial Dataset for Detection and Localization of Pedestrians and Vehicles,Pramod Abichandani,,0%
https://arxiv.org/pdf/2302.09411.pdf,MultiScale Probability Map guided Index Pooling with Attention-based learning for Road and Building Segmentation,Shirsha Bose,shirshabosecs@gmail.com,95%
https://arxiv.org/pdf/2302.09411.pdf,MultiScale Probability Map guided Index Pooling with Attention-based learning for Road and Building Segmentation,Subhasis Chaudhuri,sc@ee.iitb.ac.in,90%
https://arxiv.org/pdf/2302.09411.pdf,MultiScale Probability Map guided Index Pooling with Attention-based learning for Road and Building Segmentation,Biplab Banerjee,getbiplab@gmail.com,85%
https://arxiv.org/pdf/2302.09411.pdf,MultiScale Probability Map guided Index Pooling with Attention-based learning for Road and Building Segmentation,Ritesh Sur Chowdhury,,0%
https://arxiv.org/pdf/2302.09411.pdf,MultiScale Probability Map guided Index Pooling with Attention-based learning for Road and Building Segmentation,Debabrata Pal,,0%
https://arxiv.org/pdf/2302.09411.pdf,MultiScale Probability Map guided Index Pooling with Attention-based learning for Road and Building Segmentation,Shivashish Bose,,0%
https://arxiv.org/pdf/2302.09404.pdf,MorphGANFormer: Transformer-based Face Morphing and De-Morphing,Na Zhang,,0%
https://arxiv.org/pdf/2302.09404.pdf,MorphGANFormer: Transformer-based Face Morphing and De-Morphing,Xudong Liu,,0%
https://arxiv.org/pdf/2302.09404.pdf,MorphGANFormer: Transformer-based Face Morphing and De-Morphing,Xin Li,,0%
https://arxiv.org/pdf/2302.09404.pdf,MorphGANFormer: Transformer-based Face Morphing and De-Morphing,Guo-jun Qi,,0%
https://arxiv.org/pdf/2302.09395.pdf,When Visible-to-Thermal Facial GAN Beats Conditional Diffusion,Catherine Ordun,,0%
https://arxiv.org/pdf/2302.09395.pdf,When Visible-to-Thermal Facial GAN Beats Conditional Diffusion,Edward Raff,,0%
https://arxiv.org/pdf/2302.09395.pdf,When Visible-to-Thermal Facial GAN Beats Conditional Diffusion,Sanjay Purushotham,,0%
https://arxiv.org/pdf/2302.09394.pdf,Deep Neural Networks based Meta-Learning for Network Intrusion Detection,Asifullah Khan,asif@pieas.edu.pk,90%
https://arxiv.org/pdf/2302.09394.pdf,Deep Neural Networks based Meta-Learning for Network Intrusion Detection,Anabia Sohail,,0%
https://arxiv.org/pdf/2302.09394.pdf,Deep Neural Networks based Meta-Learning for Network Intrusion Detection,Bibi Ayisha,,0%
https://arxiv.org/pdf/2302.09394.pdf,Deep Neural Networks based Meta-Learning for Network Intrusion Detection,Irfan Hameed,,0%
https://arxiv.org/pdf/2302.09394.pdf,Deep Neural Networks based Meta-Learning for Network Intrusion Detection,Muhammad Mohsin Zafar,,0%
https://arxiv.org/pdf/2302.09394.pdf,Deep Neural Networks based Meta-Learning for Network Intrusion Detection,Hani Alquhayz,,0%
https://arxiv.org/pdf/2302.09389.pdf,Vulnerability analysis of captcha using Deep learning,Aryan Odugoudar,aryanodugoudar143@gmail.com,95%
https://arxiv.org/pdf/2302.09389.pdf,Vulnerability analysis of captcha using Deep learning,Jaskaran Singh Walia,karanwalia2k3@gmail.com,78%
https://arxiv.org/pdf/2302.09369.pdf,Calibrating the Rigged Lottery: Making All Tickets Reliable,Bowen Lei,bowenlei@stat.tamu.edu,95%
https://arxiv.org/pdf/2302.09369.pdf,Calibrating the Rigged Lottery: Making All Tickets Reliable,Dongkuan Xu,dxu27@ncsu.edu,82%
https://arxiv.org/pdf/2302.09369.pdf,Calibrating the Rigged Lottery: Making All Tickets Reliable,Ruqi Zhang,ruqiz@purdue.edu,85%
https://arxiv.org/pdf/2302.09369.pdf,Calibrating the Rigged Lottery: Making All Tickets Reliable,Bani Mallick,bmallick@stat.tamu.edu,82%
https://arxiv.org/pdf/2302.09365.pdf,Hyneter: Hybrid Network Transformer for Object Detection,Dong Chen,,0%
https://arxiv.org/pdf/2302.09365.pdf,Hyneter: Hybrid Network Transformer for Object Detection,Duoqian Miao,,0%
https://arxiv.org/pdf/2302.09365.pdf,Hyneter: Hybrid Network Transformer for Object Detection,Xuerong Zhao,,0%
https://arxiv.org/pdf/2302.09352.pdf,MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning,Hao He,hehao@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.09352.pdf,MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning,Caoyun Fan,frank92@sjtu.edu.cn,55%
https://arxiv.org/pdf/2302.09352.pdf,MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning,Yitian Li,yitian_li@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.09352.pdf,MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning,Yaohui Jin,jinyh@sjtu.edu.cn,78%
https://arxiv.org/pdf/2302.09352.pdf,MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning,Wenqing Chen,chenwq95@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.09352.pdf,MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning,Jidong Tian,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Xili Dai,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Ke Chen,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Shengbang Tong,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Jingyuan Zhang,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Xingjian Gao,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Mingyang Li,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Druv Pai,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Yuexiang Zhai,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Xiaojun Yuan,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Heung-yeung Shum,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Lionel M. Ni,,0%
https://arxiv.org/pdf/2302.09347.pdf,Closed-Loop Transcription via Convolutional Sparse Coding,Yi Ma,,0%
https://arxiv.org/pdf/2302.09344.pdf,Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics,Aahlad Puli,aahlad@nyu.edu,85%
https://arxiv.org/pdf/2302.09344.pdf,Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics,Ke Yu,yu.ke@pitt.edu,95%
https://arxiv.org/pdf/2302.09344.pdf,Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics,Rajesh Ranganath,rajeshr@cims.nyu.edu,85%
https://arxiv.org/pdf/2302.09344.pdf,Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics,Nihal Murali,nihal.murali@pitt.edu,95%
https://arxiv.org/pdf/2302.09344.pdf,Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics,Kayhan Batmanghelich,batman@bu.edu,90%
https://arxiv.org/pdf/2302.09326.pdf,An Adaptive Plug-and-Play Network for Few-Shot Learning,Hao Li,,0%
https://arxiv.org/pdf/2302.09326.pdf,An Adaptive Plug-and-Play Network for Few-Shot Learning,Li Li,,0%
https://arxiv.org/pdf/2302.09326.pdf,An Adaptive Plug-and-Play Network for Few-Shot Learning,Yunmeng Huang,,0%
https://arxiv.org/pdf/2302.09326.pdf,An Adaptive Plug-and-Play Network for Few-Shot Learning,Ning Li,,0%
https://arxiv.org/pdf/2302.09326.pdf,An Adaptive Plug-and-Play Network for Few-Shot Learning,Yongtao Zhang,,0%
https://arxiv.org/pdf/2302.09323.pdf,Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data,Anqi Qiu,bieqa@nus.edu.sg,60%
https://arxiv.org/pdf/2302.09323.pdf,Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data,Jinghan Huang,,0%
https://arxiv.org/pdf/2302.09323.pdf,Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data,Moo K. Chung,,0%
https://arxiv.org/pdf/2302.09311.pdf,Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields,Seokhwan Jang,swan.jang@samsung.com,82%
https://arxiv.org/pdf/2302.09311.pdf,Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields,Minjung Son,minjungs.son@samsung.com,95%
https://arxiv.org/pdf/2302.09311.pdf,Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields,Ji-yeon Kim,jiyeon31.kim@samsung.com,95%
https://arxiv.org/pdf/2302.09311.pdf,Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields,Sungheon Park,sh2019.park@samsung.com,82%
https://arxiv.org/pdf/2302.09311.pdf,Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields,Nahyup Kang,nahyup.kang@samsung.com,95%
https://arxiv.org/pdf/2302.09311.pdf,Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields,Young Chun Ahn,ychun.ahn@samsung.com,82%
https://arxiv.org/pdf/2302.09309.pdf,StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning,Yu-gang Jiang,ygj@fudan.edu.cn,90%
https://arxiv.org/pdf/2302.09309.pdf,StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning,Yuqian Fu,fuyq20@fudan.edu.cn,78%
https://arxiv.org/pdf/2302.09309.pdf,StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning,Yanwei Fu,yanweifu@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.09309.pdf,StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning,Yu Xie,yxie18@fudan.edu.cn,82%
https://arxiv.org/pdf/2302.11544.pdf,One-Pot Multi-Frame Denoising,Yanye Lu,yanye.lu@pku.edu.cn,95%
https://arxiv.org/pdf/2302.11544.pdf,One-Pot Multi-Frame Denoising,Lei Zhu,zhulei@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2302.11544.pdf,One-Pot Multi-Frame Denoising,Lujia Jin,jinlujia@pku.edu.cn,95%
https://arxiv.org/pdf/2302.11544.pdf,One-Pot Multi-Frame Denoising,Qian Chen,chen_qian@stu.pku.edu.cn,95%
https://arxiv.org/pdf/2302.11544.pdf,One-Pot Multi-Frame Denoising,Shi Zhao,magishe@pku.edu.cn,60%
https://arxiv.org/pdf/2302.09263.pdf,Multistage Spatial Context Models for Learned Image Compression,Fangzheng Lin,,0%
https://arxiv.org/pdf/2302.09263.pdf,Multistage Spatial Context Models for Learned Image Compression,Heming Sun,,0%
https://arxiv.org/pdf/2302.09263.pdf,Multistage Spatial Context Models for Learned Image Compression,Jinming Liu,,0%
https://arxiv.org/pdf/2302.09263.pdf,Multistage Spatial Context Models for Learned Image Compression,Jiro Katto,,0%
https://arxiv.org/pdf/2302.09260.pdf,Attribute-Specific Manipulation Based on Layer-Wise Channels,Yuanjie Yan,yanyj@smail.nju.edu.cn,82%
https://arxiv.org/pdf/2302.09260.pdf,Attribute-Specific Manipulation Based on Layer-Wise Channels,Jian Zhao,jianzhao@nju.edu.cn,95%
https://arxiv.org/pdf/2302.09260.pdf,Attribute-Specific Manipulation Based on Layer-Wise Channels,Furao Shen,frshen@nju.edu.cn,82%
https://arxiv.org/pdf/2302.09251.pdf,StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization,Elisa Ricci,e.ricci@unitn.it,82%
https://arxiv.org/pdf/2302.09251.pdf,StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization,Biplab Banerjee,getbiplab@gmail.com,85%
https://arxiv.org/pdf/2302.09251.pdf,StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization,Ankit Jha,ankitjha16@gmail.com,95%
https://arxiv.org/pdf/2302.09251.pdf,StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization,Enrico Fini,enrico.fini@unitn.it,95%
https://arxiv.org/pdf/2302.09251.pdf,StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization,Mainak Singha,mainaksingha.iitb@gmail.com,95%
https://arxiv.org/pdf/2302.09251.pdf,StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization,Shirsha Bose,shirshabosecs@gmail.com,95%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,Bo Zhou,bo.zhou@yale.edu,95%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,Michal Sofka,msofka@hyperfine.io,82%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,Jo Schlemper,,0%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,Neel Dey,,0%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,Seyed Sadegh Mohseni Salehi,,0%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,Kevin Sheth,,0%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,Chi Liu,,0%
https://arxiv.org/pdf/2302.09244.pdf,Dual-Domain Self-Supervised Learning for Accelerated Non-Cartesian MRI Reconstruction,James S. Duncan,,0%
https://arxiv.org/pdf/2302.09238.pdf,KLIF: An optimized spiking neuron unit for tuning surrogate gradient slope and membrane potential,Yilei Zhang,yilei.zhang@canterbury.ac.nz,95%
https://arxiv.org/pdf/2302.09238.pdf,KLIF: An optimized spiking neuron unit for tuning surrogate gradient slope and membrane potential,Chunming Jiang,,0%
https://arxiv.org/pdf/2302.09228.pdf,Web Photo Source Identification based on Neural Enhanced Camera Fingerprint,Honghao Huang,huanghonghao.hhh@antgroup.com,95%
https://arxiv.org/pdf/2302.09228.pdf,Web Photo Source Identification based on Neural Enhanced Camera Fingerprint,Xiaobo Zhang,ayou.zxb@antgroup.com,60%
https://arxiv.org/pdf/2302.09228.pdf,Web Photo Source Identification based on Neural Enhanced Camera Fingerprint,Huanyu Ma,huanyu.mhy@antgroup.com,85%
https://arxiv.org/pdf/2302.09228.pdf,Web Photo Source Identification based on Neural Enhanced Camera Fingerprint,Feng Qian,,0%
https://arxiv.org/pdf/2302.09228.pdf,Web Photo Source Identification based on Neural Enhanced Camera Fingerprint,Sifeng He,,0%
https://arxiv.org/pdf/2302.09228.pdf,Web Photo Source Identification based on Neural Enhanced Camera Fingerprint,Lei Yang,,0%
https://arxiv.org/pdf/2302.09227.pdf,Invertible Neural Skinning,Yash Kant,,0%
https://arxiv.org/pdf/2302.09227.pdf,Invertible Neural Skinning,Aliaksandr Siarohin,,0%
https://arxiv.org/pdf/2302.09227.pdf,Invertible Neural Skinning,Riza Alp Guler,,0%
https://arxiv.org/pdf/2302.09227.pdf,Invertible Neural Skinning,Menglei Chai,,0%
https://arxiv.org/pdf/2302.09227.pdf,Invertible Neural Skinning,Jian Ren,,0%
https://arxiv.org/pdf/2302.09227.pdf,Invertible Neural Skinning,Sergey Tulyakov,,0%
https://arxiv.org/pdf/2302.09227.pdf,Invertible Neural Skinning,Igor Gilitschenski,,0%
https://arxiv.org/pdf/2302.09221.pdf,Moby: Empowering 2D Models for Efficient Point Cloud Analytics on the Edge,Jingzong Li,,0%
https://arxiv.org/pdf/2302.09221.pdf,Moby: Empowering 2D Models for Efficient Point Cloud Analytics on the Edge,Yik Hong Cai,,0%
https://arxiv.org/pdf/2302.09221.pdf,Moby: Empowering 2D Models for Efficient Point Cloud Analytics on the Edge,Libin Liu,,0%
https://arxiv.org/pdf/2302.09221.pdf,Moby: Empowering 2D Models for Efficient Point Cloud Analytics on the Edge,Yu Mao,,0%
https://arxiv.org/pdf/2302.09221.pdf,Moby: Empowering 2D Models for Efficient Point Cloud Analytics on the Edge,Chun Jason Xue,,0%
https://arxiv.org/pdf/2302.09221.pdf,Moby: Empowering 2D Models for Efficient Point Cloud Analytics on the Edge,Hong Xu,,0%
https://arxiv.org/pdf/2302.09215.pdf,Domain Agnostic Pipeline for Retina Vessel Segmentation,Benjamin Hou,,0%
https://arxiv.org/pdf/2302.09208.pdf,Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering,Tatsuro Yamane,yamane.tatsuro.20@dois.k.u-tokyo.ac.jp,95%
https://arxiv.org/pdf/2302.09208.pdf,Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering,Pang-jo Chun,,0%
https://arxiv.org/pdf/2302.09208.pdf,Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering,Ji Dang,,0%
https://arxiv.org/pdf/2302.09208.pdf,Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering,Takayuki Okatani,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Md Mahfuzur Rahman Siddiquee,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Jay Shah,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Teresa Wu,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Catherine Chong,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Todd J. Schwedt,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Gina Dumkrieger,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Simona Nikolova,,0%
https://arxiv.org/pdf/2302.09200.pdf,Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images,Baoxin Li,,0%
https://arxiv.org/pdf/2302.09187.pdf,Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer,Nguyen Huu Phong,phong@dei.uc.pt,78%
https://arxiv.org/pdf/2302.09187.pdf,Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer,Bernardete Ribeiro,,0%
https://arxiv.org/pdf/2302.09119.pdf,A Review on Generative Adversarial Networks for Data Augmentation in Person Re-Identification Systems,Victor Uc-cetina,uccetina@correo.uady.mx,55%
https://arxiv.org/pdf/2302.09119.pdf,A Review on Generative Adversarial Networks for Data Augmentation in Person Re-Identification Systems,Laura Alvarez-gonzalez,laura.alvargonza@gmail.com,85%
https://arxiv.org/pdf/2302.09119.pdf,A Review on Generative Adversarial Networks for Data Augmentation in Person Re-Identification Systems,Anabel Martin-gonzalez,,0%
https://arxiv.org/pdf/2302.09108.pdf,ViTA: A Vision Transformer Inference Accelerator for Edge Applications,Gourav Datta,gdatta@usc.edu,82%
https://arxiv.org/pdf/2302.09108.pdf,ViTA: A Vision Transformer Inference Accelerator for Edge Applications,Souvik Kundu,souvikk.kundu@intel.com,95%
https://arxiv.org/pdf/2302.09108.pdf,ViTA: A Vision Transformer Inference Accelerator for Edge Applications,Peter A. Beerel,pabeerel@usc.edu,82%
https://arxiv.org/pdf/2302.09108.pdf,ViTA: A Vision Transformer Inference Accelerator for Edge Applications,Shashank Nag,,0%
https://arxiv.org/pdf/2302.09108.pdf,ViTA: A Vision Transformer Inference Accelerator for Edge Applications,Nitin Chandrachoodan,,0%
https://arxiv.org/pdf/2302.09057.pdf,Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent,Giannis Daras,,0%
https://arxiv.org/pdf/2302.09057.pdf,Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent,Yuval Dagan,,0%
https://arxiv.org/pdf/2302.09057.pdf,Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent,Alexandros G. Dimakis,,0%
https://arxiv.org/pdf/2302.09057.pdf,Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent,Constantinos Daskalakis,,0%
https://arxiv.org/pdf/2302.09053.pdf,OTB-morph: One-Time Biometrics via Morphing,Mahdi Ghafourian,,0%
https://arxiv.org/pdf/2302.09053.pdf,OTB-morph: One-Time Biometrics via Morphing,Julian Fierrez,,0%
https://arxiv.org/pdf/2302.09053.pdf,OTB-morph: One-Time Biometrics via Morphing,Ruben Vera-rodriguez,,0%
https://arxiv.org/pdf/2302.09053.pdf,OTB-morph: One-Time Biometrics via Morphing,Aythami Morales,,0%
https://arxiv.org/pdf/2302.09053.pdf,OTB-morph: One-Time Biometrics via Morphing,Ignacio Serna,,0%
https://arxiv.org/pdf/2302.09043.pdf,Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences,Christopher Lang,,0%
https://arxiv.org/pdf/2302.09043.pdf,Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences,Alexander Braun,,0%
https://arxiv.org/pdf/2302.09043.pdf,Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences,Lars Schillingmann,,0%
https://arxiv.org/pdf/2302.09043.pdf,Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences,Karsten Haug,,0%
https://arxiv.org/pdf/2302.09043.pdf,Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences,Abhinav Valada,,0%
https://arxiv.org/pdf/2302.09027.pdf,CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension,Xiantong Zhen,zhenxt@gmail.com,78%
https://arxiv.org/pdf/2302.09027.pdf,CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension,Ekaterina Shutova,e.shutova@uva.nl,82%
https://arxiv.org/pdf/2302.09027.pdf,CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension,Zhi Zhang,z.zhang@uva.nl,82%
https://arxiv.org/pdf/2302.09027.pdf,CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension,Helen Yannakoudakis,helen.yannakoudakis@kcl.ac.uk,95%
https://arxiv.org/pdf/2303.01255.pdf,Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,Rik Sarkar,rsarkar@inf.ed.ac.uk,82%
https://arxiv.org/pdf/2303.01255.pdf,Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,Pedro Reviriego,pedro.reviriego@upm.es,95%
https://arxiv.org/pdf/2303.01255.pdf,Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,Lauren Watson,lauren.watson@ed.ac.uk,95%
https://arxiv.org/pdf/2303.01255.pdf,Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,Gonzalo Martínez,gonzmart@pa.uc3m.es,65%
https://arxiv.org/pdf/2303.01255.pdf,Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,Marc Juarez,mjuarez@inf.ed.ac.uk,82%
https://arxiv.org/pdf/2303.01255.pdf,Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,José Alberto Hernández,,0%
https://arxiv.org/pdf/2302.09018.pdf,Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences,Bing Su,subingats@gmail.com,95%
https://arxiv.org/pdf/2302.09018.pdf,Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences,Anyi Rao,anyirao@link.cuhk.edu.hk,95%
https://arxiv.org/pdf/2302.09018.pdf,Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences,Yujie Zhou,yujiezhou@ruc.edu.cn,95%
https://arxiv.org/pdf/2302.09018.pdf,Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences,Haodong Duan,,0%
https://arxiv.org/pdf/2302.09018.pdf,Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences,Jiaqi Wang,,0%
https://arxiv.org/pdf/2302.09004.pdf,CovidExpert: A Triplet Siamese Neural Network framework for the detection of COVID-19,Gourab Roy,gourabroysec553@gmail.com,95%
https://arxiv.org/pdf/2302.09004.pdf,CovidExpert: A Triplet Siamese Neural Network framework for the detection of COVID-19,Enamul Hassan,enam-cse@sust.edu,65%
https://arxiv.org/pdf/2302.09004.pdf,CovidExpert: A Triplet Siamese Neural Network framework for the detection of COVID-19,Tareque Rahman Ornob,ornob011@gmail.com,78%
https://arxiv.org/pdf/2302.08980.pdf,Model Doctor for Diagnosing and Treating Segmentation Error,Zhijie Jia,,0%
https://arxiv.org/pdf/2302.08980.pdf,Model Doctor for Diagnosing and Treating Segmentation Error,Lin Chen,,0%
https://arxiv.org/pdf/2302.08980.pdf,Model Doctor for Diagnosing and Treating Segmentation Error,Kaiwen Hu,,0%
https://arxiv.org/pdf/2302.08980.pdf,Model Doctor for Diagnosing and Treating Segmentation Error,Lechao Cheng,,0%
https://arxiv.org/pdf/2302.08980.pdf,Model Doctor for Diagnosing and Treating Segmentation Error,Zunlei Feng,,0%
https://arxiv.org/pdf/2302.08980.pdf,Model Doctor for Diagnosing and Treating Segmentation Error,Mingli Song,,0%
https://arxiv.org/pdf/2302.08967.pdf,sMRI-PatchNet: A novel explainable patch-based deep learning network for Alzheimer's disease diagnosis and discriminative atrophy localisation with Structural MRI,Lianghao Han,lianghao.han@brunel.ac.uk,95%
https://arxiv.org/pdf/2302.08967.pdf,sMRI-PatchNet: A novel explainable patch-based deep learning network for Alzheimer's disease diagnosis and discriminative atrophy localisation with Structural MRI,Xin Zhang,x.zhang@mmu.ac.uk,82%
https://arxiv.org/pdf/2302.08967.pdf,sMRI-PatchNet: A novel explainable patch-based deep learning network for Alzheimer's disease diagnosis and discriminative atrophy localisation with Structural MRI,Liangxiu Han,l.han@mmu.ac.uk,82%
https://arxiv.org/pdf/2302.08967.pdf,sMRI-PatchNet: A novel explainable patch-based deep learning network for Alzheimer's disease diagnosis and discriminative atrophy localisation with Structural MRI,Daoqiang Zhang,dqzhang@nuaa.edu.cn,82%
https://arxiv.org/pdf/2302.08967.pdf,sMRI-PatchNet: A novel explainable patch-based deep learning network for Alzheimer's disease diagnosis and discriminative atrophy localisation with Structural MRI,Darren Dancey,d.dancey@mmu.ac.uk,82%
https://arxiv.org/pdf/2302.08967.pdf,sMRI-PatchNet: A novel explainable patch-based deep learning network for Alzheimer's disease diagnosis and discriminative atrophy localisation with Structural MRI,Haoming Chen,,0%
https://arxiv.org/pdf/2302.08958.pdf,Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts,Benyou Wang,wangbenyou@cuhk.edu.cn,95%
https://arxiv.org/pdf/2302.08958.pdf,Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts,Zhihong Chen,zhihongchen@link.cuhk.edu.cn,95%
https://arxiv.org/pdf/2302.08958.pdf,Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts,Shizhe Diao,sdiaoaa@connect.ust.hk,82%
https://arxiv.org/pdf/2302.08958.pdf,Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts,Xiang Wan,wanxiang@sribd.com,95%
https://arxiv.org/pdf/2302.08958.pdf,Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts,Guanbin Li,liguanbin@mail.sysu.edu.cn,95%
https://arxiv.org/pdf/2302.08948.pdf,Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories,Edwin Carlinet,edwin.carlinet@epita.fr,95%
https://arxiv.org/pdf/2302.08948.pdf,Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories,Bertrand Duménieu,bertrand.dumenieu@ehess.fr,95%
https://arxiv.org/pdf/2302.08948.pdf,Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories,Joseph Chazalon,joseph.chazalon@epita.fr,95%
https://arxiv.org/pdf/2302.08948.pdf,Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories,Nathalie Abadie,,0%
https://arxiv.org/pdf/2302.08947.pdf,Learning from Label Proportion with Online Pseudo-Label Decision by Regret Minimization,Shinnosuke Matsuo,,0%
https://arxiv.org/pdf/2302.08947.pdf,Learning from Label Proportion with Online Pseudo-Label Decision by Regret Minimization,Ryoma Bise,,0%
https://arxiv.org/pdf/2302.08947.pdf,Learning from Label Proportion with Online Pseudo-Label Decision by Regret Minimization,Seiichi Uchida,,0%
https://arxiv.org/pdf/2302.08947.pdf,Learning from Label Proportion with Online Pseudo-Label Decision by Regret Minimization,Daiki Suehiro,,0%
https://arxiv.org/pdf/2302.08943.pdf,Long Range Object-Level Monocular Depth Estimation for UAVs,Nicolas Jourdan,n.jourdan@ptw.tu-darmstadt.de,82%
https://arxiv.org/pdf/2302.08943.pdf,Long Range Object-Level Monocular Depth Estimation for UAVs,David Silva,silva@wingcopter.com,78%
https://arxiv.org/pdf/2302.08943.pdf,Long Range Object-Level Monocular Depth Estimation for UAVs,Nils Gählert,gaehlert@wingcopter.com,55%
https://arxiv.org/pdf/2302.08931.pdf,LDFA: Latent Diffusion Face Anonymization for Self-driving Applications,Martin Lauer,firstname.lastname@kit.edu,70%
https://arxiv.org/pdf/2302.08931.pdf,LDFA: Latent Diffusion Face Anonymization for Self-driving Applications,Marvin Klemp,,0%
https://arxiv.org/pdf/2302.08931.pdf,LDFA: Latent Diffusion Face Anonymization for Self-driving Applications,Kevin Rösch,,0%
https://arxiv.org/pdf/2302.08931.pdf,LDFA: Latent Diffusion Face Anonymization for Self-driving Applications,Royden Wagner,,0%
https://arxiv.org/pdf/2302.08931.pdf,LDFA: Latent Diffusion Face Anonymization for Self-driving Applications,Jannik Quehl,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Xu Zheng,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Yexin Liu,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Yunfan Lu,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Tongyan Hua,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Tianbo Pan,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Weiming Zhang,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Dacheng Tao,,0%
https://arxiv.org/pdf/2302.08890.pdf,Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks,Lin Wang,,0%
https://arxiv.org/pdf/2302.08878.pdf,Less is More: The Influence of Pruning on the Explainability of CNNs,Martin Nocker,martin.nocker@mci.edu,95%
https://arxiv.org/pdf/2302.08878.pdf,Less is More: The Influence of Pruning on the Explainability of CNNs,Pascal Schöttle,pascal.schoettle@mci.edu,85%
https://arxiv.org/pdf/2302.08878.pdf,Less is More: The Influence of Pruning on the Explainability of CNNs,Florian Merkle,florian.merkle@student.uibk.ac.at,95%
https://arxiv.org/pdf/2302.08878.pdf,Less is More: The Influence of Pruning on the Explainability of CNNs,Stephan Schlögl,stephan.schloegl@mci.edu,85%
https://arxiv.org/pdf/2302.08878.pdf,Less is More: The Influence of Pruning on the Explainability of CNNs,David Weber,weber_david@outlook.com,95%
https://arxiv.org/pdf/2302.08867.pdf,Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning,Jack Breen,,0%
https://arxiv.org/pdf/2302.08867.pdf,Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning,Katie Allen,,0%
https://arxiv.org/pdf/2302.08867.pdf,Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning,Kieran Zucker,,0%
https://arxiv.org/pdf/2302.08867.pdf,Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning,Geoff Hall,,0%
https://arxiv.org/pdf/2302.08867.pdf,Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning,Nicolas M. Orsi,,0%
https://arxiv.org/pdf/2302.08867.pdf,Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning,Nishant Ravikumar,,0%
https://arxiv.org/pdf/2302.08861.pdf,AliasNet: Alias Artefact Suppression Network for Accelerated Phase-Encode MRI,Marlon E. Bran Lorenzana,,0%
https://arxiv.org/pdf/2302.08861.pdf,AliasNet: Alias Artefact Suppression Network for Accelerated Phase-Encode MRI,Shekhar S. Chandra,,0%
https://arxiv.org/pdf/2302.08861.pdf,AliasNet: Alias Artefact Suppression Network for Accelerated Phase-Encode MRI,Feng Liu,,0%
https://arxiv.org/pdf/2302.08841.pdf,Lip-to-Speech Synthesis in the Wild with Multi-task Learning,Minsu Kim,,0%
https://arxiv.org/pdf/2302.08841.pdf,Lip-to-Speech Synthesis in the Wild with Multi-task Learning,Joanna Hong,,0%
https://arxiv.org/pdf/2302.08841.pdf,Lip-to-Speech Synthesis in the Wild with Multi-task Learning,Yong Man Ro,,0%
https://arxiv.org/pdf/2302.08818.pdf,Apple scab detection in orchards using deep learning on colour and multispectral images,Robert Rouš,,0%
https://arxiv.org/pdf/2302.08818.pdf,Apple scab detection in orchards using deep learning on colour and multispectral images,Joseph Peller,,0%
https://arxiv.org/pdf/2302.08818.pdf,Apple scab detection in orchards using deep learning on colour and multispectral images,Gerrit Polder,,0%
https://arxiv.org/pdf/2302.08818.pdf,Apple scab detection in orchards using deep learning on colour and multispectral images,Selwin Hageraats,,0%
https://arxiv.org/pdf/2302.08818.pdf,Apple scab detection in orchards using deep learning on colour and multispectral images,Thijs Ruigrok,,0%
https://arxiv.org/pdf/2302.08818.pdf,Apple scab detection in orchards using deep learning on colour and multispectral images,Pieter M. Blok,,0%
https://arxiv.org/pdf/2302.08808.pdf,Paint it Black: Generating paintings from text descriptions,Mahnoor Shahid,,0%
https://arxiv.org/pdf/2302.08808.pdf,Paint it Black: Generating paintings from text descriptions,Mark Koch,,0%
https://arxiv.org/pdf/2302.08808.pdf,Paint it Black: Generating paintings from text descriptions,Niklas Schneider,,0%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Florian Putz,.putz@uk-erlangen.de,78%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Yixing Huang,yixing.huang@uk-erlangen.de,95%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Philipp Sommer,,0%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Christoph Bert,,0%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Andreas Maier,,0%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Manuel Schmidt,,0%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Arnd Dörfler,,0%
https://arxiv.org/pdf/2302.08802.pdf,"Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning",Rainer Fietkau,,0%
https://arxiv.org/pdf/2302.08788.pdf,MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs,Nojun Kwak,zzzlssh|dhk1349|yjean8315|nojunk@snu.ac.kr,85%
https://arxiv.org/pdf/2302.08788.pdf,MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs,Seunghyeon Seo,,0%
https://arxiv.org/pdf/2302.08788.pdf,MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs,Donghoon Han,,0%
https://arxiv.org/pdf/2302.08788.pdf,MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs,Yeonjin Chang,,0%
https://arxiv.org/pdf/2302.08785.pdf,Few-shot 3D LiDAR Semantic Segmentation for Autonomous Driving,Yu Hu,huyu@ict.ac.cn,95%
https://arxiv.org/pdf/2302.08785.pdf,Few-shot 3D LiDAR Semantic Segmentation for Autonomous Driving,Jilin Mei,,0%
https://arxiv.org/pdf/2302.08785.pdf,Few-shot 3D LiDAR Semantic Segmentation for Autonomous Driving,Junbao Zhou,,0%
https://arxiv.org/pdf/2302.10798.pdf,Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training,Xiaoying Zhi,xiaoying.zhi@jpmchase.com,95%
https://arxiv.org/pdf/2302.10798.pdf,Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training,Fran Silavong,fran.silavong@jpmchase.com,95%
https://arxiv.org/pdf/2302.10798.pdf,Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training,Pheobe Sun,pheobe.sun@jpmchase.com,95%
https://arxiv.org/pdf/2302.10798.pdf,Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training,Varun Babbar,varun.babbar@jpmchase.com,95%
https://arxiv.org/pdf/2302.10798.pdf,Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training,Sean Moran,sean.j.moran@jpmchase.com,95%
https://arxiv.org/pdf/2302.10798.pdf,Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training,Ruibo Shi,ruibo.shi@jpmchase.com,95%
https://arxiv.org/pdf/2302.10798.pdf,Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training,Rundong Liu,eric.liu@jpmchase.com,78%
https://arxiv.org/pdf/2302.08771.pdf,Explicit and Implicit Knowledge Distillation via Unlabeled Data,Yuzheng Wang,,0%
https://arxiv.org/pdf/2302.08771.pdf,Explicit and Implicit Knowledge Distillation via Unlabeled Data,Zuhao Ge,,0%
https://arxiv.org/pdf/2302.08771.pdf,Explicit and Implicit Knowledge Distillation via Unlabeled Data,Zhaoyu Chen,,0%
https://arxiv.org/pdf/2302.08771.pdf,Explicit and Implicit Knowledge Distillation via Unlabeled Data,Xian Liu,,0%
https://arxiv.org/pdf/2302.08771.pdf,Explicit and Implicit Knowledge Distillation via Unlabeled Data,Chuangjia Ma,,0%
https://arxiv.org/pdf/2302.08771.pdf,Explicit and Implicit Knowledge Distillation via Unlabeled Data,Yunquan Sun,,0%
https://arxiv.org/pdf/2302.08771.pdf,Explicit and Implicit Knowledge Distillation via Unlabeled Data,Lizhe Qi,,0%
https://arxiv.org/pdf/2302.08769.pdf,Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,Weiming Shen,wshen@ieee.org,82%
https://arxiv.org/pdf/2302.08769.pdf,Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,Zhaoge Liu,liuzhg@hust.edu.cn,78%
https://arxiv.org/pdf/2302.08769.pdf,Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,Yunkang Cao,,0%
https://arxiv.org/pdf/2302.08769.pdf,Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,Xiaohao Xu,,0%
https://arxiv.org/pdf/2302.08765.pdf,On the Regularising Levenberg-Marquardt Method for Blinn-Phong Photometric Stereo,Michael Breuß,breuss@b-tu.de,55%
https://arxiv.org/pdf/2302.08765.pdf,On the Regularising Levenberg-Marquardt Method for Blinn-Phong Photometric Stereo,Georg Radow,radow@b-tu.de,78%
https://arxiv.org/pdf/2302.08764.pdf,Adversarial Contrastive Distillation with Adaptive Denoising,Yuzheng Wang,,0%
https://arxiv.org/pdf/2302.08764.pdf,Adversarial Contrastive Distillation with Adaptive Denoising,Zhaoyu Chen,,0%
https://arxiv.org/pdf/2302.08764.pdf,Adversarial Contrastive Distillation with Adaptive Denoising,Dingkang Yang,,0%
https://arxiv.org/pdf/2302.08764.pdf,Adversarial Contrastive Distillation with Adaptive Denoising,Yang Liu,,0%
https://arxiv.org/pdf/2302.08764.pdf,Adversarial Contrastive Distillation with Adaptive Denoising,Siao Liu,,0%
https://arxiv.org/pdf/2302.08764.pdf,Adversarial Contrastive Distillation with Adaptive Denoising,Wenqiang Zhang,,0%
https://arxiv.org/pdf/2302.08764.pdf,Adversarial Contrastive Distillation with Adaptive Denoising,Lizhe Qi,,0%
https://arxiv.org/pdf/2302.08760.pdf,3D Human Pose Lifting with Grid Convolution,Anbang Yao,anbang.yao@intel.com,95%
https://arxiv.org/pdf/2302.08760.pdf,3D Human Pose Lifting with Grid Convolution,Shandong Wang,shandong.wang@intel.com,95%
https://arxiv.org/pdf/2302.08760.pdf,3D Human Pose Lifting with Grid Convolution,Yuyang Liu,yyliu22@mails.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.08760.pdf,3D Human Pose Lifting with Grid Convolution,Enhua Wu,ehwu@um.edu.mo,82%
https://arxiv.org/pdf/2302.08760.pdf,3D Human Pose Lifting with Grid Convolution,Yangyuxuan Kang,,0%
https://arxiv.org/pdf/2303.11170.pdf,A Pervasive Framework for Human Detection and Tracking,Fesatidis Georgios,fgeorgios@uth.gr,82%
https://arxiv.org/pdf/2303.11170.pdf,A Pervasive Framework for Human Detection and Tracking,Bratsos Dimitrios,brdimitrios@uth.gr,82%
https://arxiv.org/pdf/2303.11170.pdf,A Pervasive Framework for Human Detection and Tracking,Kostas Kolomvatsos,kostasks@uth.gr,85%
https://arxiv.org/pdf/2302.08751.pdf,MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model,Seunghyeon Seo,,0%
https://arxiv.org/pdf/2302.08751.pdf,MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model,Jaeyoung Yoo,,0%
https://arxiv.org/pdf/2302.08751.pdf,MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model,Jihye Hwang,,0%
https://arxiv.org/pdf/2302.08751.pdf,MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model,Nojun Kwak,,0%
https://arxiv.org/pdf/2302.08741.pdf,New Insights for the Stability-Plasticity Dilemma in Online Continual Learning,Ho Bae,hobae@ewha.ac.kr,95%
https://arxiv.org/pdf/2302.08741.pdf,New Insights for the Stability-Plasticity Dilemma in Online Continual Learning,Sungroh Yoon,sryoon@snu.ac.kr,82%
https://arxiv.org/pdf/2302.08741.pdf,New Insights for the Stability-Plasticity Dilemma in Online Continual Learning,Dahuin Jung,annajung0625@snu.ac.kr,78%
https://arxiv.org/pdf/2302.08741.pdf,New Insights for the Stability-Plasticity Dilemma in Online Continual Learning,Dongjin Lee,,0%
https://arxiv.org/pdf/2302.08741.pdf,New Insights for the Stability-Plasticity Dilemma in Online Continual Learning,Sunwon Hong,,0%
https://arxiv.org/pdf/2302.08741.pdf,New Insights for the Stability-Plasticity Dilemma in Online Continual Learning,Hyemi Jang,,0%
https://arxiv.org/pdf/2302.08722.pdf,GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis,Yizhe Zhang,yizhe.zhang.cs@gmail.com,95%
https://arxiv.org/pdf/2302.08722.pdf,GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis,Danny Z. Chen,dchen@nd.edu,82%
https://arxiv.org/pdf/2302.08720.pdf,Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales,Nicolaas J. Annau,nannau@uvic.ca,82%
https://arxiv.org/pdf/2302.08720.pdf,Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales,Alex J. Cannon,,0%
https://arxiv.org/pdf/2302.08720.pdf,Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales,Adam H. Monahan,,0%
https://arxiv.org/pdf/2302.08715.pdf,EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment,Zicheng Zhang,,0%
https://arxiv.org/pdf/2302.08715.pdf,EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment,Wei Sun,,0%
https://arxiv.org/pdf/2302.08715.pdf,EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment,Yingjie Zhou,,0%
https://arxiv.org/pdf/2302.08715.pdf,EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment,Wei Lu,,0%
https://arxiv.org/pdf/2302.08715.pdf,EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment,Yucheng Zhu,,0%
https://arxiv.org/pdf/2302.08715.pdf,EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment,Xiongkuo Min,,0%
https://arxiv.org/pdf/2302.08715.pdf,EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment,Guangtao Zhai,,0%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Zhouchuan Xu,atuerxu@tencent.com,78%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Quanchao Hui,andrewshui@tencent.com,78%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Shupeng Su,pennsu@tencent.com,78%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Yexin Wang,yexinwang@tencent.com,95%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Yixiao Ge,yixiaoge@tencent.com,95%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Yukang Gan,brucegan@tencent.com,78%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Chang Zhou,chanzhou@tencent.com,82%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Ying Shan,yingsshan@tencent.com,95%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Xuyuan Xu,,0%
https://arxiv.org/pdf/2302.08714.pdf,Binary Embedding-based Retrieval at Tencent,Xiang Chen,,0%
https://arxiv.org/pdf/2302.08709.pdf,Multimodal Propaganda Processing,Vincent Ng,vince@hlt.utdallas.edu,90%
https://arxiv.org/pdf/2302.08709.pdf,Multimodal Propaganda Processing,Shengjie Li,,0%
https://arxiv.org/pdf/2302.08706.pdf,Fine-grained Cross-modal Fusion based Refinement for Text-to-Image Synthesis,Yang Wang,yangwang@hfut.edu.cn,95%
https://arxiv.org/pdf/2302.08706.pdf,Fine-grained Cross-modal Fusion based Refinement for Text-to-Image Synthesis,Haoran Sun,,0%
https://arxiv.org/pdf/2302.08706.pdf,Fine-grained Cross-modal Fusion based Refinement for Text-to-Image Synthesis,Haipeng Liu,,0%
https://arxiv.org/pdf/2302.08706.pdf,Fine-grained Cross-modal Fusion based Refinement for Text-to-Image Synthesis,Biao Qian,,0%
https://arxiv.org/pdf/2302.08689.pdf,Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition,Shengqin Wang,,0%
https://arxiv.org/pdf/2302.08689.pdf,Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition,Yongji Zhang,,0%
https://arxiv.org/pdf/2302.08689.pdf,Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition,Hong Qi,,0%
https://arxiv.org/pdf/2302.08689.pdf,Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition,Minghao Zhao,,0%
https://arxiv.org/pdf/2302.08689.pdf,Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition,Yu Jiang,,0%
https://arxiv.org/pdf/2302.08682.pdf,Random Padding Data Augmentation,Laicheng Zhong,laicheng.zhong@sydney.edu.au,95%
https://arxiv.org/pdf/2302.08682.pdf,Random Padding Data Augmentation,Nan Yang,n.yang@sydney.edu.au,82%
https://arxiv.org/pdf/2302.08682.pdf,Random Padding Data Augmentation,Dong Yuan,dong.yuan@sydney.edu.au,95%
https://arxiv.org/pdf/2302.08682.pdf,Random Padding Data Augmentation,Wei Bao,wei.bao@sydney.edu.au,95%
https://arxiv.org/pdf/2302.08682.pdf,Random Padding Data Augmentation,Fan Huang,fan.huang@sydney.edu.au,95%
https://arxiv.org/pdf/2302.08674.pdf,EnfoMax: Domain Entropy and Mutual Information Maximization for Domain Generalized Face Anti-spoofing,Tianyi Zheng,tyzheng@sjtu.edu.cn,82%
https://arxiv.org/pdf/2302.08672.pdf,Multimodal Subtask Graph Generation from Instructional Videos,Yunseok Jang,,0%
https://arxiv.org/pdf/2302.08672.pdf,Multimodal Subtask Graph Generation from Instructional Videos,Sungryull Sohn,,0%
https://arxiv.org/pdf/2302.08672.pdf,Multimodal Subtask Graph Generation from Instructional Videos,Lajanugen Logeswaran,,0%
https://arxiv.org/pdf/2302.08672.pdf,Multimodal Subtask Graph Generation from Instructional Videos,Tiange Luo,,0%
https://arxiv.org/pdf/2302.08672.pdf,Multimodal Subtask Graph Generation from Instructional Videos,Moontae Lee,,0%
https://arxiv.org/pdf/2302.08672.pdf,Multimodal Subtask Graph Generation from Instructional Videos,Honglak Lee,,0%
https://arxiv.org/pdf/2302.08670.pdf,Cascaded information enhancement and cross-modal attention feature fusion for multispectral pedestrian detection,Kaizheng Wang,kz.wang@foxmail.com,82%
https://arxiv.org/pdf/2302.08670.pdf,Cascaded information enhancement and cross-modal attention feature fusion for multispectral pedestrian detection,Yang Yang,,0%
https://arxiv.org/pdf/2302.08670.pdf,Cascaded information enhancement and cross-modal attention feature fusion for multispectral pedestrian detection,Kaixiong Xu,,0%
https://arxiv.org/pdf/2302.08662.pdf,Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression,Zhiyu Pan,zhiyupan@hust.edu.cn,95%
https://arxiv.org/pdf/2302.08662.pdf,Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression,Zhiguo Cao,zgcao@hust.edu.cn,82%
https://arxiv.org/pdf/2302.08662.pdf,Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression,Yinpeng Chen,,0%
https://arxiv.org/pdf/2302.08662.pdf,Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression,Jiale Zhang,,0%
https://arxiv.org/pdf/2302.08662.pdf,Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression,Hao Lu,,0%
https://arxiv.org/pdf/2302.08662.pdf,Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression,Weicai Zhong,,0%
https://arxiv.org/pdf/2302.08650.pdf,Gaussian-smoothed Imbalance Data Improves Speech Emotion Recognition,Xuefeng Liang,,0%
https://arxiv.org/pdf/2302.08650.pdf,Gaussian-smoothed Imbalance Data Improves Speech Emotion Recognition,Hexin Jiang,,0%
https://arxiv.org/pdf/2302.08650.pdf,Gaussian-smoothed Imbalance Data Improves Speech Emotion Recognition,Wenxin Xu,,0%
https://arxiv.org/pdf/2302.08650.pdf,Gaussian-smoothed Imbalance Data Improves Speech Emotion Recognition,Ying Zhou,,0%
https://arxiv.org/pdf/2302.10915.pdf,Conformers are All You Need for Visual Speech Recognition,Oscar Chang,,0%
https://arxiv.org/pdf/2302.10915.pdf,Conformers are All You Need for Visual Speech Recognition,Hank Liao,,0%
https://arxiv.org/pdf/2302.10915.pdf,Conformers are All You Need for Visual Speech Recognition,Dmitriy Serdyuk,,0%
https://arxiv.org/pdf/2302.10915.pdf,Conformers are All You Need for Visual Speech Recognition,Ankit Shah,,0%
https://arxiv.org/pdf/2302.10915.pdf,Conformers are All You Need for Visual Speech Recognition,Olivier Siohan,,0%
https://arxiv.org/pdf/2302.08646.pdf,AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving,Ang Li,angli@umd.edu,95%
https://arxiv.org/pdf/2302.08646.pdf,AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving,Zhe Chen,chenz@csijri.com,78%
https://arxiv.org/pdf/2302.08646.pdf,AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving,Tianyue Zheng,tianyue002@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.08646.pdf,AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving,Jun Luo,junluo@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.08646.pdf,AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving,Hongbo Wang,,0%
https://arxiv.org/pdf/2302.08641.pdf,Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey,Satish Kumar Singh,sk.singh@iiita.ac.in,82%
https://arxiv.org/pdf/2302.08641.pdf,Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey,Shiv Ram Dubey,srdubey@iiita.ac.in,82%
https://arxiv.org/pdf/2302.08638.pdf,Low Latency Video Denoising for Online Conferencing Using CNN Architectures,Altanai Bisht,abisht@seattleu.edu,82%
https://arxiv.org/pdf/2302.08638.pdf,Low Latency Video Denoising for Online Conferencing Using CNN Architectures,Ana Carolina De Souza Mendes,adesouzamendes@seattleu.edu,82%
https://arxiv.org/pdf/2302.08638.pdf,Low Latency Video Denoising for Online Conferencing Using CNN Architectures,Justin David Thoreson,thoresonjust@seattleu.edu,78%
https://arxiv.org/pdf/2302.08638.pdf,Low Latency Video Denoising for Online Conferencing Using CNN Architectures,Shadrokh Samavi,samavishadro@seattleu.edu,82%
https://arxiv.org/pdf/2302.08595.pdf,Frequency-domain Learning for Volumetric-based 3D Data Perception,Suya You,suya.you.civ@mail.mil,95%
https://arxiv.org/pdf/2302.08595.pdf,Frequency-domain Learning for Volumetric-based 3D Data Perception,Zifan Yu,zifanyu@asu.edu,95%
https://arxiv.org/pdf/2302.08595.pdf,Frequency-domain Learning for Volumetric-based 3D Data Perception,Fengbo Ren,renfengbo@asu.edu,95%
https://arxiv.org/pdf/2302.08594.pdf,TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation,Zhikang Zhang,zhikang.zhang@asu.edu,95%
https://arxiv.org/pdf/2302.08594.pdf,TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation,Zifan Yu,zifanyu@asu.edu,95%
https://arxiv.org/pdf/2302.08594.pdf,TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation,Meida Chen,mechen@ict.usc.edu,82%
https://arxiv.org/pdf/2302.08594.pdf,TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation,Fengbo Ren,fren5@asu.edu,82%
https://arxiv.org/pdf/2302.08594.pdf,TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation,Sanjeev Agarwal,sanjeev.agarwal.civ@army.mil,95%
https://arxiv.org/pdf/2302.08594.pdf,TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation,Suya You,,0%
https://arxiv.org/pdf/2302.08594.pdf,TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation,Raghuveer Rao,,0%
https://arxiv.org/pdf/2302.08575.pdf,Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media,Gerhard Paaß,,0%
https://arxiv.org/pdf/2302.08575.pdf,Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media,Sven Giesselbach,,0%
https://arxiv.org/pdf/2302.08572.pdf,Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers,Melissa Hall,,0%
https://arxiv.org/pdf/2302.08572.pdf,Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers,Bobbie Chern,,0%
https://arxiv.org/pdf/2302.08572.pdf,Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers,Laura Gustafson,,0%
https://arxiv.org/pdf/2302.08572.pdf,Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers,Denisse Ventura,,0%
https://arxiv.org/pdf/2302.08572.pdf,Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers,Harshad Kulkarni,,0%
https://arxiv.org/pdf/2302.08572.pdf,Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers,Candace Ross,,0%
https://arxiv.org/pdf/2302.08572.pdf,Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers,Nicolas Usunier,,0%
https://arxiv.org/pdf/2302.08510.pdf,Text-driven Visual Synthesis with Latent Diffusion Prior,Ting-hsuan Liao,,0%
https://arxiv.org/pdf/2302.08510.pdf,Text-driven Visual Synthesis with Latent Diffusion Prior,Songwei Ge,,0%
https://arxiv.org/pdf/2302.08510.pdf,Text-driven Visual Synthesis with Latent Diffusion Prior,Yiran Xu,,0%
https://arxiv.org/pdf/2302.08510.pdf,Text-driven Visual Synthesis with Latent Diffusion Prior,Yao-chih Lee,,0%
https://arxiv.org/pdf/2302.08510.pdf,Text-driven Visual Synthesis with Latent Diffusion Prior,Badour Albahar,,0%
https://arxiv.org/pdf/2302.08510.pdf,Text-driven Visual Synthesis with Latent Diffusion Prior,Jia-bin Huang,,0%
https://arxiv.org/pdf/2302.08509.pdf,3D-aware Conditional Image Synthesis,Kangle Deng,,0%
https://arxiv.org/pdf/2302.08509.pdf,3D-aware Conditional Image Synthesis,Gengshan Yang,,0%
https://arxiv.org/pdf/2302.08509.pdf,3D-aware Conditional Image Synthesis,Deva Ramanan,,0%
https://arxiv.org/pdf/2302.08509.pdf,3D-aware Conditional Image Synthesis,Jun-yan Zhu,,0%
https://arxiv.org/pdf/2302.08504.pdf,PersonNeRF: Personalized Reconstruction from Photo Collections,Chung-yi Weng,,0%
https://arxiv.org/pdf/2302.08504.pdf,PersonNeRF: Personalized Reconstruction from Photo Collections,Pratul P. Srinivasan,,0%
https://arxiv.org/pdf/2302.08504.pdf,PersonNeRF: Personalized Reconstruction from Photo Collections,Brian Curless,,0%
https://arxiv.org/pdf/2302.08504.pdf,PersonNeRF: Personalized Reconstruction from Photo Collections,Ira Kemelmacher-shlizerman,,0%
https://arxiv.org/pdf/2302.08481.pdf,Local-to-Global Information Communication for Real-Time Semantic Segmentation Network Search,Guangliang Cheng,,0%
https://arxiv.org/pdf/2302.08481.pdf,Local-to-Global Information Communication for Real-Time Semantic Segmentation Network Search,Peng Sun,,0%
https://arxiv.org/pdf/2302.08481.pdf,Local-to-Global Information Communication for Real-Time Semantic Segmentation Network Search,Ting-bing Xu,,0%
https://arxiv.org/pdf/2302.08481.pdf,Local-to-Global Information Communication for Real-Time Semantic Segmentation Network Search,Shuchang Lyu,,0%
https://arxiv.org/pdf/2302.08481.pdf,Local-to-Global Information Communication for Real-Time Semantic Segmentation Network Search,Peiwen Lin,,0%
https://arxiv.org/pdf/2302.08478.pdf,Kernelized Back-Projection Networks for Blind Super Resolution,Norimichi Ukita,ukita@toyota-ti.ac.jp,78%
https://arxiv.org/pdf/2302.08478.pdf,Kernelized Back-Projection Networks for Blind Super Resolution,Tomoki Yoshida,,0%
https://arxiv.org/pdf/2302.08478.pdf,Kernelized Back-Projection Networks for Blind Super Resolution,Yuki Kondo,,0%
https://arxiv.org/pdf/2302.08478.pdf,Kernelized Back-Projection Networks for Blind Super Resolution,Takahiro Maeda,,0%
https://arxiv.org/pdf/2302.08478.pdf,Kernelized Back-Projection Networks for Blind Super Resolution,Kazutoshi Akita,,0%
https://arxiv.org/pdf/2302.08474.pdf,Efficient 3D Object Reconstruction using Visual Transformers,Yuhan Li,yli3326@gatech.edu,82%
https://arxiv.org/pdf/2302.08474.pdf,Efficient 3D Object Reconstruction using Visual Transformers,Wei Zhou,wzhou322@gatech.edu,82%
https://arxiv.org/pdf/2302.08474.pdf,Efficient 3D Object Reconstruction using Visual Transformers,Xiaofeng Wu,xwu414@gatech.edu,82%
https://arxiv.org/pdf/2302.08474.pdf,Efficient 3D Object Reconstruction using Visual Transformers,Rohan Agarwal,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Chong Mou,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Xintao Wang,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Liangbin Xie,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Yanze Wu,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Jian Zhang,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Zhongang Qi,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Ying Shan,,0%
https://arxiv.org/pdf/2302.08453.pdf,T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models,Xiaohu Qie,,0%
https://arxiv.org/pdf/2302.08427.pdf,Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies,Emma Sarfati,,0%
https://arxiv.org/pdf/2302.08427.pdf,Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies,Alexandre Bone,,0%
https://arxiv.org/pdf/2302.08427.pdf,Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies,Marc-michel Rohe,,0%
https://arxiv.org/pdf/2302.08427.pdf,Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies,Pietro Gori,,0%
https://arxiv.org/pdf/2302.08427.pdf,Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies,Isabelle Bloch,,0%
https://arxiv.org/pdf/2302.08411.pdf,Explicit Diffusion of Gaussian Mixture Model Based Image Priors,Martin Zach,martin.zach@icg.tugraz.at,95%
https://arxiv.org/pdf/2302.08411.pdf,Explicit Diffusion of Gaussian Mixture Model Based Image Priors,Thomas Pock,"Thomas Pock
pock@icg.tugraz.at",95%
https://arxiv.org/pdf/2302.08411.pdf,Explicit Diffusion of Gaussian Mixture Model Based Image Priors,Erich Kobler,kobler@uni-bonn.de,78%
https://arxiv.org/pdf/2302.08411.pdf,Explicit Diffusion of Gaussian Mixture Model Based Image Priors,Antonin Chambolle,antonin.chambolle@ceremade.dauphine.fr,95%
https://arxiv.org/pdf/2302.09301.pdf,Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension,Henry Kvinge,,0%
https://arxiv.org/pdf/2302.09301.pdf,Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension,Davis Brown,,0%
https://arxiv.org/pdf/2302.09301.pdf,Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension,Charles Godfrey,,0%
https://arxiv.org/pdf/2302.08374.pdf,Efficiency 360: Efficient Vision Transformers,Badri N. Patro,badripatro@microsoft.com,95%
https://arxiv.org/pdf/2302.08374.pdf,Efficiency 360: Efficient Vision Transformers,Vijay Srinivas Agneeswaran,vagneeswaran@microsoft.com,82%
https://arxiv.org/pdf/2302.08366.pdf,Defect Transfer GAN: Diverse Defect Synthesis for Data Augmentation,Eduardo Monari,eduardo.monari@de.bosch.com,95%
https://arxiv.org/pdf/2302.08366.pdf,Defect Transfer GAN: Diverse Defect Synthesis for Data Augmentation,Marco F. Huber,marco.huber@ieee.org,95%
https://arxiv.org/pdf/2302.08366.pdf,Defect Transfer GAN: Diverse Defect Synthesis for Data Augmentation,Ruyu Wang,ruyu.wang@de.bosch.com,95%
https://arxiv.org/pdf/2302.08366.pdf,Defect Transfer GAN: Diverse Defect Synthesis for Data Augmentation,Sabrina Hoppe,sabrina.hoppe@de.bosch.com,95%
https://arxiv.org/pdf/2302.10237.pdf,SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry,Jie Yang,yangjie01@ict.ac.cn,95%
https://arxiv.org/pdf/2302.10237.pdf,SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry,Yu-kun Lai,LaiY4@cardiff.ac.uk,78%
https://arxiv.org/pdf/2302.10237.pdf,SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry,Leonidas J. Guibas,guibas@cs.stanford.edu,78%
https://arxiv.org/pdf/2302.10237.pdf,SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry,Lin Gao,gaolin@ict.ac.cn,95%
https://arxiv.org/pdf/2302.10237.pdf,SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry,Jia-mu Sun,sunjiamu21s@ict.ac.cn,95%
https://arxiv.org/pdf/2302.10237.pdf,SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry,Kaichun Mo,kaichunm@stanford.edu,85%
https://arxiv.org/pdf/2302.08357.pdf,Boundary Guided Learning-Free Semantic Control with Diffusion Models,Yan Yan,yyan34@iit.edu,95%
https://arxiv.org/pdf/2302.08357.pdf,Boundary Guided Learning-Free Semantic Control with Diffusion Models,Zhiwei Deng,zhiweideng@google.com,95%
https://arxiv.org/pdf/2302.08357.pdf,Boundary Guided Learning-Free Semantic Control with Diffusion Models,Ye Zhu,yezhu@princeton.edu,95%
https://arxiv.org/pdf/2302.08357.pdf,Boundary Guided Learning-Free Semantic Control with Diffusion Models,Yu Wu,wuyucs@whu.edu.cn,95%
https://arxiv.org/pdf/2302.08357.pdf,Boundary Guided Learning-Free Semantic Control with Diffusion Models,Olga Russakovsky,olgarus@princeton.edu,85%
https://arxiv.org/pdf/2302.08343.pdf,Cluster-based Deep Ensemble Learning for Emotion Classification in Internet Memes,Xiaoyu Guo,xiaoyu.guo@nuaa.edu.cn,95%
https://arxiv.org/pdf/2302.08343.pdf,Cluster-based Deep Ensemble Learning for Emotion Classification in Internet Memes,Jing Ma,,0%
https://arxiv.org/pdf/2302.08343.pdf,Cluster-based Deep Ensemble Learning for Emotion Classification in Internet Memes,Arkaitz Zubiaga,,0%
https://arxiv.org/pdf/2302.08320.pdf,Introduction to Presentation Attacks in Signature Biometrics and Recent Advances,Carlos Gonzalez-garcia,carlos.gonzalezgarcia@estudiante.uam.es,85%
https://arxiv.org/pdf/2302.08320.pdf,Introduction to Presentation Attacks in Signature Biometrics and Recent Advances,Julian Fierrez,julian.fierrez@uam.es,95%
https://arxiv.org/pdf/2302.08320.pdf,Introduction to Presentation Attacks in Signature Biometrics and Recent Advances,Ruben Tolosana,ruben.tolosana@uam.es,95%
https://arxiv.org/pdf/2302.08320.pdf,Introduction to Presentation Attacks in Signature Biometrics and Recent Advances,Javier Ortega-garcia,javier.ortega@uam.es,85%
https://arxiv.org/pdf/2302.08320.pdf,Introduction to Presentation Attacks in Signature Biometrics and Recent Advances,Ruben Vera-rodriguez,,0%
https://arxiv.org/pdf/2302.08908.pdf,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,Xingjian Shi,xjshi@amazon.com,82%
https://arxiv.org/pdf/2302.08908.pdf,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,Tong He,htong@amazon.com,85%
https://arxiv.org/pdf/2302.08908.pdf,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,Mu Li,mli@amazon.com,82%
https://arxiv.org/pdf/2302.08908.pdf,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,Tianjun Xiao,tianjux@amazon.com,84%
https://arxiv.org/pdf/2302.08908.pdf,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,Jiaxin Cheng,chengjia@isi.edu,78%
https://arxiv.org/pdf/2302.08908.pdf,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,Xiao Liang,xiao@sjtu.edu.cn,85%
https://arxiv.org/pdf/2302.08292.pdf,Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles,Léo Lemarié,firstname.lastname@navya.tech,75%
https://arxiv.org/pdf/2302.08292.pdf,Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles,Alexandre Almin,,0%
https://arxiv.org/pdf/2302.08292.pdf,Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles,Anh Duong,,0%
https://arxiv.org/pdf/2302.08292.pdf,Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles,B Ravi Kiran,,0%
https://arxiv.org/pdf/2302.08287.pdf,Unsupervised Evaluation of Out-of-distribution Detection: A Data-centric Perspective,Weihong Deng,whdeng@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.08287.pdf,Unsupervised Evaluation of Out-of-distribution Detection: A Data-centric Perspective,Liang Zheng,liang.zheng@anu.edu.au,95%
https://arxiv.org/pdf/2302.08287.pdf,Unsupervised Evaluation of Out-of-distribution Detection: A Data-centric Perspective,Yuhang Zhang,,0%
https://arxiv.org/pdf/2302.08274.pdf,Robust Human Motion Forecasting using Transformer-based Model,Esteve Valls Mascaro,,0%
https://arxiv.org/pdf/2302.08274.pdf,Robust Human Motion Forecasting using Transformer-based Model,Shuo Ma,,0%
https://arxiv.org/pdf/2302.08274.pdf,Robust Human Motion Forecasting using Transformer-based Model,Hyemin Ahn,,0%
https://arxiv.org/pdf/2302.08274.pdf,Robust Human Motion Forecasting using Transformer-based Model,Dongheui Lee,,0%
https://arxiv.org/pdf/2302.08272.pdf,Revisiting Hidden Representations in Transfer Learning for Medical Imaging,Dovile Juodelyte,,0%
https://arxiv.org/pdf/2302.08272.pdf,Revisiting Hidden Representations in Transfer Learning for Medical Imaging,Amelia Jiménez-sánchez,,0%
https://arxiv.org/pdf/2302.08272.pdf,Revisiting Hidden Representations in Transfer Learning for Medical Imaging,Veronika Cheplygina,,0%
https://arxiv.org/pdf/2302.08270.pdf,Detecting Clouds in Multispectral Satellite Images Using Quantum-Kernel Support Vector Machines,Jakub Mielczarek,jakub.mielczarek@uj.edu.pl,95%
https://arxiv.org/pdf/2302.08270.pdf,Detecting Clouds in Multispectral Satellite Images Using Quantum-Kernel Support Vector Machines,Bertrand Le Saux,Bertrand.Le.Saux@esa.int,95%
https://arxiv.org/pdf/2302.08270.pdf,Detecting Clouds in Multispectral Satellite Images Using Quantum-Kernel Support Vector Machines,Filip Szczepanek,.szczepanek@student.uj.edu.pl,78%
https://arxiv.org/pdf/2302.08270.pdf,Detecting Clouds in Multispectral Satellite Images Using Quantum-Kernel Support Vector Machines,Jakub Nalepa,jnalepa@ieee.org,82%
https://arxiv.org/pdf/2302.08270.pdf,Detecting Clouds in Multispectral Satellite Images Using Quantum-Kernel Support Vector Machines,Artur Miroszewski,artur.miroszewski@uj.edu.pl,95%
https://arxiv.org/pdf/2302.08270.pdf,Detecting Clouds in Multispectral Satellite Images Using Quantum-Kernel Support Vector Machines,Grzegorz Czelusta,grzegorz.czelusta@doctoral.uj.edu.pl,95%
https://arxiv.org/pdf/2302.08270.pdf,Detecting Clouds in Multispectral Satellite Images Using Quantum-Kernel Support Vector Machines,Bartosz Grabowski,,0%
https://arxiv.org/pdf/2302.08269.pdf,SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images,Junjie Wen,,0%
https://arxiv.org/pdf/2302.08269.pdf,SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images,Jinqiang Cui,,0%
https://arxiv.org/pdf/2302.08269.pdf,SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images,Zhenjun Zhao,,0%
https://arxiv.org/pdf/2302.08269.pdf,SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images,Ruixin Yan,,0%
https://arxiv.org/pdf/2302.08269.pdf,SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images,Zhi Gao,,0%
https://arxiv.org/pdf/2302.08269.pdf,SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images,Lihua Dou,,0%
https://arxiv.org/pdf/2302.08269.pdf,SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images,Ben M. Chen,,0%
https://arxiv.org/pdf/2302.08268.pdf,Retrieval-augmented Image Captioning,Rita Ramos,ritaparadaramos@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2302.08268.pdf,Retrieval-augmented Image Captioning,Desmond Elliott,,0%
https://arxiv.org/pdf/2302.08268.pdf,Retrieval-augmented Image Captioning,Bruno Martins,,0%
https://arxiv.org/pdf/2302.08243.pdf,New Insights on Relieving Task-Recency Bias for Online Class Incremental Learning,Shiyu Ji,jishiyu cs@163.com,95%
https://arxiv.org/pdf/2302.08243.pdf,New Insights on Relieving Task-Recency Bias for Online Class Incremental Learning,Guoqiang Liang,gqliang@nwpu.edu.cn,82%
https://arxiv.org/pdf/2302.08243.pdf,New Insights on Relieving Task-Recency Bias for Online Class Incremental Learning,Zhaoqiang Chen,chenzhaoqiang@mail.nwpu.edu.cn,95%
https://arxiv.org/pdf/2302.08243.pdf,New Insights on Relieving Task-Recency Bias for Online Class Incremental Learning,Yanning Zhang,ynzhang@nwpu.edu.cn,82%
https://arxiv.org/pdf/2302.08243.pdf,New Insights on Relieving Task-Recency Bias for Online Class Incremental Learning,Zhaojie Chen,,0%
https://arxiv.org/pdf/2302.08242.pdf,Tuning computer vision models with task rewards,Alexander Kolesnikov,akolesnikov@google.com,82%
https://arxiv.org/pdf/2302.08242.pdf,Tuning computer vision models with task rewards,André Susano Pinto,andresp@google.com,85%
https://arxiv.org/pdf/2302.08242.pdf,Tuning computer vision models with task rewards,Yuge Shi,,0%
https://arxiv.org/pdf/2302.08242.pdf,Tuning computer vision models with task rewards,Lucas Beyer,,0%
https://arxiv.org/pdf/2302.08242.pdf,Tuning computer vision models with task rewards,Xiaohua Zhai,,0%
https://arxiv.org/pdf/2302.08237.pdf,A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances,Ahmed Alia,a.alia@fz-juelich.de,82%
https://arxiv.org/pdf/2302.08237.pdf,A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances,Mohammed Maree,mohammed.maree@aaup.edu,95%
https://arxiv.org/pdf/2302.08237.pdf,A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances,Armin Seyfried,a.seyfried@fz-juelich.de,82%
https://arxiv.org/pdf/2302.08237.pdf,A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances,Mohcine Chraibi,,0%
https://arxiv.org/pdf/2302.08237.pdf,A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances,Anas Toma,,0%
https://arxiv.org/pdf/2302.08231.pdf,"3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection",Jongwoo Park,,0%
https://arxiv.org/pdf/2302.08231.pdf,"3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection",Apoorv Singh,,0%
https://arxiv.org/pdf/2302.08231.pdf,"3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection",Varun Bankiti,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Ihsan Ullah,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Dustin Carrión-ojeda,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Sergio Escalera,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Isabelle Guyon,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Mike Huisman,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Felix Mohr,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Jan N Van Rijn,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Haozhe Sun,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Joaquin Vanschoren,,0%
https://arxiv.org/pdf/2302.08909.pdf,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,Phan Anh Vu,,0%
https://arxiv.org/pdf/2302.08212.pdf,Visible-Infrared Person Re-Identification via Patch-Mixed Cross-Modality Learning,Zhihao Qian,,0%
https://arxiv.org/pdf/2302.08212.pdf,Visible-Infrared Person Re-Identification via Patch-Mixed Cross-Modality Learning,Yutian Lin,,0%
https://arxiv.org/pdf/2302.08212.pdf,Visible-Infrared Person Re-Identification via Patch-Mixed Cross-Modality Learning,Bo Du,,0%
https://arxiv.org/pdf/2302.08902.pdf,Fashion Image Retrieval with Multi-Granular Alignment,Hao Huang,huanghao07@kuaishou.com,95%
https://arxiv.org/pdf/2302.08902.pdf,Fashion Image Retrieval with Multi-Granular Alignment,Qiao Deng,dengqiao@kuaishou.com,95%
https://arxiv.org/pdf/2302.08902.pdf,Fashion Image Retrieval with Multi-Granular Alignment,Jinkuan Zhu,zhujinkuan@kuaishou.com,95%
https://arxiv.org/pdf/2302.08902.pdf,Fashion Image Retrieval with Multi-Granular Alignment,Xiyao Li,lixiyao@kuaishou.com,95%
https://arxiv.org/pdf/2302.08207.pdf,Parallax-Tolerant Unsupervised Deep Image Stitching,Lang Nie,,0%
https://arxiv.org/pdf/2302.08207.pdf,Parallax-Tolerant Unsupervised Deep Image Stitching,Chunyu Lin,,0%
https://arxiv.org/pdf/2302.08207.pdf,Parallax-Tolerant Unsupervised Deep Image Stitching,Kang Liao,,0%
https://arxiv.org/pdf/2302.08207.pdf,Parallax-Tolerant Unsupervised Deep Image Stitching,Shuaicheng Liu,,0%
https://arxiv.org/pdf/2302.08207.pdf,Parallax-Tolerant Unsupervised Deep Image Stitching,Yao Zhao,,0%
https://arxiv.org/pdf/2302.08197.pdf,OPT: One-shot Pose-Controllable Talking Head Generation,Jin Liu,,0%
https://arxiv.org/pdf/2302.08197.pdf,OPT: One-shot Pose-Controllable Talking Head Generation,Xi Wang,,0%
https://arxiv.org/pdf/2302.08197.pdf,OPT: One-shot Pose-Controllable Talking Head Generation,Xiaomeng Fu,,0%
https://arxiv.org/pdf/2302.08197.pdf,OPT: One-shot Pose-Controllable Talking Head Generation,Yesheng Chai,,0%
https://arxiv.org/pdf/2302.08197.pdf,OPT: One-shot Pose-Controllable Talking Head Generation,Cai Yu,,0%
https://arxiv.org/pdf/2302.08197.pdf,OPT: One-shot Pose-Controllable Talking Head Generation,Jiao Dai,,0%
https://arxiv.org/pdf/2302.08197.pdf,OPT: One-shot Pose-Controllable Talking Head Generation,Jizhong Han,,0%
https://arxiv.org/pdf/2302.08185.pdf,WHC: Weighted Hybrid Criterion for Filter Pruning on Convolutional Neural Networks,Shaowu Chen,,0%
https://arxiv.org/pdf/2302.08185.pdf,WHC: Weighted Hybrid Criterion for Filter Pruning on Convolutional Neural Networks,Weize Sun,,0%
https://arxiv.org/pdf/2302.08185.pdf,WHC: Weighted Hybrid Criterion for Filter Pruning on Convolutional Neural Networks,Lei Huang,,0%
https://arxiv.org/pdf/2302.08180.pdf,Cross Modal Distillation for Flood Extent Mapping,Shubhika Garg,,0%
https://arxiv.org/pdf/2302.08180.pdf,Cross Modal Distillation for Flood Extent Mapping,Ben Feinstein,,0%
https://arxiv.org/pdf/2302.08180.pdf,Cross Modal Distillation for Flood Extent Mapping,Shahar Timnat,,0%
https://arxiv.org/pdf/2302.08180.pdf,Cross Modal Distillation for Flood Extent Mapping,Vishal Batchu,,0%
https://arxiv.org/pdf/2302.08180.pdf,Cross Modal Distillation for Flood Extent Mapping,Gideon Dror,,0%
https://arxiv.org/pdf/2302.08180.pdf,Cross Modal Distillation for Flood Extent Mapping,Adi Gerzi Rosenthal,,0%
https://arxiv.org/pdf/2302.08180.pdf,Cross Modal Distillation for Flood Extent Mapping,Varun Gulshan,,0%
https://arxiv.org/pdf/2302.08175.pdf,A numerical approximation method for the Fisher-Rao distance between multivariate normal distributions,Frank Nielsen,,0%
https://arxiv.org/pdf/2302.08156.pdf,Research on road object detection algorithm based on improved YOLOX,Tao Yang,,0%
https://arxiv.org/pdf/2302.08156.pdf,Research on road object detection algorithm based on improved YOLOX,Youyu Wu,,0%
https://arxiv.org/pdf/2302.08156.pdf,Research on road object detection algorithm based on improved YOLOX,Yangxintai Tang,,0%
https://arxiv.org/pdf/2302.08149.pdf,URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation,Shuwei Shao,swshao@buaa.edu.cn,82%
https://arxiv.org/pdf/2302.08149.pdf,URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation,Zhongcai Pei,,0%
https://arxiv.org/pdf/2302.08149.pdf,URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation,Weihai Chen,,0%
https://arxiv.org/pdf/2302.08149.pdf,URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation,Ran Li,,0%
https://arxiv.org/pdf/2302.08149.pdf,URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation,Zhong Liu,,0%
https://arxiv.org/pdf/2302.08149.pdf,URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation,Zhengguo Li,,0%
https://arxiv.org/pdf/2302.08119.pdf,A Review of Uncertainty Estimation and its Application in Medical Imaging,Zhihao Chen,chen@tju.edu.cn,78%
https://arxiv.org/pdf/2302.08119.pdf,A Review of Uncertainty Estimation and its Application in Medical Imaging,Meng Wang,wangmeng9218@126.com,95%
https://arxiv.org/pdf/2302.08119.pdf,A Review of Uncertainty Estimation and its Application in Medical Imaging,Ke Zou,kezou8@gmail.com,95%
https://arxiv.org/pdf/2302.08119.pdf,A Review of Uncertainty Estimation and its Application in Medical Imaging,Xuedong Yuan,dong@163.com,90%
https://arxiv.org/pdf/2302.08119.pdf,A Review of Uncertainty Estimation and its Application in Medical Imaging,Huazhu Fu,hzfu@ieee.org,82%
https://arxiv.org/pdf/2302.08119.pdf,A Review of Uncertainty Estimation and its Application in Medical Imaging,Xiaojing Shen,shenxj@scu.edu.cn,78%
https://arxiv.org/pdf/2302.08113.pdf,MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation,Omer Bar-tal,,0%
https://arxiv.org/pdf/2302.08113.pdf,MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation,Lior Yariv,,0%
https://arxiv.org/pdf/2302.08113.pdf,MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation,Yaron Lipman,,0%
https://arxiv.org/pdf/2302.08113.pdf,MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation,Tali Dekel,,0%
https://arxiv.org/pdf/2302.08106.pdf,Towards Efficient Visual Adaption via Structural Re-parameterization,Gen Luo,luogen@stu.xmu.edu.cn,95%
https://arxiv.org/pdf/2302.08106.pdf,Towards Efficient Visual Adaption via Structural Re-parameterization,Guannan Jiang,jianggn@catl.com,78%
https://arxiv.org/pdf/2302.08106.pdf,Towards Efficient Visual Adaption via Structural Re-parameterization,Zhiyu Wang,wangzy13@catl.com,78%
https://arxiv.org/pdf/2302.08106.pdf,Towards Efficient Visual Adaption via Structural Re-parameterization,Xiaoshuai Sun,xssun@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.08106.pdf,Towards Efficient Visual Adaption via Structural Re-parameterization,Yiyi Zhou,zhouyiyi@xmu.edu.cn,95%
https://arxiv.org/pdf/2302.08106.pdf,Towards Efficient Visual Adaption via Structural Re-parameterization,Minglang Huang,huangminglang@stu.xmu.edu.cn,95%
https://arxiv.org/pdf/2302.08106.pdf,Towards Efficient Visual Adaption via Structural Re-parameterization,Rongrong Ji,rrji@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.08102.pdf,Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition,Yong Man Ro,ymro@kaist.ac.kr,82%
https://arxiv.org/pdf/2302.08102.pdf,Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition,Hyung-il Kim,hikim@etri.re.kr,82%
https://arxiv.org/pdf/2302.08102.pdf,Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition,Minsu Kim,,0%
https://arxiv.org/pdf/2302.08066.pdf,Masking and Mixing Adversarial Training,Hironobu Fujiyoshi,fujiyoshi@isc.chubu.ac.jp,78%
https://arxiv.org/pdf/2302.08066.pdf,Masking and Mixing Adversarial Training,Yasunori Ishii,ishii.yasunori@jp.panasonic.com,95%
https://arxiv.org/pdf/2302.08066.pdf,Masking and Mixing Adversarial Training,Tsubasa Hirakawa,hirakawa@mprg.cs.chubu.ac.jp,78%
https://arxiv.org/pdf/2302.08066.pdf,Masking and Mixing Adversarial Training,Takayoshi Yamashita,takayoshi@isc.chubu.ac.jp,85%
https://arxiv.org/pdf/2302.08066.pdf,Masking and Mixing Adversarial Training,Kazuki Kozuka,kozuka.kazuki@jp.panasonic.com,95%
https://arxiv.org/pdf/2302.08066.pdf,Masking and Mixing Adversarial Training,Hiroki Adachi,,0%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Raghav Goyal,rgoyal14@cs.ubc.ca,82%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Effrosyni Mavroudi,,0%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Xitong Yang,,0%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Sainbayar Sukhbaatar,,0%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Leonid Sigal,,0%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Matt Feiszli,,0%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Lorenzo Torresani,,0%
https://arxiv.org/pdf/2302.08063.pdf,MINOTAUR: Multi-task Video Grounding From Multimodal Queries,Du Tran,,0%
https://arxiv.org/pdf/2302.08062.pdf,Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews,Yukun Shi,ykshi@nju.edu.cn,82%
https://arxiv.org/pdf/2302.08062.pdf,Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews,Hairong Lv,lvhairong@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.08062.pdf,Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews,Chengbin Hou,,0%
https://arxiv.org/pdf/2302.08062.pdf,Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews,Xinyu Lin,,0%
https://arxiv.org/pdf/2302.08062.pdf,Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews,Hanhui Huang,,0%
https://arxiv.org/pdf/2302.08062.pdf,Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews,Sheng Xu,,0%
https://arxiv.org/pdf/2302.08062.pdf,Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews,Junxuan Fan,,0%
https://arxiv.org/pdf/2302.08058.pdf,Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution,Zhengyu Liang,zyliang@nudt.edu.cn,82%
https://arxiv.org/pdf/2302.08058.pdf,Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution,Jungang Yang,yangjungang@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.08058.pdf,Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution,Yingqian Wang,,0%
https://arxiv.org/pdf/2302.08058.pdf,Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution,Longguang Wang,,0%
https://arxiv.org/pdf/2302.08058.pdf,Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution,Shilin Zhou,,0%
https://arxiv.org/pdf/2302.08058.pdf,Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution,Yulan Guo,,0%
https://arxiv.org/pdf/2302.08054.pdf,Spectral 3D Computer Vision -- A Review,Ali Zia,ali.zia@csiro.au,95%
https://arxiv.org/pdf/2302.08054.pdf,Spectral 3D Computer Vision -- A Review,Vivien Rolland,vivien.rolland@csiro.au,95%
https://arxiv.org/pdf/2302.08054.pdf,Spectral 3D Computer Vision -- A Review,Yajie Sun,,0%
https://arxiv.org/pdf/2302.08054.pdf,Spectral 3D Computer Vision -- A Review,Charissa Yu,,0%
https://arxiv.org/pdf/2302.08054.pdf,Spectral 3D Computer Vision -- A Review,Jun Zhou,,0%
https://arxiv.org/pdf/2302.08052.pdf,Hierarchical Cross-modal Transformer for RGB-D Salient Object Detection,Hao Chen,haochen303@seu.edu.cn,95%
https://arxiv.org/pdf/2302.08052.pdf,Hierarchical Cross-modal Transformer for RGB-D Salient Object Detection,Feihong Shen,feihongshen@seu.edu.cn,95%
https://arxiv.org/pdf/2302.08050.pdf,Positive-unlabeled learning for binary and multi-class cell detection in histopathology images with incomplete annotations,Yaou Liu,liuyaou@bjtth.org,95%
https://arxiv.org/pdf/2302.08050.pdf,Positive-unlabeled learning for binary and multi-class cell detection in histopathology images with incomplete annotations,Zhiwen Liu,zwliu@bit.edu.cn,82%
https://arxiv.org/pdf/2302.08050.pdf,Positive-unlabeled learning for binary and multi-class cell detection in histopathology images with incomplete annotations,Fengqian Pang,fqpang@ncut.edu.cn,82%
https://arxiv.org/pdf/2302.08050.pdf,Positive-unlabeled learning for binary and multi-class cell detection in histopathology images with incomplete annotations,Chuyang Ye,chuyang.ye@bit.edu.cn,95%
https://arxiv.org/pdf/2302.08050.pdf,Positive-unlabeled learning for binary and multi-class cell detection in histopathology images with incomplete annotations,Zipei Zhao,,0%
https://arxiv.org/pdf/2302.08047.pdf,TcGAN: Semantic-Aware and Structure-Preserved GANs with Individual Vision Transformer for Fast Arbitrary One-Shot Image Generation,Yunliang Jiang,,0%
https://arxiv.org/pdf/2302.08047.pdf,TcGAN: Semantic-Aware and Structure-Preserved GANs with Individual Vision Transformer for Fast Arbitrary One-Shot Image Generation,Lili Yan,,0%
https://arxiv.org/pdf/2302.08047.pdf,TcGAN: Semantic-Aware and Structure-Preserved GANs with Individual Vision Transformer for Fast Arbitrary One-Shot Image Generation,Xiongtao Zhang,,0%
https://arxiv.org/pdf/2302.08047.pdf,TcGAN: Semantic-Aware and Structure-Preserved GANs with Individual Vision Transformer for Fast Arbitrary One-Shot Image Generation,Yong Liu,,0%
https://arxiv.org/pdf/2302.08047.pdf,TcGAN: Semantic-Aware and Structure-Preserved GANs with Individual Vision Transformer for Fast Arbitrary One-Shot Image Generation,Danfeng Sun,,0%
https://arxiv.org/pdf/2302.08046.pdf,Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space,Keyan Chen,,0%
https://arxiv.org/pdf/2302.08046.pdf,Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space,Wenyuan Li,,0%
https://arxiv.org/pdf/2302.08046.pdf,Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space,Sen Lei,,0%
https://arxiv.org/pdf/2302.08046.pdf,Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space,Jianqi Chen,,0%
https://arxiv.org/pdf/2302.08046.pdf,Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space,Xiaolong Jiang,,0%
https://arxiv.org/pdf/2302.08046.pdf,Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space,Zhengxia Zou,,0%
https://arxiv.org/pdf/2302.08046.pdf,Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space,Zhenwei Shi,,0%
https://arxiv.org/pdf/2302.08023.pdf,Object-centric Learning with Cyclic Walks between Parts and Whole,Mengmi Zhang,mengmi@i2r.a-star.edu.sg,85%
https://arxiv.org/pdf/2302.08023.pdf,Object-centric Learning with Cyclic Walks between Parts and Whole,Ziyu Wang,,0%
https://arxiv.org/pdf/2302.08023.pdf,Object-centric Learning with Cyclic Walks between Parts and Whole,Mike Zheng Shou,,0%
https://arxiv.org/pdf/2302.08011.pdf,Vision-Based Terrain Relative Navigation on High-Altitude Balloon and Sub-Orbital Rocket,Dominic Maggio,,0%
https://arxiv.org/pdf/2302.08011.pdf,Vision-Based Terrain Relative Navigation on High-Altitude Balloon and Sub-Orbital Rocket,Courtney Mario,,0%
https://arxiv.org/pdf/2302.08011.pdf,Vision-Based Terrain Relative Navigation on High-Altitude Balloon and Sub-Orbital Rocket,Brett Streetman,,0%
https://arxiv.org/pdf/2302.08011.pdf,Vision-Based Terrain Relative Navigation on High-Altitude Balloon and Sub-Orbital Rocket,Ted Steiner,,0%
https://arxiv.org/pdf/2302.08011.pdf,Vision-Based Terrain Relative Navigation on High-Altitude Balloon and Sub-Orbital Rocket,Luca Carlone,,0%
https://arxiv.org/pdf/2302.07994.pdf,À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting,Stefano Soatto,soattos@amazon.com,82%
https://arxiv.org/pdf/2302.07994.pdf,À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting,Alessandro Achille,aachille@amazon.com,82%
https://arxiv.org/pdf/2302.07994.pdf,À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting,Benjamin Bowman,benbowman314@math.ucla.edu,82%
https://arxiv.org/pdf/2302.07994.pdf,À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting,Matthew Trager,mttrager@amazon.com,82%
https://arxiv.org/pdf/2302.07994.pdf,À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting,Luca Zancato,zancato@amazon.it,78%
https://arxiv.org/pdf/2302.07994.pdf,À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting,Giovanni Paolini,paoling@amazon.com,78%
https://arxiv.org/pdf/2302.07994.pdf,À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting,Pramuditha Perera,pramudi@amazon.com,90%
https://arxiv.org/pdf/2302.07979.pdf,PRedItOR: Text Guided Image Editing with Diffusion Prior,Hareesh Ravi,,0%
https://arxiv.org/pdf/2302.07979.pdf,PRedItOR: Text Guided Image Editing with Diffusion Prior,Sachin Kelkar,,0%
https://arxiv.org/pdf/2302.07979.pdf,PRedItOR: Text Guided Image Editing with Diffusion Prior,Midhun Harikumar,,0%
https://arxiv.org/pdf/2302.07979.pdf,PRedItOR: Text Guided Image Editing with Diffusion Prior,Ajinkya Kale,,0%
https://arxiv.org/pdf/2302.07967.pdf,Self-supervised Registration and Segmentation of the Ossicles with A Single Ground Truth Label,Yike Zhang,yike.zhang@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.07967.pdf,Self-supervised Registration and Segmentation of the Ossicles with A Single Ground Truth Label,Jack Noble,jack.noble@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.07950.pdf,Self-Organising Neural Discrete Representation Learning à la Kohonen,Jürgen Schmidhuber,juergen@idsia.ch,60%
https://arxiv.org/pdf/2302.07950.pdf,Self-Organising Neural Discrete Representation Learning à la Kohonen,Kazuki Irie,kirie@fas.harvard.edu,82%
https://arxiv.org/pdf/2302.07950.pdf,Self-Organising Neural Discrete Representation Learning à la Kohonen,Róbert Csordás,rcsordas@stanford.edu,82%
https://arxiv.org/pdf/2302.07919.pdf,COVID-VTS: Fact Extraction and Verification on Short Video Platforms,Abhinav Shrivastava,abhinav@umd.edu,85%
https://arxiv.org/pdf/2302.07919.pdf,COVID-VTS: Fact Extraction and Verification on Short Video Platforms,Yaser Yacoob,yaser@umd.edu,85%
https://arxiv.org/pdf/2302.07919.pdf,COVID-VTS: Fact Extraction and Verification on Short Video Platforms,Fuxiao Liu,,0%
https://arxiv.org/pdf/2302.07917.pdf,"Evaluating Trade-offs in Computer Vision Between Attribute Privacy, Fairness and Utility",William Paul,,0%
https://arxiv.org/pdf/2302.07917.pdf,"Evaluating Trade-offs in Computer Vision Between Attribute Privacy, Fairness and Utility",Philip Mathew,,0%
https://arxiv.org/pdf/2302.07917.pdf,"Evaluating Trade-offs in Computer Vision Between Attribute Privacy, Fairness and Utility",Fady Alajaji,,0%
https://arxiv.org/pdf/2302.07917.pdf,"Evaluating Trade-offs in Computer Vision Between Attribute Privacy, Fairness and Utility",Philippe Burlina,,0%
https://arxiv.org/pdf/2303.10173.pdf,VideoSum: A Python Library for Surgical Video Summarization,Luis C. Garcia-peraza-herrera,,0%
https://arxiv.org/pdf/2303.10173.pdf,VideoSum: A Python Library for Surgical Video Summarization,Sebastien Ourselin,,0%
https://arxiv.org/pdf/2303.10173.pdf,VideoSum: A Python Library for Surgical Video Summarization,Tom Vercauteren,,0%
https://arxiv.org/pdf/2302.07864.pdf,Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild,Hshmat Sahak,hshmat.sahak@mail.utoronto.ca,95%
https://arxiv.org/pdf/2302.07864.pdf,Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild,Daniel Watson,,0%
https://arxiv.org/pdf/2302.07864.pdf,Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild,Chitwan Saharia,,0%
https://arxiv.org/pdf/2302.07864.pdf,Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild,David Fleet,,0%
https://arxiv.org/pdf/2302.07848.pdf,One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2,Trevine Oorloff,trevine@umd.edu,85%
https://arxiv.org/pdf/2302.07848.pdf,One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2,Yaser Yacoob,yaser@umd.edu,85%
https://arxiv.org/pdf/2302.07817.pdf,Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction,Yuanhui Huang,huangyh22@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.07817.pdf,Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction,Jiwen Lu,lujiwen@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.07817.pdf,Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction,Jie Zhou,jzhou@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.07817.pdf,Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction,Yunpeng Zhang,yunpengzhang97@gmail.com,95%
https://arxiv.org/pdf/2302.07817.pdf,Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction,Wenzhao Zheng,zhengwz18@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.07806.pdf,AI pipeline for accurate retinal layer segmentation using OCT 3D images,Mayank Goswami,mayank.goswami@ph.iitr.ac.in,95%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Muhammad Anwaar Khalid,,0%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Kanwal Zulfiqar,,0%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Ulfat Bashir,,0%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Areeba Shaheen,,0%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Rida Iqbal,,0%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Zarnab Rizwan,,0%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Ghina Rizwan,,0%
https://arxiv.org/pdf/2302.07797.pdf,'Aariz: A Benchmark Dataset for Automatic Cephalometric Landmark Detection and CVM Stage Classification,Muhammad Moazam Fraz,,0%
https://arxiv.org/pdf/2302.07734.pdf,TFormer: A Transmission-Friendly ViT Model for IoT Devices,Vishnu Naresh Boddeti,vishnu@msu.edu,85%
https://arxiv.org/pdf/2302.07734.pdf,TFormer: A Transmission-Friendly ViT Model for IoT Devices,Shangguang Wang,sgwang@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.07734.pdf,TFormer: A Transmission-Friendly ViT Model for IoT Devices,Felix Juefei-xu,felixu@meta.com,85%
https://arxiv.org/pdf/2302.07734.pdf,TFormer: A Transmission-Friendly ViT Model for IoT Devices,Chuntao Ding,ing@bjtu.edu.cn,90%
https://arxiv.org/pdf/2302.07734.pdf,TFormer: A Transmission-Friendly ViT Model for IoT Devices,Yun Yang,yyang@swin.edu.au,82%
https://arxiv.org/pdf/2302.07734.pdf,TFormer: A Transmission-Friendly ViT Model for IoT Devices,Zhichao Lu,luzhichaocn@gmail.com,95%
https://arxiv.org/pdf/2302.07702.pdf,Audio-Visual Contrastive Learning with Temporal Self-Supervision,Simon Jenni,jenni@adobe.com,78%
https://arxiv.org/pdf/2302.07702.pdf,Audio-Visual Contrastive Learning with Temporal Self-Supervision,Alexander Black,alex.black@surrey.ac.uk,82%
https://arxiv.org/pdf/2302.07702.pdf,Audio-Visual Contrastive Learning with Temporal Self-Supervision,John Collomosse,collomos@adobe.com,90%
https://arxiv.org/pdf/2302.07693.pdf,Fine-tuning of sign language recognition models: a technical report,Ruslan Murtazin,RBMurtazin@sberbank.ru,82%
https://arxiv.org/pdf/2302.07693.pdf,Fine-tuning of sign language recognition models: a technical report,Maxim Novopoltsev,MYNovopoltsev@sberbank.ru,82%
https://arxiv.org/pdf/2302.07693.pdf,Fine-tuning of sign language recognition models: a technical report,Leonid Verkhovtsev,LRVerkhovtsev@sberbank.ru,82%
https://arxiv.org/pdf/2302.07693.pdf,Fine-tuning of sign language recognition models: a technical report,Iuliia Zemtsova,YMZemtsova@sberbank.ru,78%
https://arxiv.org/pdf/2302.07693.pdf,Fine-tuning of sign language recognition models: a technical report,Dmitriy Milevich,DEMilevich@sberbank.ru,82%
https://arxiv.org/pdf/2302.07685.pdf,Video Probabilistic Diffusion Models in Projected Latent Space,Sihyun Yu,sihyun.yu@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.07685.pdf,Video Probabilistic Diffusion Models in Projected Latent Space,Kihyuk Sohn,kihyuks@google.com,85%
https://arxiv.org/pdf/2302.07685.pdf,Video Probabilistic Diffusion Models in Projected Latent Space,Jinwoo Shin,jinwoos@kaist.ac.kr,85%
https://arxiv.org/pdf/2302.07685.pdf,Video Probabilistic Diffusion Models in Projected Latent Space,Subin Kim,subin-kim@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Mingli Song,brooksong@zju.edu.cn,78%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Peiyuan Liu,peiyuan.19@intl.zju.edu.cn,85%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Shenghao Hao,shengyuhao@zju.edu.cn,82%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Gaoang Wang,gaoangwang@intl.zju.edu.cn,95%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Yibing Zhan,zhanyibing@jd.com,95%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Zuozhu Liu,zuozuliu@intl.zju.edu.cn,82%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Jenq-neng Hwang,hwang@uw.edu,78%
https://arxiv.org/pdf/2302.07676.pdf,DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes,Kaixun Jin,,0%
https://arxiv.org/pdf/2302.07669.pdf,Unsupervised Hashing with Similarity Distribution Calibration,Chee Seng Chan,cs.chan@um.edu.my,82%
https://arxiv.org/pdf/2302.07669.pdf,Unsupervised Hashing with Similarity Distribution Calibration,Kam Woh Ng,kamwoh.ng@surrey.ac.uk,95%
https://arxiv.org/pdf/2302.07669.pdf,Unsupervised Hashing with Similarity Distribution Calibration,Tao Xiang,t.xiang@surrey.ac.uk,82%
https://arxiv.org/pdf/2302.07669.pdf,Unsupervised Hashing with Similarity Distribution Calibration,Yi-zhe Song,y.song@surrey.ac.uk,82%
https://arxiv.org/pdf/2302.07669.pdf,Unsupervised Hashing with Similarity Distribution Calibration,Jiun Tian Hoe,jiuntian001@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2302.07669.pdf,Unsupervised Hashing with Similarity Distribution Calibration,Xiatian Zhu,xiatian.zhu@surrey.ac.uk,95%
https://arxiv.org/pdf/2302.07669.pdf,Unsupervised Hashing with Similarity Distribution Calibration,Tianyu Zhang,,0%
https://arxiv.org/pdf/2302.07667.pdf,CERiL: Continuous Event-based Reinforcement Learning,Celyn Walters,celyn.walters@surrey.ac.uk,95%
https://arxiv.org/pdf/2302.07667.pdf,CERiL: Continuous Event-based Reinforcement Learning,Simon Hadfield,,0%
https://arxiv.org/pdf/2302.07661.pdf,Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D Panoramic Color Images from LiDAR Point Clouds,Tiago Cortinhal,,0%
https://arxiv.org/pdf/2302.07661.pdf,Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D Panoramic Color Images from LiDAR Point Clouds,Eren Erdal Aksoy,,0%
https://arxiv.org/pdf/2302.07608.pdf,Uncertainty-Estimation with Normalized Logits for Out-of-Distribution Detection,Yu Qiao,yu.qiao@siat.ac.cn,95%
https://arxiv.org/pdf/2302.07608.pdf,Uncertainty-Estimation with Normalized Logits for Out-of-Distribution Detection,Mouxiao Huang,,0%
https://arxiv.org/pdf/2302.07583.pdf,ForceFormer: Exploring Social Force and Transformer for Pedestrian Trajectory Prediction,Hao Cheng,h.cheng-2@utwente.nl,82%
https://arxiv.org/pdf/2302.07583.pdf,ForceFormer: Exploring Social Force and Transformer for Pedestrian Trajectory Prediction,Weicheng Zhang,weicheng.zhang@stud.uni-hannover.de,95%
https://arxiv.org/pdf/2302.07583.pdf,ForceFormer: Exploring Social Force and Transformer for Pedestrian Trajectory Prediction,Monika Sester,Monika.Sester@ikg.uni-hannover.de,95%
https://arxiv.org/pdf/2302.07583.pdf,ForceFormer: Exploring Social Force and Transformer for Pedestrian Trajectory Prediction,Fatema T. Johora,fatema.tuj.johora@tu-clausthal.de,95%
https://arxiv.org/pdf/2302.07579.pdf,Semi-Supervised Deep Regression with Uncertainty Consistency and Variational Model Ensembling via Bayesian Neural Networks,Xiaomeng Li,eexmli@ust.hk,78%
https://arxiv.org/pdf/2302.07579.pdf,Semi-Supervised Deep Regression with Uncertainty Consistency and Variational Model Ensembling via Bayesian Neural Networks,Kwang-ting Cheng,timcheng@ust.hk,78%
https://arxiv.org/pdf/2302.07579.pdf,Semi-Supervised Deep Regression with Uncertainty Consistency and Variational Model Ensembling via Bayesian Neural Networks,Weihang Dai,wdaiaj@connect.ust.hk,82%
https://arxiv.org/pdf/2302.07577.pdf,Efficient Teacher: Semi-Supervised Object Detection for YOLOv5,Bowen Xu,bowen.xbw@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.07577.pdf,Efficient Teacher: Semi-Supervised Object Detection for YOLOv5,Wenlong Guan,wenlong.gwl@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.07577.pdf,Efficient Teacher: Semi-Supervised Object Detection for YOLOv5,Lulu Hu,chudu.hll@alibaba-inc.com,78%
https://arxiv.org/pdf/2302.07577.pdf,Efficient Teacher: Semi-Supervised Object Detection for YOLOv5,Mingtao Chen,ruiyang.cmt@alibaba-inc.com,60%
https://arxiv.org/pdf/2302.07570.pdf,Super-Resolution of BVOC Maps by Adapting Deep Learning Methods,Antonio Giganti,,0%
https://arxiv.org/pdf/2302.07570.pdf,Super-Resolution of BVOC Maps by Adapting Deep Learning Methods,Sara Mandelli,,0%
https://arxiv.org/pdf/2302.07570.pdf,Super-Resolution of BVOC Maps by Adapting Deep Learning Methods,Paolo Bestagini,,0%
https://arxiv.org/pdf/2302.07570.pdf,Super-Resolution of BVOC Maps by Adapting Deep Learning Methods,Marco Marcon,,0%
https://arxiv.org/pdf/2302.07570.pdf,Super-Resolution of BVOC Maps by Adapting Deep Learning Methods,Stefano Tubaro,,0%
https://arxiv.org/pdf/2302.11570.pdf,Plug-and-Play Deep Energy Model for Inverse problems,Mathews Jacob,mathews-jacob@uiowa.edu,95%
https://arxiv.org/pdf/2302.11570.pdf,Plug-and-Play Deep Energy Model for Inverse problems,Jyothi Rikabh Chand,jyothi-rikhabchand@uiowa.edu,95%
https://arxiv.org/pdf/2302.07516.pdf,Offline-to-Online Knowledge Distillation for Video Instance Segmentation,Sunghoon Im,sunghoonim@dgist.ac.kr,95%
https://arxiv.org/pdf/2302.07516.pdf,Offline-to-Online Knowledge Distillation for Video Instance Segmentation,Hojin Kim,hojin.kim@dgist.ac.kr,95%
https://arxiv.org/pdf/2302.07516.pdf,Offline-to-Online Knowledge Distillation for Video Instance Segmentation,Seunghun Lee,,0%
https://arxiv.org/pdf/2302.07483.pdf,EdgeYOLO: An Edge-Real-Time Object Detector,Junlin Zha,jlzha8101@163.com,82%
https://arxiv.org/pdf/2302.07483.pdf,EdgeYOLO: An Edge-Real-Time Object Detector,Gang Wang,gangwang@bit.edu.cn,95%
https://arxiv.org/pdf/2302.07483.pdf,EdgeYOLO: An Edge-Real-Time Object Detector,Shihan Liu,liushihan@bit.edu.cn,95%
https://arxiv.org/pdf/2302.07483.pdf,EdgeYOLO: An Edge-Real-Time Object Detector,Zhuo Li,zhuoli@bit.edu.cn,95%
https://arxiv.org/pdf/2302.07483.pdf,EdgeYOLO: An Edge-Real-Time Object Detector,Jian Sun,sunjian@bit.edu.cn,95%
https://arxiv.org/pdf/2302.09068.pdf,A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning,Zhisheng Tang,zhisheng@isi.edu,85%
https://arxiv.org/pdf/2302.09068.pdf,A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning,Mayank Kejriwal,kejriwal@isi.edu,78%
https://arxiv.org/pdf/2302.07468.pdf,Edge-weighted pFISTA-Net for MRI Reconstruction,Jianpeng Cao,caojianpeng@stu.xmu.edu.cn,95%
https://arxiv.org/pdf/2302.07455.pdf,A lightweight network for photovoltaic cell defect detection in electroluminescence images based on neural architecture search and knowledge distillation,Jinxia Zhang,jinxiazhang@seu.edu.cn,95%
https://arxiv.org/pdf/2302.07455.pdf,A lightweight network for photovoltaic cell defect detection in electroluminescence images based on neural architecture search and knowledge distillation,Xinyi Chen,,0%
https://arxiv.org/pdf/2302.07455.pdf,A lightweight network for photovoltaic cell defect detection in electroluminescence images based on neural architecture search and knowledge distillation,Haikun Wei,,0%
https://arxiv.org/pdf/2302.07455.pdf,A lightweight network for photovoltaic cell defect detection in electroluminescence images based on neural architecture search and knowledge distillation,Kanjian Zhang,,0%
https://arxiv.org/pdf/2302.07440.pdf,Road Redesign Technique Achieving Enhanced Road Safety by Inpainting with a Diffusion Model,Dongsoo Har,dshar@kaist.ac.kr,82%
https://arxiv.org/pdf/2302.07440.pdf,Road Redesign Technique Achieving Enhanced Road Safety by Inpainting with a Diffusion Model,Medhavi Mishra,medhavi132@gmail.com,85%
https://arxiv.org/pdf/2302.07440.pdf,Road Redesign Technique Achieving Enhanced Road Safety by Inpainting with a Diffusion Model,Sumit Mishra,sumitmishra209@gmail.com,95%
https://arxiv.org/pdf/2302.07440.pdf,Road Redesign Technique Achieving Enhanced Road Safety by Inpainting with a Diffusion Model,Taeyoung Kim,,0%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Yu Sun,sunyu@qti.qualcomm.com,95%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Min Guo,mguo@qti.qualcomm.com,82%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Junni Zou,zoujunni@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Botao Wang,botaow@qti.qualcomm.com,85%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Bowen Shi,sjtu shibowen@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Hongkai Xiong,xionghongkai@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Wenrui Dai,daiwenrui@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Han Li,,0%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Hongwei Zheng,,0%
https://arxiv.org/pdf/2302.07408.pdf,Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation,Chenlin Li,,0%
https://arxiv.org/pdf/2302.07387.pdf,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,Jiang Liu,,0%
https://arxiv.org/pdf/2302.07387.pdf,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,Hui Ding,,0%
https://arxiv.org/pdf/2302.07387.pdf,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,Zhaowei Cai,,0%
https://arxiv.org/pdf/2302.07387.pdf,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,Yuting Zhang,,0%
https://arxiv.org/pdf/2302.07387.pdf,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,Ravi Kumar Satzoda,,0%
https://arxiv.org/pdf/2302.07387.pdf,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,Vijay Mahadevan,,0%
https://arxiv.org/pdf/2302.07387.pdf,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,R. Manmatha,,0%
https://arxiv.org/pdf/2302.07360.pdf,Self-supervised learning of object pose estimation using keypoint prediction,Per-erik Forssén,per-erik.forssen@liu.se,95%
https://arxiv.org/pdf/2302.07360.pdf,Self-supervised learning of object pose estimation using keypoint prediction,Felix Järemo Lawin,felix.lawin@gmail.com,95%
https://arxiv.org/pdf/2302.07360.pdf,Self-supervised learning of object pose estimation using keypoint prediction,Zahra Gharaee,zahra.gharaee@uwaterloo.ca,95%
https://arxiv.org/pdf/2302.07354.pdf,Tag-based annotation creates better avatars,Shen Sang,shen.sang@bytedance.com,95%
https://arxiv.org/pdf/2302.07354.pdf,Tag-based annotation creates better avatars,Zeyu Cheng,University@outlook.com,60%
https://arxiv.org/pdf/2302.07354.pdf,Tag-based annotation creates better avatars,James Davis,davis@cs.ucsc.edu,78%
https://arxiv.org/pdf/2302.07354.pdf,Tag-based annotation creates better avatars,Jing Liu,jing.liu@bydance.com,95%
https://arxiv.org/pdf/2302.07354.pdf,Tag-based annotation creates better avatars,Minghao Liu,,0%
https://arxiv.org/pdf/2302.07344.pdf,Semi-Supervised Visual Tracking of Marine Animals using Autonomous Underwater Vehicles,Roger Hanlon,rhanlon@mbl.edu,82%
https://arxiv.org/pdf/2302.07344.pdf,Semi-Supervised Visual Tracking of Marine Animals using Autonomous Underwater Vehicles,T. Aran Mooney,amooney@whoi.edu,78%
https://arxiv.org/pdf/2302.07344.pdf,Semi-Supervised Visual Tracking of Marine Animals using Autonomous Underwater Vehicles,Yogesh Girdhar,ygirdhar@whoi.edu,82%
https://arxiv.org/pdf/2302.07344.pdf,Semi-Supervised Visual Tracking of Marine Animals using Autonomous Underwater Vehicles,Levi Cai,cail@mit.edu,78%
https://arxiv.org/pdf/2302.07344.pdf,Semi-Supervised Visual Tracking of Marine Animals using Autonomous Underwater Vehicles,Nathan E. Mcguire,nmcguire@whoi.edu,82%
https://arxiv.org/pdf/2302.07328.pdf,Hybrid Spiking Neural Network Fine-tuning for Hippocampus Segmentation,Jundong Liu,liuj1@ohio.edu,78%
https://arxiv.org/pdf/2302.07328.pdf,Hybrid Spiking Neural Network Fine-tuning for Hippocampus Segmentation,Ye Yue,,0%
https://arxiv.org/pdf/2302.07328.pdf,Hybrid Spiking Neural Network Fine-tuning for Hippocampus Segmentation,Marc Baltes,,0%
https://arxiv.org/pdf/2302.07328.pdf,Hybrid Spiking Neural Network Fine-tuning for Hippocampus Segmentation,Nidal Abujahar,,0%
https://arxiv.org/pdf/2302.07328.pdf,Hybrid Spiking Neural Network Fine-tuning for Hippocampus Segmentation,Tao Sun,,0%
https://arxiv.org/pdf/2302.07328.pdf,Hybrid Spiking Neural Network Fine-tuning for Hippocampus Segmentation,Charles D. Smith,,0%
https://arxiv.org/pdf/2302.07328.pdf,Hybrid Spiking Neural Network Fine-tuning for Hippocampus Segmentation,Trevor Bihl,,0%
https://arxiv.org/pdf/2302.07319.pdf,Frustratingly Simple but Effective Zero-shot Detection and Segmentation: Analysis and a Strong Baseline,Anirudth Nambirajan,anirudtn@amazon.com,60%
https://arxiv.org/pdf/2302.07319.pdf,Frustratingly Simple but Effective Zero-shot Detection and Segmentation: Analysis and a Strong Baseline,Siddhesh Khandelwal,skhandel@cs.ubc.ca,90%
https://arxiv.org/pdf/2302.07319.pdf,Frustratingly Simple but Effective Zero-shot Detection and Segmentation: Analysis and a Strong Baseline,Behjat Siddiquie,behjats@amazon.com,85%
https://arxiv.org/pdf/2302.07319.pdf,Frustratingly Simple but Effective Zero-shot Detection and Segmentation: Analysis and a Strong Baseline,Jayan Eledath,eledathj@amazon.com,78%
https://arxiv.org/pdf/2302.07319.pdf,Frustratingly Simple but Effective Zero-shot Detection and Segmentation: Analysis and a Strong Baseline,Leonid Sigal,lsigal@cs.ubc.ca,82%
https://arxiv.org/pdf/2302.07317.pdf,Algorithm Selection for Deep Active Learning with Imbalanced Datasets,Robert Nowak,rdnowak@wisc.edu,82%
https://arxiv.org/pdf/2302.07317.pdf,Algorithm Selection for Deep Active Learning with Imbalanced Datasets,Jifan Zhang,jifan@cs.wisc.edu,85%
https://arxiv.org/pdf/2302.07317.pdf,Algorithm Selection for Deep Active Learning with Imbalanced Datasets,Shuai Shao,sshao@meta.com,82%
https://arxiv.org/pdf/2302.07317.pdf,Algorithm Selection for Deep Active Learning with Imbalanced Datasets,Saurabh Verma,saurabh08@meta.com,85%
https://arxiv.org/pdf/2302.07300.pdf,MSDA: Monocular Self-supervised Domain Adaptation for 6D Object Pose Estimation,Esa Rahtu,esa.rahtu@tuni.fi,95%
https://arxiv.org/pdf/2302.07300.pdf,MSDA: Monocular Self-supervised Domain Adaptation for 6D Object Pose Estimation,Janne Heikkilä,janne.heikkila@oulu.fi,95%
https://arxiv.org/pdf/2302.07300.pdf,MSDA: Monocular Self-supervised Domain Adaptation for 6D Object Pose Estimation,Dingding Cai,dingding.cai@tuni.fi,95%
https://arxiv.org/pdf/2302.07257.pdf,ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,Sheng Wang,,0%
https://arxiv.org/pdf/2302.07257.pdf,ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,Zihao Zhao,,0%
https://arxiv.org/pdf/2302.07257.pdf,ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,Xi Ouyang,,0%
https://arxiv.org/pdf/2302.07257.pdf,ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,Qian Wang,,0%
https://arxiv.org/pdf/2302.07257.pdf,ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,Dinggang Shen,,0%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Rameswar Panda,rpanda@ibm.com,82%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Bao Pham,phamb@rpi.edu,78%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Benjamin Hoover,benjamin.hoover@ibm.com,95%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Mohammed J. Zaki,zaki@cs.rpi.edu,78%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Yuchen Liang,liangy7@rpi.edu,78%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Dmitry Krotov,krotov@ibm.com,78%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Hendrik Strobelt,hendrik.strobelt@ibm.com,95%
https://arxiv.org/pdf/2302.07253.pdf,Energy Transformer,Duen Horng Chau,,0%
https://arxiv.org/pdf/2302.07245.pdf,WSD: Wild Selfie Dataset for Face Recognition in Selfie Images,Shiv Ram Dubey,srdubey@iiita.ac.in,82%
https://arxiv.org/pdf/2302.07245.pdf,WSD: Wild Selfie Dataset for Face Recognition in Selfie Images,Laxman Kumarapu,,0%
https://arxiv.org/pdf/2302.07245.pdf,WSD: Wild Selfie Dataset for Face Recognition in Selfie Images,Snehasis Mukherjee,,0%
https://arxiv.org/pdf/2302.07245.pdf,WSD: Wild Selfie Dataset for Face Recognition in Selfie Images,Parkhi Mohan,,0%
https://arxiv.org/pdf/2302.07245.pdf,WSD: Wild Selfie Dataset for Face Recognition in Selfie Images,Sree Pragna Vinnakoti,,0%
https://arxiv.org/pdf/2302.07245.pdf,WSD: Wild Selfie Dataset for Face Recognition in Selfie Images,Subhash Karthikeya,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Krishna Murthy Jatavallabhula,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Alihusein Kuwajerwala,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Qiao Gu,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Mohd Omama,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Tao Chen,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Alaa Maalouf,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Shuang Li,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Ganesh Iyer,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Soroush Saryazdi,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Nikhil Keetha,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Ayush Tewari,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Joshua B. Tenenbaum,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Celso Miguel De Melo,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Madhava Krishna,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Liam Paull,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Florian Shkurti,,0%
https://arxiv.org/pdf/2302.07241.pdf,ConceptFusion: Open-set Multimodal 3D Mapping,Antonio Torralba,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Shangzan Zhang,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Sida Peng,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Tianrun Chen,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Linzhan Mou,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Haotong Lin,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Kaicheng Yu,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Yiyi Liao,,0%
https://arxiv.org/pdf/2302.07224.pdf,Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask,Xiaowei Zhou,,0%
https://arxiv.org/pdf/2302.07203.pdf,Synthesizing audio from tongue motion during speech using tagged MRI via transformer,Xiaofeng Liu,,0%
https://arxiv.org/pdf/2302.07203.pdf,Synthesizing audio from tongue motion during speech using tagged MRI via transformer,Fangxu Xing,,0%
https://arxiv.org/pdf/2302.07203.pdf,Synthesizing audio from tongue motion during speech using tagged MRI via transformer,Jerry L. Prince,,0%
https://arxiv.org/pdf/2302.07203.pdf,Synthesizing audio from tongue motion during speech using tagged MRI via transformer,Maureen Stone,,0%
https://arxiv.org/pdf/2302.07203.pdf,Synthesizing audio from tongue motion during speech using tagged MRI via transformer,Georges El Fakhri,,0%
https://arxiv.org/pdf/2302.07203.pdf,Synthesizing audio from tongue motion during speech using tagged MRI via transformer,Jonghye Woo,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Bo Zhou,bo.zhou@yale.edu,95%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Yu-jung Tsai,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Jiazhen Zhang,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Xueqi Guo,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Huidong Xie,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Xiongchao Chen,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Tianshun Miao,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Yihuan Lu,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,James S. Duncan,,0%
https://arxiv.org/pdf/2302.07135.pdf,Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET,Chi Liu,,0%
https://arxiv.org/pdf/2302.07184.pdf,Point Cloud Registration for LiDAR and Photogrammetric Data: a Critical Synthesis and Performance Analysis on Classic and Deep Learning Algorithms,Rongjun Qin,qin.324@osu.edu,78%
https://arxiv.org/pdf/2302.07184.pdf,Point Cloud Registration for LiDAR and Photogrammetric Data: a Critical Synthesis and Performance Analysis on Classic and Deep Learning Algorithms,Ningli Xu,,0%
https://arxiv.org/pdf/2302.07184.pdf,Point Cloud Registration for LiDAR and Photogrammetric Data: a Critical Synthesis and Performance Analysis on Classic and Deep Learning Algorithms,Shuang Song,,0%
https://arxiv.org/pdf/2302.07182.pdf,Visibility-Aware Pixelwise View Selection for Multi-View Stereo Matching,Yukun Shi,yshi21@uoguelph.ca,82%
https://arxiv.org/pdf/2302.07182.pdf,Visibility-Aware Pixelwise View Selection for Multi-View Stereo Matching,Minglun Gong,minglun@uoguelph.ca,85%
https://arxiv.org/pdf/2302.07182.pdf,Visibility-Aware Pixelwise View Selection for Multi-View Stereo Matching,Zhentao Huang,zhentao@uoguelph.ca,85%
https://arxiv.org/pdf/2302.11344.pdf,Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning,Elahe Arani,e.arani@gmail.com,82%
https://arxiv.org/pdf/2302.11344.pdf,Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning,Bahram Zonooz,bahram.zonooz@gmail.com,95%
https://arxiv.org/pdf/2302.11344.pdf,Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning,Fahad Sarfraz,fahad.sarfraz@navinfo.eu,95%
https://arxiv.org/pdf/2302.11346.pdf,Task-Aware Information Routing from Common Representation Space in Lifelong Learning,Prashant Bhat,prashant.bhat@navinfo.eu,95%
https://arxiv.org/pdf/2302.11346.pdf,Task-Aware Information Routing from Common Representation Space in Lifelong Learning,Elahe Arani,e.arani@gmail.com,82%
https://arxiv.org/pdf/2302.11346.pdf,Task-Aware Information Routing from Common Representation Space in Lifelong Learning,Bahram Zonooz,bahram.zonooz@gmail.com,95%
https://arxiv.org/pdf/2302.07689.pdf,Event-guided Multi-patch Network with Self-supervision for Non-uniform Motion Deblurring,Piotr Koniusz,piotr.koniusz@data61.csiro.au,95%
https://arxiv.org/pdf/2302.07689.pdf,Event-guided Multi-patch Network with Self-supervision for Non-uniform Motion Deblurring,Hongguang Zhang,zhang.hongguang@outlook.com,95%
https://arxiv.org/pdf/2302.07689.pdf,Event-guided Multi-patch Network with Self-supervision for Non-uniform Motion Deblurring,Limeng Zhang,,0%
https://arxiv.org/pdf/2302.07689.pdf,Event-guided Multi-patch Network with Self-supervision for Non-uniform Motion Deblurring,Yuchao Dai,,0%
https://arxiv.org/pdf/2302.07689.pdf,Event-guided Multi-patch Network with Self-supervision for Non-uniform Motion Deblurring,Hongdong Li,,0%
https://arxiv.org/pdf/2302.10912.pdf,Balanced Audiovisual Dataset for Imbalance Analysis,Wenke Xia,,0%
https://arxiv.org/pdf/2302.10912.pdf,Balanced Audiovisual Dataset for Imbalance Analysis,Xu Zhao,,0%
https://arxiv.org/pdf/2302.10912.pdf,Balanced Audiovisual Dataset for Imbalance Analysis,Xincheng Pang,,0%
https://arxiv.org/pdf/2302.10912.pdf,Balanced Audiovisual Dataset for Imbalance Analysis,Changqing Zhang,,0%
https://arxiv.org/pdf/2302.10912.pdf,Balanced Audiovisual Dataset for Imbalance Analysis,Di Hu,,0%
https://arxiv.org/pdf/2302.07121.pdf,Universal Guidance for Diffusion Models,Arpit Bansal,bansal01@umd.edu,78%
https://arxiv.org/pdf/2302.07121.pdf,Universal Guidance for Diffusion Models,Hong-min Chu,chu@umd.edu,78%
https://arxiv.org/pdf/2302.07121.pdf,Universal Guidance for Diffusion Models,Avi Schwarzschild,,0%
https://arxiv.org/pdf/2302.07121.pdf,Universal Guidance for Diffusion Models,Soumyadip Sengupta,,0%
https://arxiv.org/pdf/2302.07121.pdf,Universal Guidance for Diffusion Models,Micah Goldblum,,0%
https://arxiv.org/pdf/2302.07121.pdf,Universal Guidance for Diffusion Models,Jonas Geiping,,0%
https://arxiv.org/pdf/2302.07121.pdf,Universal Guidance for Diffusion Models,Tom Goldstein,,0%
https://arxiv.org/pdf/2302.07116.pdf,Team DETR: Guide Queries as a Professional Team in Detection Transformers,Tian Qiu,,0%
https://arxiv.org/pdf/2302.07116.pdf,Team DETR: Guide Queries as a Professional Team in Detection Transformers,Linyun Zhou,,0%
https://arxiv.org/pdf/2302.07116.pdf,Team DETR: Guide Queries as a Professional Team in Detection Transformers,Wenxiang Xu,,0%
https://arxiv.org/pdf/2302.07116.pdf,Team DETR: Guide Queries as a Professional Team in Detection Transformers,Lechao Cheng,,0%
https://arxiv.org/pdf/2302.07116.pdf,Team DETR: Guide Queries as a Professional Team in Detection Transformers,Zunlei Feng,,0%
https://arxiv.org/pdf/2302.07116.pdf,Team DETR: Guide Queries as a Professional Team in Detection Transformers,Mingli Song,,0%
https://arxiv.org/pdf/2302.12831.pdf,CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution,Axi Niu,,0%
https://arxiv.org/pdf/2302.12831.pdf,CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution,Kang Zhang,,0%
https://arxiv.org/pdf/2302.12831.pdf,CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution,Trung X. Pham,,0%
https://arxiv.org/pdf/2302.12831.pdf,CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution,Jinqiu Sun,,0%
https://arxiv.org/pdf/2302.12831.pdf,CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution,Yu Zhu,,0%
https://arxiv.org/pdf/2302.12831.pdf,CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution,In So Kweon,,0%
https://arxiv.org/pdf/2302.12831.pdf,CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution,Yanning Zhang,,0%
https://arxiv.org/pdf/2302.10764.pdf,On The Coherence of Quantitative Evaluation of Visual Explanations,Benjamin Vandersmissen,benjamin.vandersmissen@uantwerpen.be,95%
https://arxiv.org/pdf/2302.10764.pdf,On The Coherence of Quantitative Evaluation of Visual Explanations,Jose Oramas,,0%
https://arxiv.org/pdf/2302.07025.pdf,Optimal Transport for Change Detection on LiDAR Point Clouds,Marco Fiorucci,,0%
https://arxiv.org/pdf/2302.07025.pdf,Optimal Transport for Change Detection on LiDAR Point Clouds,Peter Naylor,,0%
https://arxiv.org/pdf/2302.07025.pdf,Optimal Transport for Change Detection on LiDAR Point Clouds,Makoto Yamada,,0%
https://arxiv.org/pdf/2304.06723.pdf,Introduction to Presentation Attack Detection in Fingerprint Biometrics,Javier Galbally,javier.galbally@ec.europa.eu,95%
https://arxiv.org/pdf/2304.06723.pdf,Introduction to Presentation Attack Detection in Fingerprint Biometrics,Raffaele Cappelli,raffaele.cappelli@unibo.it,95%
https://arxiv.org/pdf/2304.06723.pdf,Introduction to Presentation Attack Detection in Fingerprint Biometrics,Julian Fierrez,julian.fierrez@uam.es,95%
https://arxiv.org/pdf/2304.06723.pdf,Introduction to Presentation Attack Detection in Fingerprint Biometrics,Gian Luca Marcialis,marcialis@unica.it,78%
https://arxiv.org/pdf/2302.06992.pdf,Hard-aware Instance Adaptive Self-training for Unsupervised Cross-domain Semantic Segmentation,Wenqi Tang,wenqi@bupt.edu.cn,85%
https://arxiv.org/pdf/2302.06992.pdf,Hard-aware Instance Adaptive Self-training for Unsupervised Cross-domain Semantic Segmentation,Chuang Zhu,czhu@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.06992.pdf,Hard-aware Instance Adaptive Self-training for Unsupervised Cross-domain Semantic Segmentation,Tiejun Huang,tjhuang@pku.edu.cn,82%
https://arxiv.org/pdf/2302.06992.pdf,Hard-aware Instance Adaptive Self-training for Unsupervised Cross-domain Semantic Segmentation,Kebin Liu,liukebin@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.06992.pdf,Hard-aware Instance Adaptive Self-training for Unsupervised Cross-domain Semantic Segmentation,Jiaqi Zou,jqzou@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.06992.pdf,Hard-aware Instance Adaptive Self-training for Unsupervised Cross-domain Semantic Segmentation,Ke Mei,raykoomei@tencent.com,78%
https://arxiv.org/pdf/2302.06961.pdf,DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization,Xiaowei Ding,dingxiaowei@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.06961.pdf,DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization,Kang Dang,Kang.Dang@xjtlu.edu.cn,95%
https://arxiv.org/pdf/2302.06961.pdf,DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization,Jinfeng Wang,ihxwang@cqu.edu.cn,78%
https://arxiv.org/pdf/2302.06961.pdf,DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization,Jionglong Su,Jionglong.Su@xjtlu.edu.cn,95%
https://arxiv.org/pdf/2302.06961.pdf,DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization,Sifan Song,,0%
https://arxiv.org/pdf/2302.06961.pdf,DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization,Zilong Wang,,0%
https://arxiv.org/pdf/2302.06961.pdf,DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization,Hongxing Wang,,0%
https://arxiv.org/pdf/2302.06949.pdf,Camera Calibration without Camera Access -- A Robust Validation Technique for Extended PnP Methods,Per-erik Forssén,per-erik.forssen@liu.se,95%
https://arxiv.org/pdf/2302.06949.pdf,Camera Calibration without Camera Access -- A Robust Validation Technique for Extended PnP Methods,Johan Edstedt,johan.edstedt@liu.se,95%
https://arxiv.org/pdf/2302.06949.pdf,Camera Calibration without Camera Access -- A Robust Validation Technique for Extended PnP Methods,Emil Brissman,emil.brissman@liu.se,95%
https://arxiv.org/pdf/2302.06939.pdf,Underwater target detection based on improved YOLOv7,Nizhuan Wang,wangnizhuan1120@gmail.com,95%
https://arxiv.org/pdf/2302.06939.pdf,Underwater target detection based on improved YOLOv7,Mengduo Yang,mengduoyang@163.com,95%
https://arxiv.org/pdf/2302.06939.pdf,Underwater target detection based on improved YOLOv7,Kaiyue Liu,,0%
https://arxiv.org/pdf/2302.06939.pdf,Underwater target detection based on improved YOLOv7,Qi Sun,,0%
https://arxiv.org/pdf/2302.06939.pdf,Underwater target detection based on improved YOLOv7,Daming Sun,,0%
https://arxiv.org/pdf/2302.06918.pdf,An Image Processing Pipeline for Autonomous Deep-Space Optical Navigation,Eleonora Andreis,eleonora.andreis@polimi.it,95%
https://arxiv.org/pdf/2302.06918.pdf,An Image Processing Pipeline for Autonomous Deep-Space Optical Navigation,Francesco Topputo,francesco.topputo@polimi.it,95%
https://arxiv.org/pdf/2302.06918.pdf,An Image Processing Pipeline for Autonomous Deep-Space Optical Navigation,Paolo Panicucci,paolo.panicucci@polimi.it,95%
https://arxiv.org/pdf/2302.06908.pdf,DiffFaceSketch: High-Fidelity Face Image Synthesis with Sketch-Guided Latent Diffusion Model,Yichen Peng,yichen.peng@jaist.ac.jp,95%
https://arxiv.org/pdf/2302.06908.pdf,DiffFaceSketch: High-Fidelity Face Image Synthesis with Sketch-Guided Latent Diffusion Model,Chunqi Zhao,,0%
https://arxiv.org/pdf/2302.06908.pdf,DiffFaceSketch: High-Fidelity Face Image Synthesis with Sketch-Guided Latent Diffusion Model,Haoran Xie,,0%
https://arxiv.org/pdf/2302.06908.pdf,DiffFaceSketch: High-Fidelity Face Image Synthesis with Sketch-Guided Latent Diffusion Model,Tsukasa Fukusato,,0%
https://arxiv.org/pdf/2302.06908.pdf,DiffFaceSketch: High-Fidelity Face Image Synthesis with Sketch-Guided Latent Diffusion Model,Kazunori Miyata,,0%
https://arxiv.org/pdf/2302.06900.pdf,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,Shuhao Shi,,0%
https://arxiv.org/pdf/2302.06900.pdf,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,Kai Qiao,,0%
https://arxiv.org/pdf/2302.06900.pdf,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,Jie Yang,,0%
https://arxiv.org/pdf/2302.06900.pdf,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,Baojie Song,,0%
https://arxiv.org/pdf/2302.06900.pdf,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,Jian Chen,,0%
https://arxiv.org/pdf/2302.06900.pdf,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,Bin Yan,,0%
https://arxiv.org/pdf/2302.06898.pdf,Take a Prior from Other Tasks for Severe Blur Removal,Pei Wang,,0%
https://arxiv.org/pdf/2302.06898.pdf,Take a Prior from Other Tasks for Severe Blur Removal,Danna Xue,,0%
https://arxiv.org/pdf/2302.06898.pdf,Take a Prior from Other Tasks for Severe Blur Removal,Yu Zhu,,0%
https://arxiv.org/pdf/2302.06898.pdf,Take a Prior from Other Tasks for Severe Blur Removal,Jinqiu Sun,,0%
https://arxiv.org/pdf/2302.06898.pdf,Take a Prior from Other Tasks for Severe Blur Removal,Qingsen Yan,,0%
https://arxiv.org/pdf/2302.06898.pdf,Take a Prior from Other Tasks for Severe Blur Removal,Sung-eui Yoon,,0%
https://arxiv.org/pdf/2302.06898.pdf,Take a Prior from Other Tasks for Severe Blur Removal,Yanning Zhang,,0%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Shuai Tan,tanshuai2001@gmail.com,95%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Yuyuan Li,y2li@hdu.edu.cn,82%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Biao Gong,a.biao.gong@gmail.com,95%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Deli Zhao,zhaodeli@gmail.com,95%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Yujun Shen,shenyujun0302@gmail.com,95%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Yutong Feng,fengyutong.fyt@gmail.com,95%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Kecheng Zheng,zkechengzk@gmail.com,85%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Xiaoying Xie,,0%
https://arxiv.org/pdf/2302.06891.pdf,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,Chaochao Chen,,0%
https://arxiv.org/pdf/2302.06883.pdf,Text-Guided Scene Sketch-to-Photo Synthesis,Aprilpyone Maungmaung,,0%
https://arxiv.org/pdf/2302.06883.pdf,Text-Guided Scene Sketch-to-Photo Synthesis,Makoto Shing,,0%
https://arxiv.org/pdf/2302.06883.pdf,Text-Guided Scene Sketch-to-Photo Synthesis,Kentaro Mitsui,,0%
https://arxiv.org/pdf/2302.06883.pdf,Text-Guided Scene Sketch-to-Photo Synthesis,Kei Sawada,,0%
https://arxiv.org/pdf/2302.06883.pdf,Text-Guided Scene Sketch-to-Photo Synthesis,Fumio Okura,,0%
https://arxiv.org/pdf/2302.10283.pdf,Self-supervised learning of Split Invariant Equivariant representations,Quentin Garrido,garridoq@meta.com,78%
https://arxiv.org/pdf/2302.10283.pdf,Self-supervised learning of Split Invariant Equivariant representations,Laurent Najman,,0%
https://arxiv.org/pdf/2302.10283.pdf,Self-supervised learning of Split Invariant Equivariant representations,Yann Lecun,,0%
https://arxiv.org/pdf/2302.06874.pdf,Robust Representation Learning with Self-Distillation for Domain Generalization,Ankur Singh,,0%
https://arxiv.org/pdf/2302.06874.pdf,Robust Representation Learning with Self-Distillation for Domain Generalization,Senthilnath Jayavelu,,0%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Hideki Koike,koike@c.titech.ac.jp,78%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Qianyi Wu,qianyi.wu@monash.edu,95%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Ziwei Liu,ziwei.liu@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Tianshu Hu,hutianshu007@163.com,95%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Chen-chieh Liao,liao.c.aa@m.titech.ac.jp,78%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Yasheng Sun,sun.y.aj@m.titech.ac.jp,78%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Shio Miyafuji,miyafuji.s.aa@m.titech.ac.jp,78%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Hang Zhou,zhouhang09@baidu.com,95%
https://arxiv.org/pdf/2302.06857.pdf,Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation,Kaisiyuan Wang,kaisiyuan.wang@sydney.edu.au,95%
https://arxiv.org/pdf/2302.06848.pdf,YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-time Spatio-temporal Action Detection,Jianhua Yang,,0%
https://arxiv.org/pdf/2302.06848.pdf,YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-time Spatio-temporal Action Detection,Kun Dai,,0%
https://arxiv.org/pdf/2302.06845.pdf,SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization,Zhi Wang,wangzhi@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.06845.pdf,SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization,Yuan Meng,yuanmeng@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.06845.pdf,SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization,Wenwu Zhu,wwzhu@tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.06845.pdf,SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization,Zenghao Chai,zenghaochai@comp.nus.edu.sg,95%
https://arxiv.org/pdf/2302.06845.pdf,SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization,Chen Tang,,0%
https://arxiv.org/pdf/2302.06845.pdf,SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization,Kai Ouyang,,0%
https://arxiv.org/pdf/2302.06845.pdf,SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization,Yunpeng Bai,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Kyle Sargent,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Jing Yu Koh,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Han Zhang,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Huiwen Chang,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Charles Herrmann,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Pratul Srinivasan,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Jiajun Wu,,0%
https://arxiv.org/pdf/2302.06833.pdf,VQ3D: Learning a 3D-Aware Generative Model on ImageNet,Deqing Sun,,0%
https://arxiv.org/pdf/2302.06827.pdf,B-BACN: Bayesian Boundary-Aware Convolutional Network for Crack Characterization,Rahul Rathnakumar,,0%
https://arxiv.org/pdf/2302.06827.pdf,B-BACN: Bayesian Boundary-Aware Convolutional Network for Crack Characterization,Yutian Pang,,0%
https://arxiv.org/pdf/2302.06827.pdf,B-BACN: Bayesian Boundary-Aware Convolutional Network for Crack Characterization,Yongming Liu,,0%
https://arxiv.org/pdf/2302.06826.pdf,DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models,Yanting Zhang,ytzhang@dhu.edu.cn,82%
https://arxiv.org/pdf/2302.06826.pdf,DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models,Wenhao Chai,wenhaochai.19@intl.zju.edu.cn,95%
https://arxiv.org/pdf/2302.06826.pdf,DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models,Shengyu Hao,shengyuhao@zju.edu.cn,95%
https://arxiv.org/pdf/2302.06826.pdf,DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models,Gaoang Wang,gaoangwang@intl.zju.edu.cn,95%
https://arxiv.org/pdf/2302.06826.pdf,DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models,Shidong Cao,,0%
https://arxiv.org/pdf/2302.06826.pdf,DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models,Hangyue Chen,,0%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Davide Sapienza,davide.sapienza@unimore.it,95%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Elena Govi,,0%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Sara Aldhaheri,,0%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Marko Bertogna,,0%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Eloy Roura,,0%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Èric Pairet,,0%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Micaela Verucchi,,0%
https://arxiv.org/pdf/2302.06821.pdf,Model-Based Underwater 6D Pose Estimation from RGB,Paola Ardón,,0%
https://arxiv.org/pdf/2302.06815.pdf,Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes,Liang Liu,leoneliu@tencent.com,82%
https://arxiv.org/pdf/2302.06815.pdf,Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes,Yuxi Li,yukiyxli@tencent.com,82%
https://arxiv.org/pdf/2302.06815.pdf,Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes,Boshen Zhang,boshenzhang@tencent.com,95%
https://arxiv.org/pdf/2302.06815.pdf,Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes,Yabiao Wang,caseywang@tencent.com,78%
https://arxiv.org/pdf/2302.06815.pdf,Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes,Cai Rong Zhao,zhaocairong@tongji.edu.cn,95%
https://arxiv.org/pdf/2302.06815.pdf,Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes,Yuanpeng Tu,,0%
https://arxiv.org/pdf/2302.06815.pdf,Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes,Jiangning Zhang,,0%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Liang Liu,leoneliu@tencent.com,82%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Boshen Zhang,boshenzhang@tencent.com,95%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Yuxi Li,yukiyxli@tencent.com,82%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Yabiao Wang,jasoncjwang@tencent.com,78%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Chengjie Wang,caseywang@tencent.com,82%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Cai Rong Zhao,zhaocairong@tongji.edu.cn,95%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Yuanpeng Tu,,0%
https://arxiv.org/pdf/2302.06810.pdf,Learning from Noisy Labels with Decoupled Meta Label Purifier,Jian Li,,0%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Liang Liu,leoneliu@tencent.com,82%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Boshen Zhang,boshenzhang@tencent.com,95%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Yuxi Li,yukiyxli@tencent.com,82%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Yabiao Wang,jasoncjwang@tencent.com,78%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Chengjie Wang,caseywang@tencent.com,82%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Cai Rong Zhao,zhaocairong@tongji.edu.cn,95%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Yuanpeng Tu,,0%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Jian Li,,0%
https://arxiv.org/pdf/2302.06805.pdf,Learning with Noisy labels via Self-supervised Adversarial Noisy Masking,Jiangning Zhang,,0%
https://arxiv.org/pdf/2302.06793.pdf,HR-NeuS: Recovering High-Frequency Surface Geometry via Neural Implicit Surfaces,Erich Liang,eliang@berkeley.edu,82%
https://arxiv.org/pdf/2302.06793.pdf,HR-NeuS: Recovering High-Frequency Surface Geometry via Neural Implicit Surfaces,Kenan Deng,kenanden@amazon.com,85%
https://arxiv.org/pdf/2302.06793.pdf,HR-NeuS: Recovering High-Frequency Surface Geometry via Neural Implicit Surfaces,Xi Zhang,xizhn@amazon.com,85%
https://arxiv.org/pdf/2302.06793.pdf,HR-NeuS: Recovering High-Frequency Surface Geometry via Neural Implicit Surfaces,Chun-kai Wang,ckwang@amazon.com,82%
https://arxiv.org/pdf/2302.06755.pdf,Dataset Distillation with Convexified Implicit Gradients,Noel Loo,loo@mit.edu,78%
https://arxiv.org/pdf/2302.06755.pdf,Dataset Distillation with Convexified Implicit Gradients,Ramin Hasani,,0%
https://arxiv.org/pdf/2302.06755.pdf,Dataset Distillation with Convexified Implicit Gradients,Mathias Lechner,,0%
https://arxiv.org/pdf/2302.06755.pdf,Dataset Distillation with Convexified Implicit Gradients,Daniela Rus,,0%
https://arxiv.org/pdf/2302.06733.pdf,Robust Unsupervised StyleGAN Image Restoration,Yohan Poirier-ginter,,0%
https://arxiv.org/pdf/2302.06733.pdf,Robust Unsupervised StyleGAN Image Restoration,Jean-françois Lalonde,,0%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Ruogu Fang,ruogu.fang@ufl.edu,95%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Charlie Tran,,0%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Kai Shen,,0%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Kang Liu,,0%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Akshay Ashok,,0%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Adolfo Ramirez-zamora,,0%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Jinghua Chen,,0%
https://arxiv.org/pdf/2302.06727.pdf,Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging,Yulin Li,,0%
https://arxiv.org/pdf/2302.06698.pdf,An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection,Ramon Lawrence,ramon.lawrence@ubc.ca,95%
https://arxiv.org/pdf/2302.06698.pdf,An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection,Amritpal Singh,amritpal.singh@agr.gc.ca,95%
https://arxiv.org/pdf/2302.06698.pdf,An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection,Shahid Jahagirdar,shahid.h.j@gmail.com,85%
https://arxiv.org/pdf/2302.06698.pdf,An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection,Sam Long,lsam8910@gmail.com,85%
https://arxiv.org/pdf/2302.06698.pdf,An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection,Scott Fazackerley,scott.fazackerley@alumni.ubc.ca,95%
https://arxiv.org/pdf/2302.06698.pdf,An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection,Ritayu Nagpal,ritayu.nagpal09@gmail.com,95%
https://arxiv.org/pdf/2302.06698.pdf,An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection,Weiwei Liu,weiwei.liu2046@gmail.com,95%
https://arxiv.org/pdf/2302.06685.pdf,The Sum of Its Parts: Visual Part Segmentation for Inertial Parameter Identification of Manipulated Objects,Philippe Nadeau,,0%
https://arxiv.org/pdf/2302.06685.pdf,The Sum of Its Parts: Visual Part Segmentation for Inertial Parameter Identification of Manipulated Objects,Matthew Giamou,,0%
https://arxiv.org/pdf/2302.06685.pdf,The Sum of Its Parts: Visual Part Segmentation for Inertial Parameter Identification of Manipulated Objects,Jonathan Kelly,,0%
https://arxiv.org/pdf/2302.06684.pdf,A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000,Sontje Ihler,sontje.ihler@imes.uni-hannover.de,95%
https://arxiv.org/pdf/2302.06684.pdf,A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000,Felix Kuhnke,,0%
https://arxiv.org/pdf/2302.06684.pdf,A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000,Svenja Spindeldreier,,0%
https://arxiv.org/pdf/2302.06683.pdf,Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion,Mehryar Abbasi,,0%
https://arxiv.org/pdf/2302.06683.pdf,Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion,Parvaneh Saeedi,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Xiangning Chen,xiangning@cs.ucla.edu,85%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Esteban Real,crazydonkey@google.com,65%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Chen Liang,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Da Huang,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Kaiyuan Wang,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Yao Liu,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Hieu Pham,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Xuanyi Dong,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Thang Luong,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Cho-jui Hsieh,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Yifeng Lu,,0%
https://arxiv.org/pdf/2302.06675.pdf,Symbolic Discovery of Optimization Algorithms,Quoc V. Le,,0%
https://arxiv.org/pdf/2302.06650.pdf,Surround-View Vision-based 3D Detection for Autonomous Driving: A Survey,Varun Bankiti,varun.bankiti@motional.com,95%
https://arxiv.org/pdf/2302.06650.pdf,Surround-View Vision-based 3D Detection for Autonomous Driving: A Survey,Apoorv Singh,apoorv.singh@motional.com,95%
https://arxiv.org/pdf/2302.06643.pdf,Vision-RADAR fusion for Robotics BEV Detections: A Survey,Apoorv Singh,apoorv.singh@motional.com,95%
https://arxiv.org/pdf/2302.06608.pdf,3D-aware Blending with Generative NeRFs,Hyunsu Kim,,0%
https://arxiv.org/pdf/2302.06608.pdf,3D-aware Blending with Generative NeRFs,Gayoung Lee,,0%
https://arxiv.org/pdf/2302.06608.pdf,3D-aware Blending with Generative NeRFs,Yunjey Choi,,0%
https://arxiv.org/pdf/2302.06608.pdf,3D-aware Blending with Generative NeRFs,Jin-hwa Kim,,0%
https://arxiv.org/pdf/2302.06608.pdf,3D-aware Blending with Generative NeRFs,Jun-yan Zhu,,0%
https://arxiv.org/pdf/2302.06605.pdf,UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,Zhiwu Lu,luzhiwu@ruc.edu.cn,95%
https://arxiv.org/pdf/2302.06605.pdf,UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,Mingyu Ding,myding@berkeley.edu,82%
https://arxiv.org/pdf/2302.06605.pdf,UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,Haoyu Lu,,0%
https://arxiv.org/pdf/2302.06605.pdf,UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,Yuqi Huo,,0%
https://arxiv.org/pdf/2302.06605.pdf,UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,Guoxing Yang,,0%
https://arxiv.org/pdf/2302.06605.pdf,UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,Wei Zhan,,0%
https://arxiv.org/pdf/2302.06605.pdf,UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,Masayoshi Tomizuka,,0%
https://arxiv.org/pdf/2302.06604.pdf,ALAN: Autonomously Exploring Robotic Agents in the Real World,Russell Mendonca,,0%
https://arxiv.org/pdf/2302.06604.pdf,ALAN: Autonomously Exploring Robotic Agents in the Real World,Shikhar Bahl,,0%
https://arxiv.org/pdf/2302.06604.pdf,ALAN: Autonomously Exploring Robotic Agents in the Real World,Deepak Pathak,,0%
https://arxiv.org/pdf/2302.06594.pdf,Geometric Clifford Algebra Networks,David Ruhe,david.ruhe@gmail.com,95%
https://arxiv.org/pdf/2302.06594.pdf,Geometric Clifford Algebra Networks,Johannes Brandstetter,johannesb@microsoft.com,85%
https://arxiv.org/pdf/2302.06594.pdf,Geometric Clifford Algebra Networks,Jayesh K. Gupta,,0%
https://arxiv.org/pdf/2302.06594.pdf,Geometric Clifford Algebra Networks,Steven De Keninck,,0%
https://arxiv.org/pdf/2302.06594.pdf,Geometric Clifford Algebra Networks,Max Welling,,0%
https://arxiv.org/pdf/2302.06586.pdf,Stitchable Neural Networks,Bohan Zhuang,bohan.zhuang@gmail.com,95%
https://arxiv.org/pdf/2302.06586.pdf,Stitchable Neural Networks,Zizheng Pan,,0%
https://arxiv.org/pdf/2302.06586.pdf,Stitchable Neural Networks,Jianfei Cai,,0%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Leon Lenchik,llenchik@wakehealth.edu,82%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Akshay S. Chaudhari,akshaysc@stanford.edu,85%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Arjun Desai,arjundd@stanford.edu,85%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Bhavik N. Patel,patel.bhavik@mayo.edu,95%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Khushboo Arora,khushboo.arora@carpl.ai,95%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Louis Blankemeier,lblankem@stanford.edu,90%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Marc Willis,marc.willis@stanford.edu,95%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Sally Yao,yaohanqi@stanford.edu,78%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Andrew Wentland,alwentland@wisc.edu,82%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Robert D. Boutin,boutin@stanford.edu,78%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Eduardo Reis,edreis@stanford.edu,82%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Bhanushree Bahl,bhanushree.bahl@carpl.ai,95%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Juan Manuel Zambrano Chaves,,0%
https://arxiv.org/pdf/2302.06568.pdf,Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography,Malte Jensen,,0%
https://arxiv.org/pdf/2302.06564.pdf,A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel Training Applied to Image Recognition Problems,Martin Lanser,martin.lanser@uni-koeln.de,95%
https://arxiv.org/pdf/2302.06564.pdf,A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel Training Applied to Image Recognition Problems,Axel Klawonn,axel.klawonn@uni-koeln.de,95%
https://arxiv.org/pdf/2302.06564.pdf,A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel Training Applied to Image Recognition Problems,Janine Weber,janine.weber@uni-koeln.de,95%
https://arxiv.org/pdf/2302.06556.pdf,VA-DepthNet: A Variational Approach to Single Image Depth Prediction,Shuhang Gu,shuhanggu@uestc.edu.cn,95%
https://arxiv.org/pdf/2302.06556.pdf,VA-DepthNet: A Variational Approach to Single Image Depth Prediction,Radu Timofte,radu.timofte@uni-wuerzburg.de,95%
https://arxiv.org/pdf/2302.06556.pdf,VA-DepthNet: A Variational Approach to Single Image Depth Prediction,Luc Van Gool,vangool@vision.ee.ethz.ch,78%
https://arxiv.org/pdf/2302.06556.pdf,VA-DepthNet: A Variational Approach to Single Image Depth Prediction,Ce Liu,ce.liu@vision.ee.ethz.ch,95%
https://arxiv.org/pdf/2302.06556.pdf,VA-DepthNet: A Variational Approach to Single Image Depth Prediction,Suryansh Kumar,sukumar@vision.ee.ethz.ch,82%
https://arxiv.org/pdf/2302.06555.pdf,Do Vision and Language Models Share Concepts? A Vector Space Alignment Study,Constanza Fierro,c.fierro@di.ku.dk,82%
https://arxiv.org/pdf/2302.06555.pdf,Do Vision and Language Models Share Concepts? A Vector Space Alignment Study,Anders Søgaard,soegaard@di.ku.dk,65%
https://arxiv.org/pdf/2302.06555.pdf,Do Vision and Language Models Share Concepts? A Vector Space Alignment Study,Yova Kementchedjhieva,yova.kementchedjhieva@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.06555.pdf,Do Vision and Language Models Share Concepts? A Vector Space Alignment Study,Jiaang Li,jili@di.ku.dk,82%
https://arxiv.org/pdf/2302.06549.pdf,Between Generating Noise and Generating Images: Noise in the Correct Frequency Improves the Quality of Synthetic Histopathology Images for Digital Pathology,Yoni Peretz,yoni.savir@technion.ac.il,85%
https://arxiv.org/pdf/2302.06549.pdf,Between Generating Noise and Generating Images: Noise in the Correct Frequency Improves the Quality of Synthetic Histopathology Images for Digital Pathology,Nati Daniel,,0%
https://arxiv.org/pdf/2302.06549.pdf,Between Generating Noise and Generating Images: Noise in the Correct Frequency Improves the Quality of Synthetic Histopathology Images for Digital Pathology,Eliel Aknin,,0%
https://arxiv.org/pdf/2302.06549.pdf,Between Generating Noise and Generating Images: Noise in the Correct Frequency Improves the Quality of Synthetic Histopathology Images for Digital Pathology,Ariel Larey,,0%
https://arxiv.org/pdf/2302.06549.pdf,Between Generating Noise and Generating Images: Noise in the Correct Frequency Improves the Quality of Synthetic Histopathology Images for Digital Pathology,Guy Sela,,0%
https://arxiv.org/pdf/2302.06549.pdf,Between Generating Noise and Generating Images: Noise in the Correct Frequency Improves the Quality of Synthetic Histopathology Images for Digital Pathology,Yael Fisher,,0%
https://arxiv.org/pdf/2302.06549.pdf,Between Generating Noise and Generating Images: Noise in the Correct Frequency Improves the Quality of Synthetic Histopathology Images for Digital Pathology,Yonatan Savir,,0%
https://arxiv.org/pdf/2302.06523.pdf,Transferable Deep Metric Learning for Clustering,Jesse Read,jesse.read@polytechnique.edu,95%
https://arxiv.org/pdf/2302.06523.pdf,Transferable Deep Metric Learning for Clustering,Rim Kaddah,rim.kaddah@irt-systemx.fr,95%
https://arxiv.org/pdf/2302.06523.pdf,Transferable Deep Metric Learning for Clustering,Simo Alami. C,,0%
https://arxiv.org/pdf/2302.06514.pdf,"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction Settings: What, Why and How?",Batuhan Bal,bal.batuhan@metu.edu.tr,95%
https://arxiv.org/pdf/2302.06514.pdf,"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction Settings: What, Why and How?",Hatice Gunes,hatice.gunes@cl.cam.ac.uk,95%
https://arxiv.org/pdf/2302.06514.pdf,"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction Settings: What, Why and How?",Siyang Song,,0%
https://arxiv.org/pdf/2302.06514.pdf,"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction Settings: What, Why and How?",Micol Spitale,,0%
https://arxiv.org/pdf/2302.06514.pdf,"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction Settings: What, Why and How?",Yiming Luo,,0%
https://arxiv.org/pdf/2302.06513.pdf,DEPAS: De-novo Pathology Semantic Masks using a Generative Model,Yonatan Savir,yoni.savir@technion.ac.il,82%
https://arxiv.org/pdf/2302.06513.pdf,DEPAS: De-novo Pathology Semantic Masks using a Generative Model,Ariel Larey,,0%
https://arxiv.org/pdf/2302.06513.pdf,DEPAS: De-novo Pathology Semantic Masks using a Generative Model,Nati Daniel,,0%
https://arxiv.org/pdf/2302.06513.pdf,DEPAS: De-novo Pathology Semantic Masks using a Generative Model,Eliel Aknin,,0%
https://arxiv.org/pdf/2302.06513.pdf,DEPAS: De-novo Pathology Semantic Masks using a Generative Model,Yael Fisher,,0%
https://arxiv.org/pdf/2302.06504.pdf,Preconditioned Score-based Generative Models,Li Zhang,lizhangfd@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.06504.pdf,Preconditioned Score-based Generative Models,Hengyuan Ma,,0%
https://arxiv.org/pdf/2302.06504.pdf,Preconditioned Score-based Generative Models,Xiatian Zhu,,0%
https://arxiv.org/pdf/2302.06504.pdf,Preconditioned Score-based Generative Models,Jianfeng Feng,,0%
https://arxiv.org/pdf/2302.06494.pdf,Explicit3D: Graph Network with Spatial Inference for Single Image 3D Object Detection,Wenming Yang,yang.wenming@sz.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.06494.pdf,Explicit3D: Graph Network with Spatial Inference for Single Image 3D Object Detection,Yanjun Liu,liuyanju21@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.06492.pdf,Optical flow estimation from event-based cameras and spiking neural networks,Javier Cuadrado,,0%
https://arxiv.org/pdf/2302.06492.pdf,Optical flow estimation from event-based cameras and spiking neural networks,Ulysse Rançon,,0%
https://arxiv.org/pdf/2302.06492.pdf,Optical flow estimation from event-based cameras and spiking neural networks,Benoît Cottereau,,0%
https://arxiv.org/pdf/2302.06492.pdf,Optical flow estimation from event-based cameras and spiking neural networks,Francisco Barranco,,0%
https://arxiv.org/pdf/2302.06492.pdf,Optical flow estimation from event-based cameras and spiking neural networks,Timothée Masquelier,,0%
https://arxiv.org/pdf/2302.06441.pdf,ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion,Yimin Dou,,0%
https://arxiv.org/pdf/2302.06441.pdf,ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion,Kewen Li,,0%
https://arxiv.org/pdf/2302.06441.pdf,ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion,Wenjun Lv,,0%
https://arxiv.org/pdf/2302.06441.pdf,ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion,Timing Li,,0%
https://arxiv.org/pdf/2302.06441.pdf,ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion,Hongjie Duan,,0%
https://arxiv.org/pdf/2302.06441.pdf,ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion,Zhifeng Xu,,0%
https://arxiv.org/pdf/2302.10282.pdf,Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions,Sina Zarrieß,1first.last@uni-jena.de,60%
https://arxiv.org/pdf/2302.10282.pdf,Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions,Henrik Voigt,,0%
https://arxiv.org/pdf/2302.10282.pdf,Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions,Jan Hombeck,,0%
https://arxiv.org/pdf/2302.10282.pdf,Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions,Monique Meuschke,,0%
https://arxiv.org/pdf/2302.10282.pdf,Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions,Kai Lawonn,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Fabian Wagner,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Mareike Thies,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Noah Maul,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Laura Pfaff,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Oliver Aust,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Sabrina Pechmann,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Christopher Syben,,0%
https://arxiv.org/pdf/2302.06436.pdf,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,Andreas Maier,,0%
https://arxiv.org/pdf/2302.06432.pdf,A Deep Learning-based Global and Segmentation-based Semantic Feature Fusion Approach for Indoor Scene Classification,Tiago Barros,tiagobarros@isr.uc.pt,95%
https://arxiv.org/pdf/2302.06432.pdf,A Deep Learning-based Global and Segmentation-based Semantic Feature Fusion Approach for Indoor Scene Classification,Ana Lopes,anacris@isr.uc.pt,85%
https://arxiv.org/pdf/2302.06432.pdf,A Deep Learning-based Global and Segmentation-based Semantic Feature Fusion Approach for Indoor Scene Classification,Ricardo Pereira,ricardo.pereira@isr.uc.pt,95%
https://arxiv.org/pdf/2302.06432.pdf,A Deep Learning-based Global and Segmentation-based Semantic Feature Fusion Approach for Indoor Scene Classification,Luis Garrote,garrote@isr.uc.pt,78%
https://arxiv.org/pdf/2302.06432.pdf,A Deep Learning-based Global and Segmentation-based Semantic Feature Fusion Approach for Indoor Scene Classification,Urbano J. Nunes,urbano@isr.uc.pt,85%
https://arxiv.org/pdf/2302.06381.pdf,Self-supervised phase unwrapping in fringe projection profilometry,Wanzhong Song,songwz@scu.edu.cn,78%
https://arxiv.org/pdf/2302.06381.pdf,Self-supervised phase unwrapping in fringe projection profilometry,Xiaomin Gao,,0%
https://arxiv.org/pdf/2302.06381.pdf,Self-supervised phase unwrapping in fringe projection profilometry,Chunqian Tan,,0%
https://arxiv.org/pdf/2302.06381.pdf,Self-supervised phase unwrapping in fringe projection profilometry,Junzhe Lei,,0%
https://arxiv.org/pdf/2302.06378.pdf,Semantic Image Segmentation: Two Decades of Research,Gabriela Csurka,Gabriela.Csurka@naverlabs.com,95%
https://arxiv.org/pdf/2302.06378.pdf,Semantic Image Segmentation: Two Decades of Research,Riccardo Volpi,Riccardo.Volpi@naverlabs.com,95%
https://arxiv.org/pdf/2302.06378.pdf,Semantic Image Segmentation: Two Decades of Research,Boris Chidlovskii,Boris.Chidlovskii@naverlabs.com,95%
https://arxiv.org/pdf/2302.06358.pdf,Anticipating Next Active Objects for Egocentric Videos,Sanket Thakur,,0%
https://arxiv.org/pdf/2302.06358.pdf,Anticipating Next Active Objects for Egocentric Videos,Cigdem Beyan,,0%
https://arxiv.org/pdf/2302.06358.pdf,Anticipating Next Active Objects for Egocentric Videos,Pietro Morerio,,0%
https://arxiv.org/pdf/2302.06358.pdf,Anticipating Next Active Objects for Egocentric Videos,Vittorio Murino,,0%
https://arxiv.org/pdf/2302.06358.pdf,Anticipating Next Active Objects for Egocentric Videos,Alessio Del Bue,,0%
https://arxiv.org/pdf/2302.06356.pdf,Detection and Segmentation of Pancreas using Morphological Snakes and Deep Convolutional Neural Networks,Agapi Davradou,adavradou@gmail.com,82%
https://arxiv.org/pdf/2302.06353.pdf,Contour-based Interactive Segmentation,Danil Galeev,d.galeev@samsung.com,82%
https://arxiv.org/pdf/2302.06353.pdf,Contour-based Interactive Segmentation,Anna Vorontsova,a.vorontsova@samsung.com,82%
https://arxiv.org/pdf/2302.06353.pdf,Contour-based Interactive Segmentation,Polina Popenova,p.popenova@samsung.com,82%
https://arxiv.org/pdf/2302.06353.pdf,Contour-based Interactive Segmentation,Anton Konushin,a.konushin@samsung.com,82%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Francesco Santini,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Jakob Wasserthal,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Abramo Agosti,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Xeni Deligianni,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Kevin R. Keene,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Hermien E. Kan,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Stefan Sommer,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Fengdan Wang,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Claudia Weidensteiner,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Giulia Manco,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Matteo Paoletti,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Valentina Mazzoli,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Arjun Desai,,0%
https://arxiv.org/pdf/2302.06352.pdf,"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation",Anna Pichiecchio,,0%
https://arxiv.org/pdf/2302.06350.pdf,VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-Modal Information Retrieval,Georgina Cosma,g.cosma@lboro.ac.uk,82%
https://arxiv.org/pdf/2302.06350.pdf,VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-Modal Information Retrieval,Axel Finke,a.finke@lboro.ac.uk,82%
https://arxiv.org/pdf/2302.06350.pdf,VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-Modal Information Retrieval,Yan Gong,y.gong2@lboro.ac.uk,82%
https://arxiv.org/pdf/2302.06335.pdf,Online Arbitrary Shaped Clustering through Correlated Gaussian Functions,Ole Christian Eidheim,ole.c.eidheim@ntnu.no,95%
https://arxiv.org/pdf/2302.06318.pdf,Towards Writing Style Adaptation in Handwriting Recognition,Martin Kišš,ikiss@fit.vutbr.cz,78%
https://arxiv.org/pdf/2302.06318.pdf,Towards Writing Style Adaptation in Handwriting Recognition,Jan Kohút,ikohut@fit.vutbr.cz,78%
https://arxiv.org/pdf/2302.06318.pdf,Towards Writing Style Adaptation in Handwriting Recognition,Michal Hradiš,ihradis@fit.vutbr.cz,78%
https://arxiv.org/pdf/2302.06308.pdf,Fine-tuning Is a Surprisingly Effective Domain Adaptation Baseline in Handwriting Recognition,Jan Kohút,ikohut@fit.vutbr.cz,78%
https://arxiv.org/pdf/2302.06308.pdf,Fine-tuning Is a Surprisingly Effective Domain Adaptation Baseline in Handwriting Recognition,Michal Hradiš,ihradis@fit.vutbr.cz,78%
https://arxiv.org/pdf/2302.06301.pdf,A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment,Yahya Zweiri,yahya.zweiri@ku.ac.ae,95%
https://arxiv.org/pdf/2302.06301.pdf,A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment,Xiaoqian Huang,,0%
https://arxiv.org/pdf/2302.06301.pdf,A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment,Kachole Sanket,,0%
https://arxiv.org/pdf/2302.06301.pdf,A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment,Abdulla Ayyad,,0%
https://arxiv.org/pdf/2302.06301.pdf,A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment,Fariborz Baghaei Naeini,,0%
https://arxiv.org/pdf/2302.06301.pdf,A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment,Dimitrios Makris,,0%
https://arxiv.org/pdf/2302.06298.pdf,Hyperspectral Image Super Resolution with Real Unaligned RGB Guidance,Zeqiang Lai,,0%
https://arxiv.org/pdf/2302.06298.pdf,Hyperspectral Image Super Resolution with Real Unaligned RGB Guidance,Ying Fu,,0%
https://arxiv.org/pdf/2302.06298.pdf,Hyperspectral Image Super Resolution with Real Unaligned RGB Guidance,Jun Zhang,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Chinedu Innocent Nwoye,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Tong Yu,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Saurav Sharma,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Aditya Murali,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Deepak Alapatt,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Armine Vardazaryan,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Kun Yuan,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Jonas Hajek,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Wolfgang Reiter,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Amine Yamlahi,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Finn-henri Smidt,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Xiaoyang Zou,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Guoyan Zheng,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Bruno Oliveira,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Helena R. Torres,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Satoshi Kondo,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Satoshi Kasai,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Felix Holm,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Ege Özsoy,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Shuangchun Gui,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Han Li,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Sista Raviteja,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Rachana Sathish,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Pranav Poudel,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Binod Bhattarai,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Ziheng Wang,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Guo Rui,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Melanie Schellenberg,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,João L. Vilaça,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Tobias Czempiel,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Zhenkun Wang,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Debdoot Sheet,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Shrawan Kumar Thapa,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Max Berniker,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Patrick Godau,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Pedro Morais,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Sudarshan Regmi,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Thuy Nuong Tran,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Jaime Fonseca,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Jan-hinrich Nölke,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Estevão Lima,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Eduard Vazquez,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Lena Maier-hein,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Nassir Navab,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Pietro Mascagni,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Barbara Seeliger,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Cristians Gonzalez,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Didier Mutter,,0%
https://arxiv.org/pdf/2302.06294.pdf,CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection,Nicolas Padoy,,0%
https://arxiv.org/pdf/2302.06293.pdf,Content-Adaptive Motion Rate Adaption for Learned Video Compression,Chih-hsuan Lin,,0%
https://arxiv.org/pdf/2302.06293.pdf,Content-Adaptive Motion Rate Adaption for Learned Video Compression,Yi-hsin Chen,,0%
https://arxiv.org/pdf/2302.06293.pdf,Content-Adaptive Motion Rate Adaption for Learned Video Compression,Wen-hsiao Peng,,0%
https://arxiv.org/pdf/2302.06291.pdf,Surface-biased Multi-Level Context 3D Object Detection,Rao Anwer,rao.anwer@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.06291.pdf,Surface-biased Multi-Level Context 3D Object Detection,Sultan Abu Ghazal,sultan.abughazal@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.06291.pdf,Surface-biased Multi-Level Context 3D Object Detection,Jean Lahoud,jean.lahoud@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.06287.pdf,Render-and-Compare: Cross-View 6 DoF Localization from Noisy Prior,Yuxiang Liu,liuyuxiang17@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.06287.pdf,Render-and-Compare: Cross-View 6 DoF Localization from Noisy Prior,Yu Liu,jasonyuliu@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.06287.pdf,Render-and-Compare: Cross-View 6 DoF Localization from Noisy Prior,Rouwan Wu,wurouwan97@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.06287.pdf,Render-and-Compare: Cross-View 6 DoF Localization from Noisy Prior,Juelin Zhu,zhujuelin@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.06287.pdf,Render-and-Compare: Cross-View 6 DoF Localization from Noisy Prior,Xiaoya Cheng,chengxy@nudt.edu.cn,78%
https://arxiv.org/pdf/2302.06287.pdf,Render-and-Compare: Cross-View 6 DoF Localization from Noisy Prior,Shen Yan,yanshen12@nudt.edu.cn,95%
https://arxiv.org/pdf/2302.06287.pdf,Render-and-Compare: Cross-View 6 DoF Localization from Noisy Prior,Maojun Zhang,mjzhang@nudt.edu.cn,82%
https://arxiv.org/pdf/2302.06279.pdf,Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,Oguzhan Ersoy,oguzhan.ersoy@ru.nl,95%
https://arxiv.org/pdf/2302.06279.pdf,Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,Aitor Urbieta,aurbieta@ikerlan.es,82%
https://arxiv.org/pdf/2302.06279.pdf,Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,Gorka Abad,abad.gorka@ru.nl,95%
https://arxiv.org/pdf/2302.06279.pdf,Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,Stjepan Picek,stjepan.picek@ru.nl,95%
https://arxiv.org/pdf/2302.06251.pdf,Optimizing CT Scan Geometries With and Without Gradients,Mareike Thies,,0%
https://arxiv.org/pdf/2302.06251.pdf,Optimizing CT Scan Geometries With and Without Gradients,Fabian Wagner,,0%
https://arxiv.org/pdf/2302.06251.pdf,Optimizing CT Scan Geometries With and Without Gradients,Noah Maul,,0%
https://arxiv.org/pdf/2302.06251.pdf,Optimizing CT Scan Geometries With and Without Gradients,Laura Pfaff,,0%
https://arxiv.org/pdf/2302.06251.pdf,Optimizing CT Scan Geometries With and Without Gradients,Linda-sophie Schneider,,0%
https://arxiv.org/pdf/2302.06251.pdf,Optimizing CT Scan Geometries With and Without Gradients,Christopher Syben,,0%
https://arxiv.org/pdf/2302.06251.pdf,Optimizing CT Scan Geometries With and Without Gradients,Andreas Maier,,0%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,Jie Ren,jjren@google.com,82%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,James Urquhart Allingham,,0%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,Michael W Dusenberry,,0%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,Xiuye Gu,,0%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,Yin Cui,,0%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,Dustin Tran,,0%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,Jeremiah Zhe Liu,,0%
https://arxiv.org/pdf/2302.06235.pdf,A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models,Balaji Lakshminarayanan,,0%
https://arxiv.org/pdf/2302.06195.pdf,Exploring Navigation Maps for Learning-Based Motion Prediction,Julian Schmidt,julian.sj.schmidt@mercedes-benz.com,95%
https://arxiv.org/pdf/2302.06195.pdf,Exploring Navigation Maps for Learning-Based Motion Prediction,Julian Jordan,,0%
https://arxiv.org/pdf/2302.06195.pdf,Exploring Navigation Maps for Learning-Based Motion Prediction,Franz Gritschneder,,0%
https://arxiv.org/pdf/2302.06195.pdf,Exploring Navigation Maps for Learning-Based Motion Prediction,Thomas Monninger,,0%
https://arxiv.org/pdf/2302.06195.pdf,Exploring Navigation Maps for Learning-Based Motion Prediction,Klaus Dietmayer,,0%
https://arxiv.org/pdf/2302.06194.pdf,Capsules as viewpoint learners for human pose estimation,Nicola Conci,nicola.conci@unitn.it,95%
https://arxiv.org/pdf/2302.06194.pdf,Capsules as viewpoint learners for human pose estimation,Nicola Garau,,0%
https://arxiv.org/pdf/2302.06185.pdf,PUPS: Point Cloud Unified Panoptic Segmentation,Huanyu Wang,huanyuhello@zju.edu.cn,85%
https://arxiv.org/pdf/2302.06185.pdf,PUPS: Point Cloud Unified Panoptic Segmentation,Shihao Su,shihaocs@zju.edu.cn,85%
https://arxiv.org/pdf/2302.06185.pdf,PUPS: Point Cloud Unified Panoptic Segmentation,Jianyun Xu,xujianyun.xjy@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.06185.pdf,PUPS: Point Cloud Unified Panoptic Segmentation,Zhenwei Miao,zhenwei.mzw@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.06185.pdf,PUPS: Point Cloud Unified Panoptic Segmentation,Xi Li,xilizju@zju.edu.cn,95%
https://arxiv.org/pdf/2302.06185.pdf,PUPS: Point Cloud Unified Panoptic Segmentation,Dayang Hao,haodayang@gmail.com,95%
https://arxiv.org/pdf/2302.06185.pdf,PUPS: Point Cloud Unified Panoptic Segmentation,Xin Zhan,zhanxin.zx@alibaba-inc.com,95%
https://arxiv.org/pdf/2302.06183.pdf,Anti-Compression Contrastive Facial Forgery Detection,Surya Nepal,surya.nepal@data61.csiro.au,95%
https://arxiv.org/pdf/2302.06183.pdf,Anti-Compression Contrastive Facial Forgery Detection,Chang Xu,c.xu@sydney.edu.au,82%
https://arxiv.org/pdf/2302.06183.pdf,Anti-Compression Contrastive Facial Forgery Detection,Siqi Ma,siqi.ma@adfa.edu.au,95%
https://arxiv.org/pdf/2302.06183.pdf,Anti-Compression Contrastive Facial Forgery Detection,Jiajun Huang,,0%
https://arxiv.org/pdf/2302.06183.pdf,Anti-Compression Contrastive Facial Forgery Detection,Xinqi Zhu,,0%
https://arxiv.org/pdf/2302.06183.pdf,Anti-Compression Contrastive Facial Forgery Detection,Chengbin Du,,0%
https://arxiv.org/pdf/2302.06175.pdf,Learning and Aggregating Lane Graphs for Urban Automated Driving,Martin Büchner,,0%
https://arxiv.org/pdf/2302.06175.pdf,Learning and Aggregating Lane Graphs for Urban Automated Driving,Jannik Zürn,,0%
https://arxiv.org/pdf/2302.06175.pdf,Learning and Aggregating Lane Graphs for Urban Automated Driving,Ion-george Todoran,,0%
https://arxiv.org/pdf/2302.06175.pdf,Learning and Aggregating Lane Graphs for Urban Automated Driving,Abhinav Valada,,0%
https://arxiv.org/pdf/2302.06175.pdf,Learning and Aggregating Lane Graphs for Urban Automated Driving,Wolfram Burgard,,0%
https://arxiv.org/pdf/2302.10275.pdf,Semantic Feature Integration network for Fine-grained Visual Classification,Yueyang Li,lyueyang@jiangnan.edu.cn,85%
https://arxiv.org/pdf/2302.10275.pdf,Semantic Feature Integration network for Fine-grained Visual Classification,Haichi Luo,luohaichi@jiangnan.edu.cn,95%
https://arxiv.org/pdf/2302.10275.pdf,Semantic Feature Integration network for Fine-grained Visual Classification,Hui Wang,,0%
https://arxiv.org/pdf/2302.06149.pdf,Contour Context: Abstract Structural Distribution for 3D LiDAR Loop Detection and Metric Pose Estimation,Shaojie Shen,eeshaojie@ust.hk,85%
https://arxiv.org/pdf/2302.06149.pdf,Contour Context: Abstract Structural Distribution for 3D LiDAR Loop Detection and Metric Pose Estimation,Binqian Jiang,bjiangah@connect.ust.hk,82%
https://arxiv.org/pdf/2302.06148.pdf,CoMAE: Single Model Hybrid Pre-training on Small-Scale RGB-D Datasets,Jiange Yang,jiangeyang.jgy@gmail.com,95%
https://arxiv.org/pdf/2302.06148.pdf,CoMAE: Single Model Hybrid Pre-training on Small-Scale RGB-D Datasets,Sheng Guo,guosheng1001@gmail.com,95%
https://arxiv.org/pdf/2302.06148.pdf,CoMAE: Single Model Hybrid Pre-training on Small-Scale RGB-D Datasets,Gangshan Wu,gswu@nju.edu.cn,82%
https://arxiv.org/pdf/2302.06148.pdf,CoMAE: Single Model Hybrid Pre-training on Small-Scale RGB-D Datasets,Limin Wang,lmwang@nju.edu.cn,82%
https://arxiv.org/pdf/2302.06134.pdf,RFC-Net: Learning High Resolution Global Features for Medical Image Segmentation on a Computational Budget,Sourajit Saha,2ssaha3@umbc.edu,78%
https://arxiv.org/pdf/2302.06134.pdf,RFC-Net: Learning High Resolution Global Features for Medical Image Segmentation on a Computational Budget,David Chapman,5dchapman@cs.miami.edu,78%
https://arxiv.org/pdf/2302.06134.pdf,RFC-Net: Learning High Resolution Global Features for Medical Image Segmentation on a Computational Budget,Md Osman Gani,3mogani@umbc.edu,78%
https://arxiv.org/pdf/2302.06134.pdf,RFC-Net: Learning High Resolution Global Features for Medical Image Segmentation on a Computational Budget,Tim Oates,4oates@umbc.edu,78%
https://arxiv.org/pdf/2302.06134.pdf,RFC-Net: Learning High Resolution Global Features for Medical Image Segmentation on a Computational Budget,Shaswati Saha,,0%
https://arxiv.org/pdf/2302.06133.pdf,Deep Transfer Tensor Factorization for Multi-View Learning,Penghao Jiang,,0%
https://arxiv.org/pdf/2302.06133.pdf,Deep Transfer Tensor Factorization for Multi-View Learning,Ke Xin,,0%
https://arxiv.org/pdf/2302.06133.pdf,Deep Transfer Tensor Factorization for Multi-View Learning,Chunxi Li,,0%
https://arxiv.org/pdf/2302.06130.pdf,Learning to Scale Temperature in Masked Self-Attention for Image Inpainting,Xiang Zhou,zhoux2020@mail.sustech.edu.cn,78%
https://arxiv.org/pdf/2302.06130.pdf,Learning to Scale Temperature in Masked Self-Attention for Image Inpainting,Yuan Zeng,hi.zengyuan@gmail.com,95%
https://arxiv.org/pdf/2302.06130.pdf,Learning to Scale Temperature in Masked Self-Attention for Image Inpainting,Yi Gong,gongy@sustech.edu.cn,78%
https://arxiv.org/pdf/2302.06112.pdf,How to Use Dropout Correctly on Residual Networks with Batch Normalization,Bum Jun Kim,,0%
https://arxiv.org/pdf/2302.06112.pdf,How to Use Dropout Correctly on Residual Networks with Batch Normalization,Hyeyeon Choi,,0%
https://arxiv.org/pdf/2302.06112.pdf,How to Use Dropout Correctly on Residual Networks with Batch Normalization,Hyeonah Jang,,0%
https://arxiv.org/pdf/2302.06112.pdf,How to Use Dropout Correctly on Residual Networks with Batch Normalization,Donggeon Lee,,0%
https://arxiv.org/pdf/2302.06112.pdf,How to Use Dropout Correctly on Residual Networks with Batch Normalization,Sang Woo Kim,,0%
https://arxiv.org/pdf/2302.06098.pdf,Towards Local Visual Modeling for Image Captioning,Yiwei Ma,,0%
https://arxiv.org/pdf/2302.06098.pdf,Towards Local Visual Modeling for Image Captioning,Jiayi Ji,,0%
https://arxiv.org/pdf/2302.06098.pdf,Towards Local Visual Modeling for Image Captioning,Xiaoshuai Sun,,0%
https://arxiv.org/pdf/2302.06098.pdf,Towards Local Visual Modeling for Image Captioning,Yiyi Zhou,,0%
https://arxiv.org/pdf/2302.06098.pdf,Towards Local Visual Modeling for Image Captioning,Rongrong Ji,,0%
https://arxiv.org/pdf/2302.06096.pdf,Dual-layer Image Compression via Adaptive Downsampling and Spatially Varying Upconversion,Xiaolin Wu,xwu@ece.mcmaster.ca,82%
https://arxiv.org/pdf/2302.06096.pdf,Dual-layer Image Compression via Adaptive Downsampling and Spatially Varying Upconversion,Xi Zhang,,0%
https://arxiv.org/pdf/2302.06093.pdf,Learning-Based Defect Recognitions for Autonomous UAV Inspections,Kangcheng Liu,kcliu@mae.cuhk.edu.hk,82%
https://arxiv.org/pdf/2302.06091.pdf,Boosted ab initio Cryo-EM 3D Reconstruction with ACE-EM,Lin Yao,,0%
https://arxiv.org/pdf/2302.06091.pdf,Boosted ab initio Cryo-EM 3D Reconstruction with ACE-EM,Ruihan Xu,,0%
https://arxiv.org/pdf/2302.06091.pdf,Boosted ab initio Cryo-EM 3D Reconstruction with ACE-EM,Zhifeng Gao,,0%
https://arxiv.org/pdf/2302.06091.pdf,Boosted ab initio Cryo-EM 3D Reconstruction with ACE-EM,Guolin Ke,,0%
https://arxiv.org/pdf/2302.06091.pdf,Boosted ab initio Cryo-EM 3D Reconstruction with ACE-EM,Yuhang Wang,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Fei Kong,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Xiyue Wang,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Jinxi Xiang,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Sen Yang,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Xinran Wang,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Meng Yue,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Jun Zhang,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Junhan Zhao,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Xiao Han,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Yuhan Dong,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Biyue Zhu,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Fang Wang,,0%
https://arxiv.org/pdf/2302.06089.pdf,Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading,Yueping Liu,,0%
https://arxiv.org/pdf/2302.06081.pdf,Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval,Peng Hu,penghu.ml@gmail.com,95%
https://arxiv.org/pdf/2302.06081.pdf,Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval,Ming Yan,yanmingtop@gmail.com,95%
https://arxiv.org/pdf/2302.06081.pdf,Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval,Xu Wang,wangxu.scu@gmail.com,95%
https://arxiv.org/pdf/2302.06081.pdf,Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval,Dezhong Peng,,0%
https://arxiv.org/pdf/2302.06078.pdf,"NYCU-TWO at Memotion 3: Good Foundation, Good Teacher, then you have Good Meme Analysis",Wen-chih Peng,wcpeng@nctu.edu.tw,82%
https://arxiv.org/pdf/2302.06078.pdf,"NYCU-TWO at Memotion 3: Good Foundation, Good Teacher, then you have Good Meme Analysis",Ting-yun Ou,outingyun.cs11@nycu.edu.tw,95%
https://arxiv.org/pdf/2302.06078.pdf,"NYCU-TWO at Memotion 3: Good Foundation, Good Teacher, then you have Good Meme Analysis",Kuang-da Wang,gdwang.cs10@nycu.edu.tw,78%
https://arxiv.org/pdf/2302.06078.pdf,"NYCU-TWO at Memotion 3: Good Foundation, Good Teacher, then you have Good Meme Analysis",Yu-chien Tang,,0%
https://arxiv.org/pdf/2302.06072.pdf,Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation,Jianzhuang Liu,liu.jianzhuang@huawei.com,95%
https://arxiv.org/pdf/2302.06072.pdf,Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation,Yi Zhu,zhuyi36@huawei.com,95%
https://arxiv.org/pdf/2302.06072.pdf,Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation,Liang Lin,linliang@ieee.org,95%
https://arxiv.org/pdf/2302.06072.pdf,Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation,Bingqian Lin,,0%
https://arxiv.org/pdf/2302.06072.pdf,Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation,Xiaodan Liang,,0%
https://arxiv.org/pdf/2302.06060.pdf,Threatening Patch Attacks on Object Detection in Optical Remote Sensing Images,Gong Cheng,gcheng@nwpu.edu.cn,82%
https://arxiv.org/pdf/2302.06060.pdf,Threatening Patch Attacks on Object Detection in Optical Remote Sensing Images,Xuxiang Sun,,0%
https://arxiv.org/pdf/2302.06060.pdf,Threatening Patch Attacks on Object Detection in Optical Remote Sensing Images,Lei Pei,,0%
https://arxiv.org/pdf/2302.06060.pdf,Threatening Patch Attacks on Object Detection in Optical Remote Sensing Images,Hongda Li,,0%
https://arxiv.org/pdf/2302.06060.pdf,Threatening Patch Attacks on Object Detection in Optical Remote Sensing Images,Junwei Han,,0%
https://arxiv.org/pdf/2302.06058.pdf,Bi-directional Masks for Efficient N:M Sparse Training,Rongrong Ji,rrji@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.06058.pdf,Bi-directional Masks for Efficient N:M Sparse Training,Yuxin Zhang,,0%
https://arxiv.org/pdf/2302.06058.pdf,Bi-directional Masks for Efficient N:M Sparse Training,Yiting Luo,,0%
https://arxiv.org/pdf/2302.06058.pdf,Bi-directional Masks for Efficient N:M Sparse Training,Mingbao Lin,,0%
https://arxiv.org/pdf/2302.06058.pdf,Bi-directional Masks for Efficient N:M Sparse Training,Yunshan Zhong,,0%
https://arxiv.org/pdf/2302.06058.pdf,Bi-directional Masks for Efficient N:M Sparse Training,Jingjing Xie,,0%
https://arxiv.org/pdf/2302.06058.pdf,Bi-directional Masks for Efficient N:M Sparse Training,Fei Chao,,0%
https://arxiv.org/pdf/2302.06052.pdf,CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction,Ziyi Li,liziyi@hust.edu.cn,95%
https://arxiv.org/pdf/2302.06052.pdf,CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction,Xiaolin Hu,xlhu@mail.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.06052.pdf,CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction,Gang Zhang,zhang-g19@mails.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.06052.pdf,CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction,Jianmin Li,lijianmin@mail.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.06052.pdf,CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction,Chufeng Tang,,0%
https://arxiv.org/pdf/2302.06039.pdf,Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection,Feras Dayoub,feras.dayoub@adelaide.edu.au,95%
https://arxiv.org/pdf/2302.06039.pdf,Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection,Christopher Lehnert,c.lehnert@qut.edu.au,82%
https://arxiv.org/pdf/2302.06039.pdf,Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection,Nicolas Harvey Chapman,nicolasharvey.chapman@hdr.qut.edu.au,95%
https://arxiv.org/pdf/2302.06039.pdf,Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection,Will Browne,will.browne@qut.edu.au,95%
https://arxiv.org/pdf/2302.06019.pdf,A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training,Jingnan Shi,jnshi@mit.edu,82%
https://arxiv.org/pdf/2302.06019.pdf,A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training,Rajat Talak,talak@mit.edu,78%
https://arxiv.org/pdf/2302.06019.pdf,A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training,Dominic Maggio,drmaggio@mit.edu,82%
https://arxiv.org/pdf/2302.06019.pdf,A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training,Luca Carlone,lcarlone@mit.edu,82%
https://arxiv.org/pdf/2302.06015.pdf,"A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity",Pin-yu Chen,Pin-Yu.Chen@ibm.com,95%
https://arxiv.org/pdf/2302.06015.pdf,"A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity",Hongkang Li,lih35@rpi.edu,78%
https://arxiv.org/pdf/2302.06015.pdf,"A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity",Meng Wang,wangm7@rpi.edu,78%
https://arxiv.org/pdf/2302.06015.pdf,"A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity",Sijia Liu,,0%
https://arxiv.org/pdf/2302.06009.pdf,Policy-Induced Self-Supervision Improves Representation Finetuning in Visual RL,Sébastien M. R. Arnold,seb.arnold@usc.edu,82%
https://arxiv.org/pdf/2302.06009.pdf,Policy-Induced Self-Supervision Improves Representation Finetuning in Visual RL,Fei Sha,fsha@google.com,82%
https://arxiv.org/pdf/2302.10904.pdf,Deep Learning in Healthcare: An In-Depth Analysis,Farid Ghareh Mohammadi,farid.ghm@uga.edu,85%
https://arxiv.org/pdf/2302.10904.pdf,Deep Learning in Healthcare: An In-Depth Analysis,Khaled M. Rasheed,khaled@uga.edu,85%
https://arxiv.org/pdf/2302.10904.pdf,Deep Learning in Healthcare: An In-Depth Analysis,Farzan Shenavarmasouleh,,0%
https://arxiv.org/pdf/2302.10904.pdf,Deep Learning in Healthcare: An In-Depth Analysis,Hamid R. Arabnia,,0%
https://arxiv.org/pdf/2302.10281.pdf,LiT Tuned Models for Efficient Species Detection,Chinmay Hegde,chinmay.h@nyu.edu,85%
https://arxiv.org/pdf/2302.10281.pdf,LiT Tuned Models for Efficient Species Detection,Andre Nakkab,,0%
https://arxiv.org/pdf/2302.10281.pdf,LiT Tuned Models for Efficient Species Detection,Benjamin Feuer,,0%
https://arxiv.org/pdf/2302.05991.pdf,Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications,Seth Z. Zhao,sethzhao506@berkeley.edu,95%
https://arxiv.org/pdf/2302.05991.pdf,Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications,Chuanyu Pan,chuanyu pan@berkeley.edu,95%
https://arxiv.org/pdf/2302.05991.pdf,Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications,Allen Y. Yang,allenyang@berkeley.edu,95%
https://arxiv.org/pdf/2302.05991.pdf,Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications,Weiyu Feng,weiyu feng@berkeley.edu,95%
https://arxiv.org/pdf/2302.05991.pdf,Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications,Adam Chang,,0%
https://arxiv.org/pdf/2302.05991.pdf,Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications,Yichen Chen,,0%
https://arxiv.org/pdf/2302.05991.pdf,Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications,Zekun Wang,,0%
https://arxiv.org/pdf/2302.05968.pdf,Self-supervised pseudo-colorizing of masked cells,Royden Wagner,royden.wagner@kit.edu,95%
https://arxiv.org/pdf/2302.05968.pdf,Self-supervised pseudo-colorizing of masked cells,Carlos Fernandez Lopez,,0%
https://arxiv.org/pdf/2302.05968.pdf,Self-supervised pseudo-colorizing of masked cells,Christoph Stiller,,0%
https://arxiv.org/pdf/2302.07740.pdf,Team Triple-Check at Factify 2: Parameter-Efficient Large Foundation Models with Feature Representations for Multi-Modal Fact Verification,Wen-chih Peng,wcpeng@cs.nycu.edu.tw,82%
https://arxiv.org/pdf/2302.07740.pdf,Team Triple-Check at Factify 2: Parameter-Efficient Large Foundation Models with Feature Representations for Multi-Modal Fact Verification,Hong-wei Wu,johnnyhwu.cs11@nycu.edu.tw,78%
https://arxiv.org/pdf/2302.07740.pdf,Team Triple-Check at Factify 2: Parameter-Efficient Large Foundation Models with Feature Representations for Multi-Modal Fact Verification,Wei-wei Du,wwdu.cs10@nycu.edu.tw,82%
https://arxiv.org/pdf/2302.07740.pdf,Team Triple-Check at Factify 2: Parameter-Efficient Large Foundation Models with Feature Representations for Multi-Modal Fact Verification,Wei-yao Wang,,0%
https://arxiv.org/pdf/2302.05936.pdf,Generalized Few-Shot Continual Learning with Contrastive Mixture of Adapters,Zitong Yu,zitong.yu@ieee.org,95%
https://arxiv.org/pdf/2302.05936.pdf,Generalized Few-Shot Continual Learning with Contrastive Mixture of Adapters,Xun Wang,bnuwangxun@gmail.com,95%
https://arxiv.org/pdf/2302.05936.pdf,Generalized Few-Shot Continual Learning with Contrastive Mixture of Adapters,Alex C. Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.05936.pdf,Generalized Few-Shot Continual Learning with Contrastive Mixture of Adapters,Li Liu,dreamliu2010@gmail.com,95%
https://arxiv.org/pdf/2302.05936.pdf,Generalized Few-Shot Continual Learning with Contrastive Mixture of Adapters,Rizhao Cai,rzcai@ntu.edu.sg,82%
https://arxiv.org/pdf/2302.05936.pdf,Generalized Few-Shot Continual Learning with Contrastive Mixture of Adapters,Yawen Cui,,0%
https://arxiv.org/pdf/2302.05923.pdf,Uncertainty-Aware AB3DMOT by Variational 3D Object Detection,Alexandros Iosifidis,ai@ece.au.dk,90%
https://arxiv.org/pdf/2302.05923.pdf,Uncertainty-Aware AB3DMOT by Variational 3D Object Detection,Illia Oleksiienko,io@ece.au.dk,90%
https://arxiv.org/pdf/2302.05916.pdf,Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes,Qiang Wen,qwenab@connect.ust.hk,82%
https://arxiv.org/pdf/2302.05916.pdf,Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes,Yue Wu,ywudg@connect.ust.hk,82%
https://arxiv.org/pdf/2302.05916.pdf,Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes,Qifeng Chen,,0%
https://arxiv.org/pdf/2302.05914.pdf,Variational Voxel Pseudo Image Tracking,Nikolaos Passalis,passalis@csd.auth.gr,78%
https://arxiv.org/pdf/2302.05914.pdf,Variational Voxel Pseudo Image Tracking,Anastasios Tefas,tefas@csd.auth.gr,78%
https://arxiv.org/pdf/2302.05914.pdf,Variational Voxel Pseudo Image Tracking,Alexandros Iosifidis,ai@ece.au.dk,90%
https://arxiv.org/pdf/2302.05914.pdf,Variational Voxel Pseudo Image Tracking,Paraskevi Nousi,paranous@csd.auth.gr,65%
https://arxiv.org/pdf/2302.05914.pdf,Variational Voxel Pseudo Image Tracking,Illia Oleksiienko,io@ece.au.dk,90%
https://arxiv.org/pdf/2302.05905.pdf,Single Motion Diffusion,Sigal Raab,sigalraab@tauex.tau.ac.il,95%
https://arxiv.org/pdf/2302.05905.pdf,Single Motion Diffusion,Inbal Leibovitch,inball1@mail.tau.ac.il,85%
https://arxiv.org/pdf/2302.05905.pdf,Single Motion Diffusion,Guy Tevet,guytevet@mail.tau.ac.il,95%
https://arxiv.org/pdf/2302.05905.pdf,Single Motion Diffusion,Moab Arar,,0%
https://arxiv.org/pdf/2302.05905.pdf,Single Motion Diffusion,Amit H. Bermano,,0%
https://arxiv.org/pdf/2302.05905.pdf,Single Motion Diffusion,Daniel Cohen-or,,0%
https://arxiv.org/pdf/2302.10763.pdf,Contrastive Learning and the Emergence of Attributes Associations,Daniel N. Nissani,dnissani@post.bgu.ac.il,82%
https://arxiv.org/pdf/2302.05881.pdf,A generalizable framework for low-rank tensor completion with numerical priors,Kaizhu Huang,kaizhu.huang@dukekunshan.edu.cn,95%
https://arxiv.org/pdf/2302.05881.pdf,A generalizable framework for low-rank tensor completion with numerical priors,Shiran Yuan,,0%
https://arxiv.org/pdf/2302.05872.pdf,I$^2$SB: Image-to-Image Schrödinger Bridge,Guan-horng Liu,ghliu@gatech.edu,82%
https://arxiv.org/pdf/2302.05872.pdf,I$^2$SB: Image-to-Image Schrödinger Bridge,Arash Vahdat,,0%
https://arxiv.org/pdf/2302.05872.pdf,I$^2$SB: Image-to-Image Schrödinger Bridge,De-an Huang,,0%
https://arxiv.org/pdf/2302.05872.pdf,I$^2$SB: Image-to-Image Schrödinger Bridge,Evangelos A. Theodorou,,0%
https://arxiv.org/pdf/2302.05872.pdf,I$^2$SB: Image-to-Image Schrödinger Bridge,Weili Nie,,0%
https://arxiv.org/pdf/2302.05872.pdf,I$^2$SB: Image-to-Image Schrödinger Bridge,Anima Anandkumar,,0%
https://arxiv.org/pdf/2302.05846.pdf,OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching,Lijun Zhao,zhaolj@hit.edu.cn,78%
https://arxiv.org/pdf/2302.05846.pdf,OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching,Ke Wang,wangke@hit.edu.cn,95%
https://arxiv.org/pdf/2302.05846.pdf,OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching,Tao Xie,xietao1997@hit.edu.cn,95%
https://arxiv.org/pdf/2302.05846.pdf,OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching,Kun Dai,,0%
https://arxiv.org/pdf/2302.05846.pdf,OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching,Zhiqiang Jiang,,0%
https://arxiv.org/pdf/2302.05846.pdf,OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching,Ruifeng Li,,0%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Yaqing Ding,dingyaqing@njust.edu.cn,95%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Qianliang Wu,wuqianliang@njust.edu.cn,95%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Jian Yang,jiang.hao.bo@njust.edu.cn,85%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Jin Xie,csjxie@njust.edu.cn,78%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Guofeng Mei,guofeng.mei@student.uts.edu.au,95%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Yaqi Shen,,0%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Haobo Jiang,,0%
https://arxiv.org/pdf/2302.05844.pdf,Graph Matching Optimization Network for Point Cloud Registration,Lei Luo,,0%
https://arxiv.org/pdf/2302.05830.pdf,NephroNet: A Novel Program for Identifying Renal Cell Carcinoma and Generating Synthetic Training Images with Convolutional Neural Networks and Diffusion Models,Yashvir Sabharwal,,0%
https://arxiv.org/pdf/2302.05803.pdf,TPE-Net: Track Point Extraction and Association Network for Rail Path Proposal Generation,David Beach,David.Beach@thalesgroup.com,95%
https://arxiv.org/pdf/2302.05803.pdf,TPE-Net: Track Point Extraction and Association Network for Rail Path Proposal Generation,Veronica Marin,Veronica.Marin@thalesgroup.com,95%
https://arxiv.org/pdf/2302.05803.pdf,TPE-Net: Track Point Extraction and Association Network for Rail Path Proposal Generation,Jungwon Kang,Jungwon.Kang@thalesgroup.com,95%
https://arxiv.org/pdf/2302.05803.pdf,TPE-Net: Track Point Extraction and Association Network for Rail Path Proposal Generation,Gunho Sohn,gsohn@yorku.ca,82%
https://arxiv.org/pdf/2302.05803.pdf,TPE-Net: Track Point Extraction and Association Network for Rail Path Proposal Generation,Mohammadjavad Ghorbanalivakili,,0%
https://arxiv.org/pdf/2302.05788.pdf,Fairness-aware Multi-view Clustering,Lecheng Zheng,lecheng4@illinois.edu,85%
https://arxiv.org/pdf/2302.05788.pdf,Fairness-aware Multi-view Clustering,Yada Zhu,yzhu@us.ibm.com,82%
https://arxiv.org/pdf/2302.05788.pdf,Fairness-aware Multi-view Clustering,Jingrui He,grui@illinois.edu,90%
https://arxiv.org/pdf/2302.05776.pdf,Stochastic Surprisal: An inferential measurement of Free Energy in Neural Networks,Mohit Prabhushankar,mohit.p@gatech.edu,85%
https://arxiv.org/pdf/2302.05776.pdf,Stochastic Surprisal: An inferential measurement of Free Energy in Neural Networks,Ghassan Alregib,alregib@gatech.edu,78%
https://arxiv.org/pdf/2302.05759.pdf,Improving Sign Recognition with Phonology,Lee Kezar,lkezar@usc.edu,82%
https://arxiv.org/pdf/2302.05759.pdf,Improving Sign Recognition with Phonology,Jesse Thomason,jessetho@usc.edu,85%
https://arxiv.org/pdf/2302.05759.pdf,Improving Sign Recognition with Phonology,Zed Sevcikova Sehyr,,0%
https://arxiv.org/pdf/2302.05757.pdf,Multispectral Contrastive Learning with Viewmaker Networks,Alex Tamkin,atamkin@stanford.edu,82%
https://arxiv.org/pdf/2302.05757.pdf,Multispectral Contrastive Learning with Viewmaker Networks,Noah Goodman,ngoodman@stanford.edu,82%
https://arxiv.org/pdf/2302.05757.pdf,Multispectral Contrastive Learning with Viewmaker Networks,Jasmine Bayrooti,jbayrooti@stanford.edu,82%
https://arxiv.org/pdf/2302.05753.pdf,DaliID: Distortion-Adaptive Learned Invariance for Identification Models,Wes Robbins,wrobbins@uccs.edu,82%
https://arxiv.org/pdf/2302.05753.pdf,DaliID: Distortion-Adaptive Learned Invariance for Identification Models,Terrance E. Boult,tboult@vast.uccs.edu,82%
https://arxiv.org/pdf/2302.05753.pdf,DaliID: Distortion-Adaptive Learned Invariance for Identification Models,Gabriel Bertocco,gabriel.bertocco@ic.unicamp.br,95%
https://arxiv.org/pdf/2302.05746.pdf,Removing Image Artifacts From Scratched Lens Protectors,Bihan Wen,bihan.wen@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.05746.pdf,Removing Image Artifacts From Scratched Lens Protectors,Renjie Wan,renjiewan@comp.hkbu.edu.hk,95%
https://arxiv.org/pdf/2302.05746.pdf,Removing Image Artifacts From Scratched Lens Protectors,Alex C. Kot,eackot@ntu.edu.sg,78%
https://arxiv.org/pdf/2302.05746.pdf,Removing Image Artifacts From Scratched Lens Protectors,Lap-pui Chau,lap-pui.chau@polyu.edu.hk,95%
https://arxiv.org/pdf/2302.05746.pdf,Removing Image Artifacts From Scratched Lens Protectors,Wenhan Yang,yangwh@pcl.ac.cn,78%
https://arxiv.org/pdf/2302.05746.pdf,Removing Image Artifacts From Scratched Lens Protectors,Yufei Wang,yufei001@ntu.edu.sg,85%
https://arxiv.org/pdf/2302.05744.pdf,Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing,Zitong Yu,,0%
https://arxiv.org/pdf/2302.05744.pdf,Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing,Rizhao Cai,,0%
https://arxiv.org/pdf/2302.05744.pdf,Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing,Yawen Cui,,0%
https://arxiv.org/pdf/2302.05744.pdf,Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing,Xin Liu,,0%
https://arxiv.org/pdf/2302.05744.pdf,Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing,Yongjian Hu,,0%
https://arxiv.org/pdf/2302.05744.pdf,Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing,Alex Kot,,0%
https://arxiv.org/pdf/2302.05727.pdf,Flexible-modal Deception Detection with Audio-Visual Adapter,Zhaoxu Li,,0%
https://arxiv.org/pdf/2302.05727.pdf,Flexible-modal Deception Detection with Audio-Visual Adapter,Zitong Yu,,0%
https://arxiv.org/pdf/2302.05727.pdf,Flexible-modal Deception Detection with Audio-Visual Adapter,Nithish Muthuchamy Selvaraj,,0%
https://arxiv.org/pdf/2302.05727.pdf,Flexible-modal Deception Detection with Audio-Visual Adapter,Xiaobao Guo,,0%
https://arxiv.org/pdf/2302.05727.pdf,Flexible-modal Deception Detection with Audio-Visual Adapter,Bingquan Shen,,0%
https://arxiv.org/pdf/2302.05727.pdf,Flexible-modal Deception Detection with Audio-Visual Adapter,Adams Wai-kin Kong,,0%
https://arxiv.org/pdf/2302.05727.pdf,Flexible-modal Deception Detection with Audio-Visual Adapter,Alex Kot,,0%
https://arxiv.org/pdf/2302.05673.pdf,ConMAE: Contour Guided MAE for Unsupervised Vehicle Re-Identification,Jianwu Fang,fangjianwu@chd.edu.cn,95%
https://arxiv.org/pdf/2302.05673.pdf,ConMAE: Contour Guided MAE for Unsupervised Vehicle Re-Identification,Jing Yang,,0%
https://arxiv.org/pdf/2302.05673.pdf,ConMAE: Contour Guided MAE for Unsupervised Vehicle Re-Identification,Hongke Xu,,0%
https://arxiv.org/pdf/2302.05666.pdf,Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels,Zifu Wang,zifu.wang@kuleuven.be,95%
https://arxiv.org/pdf/2302.05666.pdf,Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels,Xuefei Ning,,0%
https://arxiv.org/pdf/2302.05666.pdf,Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels,Matthew B. Blaschko,,0%
https://arxiv.org/pdf/2302.05637.pdf,Dual Relation Knowledge Distillation for Object Detection,Gang Zhang,zhanggang03@baidu.com,95%
https://arxiv.org/pdf/2302.05637.pdf,Dual Relation Knowledge Distillation for Object Detection,Zhenliang Ni,nizhenliang@outlook.com,95%
https://arxiv.org/pdf/2302.05637.pdf,Dual Relation Knowledge Distillation for Object Detection,Fukui Yang,yangfukui@baidu.com,95%
https://arxiv.org/pdf/2302.05637.pdf,Dual Relation Knowledge Distillation for Object Detection,Shengzhao Wen,wenshengzhao@baidu.com,95%
https://arxiv.org/pdf/2302.05632.pdf,Operation-level Progressive Differentiable Architecture Search,Xunyu Zhu,,0%
https://arxiv.org/pdf/2302.05632.pdf,Operation-level Progressive Differentiable Architecture Search,Jian Li,,0%
https://arxiv.org/pdf/2302.05632.pdf,Operation-level Progressive Differentiable Architecture Search,Yong Liu,,0%
https://arxiv.org/pdf/2302.05632.pdf,Operation-level Progressive Differentiable Architecture Search,Weiping Wang,,0%
https://arxiv.org/pdf/2302.05629.pdf,Improving Differentiable Architecture Search via Self-Distillation,Yong Liu,liuyonggsai@ruc.edu.cn,95%
https://arxiv.org/pdf/2302.05629.pdf,Improving Differentiable Architecture Search via Self-Distillation,Jian Li,lijian9026@iie.ac.cn,95%
https://arxiv.org/pdf/2302.05629.pdf,Improving Differentiable Architecture Search via Self-Distillation,Weiping Wang,wangweiping@iie.ac.cn,95%
https://arxiv.org/pdf/2302.05629.pdf,Improving Differentiable Architecture Search via Self-Distillation,Xunyu Zhu,zhuxunyu@iie.ac.cn,95%
https://arxiv.org/pdf/2302.05624.pdf,A novel approach to generate datasets with XAI ground truth to evaluate image models,Antoni Jaume-i-capó,antoni.jaume@uib.es,85%
https://arxiv.org/pdf/2302.05624.pdf,A novel approach to generate datasets with XAI ground truth to evaluate image models,Gabriel Moyà-alcover,gabriel.moya@uib.es,85%
https://arxiv.org/pdf/2302.05624.pdf,A novel approach to generate datasets with XAI ground truth to evaluate image models,Miquel Miró-nicolau,miquel.miro@uib.es,85%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Xingchen Cui,cuixingchen@inspur.com,95%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Yingjie Zhang,zhangyj-s@inspur.com,78%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Wenqi Xu,wenqixu99@gmail.com,95%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Dongchao Wen,wendongchao@inspur.com,95%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Weihong Deng,dengweihong2020@foxmail.com,95%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Yichen Lu,yichen.iu@outlook.com,85%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Xu Ling,lingxu421@gmail.com,95%
https://arxiv.org/pdf/2302.05621.pdf,Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline,Hongzhi Shi,shihzh@inspur.com,78%
https://arxiv.org/pdf/2302.10276.pdf,See Your Heart: Psychological states Interpretation through Visual Creations,Likun Yang,,0%
https://arxiv.org/pdf/2302.10276.pdf,See Your Heart: Psychological states Interpretation through Visual Creations,Xiaokun Feng,,0%
https://arxiv.org/pdf/2302.10276.pdf,See Your Heart: Psychological states Interpretation through Visual Creations,Xiaotang Chen,,0%
https://arxiv.org/pdf/2302.10276.pdf,See Your Heart: Psychological states Interpretation through Visual Creations,Shiyu Zhang,,0%
https://arxiv.org/pdf/2302.10276.pdf,See Your Heart: Psychological states Interpretation through Visual Creations,Kaiqi Huang,,0%
https://arxiv.org/pdf/2302.05615.pdf,Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis,Yankai Jiang,,0%
https://arxiv.org/pdf/2302.05615.pdf,Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis,Mingze Sun,,0%
https://arxiv.org/pdf/2302.05615.pdf,Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis,Heng Guo,,0%
https://arxiv.org/pdf/2302.05615.pdf,Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis,Xiaoyu Bai,,0%
https://arxiv.org/pdf/2302.05615.pdf,Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis,Ke Yan,,0%
https://arxiv.org/pdf/2302.05615.pdf,Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis,Le Lu,,0%
https://arxiv.org/pdf/2302.05615.pdf,Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis,Minfeng Xu,,0%
https://arxiv.org/pdf/2302.05608.pdf,Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis,Sathya N. Ravi,sathya@uic.edu,85%
https://arxiv.org/pdf/2302.05608.pdf,Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis,Zhu Wang,zwang260@uic.edu,82%
https://arxiv.org/pdf/2302.05608.pdf,Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis,Sourav Medya,medya@uic.edu,78%
https://arxiv.org/pdf/2302.05598.pdf,Multi-class Brain Tumor Segmentation using Graph Attention Network,Thangarajah Akilan,takilan@lakeheadu.ca,82%
https://arxiv.org/pdf/2302.05598.pdf,Multi-class Brain Tumor Segmentation using Graph Attention Network,Dhrumil Patel,pateld72@lakeheadu.ca,78%
https://arxiv.org/pdf/2302.05598.pdf,Multi-class Brain Tumor Segmentation using Graph Attention Network,Rudra Saxena,rsaxena1@lakeheadu.ca,82%
https://arxiv.org/pdf/2302.05598.pdf,Multi-class Brain Tumor Segmentation using Graph Attention Network,Dhruv Patel,,0%
https://arxiv.org/pdf/2302.05576.pdf,Sketch Less Face Image Retrieval: A New Challenge,Liang Wang,wanggy@cqupt.edu.cn,78%
https://arxiv.org/pdf/2302.05576.pdf,Sketch Less Face Image Retrieval: A New Challenge,Dawei Dai,dw_dai@163.com,82%
https://arxiv.org/pdf/2302.05576.pdf,Sketch Less Face Image Retrieval: A New Challenge,Yutang Li,,0%
https://arxiv.org/pdf/2302.05576.pdf,Sketch Less Face Image Retrieval: A New Challenge,Shiyu Fu,,0%
https://arxiv.org/pdf/2302.05576.pdf,Sketch Less Face Image Retrieval: A New Challenge,Shuyin Xia,,0%
https://arxiv.org/pdf/2302.05576.pdf,Sketch Less Face Image Retrieval: A New Challenge,Guoyin Wang,,0%
https://arxiv.org/pdf/2302.05573.pdf,3D Colored Shape Reconstruction from a Single RGB Image through Diffusion,Bin Liu,nyliubin@nchu.edu.cn,95%
https://arxiv.org/pdf/2302.05573.pdf,3D Colored Shape Reconstruction from a Single RGB Image through Diffusion,Bo Li,libonchu@outlook.com,95%
https://arxiv.org/pdf/2302.05573.pdf,3D Colored Shape Reconstruction from a Single RGB Image through Diffusion,Xiaolin Wei,,0%
https://arxiv.org/pdf/2302.05573.pdf,3D Colored Shape Reconstruction from a Single RGB Image through Diffusion,Fengwei Chen,,0%
https://arxiv.org/pdf/2302.05543.pdf,Adding Conditional Control to Text-to-Image Diffusion Models,Anyi Rao,anyirao@cs.stanford.edu,95%
https://arxiv.org/pdf/2302.05543.pdf,Adding Conditional Control to Text-to-Image Diffusion Models,Lvmin Zhang,lvmin@cs.stanford.edu,85%
https://arxiv.org/pdf/2302.05543.pdf,Adding Conditional Control to Text-to-Image Diffusion Models,Maneesh Agrawala,maneesh@cs.stanford.edu,85%
https://arxiv.org/pdf/2302.05541.pdf,Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data,Lan Fu,lanf@email.sc.edu,85%
https://arxiv.org/pdf/2302.05541.pdf,Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data,Zhiyuan Liu,zhiy@cs.unc.edu,90%
https://arxiv.org/pdf/2302.05541.pdf,Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data,Hongkai Yu,h.yu19@csuohio.edu,82%
https://arxiv.org/pdf/2302.05541.pdf,Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data,Jinlong Li,lijinlong1117@chd.edu.cn,95%
https://arxiv.org/pdf/2302.05541.pdf,Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data,Jeff Simmons,jeff.simmons.3@us.af.mil,95%
https://arxiv.org/pdf/2302.05541.pdf,Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data,Song Wang,songwang@cec.sc.edu,95%
https://arxiv.org/pdf/2302.05499.pdf,CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition,Se-young Yun,yunseyoung@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.05499.pdf,CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition,Sumyeong Ahn,sumyeongahn@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.05499.pdf,CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition,Jongwoo Ko,jongwoo.ko@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.05496.pdf,MaskSketch: Unpaired Structure-guided Masked Image Generation,Dina Bashkirova,,0%
https://arxiv.org/pdf/2302.05496.pdf,MaskSketch: Unpaired Structure-guided Masked Image Generation,Jose Lezama,,0%
https://arxiv.org/pdf/2302.05496.pdf,MaskSketch: Unpaired Structure-guided Masked Image Generation,Kihyuk Sohn,,0%
https://arxiv.org/pdf/2302.05496.pdf,MaskSketch: Unpaired Structure-guided Masked Image Generation,Kate Saenko,,0%
https://arxiv.org/pdf/2302.05496.pdf,MaskSketch: Unpaired Structure-guided Masked Image Generation,Irfan Essa,,0%
https://arxiv.org/pdf/2302.05488.pdf,Element-Wise Attention Layers: an option for optimization,Giovanni Araujo Bacochina,,0%
https://arxiv.org/pdf/2302.05488.pdf,Element-Wise Attention Layers: an option for optimization,Rodrigo Clemente Thom De Souza,,0%
https://arxiv.org/pdf/2302.05486.pdf,RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&3D Pairs,Xun Cao,caoxun@nju.edu.cn,95%
https://arxiv.org/pdf/2302.05486.pdf,RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&3D Pairs,Longwei Guo,guolongwei@smail.nju.edu.cn,95%
https://arxiv.org/pdf/2302.05486.pdf,RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&3D Pairs,Yuanxun Lu,luyuanxun@smail.nju.edu.cn,95%
https://arxiv.org/pdf/2302.05486.pdf,RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&3D Pairs,Hao Zhu,zhuhaoese@nju.edu.cn,95%
https://arxiv.org/pdf/2302.05486.pdf,RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&3D Pairs,Menghua Wu,menghuawu@smail.nju.edu.cn,95%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Mostafa Dehghani,dehghani@google.com,78%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Josip Djolonga,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Basil Mustafa,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Piotr Padlewski,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Jonathan Heek,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Justin Gilmer,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Andreas Steiner,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Mathilde Caron,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Robert Geirhos,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Ibrahim Alabdulmohsin,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Rodolphe Jenatton,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Lucas Beyer,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Michael Tschannen,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Anurag Arnab,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Xiao Wang,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Carlos Riquelme,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Matthias Minderer,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Joan Puigcerver,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Utku Evci,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Manoj Kumar,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Sjoerd Van Steenkiste,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Gamaleldin F. Elsayed,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Aravindh Mahendran,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Fisher Yu,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Avital Oliver,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Fantine Huot,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Jasmijn Bastings,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Mark Patrick Collier,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Alexey Gritsenko,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Vighnesh Birodkar,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Cristina Vasconcelos,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Yi Tay,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Thomas Mensink,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Alexander Kolesnikov,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Filip Pavetić,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Dustin Tran,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Thomas Kipf,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Mario Lučić,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Xiaohua Zhai,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Daniel Keysers,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Jeremiah Harmsen,,0%
https://arxiv.org/pdf/2302.05442.pdf,Scaling Vision Transformers to 22 Billion Parameters,Neil Houlsby,,0%
https://arxiv.org/pdf/2302.05438.pdf,Deep Learning on Implicit Neural Representations of Shapes,Riccardo Spezialetti,riccardo.spezialetti@unibo.it,95%
https://arxiv.org/pdf/2302.05438.pdf,Deep Learning on Implicit Neural Representations of Shapes,Adriano Cardace,adriano.cardace2@unibo.it@unibo.it,95%
https://arxiv.org/pdf/2302.05438.pdf,Deep Learning on Implicit Neural Representations of Shapes,Luca De Luigi,luca.deluigi4@unibo.it,95%
https://arxiv.org/pdf/2302.05438.pdf,Deep Learning on Implicit Neural Representations of Shapes,Pierluigi Zama Ramirez,,0%
https://arxiv.org/pdf/2302.05438.pdf,Deep Learning on Implicit Neural Representations of Shapes,Samuele Salti,,0%
https://arxiv.org/pdf/2302.05438.pdf,Deep Learning on Implicit Neural Representations of Shapes,Luigi Di Stefano,,0%
https://arxiv.org/pdf/2302.05435.pdf,A deep convolutional neural network for salt-and-pepper noise removal using selective convolutional blocks,Ahmad Ali Rafiee,aa.rafiee@shirazu.ac.ir,82%
https://arxiv.org/pdf/2302.05435.pdf,A deep convolutional neural network for salt-and-pepper noise removal using selective convolutional blocks,Mahmoud Farhang,mfarhang@shirazu.ac.ir,82%
https://arxiv.org/pdf/2302.05432.pdf,Tackling Bias in the Dice Similarity Coefficient: Introducing nDSC for White Matter Lesion Segmentation,Vatsal Raina,,0%
https://arxiv.org/pdf/2302.05432.pdf,Tackling Bias in the Dice Similarity Coefficient: Introducing nDSC for White Matter Lesion Segmentation,Nataliia Molchanova,,0%
https://arxiv.org/pdf/2302.05432.pdf,Tackling Bias in the Dice Similarity Coefficient: Introducing nDSC for White Matter Lesion Segmentation,Mara Graziani,,0%
https://arxiv.org/pdf/2302.05432.pdf,Tackling Bias in the Dice Similarity Coefficient: Introducing nDSC for White Matter Lesion Segmentation,Andrey Malinin,,0%
https://arxiv.org/pdf/2302.05432.pdf,Tackling Bias in the Dice Similarity Coefficient: Introducing nDSC for White Matter Lesion Segmentation,Henning Muller,,0%
https://arxiv.org/pdf/2302.05432.pdf,Tackling Bias in the Dice Similarity Coefficient: Introducing nDSC for White Matter Lesion Segmentation,Meritxell Bach Cuadra,,0%
https://arxiv.org/pdf/2302.05432.pdf,Tackling Bias in the Dice Similarity Coefficient: Introducing nDSC for White Matter Lesion Segmentation,Mark Gales,,0%
https://arxiv.org/pdf/2302.05382.pdf,A function space perspective on stochastic shape evolution,Elizabeth Baker,,0%
https://arxiv.org/pdf/2302.05382.pdf,A function space perspective on stochastic shape evolution,Thomas Besnier,,0%
https://arxiv.org/pdf/2302.05382.pdf,A function space perspective on stochastic shape evolution,Stefan Sommer,,0%
https://arxiv.org/pdf/2302.05379.pdf,Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation,Raffaello Camoriano,raffaello.camoriano@polito.it,95%
https://arxiv.org/pdf/2302.05379.pdf,Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation,Andrea Maracani,name.surname@iit.it,70%
https://arxiv.org/pdf/2302.05379.pdf,Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation,Lorenzo Rosasco,lrosasco@mit.edu,82%
https://arxiv.org/pdf/2302.05379.pdf,Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation,Elisa Maiettini,,0%
https://arxiv.org/pdf/2302.05379.pdf,Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation,Davide Talon,,0%
https://arxiv.org/pdf/2302.05379.pdf,Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation,Lorenzo Natale,,0%
https://arxiv.org/pdf/2302.05374.pdf,LCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance,Muhammad Asif Khan,mkhan@qu.edu.qa,82%
https://arxiv.org/pdf/2302.05374.pdf,LCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance,Ridha Hamila,hamila@qu.edu.qa,78%
https://arxiv.org/pdf/2302.05374.pdf,LCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance,Hamid Menouar,hamidm@qmic.com,85%
https://arxiv.org/pdf/2302.05361.pdf,Leveraging Inpainting for Single-Image Shadow Removal,Xiaoguang Li,,0%
https://arxiv.org/pdf/2302.05361.pdf,Leveraging Inpainting for Single-Image Shadow Removal,Qing Guo,,0%
https://arxiv.org/pdf/2302.05361.pdf,Leveraging Inpainting for Single-Image Shadow Removal,Rabab Abdelfattah,,0%
https://arxiv.org/pdf/2302.05361.pdf,Leveraging Inpainting for Single-Image Shadow Removal,Di Lin,,0%
https://arxiv.org/pdf/2302.05361.pdf,Leveraging Inpainting for Single-Image Shadow Removal,Wei Feng,,0%
https://arxiv.org/pdf/2302.05361.pdf,Leveraging Inpainting for Single-Image Shadow Removal,Ivor Tsang,,0%
https://arxiv.org/pdf/2302.05361.pdf,Leveraging Inpainting for Single-Image Shadow Removal,Song Wang,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Ilayda Yaman,ilayda.yaman@eit.lth.se,95%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Guoda Tian,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Martin Larsson,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Patrik Persson,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Michiel Sandra,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Alexander Dürr,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Erik Tegler,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Nikhil Challa,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Henrik Garde,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Fredrik Tufvesson,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Kalle Åström,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Ove Edfors,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Steffen Malkowsky,,0%
https://arxiv.org/pdf/2302.05309.pdf,"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization",Liang Liu,,0%
https://arxiv.org/pdf/2302.05262.pdf,Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection,Elke Schlager,,0%
https://arxiv.org/pdf/2302.05262.pdf,Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection,Andreas Windisch,,0%
https://arxiv.org/pdf/2302.05262.pdf,Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection,Lukas Hanna,,0%
https://arxiv.org/pdf/2302.05262.pdf,Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection,Thomas Klünsner,,0%
https://arxiv.org/pdf/2302.05262.pdf,Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection,Elias Jan Hagendorfer,,0%
https://arxiv.org/pdf/2302.05262.pdf,Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection,Tamara Teppernegg,,0%
https://arxiv.org/pdf/2302.06414.pdf,LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for Real Time Semantic Grid Prediction,Manuel Alejandro Diaz-zapata,manuel.diaz-zapata@inria.fr,95%
https://arxiv.org/pdf/2302.06414.pdf,LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for Real Time Semantic Grid Prediction,David Sierra González,,0%
https://arxiv.org/pdf/2302.06414.pdf,LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for Real Time Semantic Grid Prediction,Özgür Erkent,,0%
https://arxiv.org/pdf/2302.06414.pdf,LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for Real Time Semantic Grid Prediction,Jilles Dibangoye,,0%
https://arxiv.org/pdf/2302.06414.pdf,LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for Real Time Semantic Grid Prediction,Christian Laugier,,0%
https://arxiv.org/pdf/2302.05213.pdf,CEN-HDR: Computationally Efficient neural Network for real-time High Dynamic Range imaging,Dominique Ginhac,dginhac@u-bourgogne.fr,82%
https://arxiv.org/pdf/2302.05213.pdf,CEN-HDR: Computationally Efficient neural Network for real-time High Dynamic Range imaging,Steven Tel,steven.tel@u-bourgogne.fr,95%
https://arxiv.org/pdf/2302.05213.pdf,CEN-HDR: Computationally Efficient neural Network for real-time High Dynamic Range imaging,Barthélémy Heyrman,barthelemy.heyrman@u-bourgogne.fr,95%
https://arxiv.org/pdf/2302.05211.pdf,CGA-PoseNet: Camera Pose Regression via a 1D-Up Approach to Conformal Geometric Algebra,Alberto Pepe,,0%
https://arxiv.org/pdf/2302.05211.pdf,CGA-PoseNet: Camera Pose Regression via a 1D-Up Approach to Conformal Geometric Algebra,Joan Lasenby,,0%
https://arxiv.org/pdf/2302.05201.pdf,PointWavelet: Learning in Spectral Domain for 3D Point Cloud Analysis,Baosheng Yu,"baosheng.yu;
dacheng.tao@sydney.edu.au",95%
https://arxiv.org/pdf/2302.05201.pdf,PointWavelet: Learning in Spectral Domain for 3D Point Cloud Analysis,Cheng Wen,cwen6671; jlon7198@uni.sydney.edu.au,82%
https://arxiv.org/pdf/2302.05201.pdf,PointWavelet: Learning in Spectral Domain for 3D Point Cloud Analysis,Dacheng Tao,dacheng.tao@sydney.edu.au,95%
https://arxiv.org/pdf/2302.05201.pdf,PointWavelet: Learning in Spectral Domain for 3D Point Cloud Analysis,Jianzhi Long,jlon7198@uni.sydney.edu.au,65%
https://arxiv.org/pdf/2302.05200.pdf,End-to-end Semantic Object Detection with Cross-Modal Alignment,Ivanovitch Silva,ivanovitch.silva@ufrn.br,95%
https://arxiv.org/pdf/2302.05200.pdf,End-to-end Semantic Object Detection with Cross-Modal Alignment,Allan Martins,allan@dee.ufrn.br,85%
https://arxiv.org/pdf/2302.05200.pdf,End-to-end Semantic Object Detection with Cross-Modal Alignment,Silvan Ferreira,silvan.junior.051@ufrn.edu.br,85%
https://arxiv.org/pdf/2302.05195.pdf,Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime,Thomas Stegmüller,thomas.stegmuller@epfl.ch,95%
https://arxiv.org/pdf/2302.05195.pdf,Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime,Christian Abbet,,0%
https://arxiv.org/pdf/2302.05195.pdf,Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime,Behzad Bozorgtabar,,0%
https://arxiv.org/pdf/2302.05195.pdf,Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime,Holly Clarke,,0%
https://arxiv.org/pdf/2302.05195.pdf,Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime,Patrick Petignat,,0%
https://arxiv.org/pdf/2302.05195.pdf,Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime,Pierre Vassilakos,,0%
https://arxiv.org/pdf/2302.05195.pdf,Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime,Jean-philippe Thiran,,0%
https://arxiv.org/pdf/2302.05192.pdf,Virtually increasing the measurement frequency of LIDAR sensor utilizing a single RGB camera,Tamas Sziranyi,tamas.sziranyi@sztaki.hu,95%
https://arxiv.org/pdf/2302.05192.pdf,Virtually increasing the measurement frequency of LIDAR sensor utilizing a single RGB camera,Zoltan Rozsa,zoltan.rozsa@sztaki.hu,95%
https://arxiv.org/pdf/2302.10305.pdf,Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance,Nojun Kwak,nojunk@snu.ac.kr,85%
https://arxiv.org/pdf/2302.10305.pdf,Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance,Chaerin Kong,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Deyun Zhang,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Shijia Geng,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Yang Zhou,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Weilun Xu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Guodong Wei,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Kai Wang,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Jie Yu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Qiang Zhu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Yongkui Li,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Yonghong Zhao,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Xingyue Chen,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Rui Zhang,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Zhaoji Fu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Rongbo Zhou,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Yanqi E,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Sumei Fan,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Qinghao Zhao,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Chuandong Cheng,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Nan Peng,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Liang Zhang,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Linlin Zheng,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Jianjun Chu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Hongbin Xu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Chen Tan,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Jian Liu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Huayue Tao,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Tong Liu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Kangyin Chen,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Chenyang Jiang,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Xingpeng Liu,,0%
https://arxiv.org/pdf/2302.10301.pdf,Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images,Shenda Hong,,0%
https://arxiv.org/pdf/2302.05160.pdf,Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection,Junqing Yu,yjqing@hust.edu.cn,75%
https://arxiv.org/pdf/2302.05160.pdf,Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection,Wei Yang,weiyangcs@hust.edu.cn,95%
https://arxiv.org/pdf/2302.05160.pdf,Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection,Hang Zhou,,0%
https://arxiv.org/pdf/2302.05155.pdf,TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation,Hyesu Lim,,0%
https://arxiv.org/pdf/2302.05155.pdf,TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation,Byeonggeun Kim,,0%
https://arxiv.org/pdf/2302.05155.pdf,TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation,Jaegul Choo,,0%
https://arxiv.org/pdf/2302.05155.pdf,TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation,Sungha Choi,,0%
https://arxiv.org/pdf/2302.05154.pdf,Industrial and Medical Anomaly Detection Through Cycle-Consistent Adversarial Networks,Arnaud Bougaham,arnaud.bougaham@unamur.be,95%
https://arxiv.org/pdf/2302.05154.pdf,Industrial and Medical Anomaly Detection Through Cycle-Consistent Adversarial Networks,Valentin Delchevalerie,,0%
https://arxiv.org/pdf/2302.05154.pdf,Industrial and Medical Anomaly Detection Through Cycle-Consistent Adversarial Networks,Mohammed El Adoui,,0%
https://arxiv.org/pdf/2302.05154.pdf,Industrial and Medical Anomaly Detection Through Cycle-Consistent Adversarial Networks,Benoît Frénay,,0%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Ruogu Fang,ruogu.fang@ufl.edu,95%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Skylar E. Stolte,,0%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Kyle Volle,,0%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Aprinda Indahlastari,,0%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Alejandro Albizu,,0%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Adam J. Woods,,0%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Kevin Brink,,0%
https://arxiv.org/pdf/2302.05142.pdf,DOMINO: Domain-aware Loss for Deep Learning Calibration,Matthew Hale,,0%
https://arxiv.org/pdf/2302.05132.pdf,GCNet: Probing Self-Similarity Learning for Generalized Counting Network,Minglun Gong,minglun@uoguelph.ca,85%
https://arxiv.org/pdf/2302.05132.pdf,GCNet: Probing Self-Similarity Learning for Generalized Counting Network,Jun Zhou,jun90@dlmu.edu.cn,85%
https://arxiv.org/pdf/2302.05132.pdf,GCNet: Probing Self-Similarity Learning for Generalized Counting Network,Mingjie Wang,mingjiew@zstu.edu.cn,85%
https://arxiv.org/pdf/2302.05132.pdf,GCNet: Probing Self-Similarity Learning for Generalized Counting Network,Graham W. Taylor,gwtaylor@uoguelph.ca,82%
https://arxiv.org/pdf/2302.05132.pdf,GCNet: Probing Self-Similarity Learning for Generalized Counting Network,Yande Li,yande@lzu.edu.cn,85%
https://arxiv.org/pdf/2302.05119.pdf,Fast Learnings of Coupled Nonnegative Tensor Decomposition Using Optimal Gradient and Low-rank Approximation,Xiulin Wang,xiulin.wang@foxmail.com,95%
https://arxiv.org/pdf/2302.05119.pdf,Fast Learnings of Coupled Nonnegative Tensor Decomposition Using Optimal Gradient and Low-rank Approximation,Fengyu Cong,cong@dlut.edu.cn,78%
https://arxiv.org/pdf/2302.05119.pdf,Fast Learnings of Coupled Nonnegative Tensor Decomposition Using Optimal Gradient and Low-rank Approximation,Jing Liu,liujing@dmu.edu.cn,95%
https://arxiv.org/pdf/2302.05116.pdf,Example-Based Sampling with Diffusion Models,Bastien Doignies,,0%
https://arxiv.org/pdf/2302.05116.pdf,Example-Based Sampling with Diffusion Models,Nicolas Bonneel,,0%
https://arxiv.org/pdf/2302.05116.pdf,Example-Based Sampling with Diffusion Models,David Coeurjolly,,0%
https://arxiv.org/pdf/2302.05116.pdf,Example-Based Sampling with Diffusion Models,Julie Digne,,0%
https://arxiv.org/pdf/2302.05116.pdf,Example-Based Sampling with Diffusion Models,Loïs Paulin,,0%
https://arxiv.org/pdf/2302.05116.pdf,Example-Based Sampling with Diffusion Models,Jean-claude Iehl,,0%
https://arxiv.org/pdf/2302.05116.pdf,Example-Based Sampling with Diffusion Models,Victor Ostromoukhov,,0%
https://arxiv.org/pdf/2302.05114.pdf,Exploiting Neighborhood Structural Features for Change Detection,Mengmeng Wang,,0%
https://arxiv.org/pdf/2302.05114.pdf,Exploiting Neighborhood Structural Features for Change Detection,Zhiqiang Han,,0%
https://arxiv.org/pdf/2302.05114.pdf,Exploiting Neighborhood Structural Features for Change Detection,Peizhen Yang,,0%
https://arxiv.org/pdf/2302.05114.pdf,Exploiting Neighborhood Structural Features for Change Detection,Bai Zhu,,0%
https://arxiv.org/pdf/2302.05114.pdf,Exploiting Neighborhood Structural Features for Change Detection,Ming Hao,,0%
https://arxiv.org/pdf/2302.05114.pdf,Exploiting Neighborhood Structural Features for Change Detection,Jianwei Fan,,0%
https://arxiv.org/pdf/2302.05114.pdf,Exploiting Neighborhood Structural Features for Change Detection,Yuanxin Ye,,0%
https://arxiv.org/pdf/2302.05109.pdf,Adjacent-Level Feature Cross-Fusion With 3-D CNN for Remote Sensing Image Change Detection,Liang Zhou,gmail@163.com,60%
https://arxiv.org/pdf/2302.05109.pdf,Adjacent-Level Feature Cross-Fusion With 3-D CNN for Remote Sensing Image Change Detection,Jianwei Fan,fanjw@xynu.edu.cn,78%
https://arxiv.org/pdf/2302.05109.pdf,Adjacent-Level Feature Cross-Fusion With 3-D CNN for Remote Sensing Image Change Detection,Yuanxin Ye,yeyuanxin@home.swjtu.edu.cn,95%
https://arxiv.org/pdf/2302.05109.pdf,Adjacent-Level Feature Cross-Fusion With 3-D CNN for Remote Sensing Image Change Detection,Mengmeng Wang,,0%
https://arxiv.org/pdf/2302.05109.pdf,Adjacent-Level Feature Cross-Fusion With 3-D CNN for Remote Sensing Image Change Detection,Guangyang Lei,,0%
https://arxiv.org/pdf/2302.05109.pdf,Adjacent-Level Feature Cross-Fusion With 3-D CNN for Remote Sensing Image Change Detection,Yao Qin,,0%
https://arxiv.org/pdf/2302.05105.pdf,Text recognition on images using pre-trained CNN,Afgani Fajar Rizky,,0%
https://arxiv.org/pdf/2302.05105.pdf,Text recognition on images using pre-trained CNN,Novanto Yudistira,,0%
https://arxiv.org/pdf/2302.05105.pdf,Text recognition on images using pre-trained CNN,Edy Santoso,,0%
https://arxiv.org/pdf/2302.05098.pdf,Confidence-based Reliable Learning under Dual Noises,Peng Cui,xpeng.cui@gmail.com,95%
https://arxiv.org/pdf/2302.05098.pdf,Confidence-based Reliable Learning under Dual Noises,Zhijie Deng,zhijied@sjtu.edu.cn,85%
https://arxiv.org/pdf/2302.05098.pdf,Confidence-based Reliable Learning under Dual Noises,Yang Yue,yueyang22@mails.tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.05098.pdf,Confidence-based Reliable Learning under Dual Noises,Jun Zhu,,0%
https://arxiv.org/pdf/2302.05097.pdf,CCDN: Checkerboard Corner Detection Network for Robust Camera Calibration,Qi Zhang,edisonzhangqi@hust.edu.cn,95%
https://arxiv.org/pdf/2302.05097.pdf,CCDN: Checkerboard Corner Detection Network for Robust Camera Calibration,Ben Chen,benchen@hust.edu.cn,95%
https://arxiv.org/pdf/2302.05097.pdf,CCDN: Checkerboard Corner Detection Network for Robust Camera Calibration,Caihua Xiong,chxiong@hust.edu.cn,82%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Azzedine Boukerche,aboukerc@uOttawa.ca,90%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Yan Wang,yanwang19@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Jun Liu,jun_liu@sutd.edu.sg,95%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Jing Liu,jingliu19@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Yang Liu,yang_liu20@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Liang Song,songl@fudan.edu.cn,78%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Peng Sun,peng.sun568@duke.edu,95%
https://arxiv.org/pdf/2302.05087.pdf,Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models,Dingkang Yang,,0%
https://arxiv.org/pdf/2302.05086.pdf,Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples,Yiwen Guo,guoyiwen89@gmail.com,95%
https://arxiv.org/pdf/2302.05086.pdf,Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples,Hao Chen,chen@ucdavis.edu,78%
https://arxiv.org/pdf/2302.05086.pdf,Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples,Wangmeng Zuo,wmzuo@hit.edu.cn,82%
https://arxiv.org/pdf/2302.05086.pdf,Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples,Qizhang Li,liqizhang95@gmail.com,95%
https://arxiv.org/pdf/2302.05080.pdf,Long-Tailed Partial Label Learning via Dynamic Rebalancing,Ya Zhang,ya zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.05080.pdf,Long-Tailed Partial Label Learning via Dynamic Rebalancing,Yanfeng Wang,wangyanfeng@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.05080.pdf,Long-Tailed Partial Label Learning via Dynamic Rebalancing,Zhihan Zhou,zhihanzhou@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.05080.pdf,Long-Tailed Partial Label Learning via Dynamic Rebalancing,Feng Hong,feng.hong@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.05080.pdf,Long-Tailed Partial Label Learning via Dynamic Rebalancing,Jiangchao Yao,,0%
https://arxiv.org/pdf/2302.05075.pdf,BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization,Houqiang Li,lihq@ustc.edu.cn,78%
https://arxiv.org/pdf/2302.05075.pdf,BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization,Jiaxin Shi,shijiaxin3@huawei.com,95%
https://arxiv.org/pdf/2302.05075.pdf,BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization,Hezhen Hu,alexhu@mail.ustc.edu.cn,78%
https://arxiv.org/pdf/2302.05075.pdf,BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization,Weichao Zhao,,0%
https://arxiv.org/pdf/2302.05075.pdf,BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization,Wengang Zhou,,0%
https://arxiv.org/pdf/2302.05071.pdf,EVC: Towards Real-Time Neural Image Compression with Mask Decay,Yan Lu,yanlu@microsoft.com,95%
https://arxiv.org/pdf/2302.05071.pdf,EVC: Towards Real-Time Neural Image Compression with Mask Decay,Bin Li,libin@microsoft.com,95%
https://arxiv.org/pdf/2302.05071.pdf,EVC: Towards Real-Time Neural Image Compression with Mask Decay,Guo-hua Wang,wangguohua@lamda.nju.edu.cn,95%
https://arxiv.org/pdf/2302.05071.pdf,EVC: Towards Real-Time Neural Image Compression with Mask Decay,Jiahao Li,li.jiahao@microsoft.com,95%
https://arxiv.org/pdf/2302.05043.pdf,A Review of Predictive and Contrastive Self-supervised Learning for Medical Images,Wei-chien Wang,wwan7784@uni.sydney.edu.au,65%
https://arxiv.org/pdf/2302.05043.pdf,A Review of Predictive and Contrastive Self-supervised Learning for Medical Images,Euijoon Ahn,,0%
https://arxiv.org/pdf/2302.05043.pdf,A Review of Predictive and Contrastive Self-supervised Learning for Medical Images,Dagan Feng,,0%
https://arxiv.org/pdf/2302.05043.pdf,A Review of Predictive and Contrastive Self-supervised Learning for Medical Images,Jinman Kim,,0%
https://arxiv.org/pdf/2302.05041.pdf,Data-Driven Stochastic Motion Evaluation and Optimization with Image by Spatially-Aligned Temporal Encoding,Takeru Oba,,0%
https://arxiv.org/pdf/2302.05041.pdf,Data-Driven Stochastic Motion Evaluation and Optimization with Image by Spatially-Aligned Temporal Encoding,Norimichi Ukita,,0%
https://arxiv.org/pdf/2302.10284.pdf,"OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2",Jiannan Zhao,jzhao@gxu.edu.cn,82%
https://arxiv.org/pdf/2302.10284.pdf,"OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2",Feng Shuang,,0%
https://arxiv.org/pdf/2302.10284.pdf,"OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2",Yanpeng Zhu,,0%
https://arxiv.org/pdf/2302.10284.pdf,"OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2",Yupeng Xie,,0%
https://arxiv.org/pdf/2302.10284.pdf,"OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2",Lei Zhao,,0%
https://arxiv.org/pdf/2302.10284.pdf,"OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2",Quansheng Xie,,0%
https://arxiv.org/pdf/2302.10284.pdf,"OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2",Shigang Yue,,0%
https://arxiv.org/pdf/2302.05033.pdf,Short-Term Aggregated Residential Load Forecasting using BiLSTM and CNN-BiLSTM,Raymond I. Fernandez,rifernan@uh.edu,65%
https://arxiv.org/pdf/2302.05033.pdf,Short-Term Aggregated Residential Load Forecasting using BiLSTM and CNN-BiLSTM,Xingpeng Li,xli82@uh.edu,82%
https://arxiv.org/pdf/2302.05033.pdf,Short-Term Aggregated Residential Load Forecasting using BiLSTM and CNN-BiLSTM,Bharat Bohara,bbohara@uh.edu,82%
https://arxiv.org/pdf/2302.05033.pdf,Short-Term Aggregated Residential Load Forecasting using BiLSTM and CNN-BiLSTM,Vysali Gollapudi,,0%
https://arxiv.org/pdf/2302.05027.pdf,Deep Seam Prediction for Image Stitching Based on Selection Consistency Loss,Senmao Cheng,,0%
https://arxiv.org/pdf/2302.05027.pdf,Deep Seam Prediction for Image Stitching Based on Selection Consistency Loss,Fan Yang,,0%
https://arxiv.org/pdf/2302.05027.pdf,Deep Seam Prediction for Image Stitching Based on Selection Consistency Loss,Zhi Chen,,0%
https://arxiv.org/pdf/2302.05027.pdf,Deep Seam Prediction for Image Stitching Based on Selection Consistency Loss,Nanjun Yuan,,0%
https://arxiv.org/pdf/2302.05027.pdf,Deep Seam Prediction for Image Stitching Based on Selection Consistency Loss,Wenbing Tao,,0%
https://arxiv.org/pdf/2302.05018.pdf,Predicting Out-of-Distribution Error with Confidence Optimal Transport,Runtian Zhai,rzhai@cs.cmu.edu,82%
https://arxiv.org/pdf/2302.05018.pdf,Predicting Out-of-Distribution Error with Confidence Optimal Transport,Katia Sycara,sycara@cs.cmu.edu,78%
https://arxiv.org/pdf/2302.05018.pdf,Predicting Out-of-Distribution Error with Confidence Optimal Transport,Zhenlin Wang,zhenlinw@cs.cmu.edu,85%
https://arxiv.org/pdf/2302.05018.pdf,Predicting Out-of-Distribution Error with Confidence Optimal Transport,Soheil Kolouri,soheil.kolouri@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.05018.pdf,Predicting Out-of-Distribution Error with Confidence Optimal Transport,Yuzhe Lu,yuzhelu@cs.cmu.edu,95%
https://arxiv.org/pdf/2302.05018.pdf,Predicting Out-of-Distribution Error with Confidence Optimal Transport,Joseph Campbell,jcampbell@cs.cmu.edu,82%
https://arxiv.org/pdf/2302.05017.pdf,A survey on facial image deblurring,Bingnan Wang,wangbingnan21@mails.ucas.ac.cn,95%
https://arxiv.org/pdf/2302.05017.pdf,A survey on facial image deblurring,Quan Zheng,zhengquan@iscas.ac.cn,95%
https://arxiv.org/pdf/2302.05017.pdf,A survey on facial image deblurring,Fanjiang Xu,fanjiang@iscas.ac.cn,85%
https://arxiv.org/pdf/2302.05016.pdf,Is Multimodal Vision Supervision Beneficial to Language?,Vasudev Lal,vasudev.lal@intel.com,95%
https://arxiv.org/pdf/2302.05016.pdf,Is Multimodal Vision Supervision Beneficial to Language?,Avinash Madasu,avinashmadasu17@gmail.com,95%
https://arxiv.org/pdf/2302.05011.pdf,Context Understanding in Computer Vision: A Survey,Xuan Wang,xwang4@gradcenter.cuny.edu,82%
https://arxiv.org/pdf/2302.05011.pdf,Context Understanding in Computer Vision: A Survey,Zhigang Zhu,,0%
https://arxiv.org/pdf/2302.04977.pdf,Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines,Vitaly Shmatikov,shmat@cs.cornell.edu,90%
https://arxiv.org/pdf/2302.04977.pdf,Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines,Eugene Bagdasaryan,eugene@cs.cornell.edu,85%
https://arxiv.org/pdf/2302.04973.pdf,Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames,Ondrej Biza,biza.o@northeastern.edu,78%
https://arxiv.org/pdf/2302.04973.pdf,Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames,Sjoerd Van Steenkiste,,0%
https://arxiv.org/pdf/2302.04973.pdf,Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames,Mehdi S. M. Sajjadi,,0%
https://arxiv.org/pdf/2302.04973.pdf,Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames,Gamaleldin F. Elsayed,,0%
https://arxiv.org/pdf/2302.04973.pdf,Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames,Aravindh Mahendran,,0%
https://arxiv.org/pdf/2302.04973.pdf,Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames,Thomas Kipf,,0%
https://arxiv.org/pdf/2302.04936.pdf,Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data,Raymond Leung,raymond.leung@sydney.edu.au,95%
https://arxiv.org/pdf/2302.04936.pdf,Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data,Arman Melkumyan,arman.melkumyan@sydney.edu.au,95%
https://arxiv.org/pdf/2302.04936.pdf,Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data,Lloyd Windrim,,0%
https://arxiv.org/pdf/2302.04936.pdf,Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data,Richard J. Murphy,,0%
https://arxiv.org/pdf/2302.04936.pdf,Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data,Anna Chlingaryan,,0%
https://arxiv.org/pdf/2302.04871.pdf,In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing,Yiran Xu,,0%
https://arxiv.org/pdf/2302.04871.pdf,In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing,Zhixin Shu,,0%
https://arxiv.org/pdf/2302.04871.pdf,In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing,Cameron Smith,,0%
https://arxiv.org/pdf/2302.04871.pdf,In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing,Seoung Wug Oh,,0%
https://arxiv.org/pdf/2302.04871.pdf,In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing,Jia-bin Huang,,0%
https://arxiv.org/pdf/2302.04870.pdf,Offsite-Tuning: Transfer Learning without Full Model,Guangxuan Xiao,,0%
https://arxiv.org/pdf/2302.04870.pdf,Offsite-Tuning: Transfer Learning without Full Model,Ji Lin,,0%
https://arxiv.org/pdf/2302.04870.pdf,Offsite-Tuning: Transfer Learning without Full Model,Song Han,,0%
https://arxiv.org/pdf/2302.04869.pdf,Reversible Vision Transformers,Karttikeya Mangalam,,0%
https://arxiv.org/pdf/2302.04869.pdf,Reversible Vision Transformers,Haoqi Fan,,0%
https://arxiv.org/pdf/2302.04869.pdf,Reversible Vision Transformers,Yanghao Li,,0%
https://arxiv.org/pdf/2302.04869.pdf,Reversible Vision Transformers,Chao-yuan Wu,,0%
https://arxiv.org/pdf/2302.04869.pdf,Reversible Vision Transformers,Bo Xiong,,0%
https://arxiv.org/pdf/2302.04869.pdf,Reversible Vision Transformers,Christoph Feichtenhofer,,0%
https://arxiv.org/pdf/2302.04869.pdf,Reversible Vision Transformers,Jitendra Malik,,0%
https://arxiv.org/pdf/2302.04868.pdf,MEGANE: Morphable Eyeglass and Avatar Network,Junxuan Li,,0%
https://arxiv.org/pdf/2302.04868.pdf,MEGANE: Morphable Eyeglass and Avatar Network,Shunsuke Saito,,0%
https://arxiv.org/pdf/2302.04868.pdf,MEGANE: Morphable Eyeglass and Avatar Network,Tomas Simon,,0%
https://arxiv.org/pdf/2302.04868.pdf,MEGANE: Morphable Eyeglass and Avatar Network,Stephen Lombardi,,0%
https://arxiv.org/pdf/2302.04868.pdf,MEGANE: Morphable Eyeglass and Avatar Network,Hongdong Li,,0%
https://arxiv.org/pdf/2302.04868.pdf,MEGANE: Morphable Eyeglass and Avatar Network,Jason Saragih,,0%
https://arxiv.org/pdf/2302.04867.pdf,UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models,Wenliang Zhao,,0%
https://arxiv.org/pdf/2302.04867.pdf,UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models,Lujia Bai,,0%
https://arxiv.org/pdf/2302.04867.pdf,UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models,Yongming Rao,,0%
https://arxiv.org/pdf/2302.04867.pdf,UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models,Jie Zhou,,0%
https://arxiv.org/pdf/2302.04867.pdf,UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models,Jiwen Lu,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Shun Iwase,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Shunsuke Saito,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Tomas Simon,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Stephen Lombardi,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Timur Bagautdinov,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Rohan Joshi,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Fabian Prada,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Takaaki Shiratori,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Yaser Sheikh,,0%
https://arxiv.org/pdf/2302.04866.pdf,RelightableHands: Efficient Neural Relighting of Articulated Hand Models,Jason Saragih,,0%
https://arxiv.org/pdf/2302.04865.pdf,ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion,Cynthia Lu,cynthilu@amazon.com,82%
https://arxiv.org/pdf/2302.04865.pdf,ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion,Ying Shen,ying22@illinois.edu,85%
https://arxiv.org/pdf/2302.04865.pdf,ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion,Daniel Bis,bisdb@amazon.com,78%
https://arxiv.org/pdf/2302.04865.pdf,ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion,Ismini Lourentzou,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Guandao Yang,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Sagie Benaim,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Varun Jampani,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Kyle Genova,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Jonathan T. Barron,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Thomas Funkhouser,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Bharath Hariharan,,0%
https://arxiv.org/pdf/2302.04862.pdf,Polynomial Neural Fields for Subband Decomposition and Manipulation,Serge Belongie,,0%
https://arxiv.org/pdf/2302.04860.pdf,Diverse Human Motion Prediction Guided by Multi-Level Spatial-Temporal Anchors,Liang-yan Gui,lgui@illinois.edu,82%
https://arxiv.org/pdf/2302.04860.pdf,Diverse Human Motion Prediction Guided by Multi-Level Spatial-Temporal Anchors,Sirui Xu,siruixu2@illinois.edu,95%
https://arxiv.org/pdf/2302.04860.pdf,Diverse Human Motion Prediction Guided by Multi-Level Spatial-Temporal Anchors,Yu-xiong Wang,yxw@illinois.edu,90%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Zhuolin Yang,zhuolin5@illinois.edu,85%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Wei Ping,wping@nvidia.com,82%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Zihan Liu,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Vijay Korthikanti,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Weili Nie,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,De-an Huang,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Linxi Fan,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Zhiding Yu,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Shiyi Lan,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Bo Li,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Ming-yu Liu,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Yuke Zhu,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Mohammad Shoeybi,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Bryan Catanzaro,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Chaowei Xiao,,0%
https://arxiv.org/pdf/2302.04858.pdf,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,Anima Anandkumar,,0%
https://arxiv.org/pdf/2302.04856.pdf,One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation,Saurabh Gupta,saurabhg@illinois.edu,85%
https://arxiv.org/pdf/2302.04856.pdf,One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation,Matthew Chang,,0%
https://arxiv.org/pdf/2302.04855.pdf,Trading Information between Latents in Hierarchical Variational Autoencoders,Tim Z. Xiao,zhenzhong.xiao@uni-tuebingen.de,78%
https://arxiv.org/pdf/2302.04855.pdf,Trading Information between Latents in Hierarchical Variational Autoencoders,Robert Bamler,robert.bamler@uni-tuebingen.de,95%
https://arxiv.org/pdf/2302.04850.pdf,Robot Synesthesia: A Sound and Emotion Guided AI Painter,Vihaan Misra,vihaanm@andrew.cmu.edu,85%
https://arxiv.org/pdf/2302.04850.pdf,Robot Synesthesia: A Sound and Emotion Guided AI Painter,Jean Oh,hyaejino@andrew.cmu.edu,85%
https://arxiv.org/pdf/2302.04850.pdf,Robot Synesthesia: A Sound and Emotion Guided AI Painter,Peter Schaldenbrand,pschalde@andrew.cmu.edu,90%
https://arxiv.org/pdf/2302.04841.pdf,Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics,Max Ryabinin,mryabinin0@gmail.com,82%
https://arxiv.org/pdf/2302.04841.pdf,Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics,Anton Voronov,,0%
https://arxiv.org/pdf/2302.04841.pdf,Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics,Mikhail Khoroshikh,,0%
https://arxiv.org/pdf/2302.04841.pdf,Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics,Artem Babenko,,0%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,Viraj Prabhu,rajp@gatech.edu,90%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,James Lucas,jlucas@nvidia.com,82%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,David Acuna,,0%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,Andrew Liao,,0%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,Rafid Mahmood,,0%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,Marc T. Law,,0%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,Judy Hoffman,,0%
https://arxiv.org/pdf/2302.04832.pdf,Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting,Sanja Fidler,,0%
https://arxiv.org/pdf/2302.04824.pdf,Lithium Metal Battery Quality Control via Transformer-CNN Segmentation,Jerome Quenum,jquenum@berkeley.edu,82%
https://arxiv.org/pdf/2302.04824.pdf,Lithium Metal Battery Quality Control via Transformer-CNN Segmentation,Iryna Zenyuk,izenyuk@uci.edu,82%
https://arxiv.org/pdf/2302.04824.pdf,Lithium Metal Battery Quality Control via Transformer-CNN Segmentation,Daniela Ushizima,dushizima@lbl.gov,82%
https://arxiv.org/pdf/2302.04820.pdf,High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model,Stevo Racković,stevo.rackovic@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2302.04820.pdf,High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model,Cláudia Soares,,0%
https://arxiv.org/pdf/2302.04820.pdf,High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model,Dušan Jakovetić,,0%
https://arxiv.org/pdf/2302.04820.pdf,High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model,Zoranka Desnica,,0%
https://arxiv.org/pdf/2302.04815.pdf,To Perceive or Not to Perceive: Lightweight Stacked Hourglass Network,Jameel Hassan Abdul Samadh,jameel.hassan@mbzuai.ac.ae,85%
https://arxiv.org/pdf/2302.04815.pdf,To Perceive or Not to Perceive: Lightweight Stacked Hourglass Network,Salwa K. Al Khatib,salwa.khatib@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.04800.pdf,Drawing Attention to Detail: Pose Alignment through Self-Attention for Fine-Grained Object Classification,Mohamed El Amine Boudjoghra,mohamed.boudjoghra@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.04800.pdf,Drawing Attention to Detail: Pose Alignment through Self-Attention for Fine-Grained Object Classification,Jameel Hassan,jameel.hassan@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.04800.pdf,Drawing Attention to Detail: Pose Alignment through Self-Attention for Fine-Grained Object Classification,Salwa Al Khatib,salwa.khatib@mbzuai.ac.ae,95%
https://arxiv.org/pdf/2302.04774.pdf,3D Human Pose and Shape Estimation via HybrIK-Transformer,Boris N. Oreshkin,boris.oreshkin@gmail.com,95%
https://arxiv.org/pdf/2302.04729.pdf,Constrained Empirical Risk Minimization: Theory and Practice,Eric Marcus,,0%
https://arxiv.org/pdf/2302.04729.pdf,Constrained Empirical Risk Minimization: Theory and Practice,Ray Sheombarsing,,0%
https://arxiv.org/pdf/2302.04729.pdf,Constrained Empirical Risk Minimization: Theory and Practice,Jan-jakob Sonke,,0%
https://arxiv.org/pdf/2302.04729.pdf,Constrained Empirical Risk Minimization: Theory and Practice,Jonas Teuwen,,0%
https://arxiv.org/pdf/2302.04694.pdf,Partial Optimality in Cubic Correlation Clustering,David Stein,david.stein1@tu-dresden.de,95%
https://arxiv.org/pdf/2302.04694.pdf,Partial Optimality in Cubic Correlation Clustering,Silvia Di Gregorio,silvia.di_gregorio@tu-dresden.de,95%
https://arxiv.org/pdf/2302.04694.pdf,Partial Optimality in Cubic Correlation Clustering,Bjoern Andres,bjoern.andres@tu-dresden.de,95%
https://arxiv.org/pdf/2302.04677.pdf,Mixed-order self-paced curriculum learning for universal lesion detection,Han Li,,0%
https://arxiv.org/pdf/2302.04677.pdf,Mixed-order self-paced curriculum learning for universal lesion detection,Hu Han,,0%
https://arxiv.org/pdf/2302.04677.pdf,Mixed-order self-paced curriculum learning for universal lesion detection,S. Kevin Zhou,,0%
https://arxiv.org/pdf/2302.04638.pdf,Better Diffusion Models Further Improve Adversarial Training,Tianyu Pang,tianyupang@sea.com,95%
https://arxiv.org/pdf/2302.04638.pdf,Better Diffusion Models Further Improve Adversarial Training,Weiwei Liu,liuweiwei863@gmail.com,95%
https://arxiv.org/pdf/2302.04638.pdf,Better Diffusion Models Further Improve Adversarial Training,Zekai Wang,,0%
https://arxiv.org/pdf/2302.04638.pdf,Better Diffusion Models Further Improve Adversarial Training,Chao Du,,0%
https://arxiv.org/pdf/2302.04638.pdf,Better Diffusion Models Further Improve Adversarial Training,Min Lin,,0%
https://arxiv.org/pdf/2302.04638.pdf,Better Diffusion Models Further Improve Adversarial Training,Shuicheng Yan,,0%
https://arxiv.org/pdf/2302.04625.pdf,Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms,Pau Climent-perez,pau.climent@ua.es,85%
https://arxiv.org/pdf/2302.04625.pdf,Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms,Kooshan Hashemifard,k.hashemifard@ua.es,82%
https://arxiv.org/pdf/2302.04625.pdf,Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms,Francisco Florez-revuelta,francisco.florez@ua.es,85%
https://arxiv.org/pdf/2302.04607.pdf,Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search,Xuelong Li,li@nwpu.edu.cn,78%
https://arxiv.org/pdf/2302.04607.pdf,Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search,Jiabei Wang,jiabeiwang@tju.edu.cn,95%
https://arxiv.org/pdf/2302.04607.pdf,Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search,Zhuang Shao,Zhuang.Shao@warwick.ac.uk,95%
https://arxiv.org/pdf/2302.04607.pdf,Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search,Hanqing Sun,hqSun@tju.edu.cn,82%
https://arxiv.org/pdf/2302.04607.pdf,Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search,Yanwei Pang,,0%
https://arxiv.org/pdf/2302.04607.pdf,Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search,Jiale Cao,,0%
https://arxiv.org/pdf/2302.04589.pdf,MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection,Jian Liang,liangjian92@gmail.com,95%
https://arxiv.org/pdf/2302.04589.pdf,MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection,Bo Jiang,jiangbo@ahu.edu.cn,95%
https://arxiv.org/pdf/2302.04589.pdf,MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection,Aihua Zheng,ahzheng214@foxmail.com,82%
https://arxiv.org/pdf/2302.04589.pdf,MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection,Ran He,rhe@nlpr.ia.ac.cn,82%
https://arxiv.org/pdf/2302.04589.pdf,MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection,Yuhe Ding,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Soumick Chatterjee,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Hana Haseljić,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Robert Frysch,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Vojtěch Kulvait,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Vladimir Semshchikov,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Bennet Hensen,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Frank Wacker,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Inga Brüschx,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Thomas Werncke,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Oliver Speck,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Andreas Nürnberger,,0%
https://arxiv.org/pdf/2302.04585.pdf,Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique,Georg Rose,,0%
https://arxiv.org/pdf/2302.04584.pdf,Complex Network for Complex Problems: A comparative study of CNN and Complex-valued CNN,Soumick Chatterjee,,0%
https://arxiv.org/pdf/2302.04584.pdf,Complex Network for Complex Problems: A comparative study of CNN and Complex-valued CNN,Pavan Tummala,,0%
https://arxiv.org/pdf/2302.04584.pdf,Complex Network for Complex Problems: A comparative study of CNN and Complex-valued CNN,Oliver Speck,,0%
https://arxiv.org/pdf/2302.04584.pdf,Complex Network for Complex Problems: A comparative study of CNN and Complex-valued CNN,Andreas Nürnberger,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Tao Song,songt333@sjtu.edu.cn,78%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Chumeng Liang,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Xiaoyu Wu,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Yang Hua,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Jiaru Zhang,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Yiming Xue,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Zhengui Xue,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Ruhui Ma,,0%
https://arxiv.org/pdf/2302.04578.pdf,Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples,Haibing Guan,,0%
https://arxiv.org/pdf/2302.04544.pdf,GMConv: Modulating Effective Receptive Fields for Convolutional Kernels,Stephen Lin,stevelin@microsoft.com,82%
https://arxiv.org/pdf/2302.04544.pdf,GMConv: Modulating Effective Receptive Fields for Convolutional Kernels,Jia Ning,ninja@hust.edu.cn,75%
https://arxiv.org/pdf/2302.04544.pdf,GMConv: Modulating Effective Receptive Fields for Convolutional Kernels,Qi Chen,,0%
https://arxiv.org/pdf/2302.04544.pdf,GMConv: Modulating Effective Receptive Fields for Convolutional Kernels,Chao Li,,0%
https://arxiv.org/pdf/2302.04544.pdf,GMConv: Modulating Effective Receptive Fields for Convolutional Kernels,Kun He,,0%
https://arxiv.org/pdf/2302.04542.pdf,Efficient Attention via Control Variates,Lin Zheng,lzheng2@cs.hku.hk,82%
https://arxiv.org/pdf/2302.04542.pdf,Efficient Attention via Control Variates,Jianbo Yuan,jianbo.yuan@bytedance.com,95%
https://arxiv.org/pdf/2302.04542.pdf,Efficient Attention via Control Variates,Chong Wang,mr.chongwang@apple.com,95%
https://arxiv.org/pdf/2302.04542.pdf,Efficient Attention via Control Variates,Lingpeng Kong,,0%
https://arxiv.org/pdf/2302.04527.pdf,Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer,Dichao Liu,liu@outlook.jp,78%
https://arxiv.org/pdf/2302.04527.pdf,Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer,Toshihiko Yamasaki,,0%
https://arxiv.org/pdf/2302.04527.pdf,Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer,Yu Wang,,0%
https://arxiv.org/pdf/2302.04527.pdf,Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer,Kenji Mase,,0%
https://arxiv.org/pdf/2302.04527.pdf,Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer,Jien Kato,,0%
https://arxiv.org/pdf/2302.04521.pdf,IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection,Yuntao Zou,zouyuntao@hust.edu.cn,95%
https://arxiv.org/pdf/2302.04521.pdf,IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection,Xiaoibin Wang,,0%
https://arxiv.org/pdf/2302.04521.pdf,IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection,Shuang Gao,,0%
https://arxiv.org/pdf/2302.04521.pdf,IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection,Jianlan Guo,,0%
https://arxiv.org/pdf/2302.04521.pdf,IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection,Chu Wang,,0%
https://arxiv.org/pdf/2302.04495.pdf,"3D reconstruction from spherical images: A review of techniques, applications, and prospects",Duojie Weng,ceweng@polyu.edu.hk,78%
https://arxiv.org/pdf/2302.04495.pdf,"3D reconstruction from spherical images: A review of techniques, applications, and prospects",Kan You,youkan@cug.edu.cn,95%
https://arxiv.org/pdf/2302.04495.pdf,"3D reconstruction from spherical images: A review of techniques, applications, and prospects",Wu Chen,wu.chen@polyu.edu.hk,95%
https://arxiv.org/pdf/2302.04495.pdf,"3D reconstruction from spherical images: A review of techniques, applications, and prospects",San Jiang,jiangsan@cug.edu.cn,95%
https://arxiv.org/pdf/2302.04495.pdf,"3D reconstruction from spherical images: A review of techniques, applications, and prospects",Yaxin Li,yaxin.pu.li@connect.polyu.hk,95%
https://arxiv.org/pdf/2302.04486.pdf,A General Mobile Manipulator Automation Framework for Flexible Manufacturing in Hostile Industrial Environments,Chuanyu Yang,chuanyu.yang@amigaga.com,95%
https://arxiv.org/pdf/2302.04486.pdf,A General Mobile Manipulator Automation Framework for Flexible Manufacturing in Hostile Industrial Environments,Jinnian Pu,jipu0216@uni.sydney.edu.au,82%
https://arxiv.org/pdf/2302.04486.pdf,A General Mobile Manipulator Automation Framework for Flexible Manufacturing in Hostile Industrial Environments,Can Pu,can.pu@amigaga.com,95%
https://arxiv.org/pdf/2302.04486.pdf,A General Mobile Manipulator Automation Framework for Flexible Manufacturing in Hostile Industrial Environments,Robert B. Fisher,,0%
https://arxiv.org/pdf/2302.10896.pdf,IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness,Xiaoyun Xu,xiaoyun.xu@ru.nl,95%
https://arxiv.org/pdf/2302.10896.pdf,IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness,Stjepan Picek,stjepan.picek@ru.nl,95%
https://arxiv.org/pdf/2302.10896.pdf,IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness,Guilherme Perin,guilhermeperin7@gmail.com,95%
https://arxiv.org/pdf/2302.04476.pdf,Towards Geospatial Foundation Models via Continual Pretraining,Boran Han,boranhan@amazon.com,95%
https://arxiv.org/pdf/2302.04476.pdf,Towards Geospatial Foundation Models via Continual Pretraining,Matias Mendieta,matias.mendieta@ucf.edu,95%
https://arxiv.org/pdf/2302.04476.pdf,Towards Geospatial Foundation Models via Continual Pretraining,Yi Zhu,yi@boson.ai,85%
https://arxiv.org/pdf/2302.04476.pdf,Towards Geospatial Foundation Models via Continual Pretraining,Xingjian Shi,xshiab@connect.ust.hk,82%
https://arxiv.org/pdf/2302.04476.pdf,Towards Geospatial Foundation Models via Continual Pretraining,Chen Chen,chen.chen@crcv.ucf.edu,95%
https://arxiv.org/pdf/2302.04447.pdf,Contour Completion using Deep Structural Priors,Morteza Rezanejad,morteza.rezanejad@utoronto.ca,95%
https://arxiv.org/pdf/2302.04447.pdf,Contour Completion using Deep Structural Priors,Ali Shiraee,,0%
https://arxiv.org/pdf/2302.04447.pdf,Contour Completion using Deep Structural Priors,Mohammad Khodadad,,0%
https://arxiv.org/pdf/2302.04447.pdf,Contour Completion using Deep Structural Priors,Dirk B. Walther,,0%
https://arxiv.org/pdf/2302.04447.pdf,Contour Completion using Deep Structural Priors,Hamidreza Mahyar,,0%
https://arxiv.org/pdf/2302.04440.pdf,Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples,Marco Jiralerspong,marco.jiralerspong@mila.quebec,95%
https://arxiv.org/pdf/2302.04440.pdf,Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples,Avishek Joey Bose,,0%
https://arxiv.org/pdf/2302.04440.pdf,Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples,Ian Gemp,,0%
https://arxiv.org/pdf/2302.04440.pdf,Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples,Chongli Qin,,0%
https://arxiv.org/pdf/2302.04440.pdf,Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples,Yoram Bachrach,,0%
https://arxiv.org/pdf/2302.04440.pdf,Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples,Gauthier Gidel,,0%
https://arxiv.org/pdf/2302.04427.pdf,Zero-Knowledge Zero-Shot Learning for Novel Visual Category Discovery,Hongfu Liu,hongfuliu@brandeis.edu,95%
https://arxiv.org/pdf/2302.04427.pdf,Zero-Knowledge Zero-Shot Learning for Novel Visual Category Discovery,Zhaonan Li,,0%
https://arxiv.org/pdf/2302.04419.pdf,An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning,Sungjin Ahn,sjn.ahn@gmail.com,82%
https://arxiv.org/pdf/2302.04419.pdf,An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning,Jaesik Yoon,,0%
https://arxiv.org/pdf/2302.04419.pdf,An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning,Yi-fu Wu,,0%
https://arxiv.org/pdf/2302.04419.pdf,An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning,Heechul Bae,,0%
https://arxiv.org/pdf/2303.13536.pdf,Help the Blind See: Assistance for the Visually Impaired through Augmented Acoustic Simulation,Ritik Jalisatgi,rjalisatgi943@student.fuhsd.org,82%
https://arxiv.org/pdf/2303.13536.pdf,Help the Blind See: Assistance for the Visually Impaired through Augmented Acoustic Simulation,Alexander Mehta,amehta633@student.fuhsd.org,82%
https://arxiv.org/pdf/2302.04395.pdf,Optimized Hybrid Focal Margin Loss for Crack Segmentation,Jiajie Chen,,0%
https://arxiv.org/pdf/2302.04358.pdf,Mitigating Bias in Visual Transformers via Targeted Alignment,Arvindkumar Krishnakumar,akrishna@gatech.edu,90%
https://arxiv.org/pdf/2302.04358.pdf,Mitigating Bias in Visual Transformers via Targeted Alignment,Sruthi Sudhakar,sruthis@gatech.edu,85%
https://arxiv.org/pdf/2302.04358.pdf,Mitigating Bias in Visual Transformers via Targeted Alignment,Viraj Prabhu,virajp@gatech.edu,85%
https://arxiv.org/pdf/2302.04358.pdf,Mitigating Bias in Visual Transformers via Targeted Alignment,Judy Hoffman,judy@gatech.edu,85%
https://arxiv.org/pdf/2302.04341.pdf,Neonatal Face and Facial Landmark Detection from Video Recordings,Ethan Grooby,ethan.grooby@monash.edu,95%
https://arxiv.org/pdf/2302.04341.pdf,Neonatal Face and Facial Landmark Detection from Video Recordings,Chiranjibi Sitaula,,0%
https://arxiv.org/pdf/2302.04341.pdf,Neonatal Face and Facial Landmark Detection from Video Recordings,Soodeh Ahani,,0%
https://arxiv.org/pdf/2302.04341.pdf,Neonatal Face and Facial Landmark Detection from Video Recordings,Liisa Holsti,,0%
https://arxiv.org/pdf/2302.04341.pdf,Neonatal Face and Facial Landmark Detection from Video Recordings,Atul Malhotra,,0%
https://arxiv.org/pdf/2302.04341.pdf,Neonatal Face and Facial Landmark Detection from Video Recordings,Guy A. Dumont,,0%
https://arxiv.org/pdf/2302.04341.pdf,Neonatal Face and Facial Landmark Detection from Video Recordings,Faezeh Marzbanrad,,0%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Shannon Reid,s.reid@uncc.edu,82%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Ghazal Alinezhad Noghre,galinezh@uncc.edu,60%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Hamed Tabkhi,htabkhiv@uncc.edu,82%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Babak Rahimi Ardabili,sbhaska1@uncc.edu,60%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Arun Ravindran,arun.ravindran@uncc.edu,95%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Armin Danesh Pazho,brahimia@uncc.edu,60%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Christopher Neff,,0%
https://arxiv.org/pdf/2302.04310.pdf,Understanding Policy and Technical Aspects of AI-Enabled Smart Video Surveillance to Address Public Safety,Sai Datta Bhaskararayuni,,0%
https://arxiv.org/pdf/2302.04308.pdf,Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation,Xuan Xu,xuaxu@cs.stonybrook.edu,82%
https://arxiv.org/pdf/2302.04308.pdf,Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation,Xiaoling Hu,xiaolhu@cs.stonybrook.edu,82%
https://arxiv.org/pdf/2302.04308.pdf,Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation,Chao Chen,chao.chen.1@stonybrook.edu,95%
https://arxiv.org/pdf/2302.04308.pdf,Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation,Joseph Bae,joseph.bae@stonybrook.edu,95%
https://arxiv.org/pdf/2302.04308.pdf,Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation,Prateek Prasanna,prateek.prasanna@stonybrook.edu,95%
https://arxiv.org/pdf/2302.04308.pdf,Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation,Aishik Konwer,akonwer@cs.stonybrook.edu,82%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Xinran Tang,xinran tang@g.harvard.edu,95%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Mengyuan Li,mengyuan li@g.harvard.edu,95%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Zixi Chen,zixichen@g.harvard.edu,95%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Caleb Robinson,caleb.robinson@microsoft.com,95%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Simone Fobi Nsutezo,sfobinsutezo@microsoft.com,82%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Anthony Ortiz,anthony.ortiz@microsoft.com,95%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Varshini Reddy,varshinibogolu@g.harvard.edu,85%
https://arxiv.org/pdf/2302.04305.pdf,Mask Conditional Synthetic Satellite Imagery,Van Anh Le,vananhle@g.harvard.edu,95%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Xiuyu Li,,0%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Yijiang Liu,,0%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Long Lian,,0%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Huanrui Yang,,0%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Zhen Dong,,0%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Daniel Kang,,0%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Shanghang Zhang,,0%
https://arxiv.org/pdf/2302.04304.pdf,Q-Diffusion: Quantizing Diffusion Models,Kurt Keutzer,,0%
https://arxiv.org/pdf/2302.04303.pdf,Adapting Pre-trained Vision Transformers from 2D to 3D through Weight Inflation Improves Medical Image Segmentation,Zhengping Zhou,zpzhou@stanford.edu,82%
https://arxiv.org/pdf/2302.04303.pdf,Adapting Pre-trained Vision Transformers from 2D to 3D through Weight Inflation Improves Medical Image Segmentation,Serena Yeung,syyeung@stanford.edu,82%
https://arxiv.org/pdf/2302.04303.pdf,Adapting Pre-trained Vision Transformers from 2D to 3D through Weight Inflation Improves Medical Image Segmentation,Matthew P. Lungren,mlungren@stanford.edu,82%
https://arxiv.org/pdf/2302.04303.pdf,Adapting Pre-trained Vision Transformers from 2D to 3D through Weight Inflation Improves Medical Image Segmentation,Shih-cheng Huang,mschuang@stanford.edu,78%
https://arxiv.org/pdf/2302.04303.pdf,Adapting Pre-trained Vision Transformers from 2D to 3D through Weight Inflation Improves Medical Image Segmentation,Yuhui Zhang,yuhuiz@stanford.edu,85%
https://arxiv.org/pdf/2302.04269.pdf,Diagnosing and Rectifying Vision Models using Language,Jeff Z. Haochen,jhaochen@stanford.edu,82%
https://arxiv.org/pdf/2302.04269.pdf,Diagnosing and Rectifying Vision Models using Language,Kuan-chieh Wang,wangkua1@stanford.edu,78%
https://arxiv.org/pdf/2302.04269.pdf,Diagnosing and Rectifying Vision Models using Language,James Zou,jamesz@stanford.edu,85%
https://arxiv.org/pdf/2302.04269.pdf,Diagnosing and Rectifying Vision Models using Language,Serena Yeung,syyeung@stanford.edu,82%
https://arxiv.org/pdf/2302.04269.pdf,Diagnosing and Rectifying Vision Models using Language,Shih-cheng Huang,mschuang@stanford.edu,78%
https://arxiv.org/pdf/2302.04269.pdf,Diagnosing and Rectifying Vision Models using Language,Yuhui Zhang,yuhuiz@stanford.edu,85%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Min Xian,mxian@uidaho.edu,82%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Tiankai Yao,Tiankai.yao@inl.gov,95%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Shoukun Sun,,0%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Fei Xu,,0%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Lu Cai,,0%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Daniele Salvato,,0%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Fidelma Dilemma,,0%
https://arxiv.org/pdf/2302.12833.pdf,An Efficient Instance Segmentation Approach for Extracting Fission Gas Bubbles on U-10Zr Annular Fuel,Luca Capriotti,,0%
https://arxiv.org/pdf/2302.04265.pdf,PFGM++: Unlocking the Potential of Physics-Inspired Generative Models,Yilun Xu,ylxu@mit.edu,82%
https://arxiv.org/pdf/2302.04265.pdf,PFGM++: Unlocking the Potential of Physics-Inspired Generative Models,Ziming Liu,,0%
https://arxiv.org/pdf/2302.04265.pdf,PFGM++: Unlocking the Potential of Physics-Inspired Generative Models,Yonglong Tian,,0%
https://arxiv.org/pdf/2302.04265.pdf,PFGM++: Unlocking the Potential of Physics-Inspired Generative Models,Shangyuan Tong,,0%
https://arxiv.org/pdf/2302.04265.pdf,PFGM++: Unlocking the Potential of Physics-Inspired Generative Models,Max Tegmark,,0%
https://arxiv.org/pdf/2302.04265.pdf,PFGM++: Unlocking the Potential of Physics-Inspired Generative Models,Tommi Jaakkola,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Matthew Tancik,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Ethan Weber,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Evonne Ng,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Ruilong Li,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Brent Yi,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Justin Kerr,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Terrance Wang,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Alexander Kristoffersen,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Jake Austin,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Kamyar Salahi,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Abhik Ahuja,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,David Mcallister,,0%
https://arxiv.org/pdf/2302.04264.pdf,Nerfstudio: A Modular Framework for Neural Radiance Field Development,Angjoo Kanazawa,,0%
https://arxiv.org/pdf/2302.07088.pdf,Assessment of Vehicular Vision Obstruction Due to Driver-Side B-Pillar and Remediation with Blind Spot Eliminator,Dilara Baysal,dbaysal@purdue.edu,82%
https://arxiv.org/pdf/2302.04246.pdf,Shortcut Detection with Variational Autoencoders,Shahbaz Khan,baz.khan@tum.de,78%
https://arxiv.org/pdf/2302.04246.pdf,Shortcut Detection with Variational Autoencoders,Nicolas M. Müller,nicolas.mueller@aisec.fraunhofer.de,85%
https://arxiv.org/pdf/2302.04246.pdf,Shortcut Detection with Variational Autoencoders,Simon Roschmann,simon.roschmann@tum.de,95%
https://arxiv.org/pdf/2302.04246.pdf,Shortcut Detection with Variational Autoencoders,Philip Sperl,,0%
https://arxiv.org/pdf/2302.04246.pdf,Shortcut Detection with Variational Autoencoders,Konstantin Böttinger,,0%
https://arxiv.org/pdf/2302.04233.pdf,SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images,Nikhil Gosala,,0%
https://arxiv.org/pdf/2302.04233.pdf,SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images,Kürsat Petek,,0%
https://arxiv.org/pdf/2302.04233.pdf,SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images,Paulo L. J. Drews-jr,,0%
https://arxiv.org/pdf/2302.04233.pdf,SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images,Wolfram Burgard,,0%
https://arxiv.org/pdf/2302.04233.pdf,SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images,Abhinav Valada,,0%
https://arxiv.org/pdf/2302.04177.pdf,A Dynamic Graph CNN with Cross-Representation Distillation for Event-Based Recognition,Hao Chen,haochen593@gmail.com,95%
https://arxiv.org/pdf/2302.04177.pdf,A Dynamic Graph CNN with Cross-Representation Distillation for Event-Based Recognition,Yongjian Deng,yjdeng@bjut.edu.cn,82%
https://arxiv.org/pdf/2302.04177.pdf,A Dynamic Graph CNN with Cross-Representation Distillation for Event-Based Recognition,Youfu Li,meyfli@cityu.edu.hk,78%
https://arxiv.org/pdf/2302.04177.pdf,A Dynamic Graph CNN with Cross-Representation Distillation for Event-Based Recognition,Hai Liu,hailiu0204@ccnu.edu.cn,95%
https://arxiv.org/pdf/2302.04177.pdf,A Dynamic Graph CNN with Cross-Representation Distillation for Event-Based Recognition,Bochen Xie,,0%
https://arxiv.org/pdf/2302.07137.pdf,Deep Non-Monotonic Reasoning for Visual Abstract Reasoning Tasks,Maithilee Kunda,mkunda@vanderbilt.edu,82%
https://arxiv.org/pdf/2302.07137.pdf,Deep Non-Monotonic Reasoning for Visual Abstract Reasoning Tasks,Deepayan Sanyal,deepayan.sanyal@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.07137.pdf,Deep Non-Monotonic Reasoning for Visual Abstract Reasoning Tasks,Yuan Yang,yuan.yang@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.07137.pdf,Deep Non-Monotonic Reasoning for Visual Abstract Reasoning Tasks,James Ainooson,james.ainooson@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.07137.pdf,Deep Non-Monotonic Reasoning for Visual Abstract Reasoning Tasks,Joel Michelson,joel.p.michelson@vanderbilt.edu,95%
https://arxiv.org/pdf/2302.04173.pdf,A Survey of Feature detection methods for localisation of plain sections of Axial Brain Magnetic Resonance Imaging,Jan Novotný,jan.novotny@physics.slu.cz,95%
https://arxiv.org/pdf/2302.04173.pdf,A Survey of Feature detection methods for localisation of plain sections of Axial Brain Magnetic Resonance Imaging,Jiří Martinů,,0%
https://arxiv.org/pdf/2302.04173.pdf,A Survey of Feature detection methods for localisation of plain sections of Axial Brain Magnetic Resonance Imaging,Karel Adámek,,0%
https://arxiv.org/pdf/2302.04173.pdf,A Survey of Feature detection methods for localisation of plain sections of Axial Brain Magnetic Resonance Imaging,Petr Čermák,,0%
https://arxiv.org/pdf/2302.04173.pdf,A Survey of Feature detection methods for localisation of plain sections of Axial Brain Magnetic Resonance Imaging,Jiří Kozel,,0%
https://arxiv.org/pdf/2302.04173.pdf,A Survey of Feature detection methods for localisation of plain sections of Axial Brain Magnetic Resonance Imaging,David Školoudík,,0%
https://arxiv.org/pdf/2302.04149.pdf,Domain Adaptation of Synthetic Driving Datasets for Real-World Autonomous Driving,Koustav Mullick,koustav.mullick@in.bosch.com,95%
https://arxiv.org/pdf/2302.04149.pdf,Domain Adaptation of Synthetic Driving Datasets for Real-World Autonomous Driving,Amit Arvind Kale,amitarvind.kale@in.bosch.com,95%
https://arxiv.org/pdf/2302.04149.pdf,Domain Adaptation of Synthetic Driving Datasets for Real-World Autonomous Driving,Harshil Jain,,0%
https://arxiv.org/pdf/2302.04149.pdf,Domain Adaptation of Synthetic Driving Datasets for Real-World Autonomous Driving,Sanchit Gupta,,0%
https://arxiv.org/pdf/2302.04143.pdf,Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models,Corey W. Arnold,cwarnold@ucla.edu,82%
https://arxiv.org/pdf/2302.04143.pdf,Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models,Haoyue Zhang,HaoyueZhang@mednet.ucla.edu,95%
https://arxiv.org/pdf/2302.04143.pdf,Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models,Kambiz Nael,kanael@mednet.ucla.edu,82%
https://arxiv.org/pdf/2302.04143.pdf,Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models,Eric J. Yang,EricJYang@ucla.edu,95%
https://arxiv.org/pdf/2302.04143.pdf,Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models,Jennifer S. Polson,jpolson@g.ucla.edu,82%
https://arxiv.org/pdf/2302.04143.pdf,Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models,William Speier,Speier@ucla.edu,78%
https://arxiv.org/pdf/2302.04135.pdf,Multi-Modal Evaluation Approach for Medical Image Segmentation,Abdelghani Chibani,achibani@gmail.com,82%
https://arxiv.org/pdf/2302.04135.pdf,Multi-Modal Evaluation Approach for Medical Image Segmentation,Mohammadreza Razzazi,razzazi@aut.ac.ir,78%
https://arxiv.org/pdf/2302.04135.pdf,Multi-Modal Evaluation Approach for Medical Image Segmentation,Aomar Osmani,ao@lipn.univ-paris13.fr,90%
https://arxiv.org/pdf/2302.04135.pdf,Multi-Modal Evaluation Approach for Medical Image Segmentation,Seyed M. R. Modaresi,,0%
https://arxiv.org/pdf/2302.04129.pdf,Hyperspectral Image Compression Using Implicit Neural Representation,Shima Rezasoltani,,0%
https://arxiv.org/pdf/2302.04129.pdf,Hyperspectral Image Compression Using Implicit Neural Representation,Faisal Z. Qureshi,,0%
https://arxiv.org/pdf/2302.04108.pdf,Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,Adham Atyabi,aatyabi@uccs.edu,82%
https://arxiv.org/pdf/2302.04108.pdf,Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,Fatemeh Afghah,fafghah@clemson.edu,82%
https://arxiv.org/pdf/2302.04108.pdf,Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,Fatemeh Lotfi,ﬂotﬁ@clemson.edu,82%
https://arxiv.org/pdf/2302.04108.pdf,Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,Hossein Rajoli,hrajoli@clemson.edu,82%
https://arxiv.org/pdf/2302.04075.pdf,Best Practices in Active Learning for Semantic Segmentation,Sudhanshu Mittal,mittal@cs.uni-freiburg.de,78%
https://arxiv.org/pdf/2302.04075.pdf,Best Practices in Active Learning for Semantic Segmentation,Thomas Brox,brox@cs.uni-freiburg.de,78%
https://arxiv.org/pdf/2302.04075.pdf,Best Practices in Active Learning for Semantic Segmentation,Joshua Niemeijer,Joshua.Niemeijer@dlr.de,95%
https://arxiv.org/pdf/2302.04075.pdf,Best Practices in Active Learning for Semantic Segmentation,Jörg P. Schäfer,Joerg.Schaefer@dlr.de,85%
https://arxiv.org/pdf/2302.04064.pdf,Weakly-supervised Representation Learning for Video Alignment and Analysis,Guy Bar-shalom,guy.b@cs.technion.ac.il,85%
https://arxiv.org/pdf/2302.04064.pdf,Weakly-supervised Representation Learning for Video Alignment and Analysis,George Leifman,gleifman@verily.com,82%
https://arxiv.org/pdf/2302.04064.pdf,Weakly-supervised Representation Learning for Video Alignment and Analysis,Michael Elad,melad@verily.com,82%
https://arxiv.org/pdf/2302.04064.pdf,Weakly-supervised Representation Learning for Video Alignment and Analysis,Ehud Rivlin,ehud@verily.com,85%
https://arxiv.org/pdf/2302.04060.pdf,"A Systematic Evaluation and Benchmark for Embedding-Aware Generative Models: Features, Models, and Any-shot Scenarios",Jiancheng Zhao,jiancheng@zju.edu.cn,85%
https://arxiv.org/pdf/2302.04060.pdf,"A Systematic Evaluation and Benchmark for Embedding-Aware Generative Models: Features, Models, and Any-shot Scenarios",Chunhui Zhao,chhzhao@zju.edu.cn,82%
https://arxiv.org/pdf/2302.04060.pdf,"A Systematic Evaluation and Benchmark for Embedding-Aware Generative Models: Features, Models, and Any-shot Scenarios",Liangjun Feng,liangjunfeng@zju.edu.cn,95%
https://arxiv.org/pdf/2302.04032.pdf,A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions,Rajkumar Saini,lastname@ltu.se,55%
https://arxiv.org/pdf/2302.04032.pdf,A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions,Gustav Grund Pihlgren,gustav.pihlgren@umu.se,95%
https://arxiv.org/pdf/2302.04032.pdf,A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions,Konstantina Nikolaidou,,0%
https://arxiv.org/pdf/2302.04032.pdf,A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions,Prakash Chandra Chhipa,,0%
https://arxiv.org/pdf/2302.04032.pdf,A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions,Nosheen Abid,,0%
https://arxiv.org/pdf/2302.04032.pdf,A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions,Fredrik Sandin,,0%
https://arxiv.org/pdf/2302.04032.pdf,A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions,Marcus Liwicki,,0%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Shiwei Zhang,zhangjin.zsw@alibaba-inc.com,78%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Yingya Zhang,yingya.zyy@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Di Luan,dluan@connect.ust.hk,82%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Jun Cen,jcenaa@connect.ust.hk,82%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Shaojie Shen,eeshaojie@ust.hk,85%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Yixuan Pei,peiyixuan@stu.xjtu.edu.cn,95%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Deli Zhao,deli.zdl@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.04002.pdf,The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition,Qifeng Chen,,0%
https://arxiv.org/pdf/2302.03992.pdf,Convolutional Neural Networks Trained to Identify Words Provide a Surprisingly Good Account of Visual Form Priming Effects,Dong Yin,,0%
https://arxiv.org/pdf/2302.03992.pdf,Convolutional Neural Networks Trained to Identify Words Provide a Surprisingly Good Account of Visual Form Priming Effects,Valerio Biscione,,0%
https://arxiv.org/pdf/2302.03992.pdf,Convolutional Neural Networks Trained to Identify Words Provide a Surprisingly Good Account of Visual Form Priming Effects,Jeffrey Bowers,,0%
https://arxiv.org/pdf/2302.03987.pdf,Multiview Representation Learning from Crowdsourced Triplet Comparisons,Jiyi Li,jyli@yamanashi.ac.jp,82%
https://arxiv.org/pdf/2302.03987.pdf,Multiview Representation Learning from Crowdsourced Triplet Comparisons,Hisashi Kashima,kashima@i.kyoto-u.ac.jp,78%
https://arxiv.org/pdf/2302.03987.pdf,Multiview Representation Learning from Crowdsourced Triplet Comparisons,Xiaotian Lu,lu@ml.ist.i.kyoto-u.ac.jp,78%
https://arxiv.org/pdf/2302.03987.pdf,Multiview Representation Learning from Crowdsourced Triplet Comparisons,Koh Takeuchi,takeuchi@i.kyoto-u.ac.jp,78%
https://arxiv.org/pdf/2302.03985.pdf,Cross-Layer Retrospective Retrieving via Layer Attention,Guodong Li,gdli@hku.hk,82%
https://arxiv.org/pdf/2302.03985.pdf,Cross-Layer Retrospective Retrieving via Layer Attention,Guangjian Tian,Tian.Guangjian@huawei.com,95%
https://arxiv.org/pdf/2302.03985.pdf,Cross-Layer Retrospective Retrieving via Layer Attention,Yuxi Cai,caiyuxi@connect.hku.hk,95%
https://arxiv.org/pdf/2302.03985.pdf,Cross-Layer Retrospective Retrieving via Layer Attention,Jintai Chen,jtigerchen@zju.edu.cn,82%
https://arxiv.org/pdf/2302.03985.pdf,Cross-Layer Retrospective Retrieving via Layer Attention,Yanwen Fang,,0%
https://arxiv.org/pdf/2302.03985.pdf,Cross-Layer Retrospective Retrieving via Layer Attention,Jingyu Zhao,,0%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,G. Bassi,giovanni.bassi@cern.ch,82%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,L. Giambastiani,,0%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,K. Hennessy,,0%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,F. Lazzari,,0%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,M. J. Morello,,0%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,T. Pajero,,0%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,A. Fernandez Prieto,,0%
https://arxiv.org/pdf/2302.03972.pdf,A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector,G. Punzi,,0%
https://arxiv.org/pdf/2302.03956.pdf,Neural Congealing: Aligning Images to a Joint Semantic Atlas,Dolev Ofri-amar,,0%
https://arxiv.org/pdf/2302.03956.pdf,Neural Congealing: Aligning Images to a Joint Semantic Atlas,Michal Geyer,,0%
https://arxiv.org/pdf/2302.03956.pdf,Neural Congealing: Aligning Images to a Joint Semantic Atlas,Yoni Kasten,,0%
https://arxiv.org/pdf/2302.03956.pdf,Neural Congealing: Aligning Images to a Joint Semantic Atlas,Tali Dekel,,0%
https://arxiv.org/pdf/2302.04676.pdf,Stacked Cross-modal Feature Consolidation Attention Networks for Image Captioning,Mohsen Ebrahimi Moghaddam,moghadam@sbu.ac.ir,65%
https://arxiv.org/pdf/2302.04676.pdf,Stacked Cross-modal Feature Consolidation Attention Networks for Image Captioning,Mozhgan Pourkeshavarz,,0%
https://arxiv.org/pdf/2302.04676.pdf,Stacked Cross-modal Feature Consolidation Attention Networks for Image Captioning,Shahabedin Nabavi,,0%
https://arxiv.org/pdf/2302.04676.pdf,Stacked Cross-modal Feature Consolidation Attention Networks for Image Captioning,Mehrnoush Shamsfard,,0%
https://arxiv.org/pdf/2302.03934.pdf,Spatiotemporal Deformation Perception for Fisheye Video Rectification,Yao Zhao,yzhao@bjtu.edu.cn,82%
https://arxiv.org/pdf/2302.03934.pdf,Spatiotemporal Deformation Perception for Fisheye Video Rectification,Shangrong Yang,sr yang@bjtu.edu.cn,82%
https://arxiv.org/pdf/2302.03934.pdf,Spatiotemporal Deformation Perception for Fisheye Video Rectification,Kang Liao,kang liao@bjtu.edu.cn,95%
https://arxiv.org/pdf/2302.03934.pdf,Spatiotemporal Deformation Perception for Fisheye Video Rectification,Chunyu Lin,cylin@bjtu.edu.cn,82%
https://arxiv.org/pdf/2302.03932.pdf,Multi-view Feature Extraction based on Dual Contrastive Head,Hongjie Zhang,,0%
https://arxiv.org/pdf/2302.10756.pdf,Unsupervised Seismic Footprint Removal With Physical Prior Augmented Deep Autoencoder,Feng Qian,fengqian@uestc.edu.cn,95%
https://arxiv.org/pdf/2302.10756.pdf,Unsupervised Seismic Footprint Removal With Physical Prior Augmented Deep Autoencoder,Jinliang Tang,tangjl.swty@sinopec.com,78%
https://arxiv.org/pdf/2302.10756.pdf,Unsupervised Seismic Footprint Removal With Physical Prior Augmented Deep Autoencoder,Yingjie Zhou,yjzhou@scu.edu.cn,82%
https://arxiv.org/pdf/2302.10756.pdf,Unsupervised Seismic Footprint Removal With Physical Prior Augmented Deep Autoencoder,Yuehua Yue,,0%
https://arxiv.org/pdf/2302.10756.pdf,Unsupervised Seismic Footprint Removal With Physical Prior Augmented Deep Autoencoder,Yu He,,0%
https://arxiv.org/pdf/2302.10756.pdf,Unsupervised Seismic Footprint Removal With Physical Prior Augmented Deep Autoencoder,Hongtao Yu,,0%
https://arxiv.org/pdf/2302.10756.pdf,Unsupervised Seismic Footprint Removal With Physical Prior Augmented Deep Autoencoder,Guangmin Hu,,0%
https://arxiv.org/pdf/2302.03922.pdf,Gestalt-Guided Image Understanding for Few-Shot Learning,Kun Song,songkun@xs.ustb.edu.cn,95%
https://arxiv.org/pdf/2302.03922.pdf,Gestalt-Guided Image Understanding for Few-Shot Learning,Jiansheng Chen,jschen@ustb.edu.cn,82%
https://arxiv.org/pdf/2302.03922.pdf,Gestalt-Guided Image Understanding for Few-Shot Learning,Yuchen Wu,yuchen.wu@xs.ustb.edu.cn,95%
https://arxiv.org/pdf/2302.03922.pdf,Gestalt-Guided Image Understanding for Few-Shot Learning,Tianyu Hu,Tianyu@ustb.edu.cn,85%
https://arxiv.org/pdf/2302.03922.pdf,Gestalt-Guided Image Understanding for Few-Shot Learning,Huimin Ma,,0%
https://arxiv.org/pdf/2302.03914.pdf,Generalized Few-Shot 3D Object Detection of LiDAR Point Cloud for Autonomous Driving,Jianbing Shen,shenjianbingcg@gmail.com,95%
https://arxiv.org/pdf/2302.03914.pdf,Generalized Few-Shot 3D Object Detection of LiDAR Point Cloud for Autonomous Driving,Jiawei Liu,liu@bit.edu.cn,78%
https://arxiv.org/pdf/2302.03914.pdf,Generalized Few-Shot 3D Object Detection of LiDAR Point Cloud for Autonomous Driving,Sanyuan Zhao,zhaosanyuan@bit.edu.cn,95%
https://arxiv.org/pdf/2302.03914.pdf,Generalized Few-Shot 3D Object Detection of LiDAR Point Cloud for Autonomous Driving,Xingping Dong,xingping.dong@gmail.com,95%
https://arxiv.org/pdf/2302.03911.pdf,Multi-site Organ Segmentation with Federated Partial Supervision and Site Adaptation,S. Kevin Zhou,skevinzhou@ustc.edu.cn,82%
https://arxiv.org/pdf/2302.03911.pdf,Multi-site Organ Segmentation with Federated Partial Supervision and Site Adaptation,Pengbo Liu,,0%
https://arxiv.org/pdf/2302.03911.pdf,Multi-site Organ Segmentation with Federated Partial Supervision and Site Adaptation,Mengke Sun,,0%
https://arxiv.org/pdf/2302.03900.pdf,Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models,Jong Chul Ye,jong.ye@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.03900.pdf,Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models,Hyeonho Jeong,,0%
https://arxiv.org/pdf/2302.03900.pdf,Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models,Gihyun Kwon,,0%
https://arxiv.org/pdf/2302.10296.pdf,On Function-Coupled Watermarks for Deep Neural Networks,Xiangyu Wen,,0%
https://arxiv.org/pdf/2302.10296.pdf,On Function-Coupled Watermarks for Deep Neural Networks,Yu Li,,0%
https://arxiv.org/pdf/2302.10296.pdf,On Function-Coupled Watermarks for Deep Neural Networks,Wei Jiang,,0%
https://arxiv.org/pdf/2302.10296.pdf,On Function-Coupled Watermarks for Deep Neural Networks,Qiang Xu,,0%
https://arxiv.org/pdf/2302.03875.pdf,Neural Artistic Style Transfer with Conditional Adversaria,P. N. Deelaka,,0%
https://arxiv.org/pdf/2302.03873.pdf,Geometric Perception based Efficient Text Recognition,P. N. Deelaka,,0%
https://arxiv.org/pdf/2302.03873.pdf,Geometric Perception based Efficient Text Recognition,D. R. Jayakodi,,0%
https://arxiv.org/pdf/2302.03873.pdf,Geometric Perception based Efficient Text Recognition,D. Y. Silva,,0%
https://arxiv.org/pdf/2302.03868.pdf,A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation,Beatrice Riviere,riviere@rice.edu,78%
https://arxiv.org/pdf/2302.03868.pdf,A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation,David Fuentes,dtfuentes@mdanderson.org,82%
https://arxiv.org/pdf/2302.03868.pdf,A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation,Adrian Celaya,aecelaya@rice.edu,82%
https://arxiv.org/pdf/2302.03861.pdf,SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images,Quanzheng Li,quanzheng@mgh.harvard.edu,85%
https://arxiv.org/pdf/2302.03861.pdf,SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images,Kuang Gong,kgong@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.03861.pdf,SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images,Junyu Chen,jchen245@jhmi.edu,82%
https://arxiv.org/pdf/2302.03861.pdf,SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images,Se-in Jang,sjang7@mgh.harvard.edu,82%
https://arxiv.org/pdf/2302.03861.pdf,SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images,Gary Y. Li,bettergary@gmail.com,85%
https://arxiv.org/pdf/2302.03860.pdf,EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions,Xinwei Ju,x.ju21@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.03860.pdf,EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions,Frank Po Wen Lo,po.lo15@imperial.ac.uk,78%
https://arxiv.org/pdf/2302.03860.pdf,EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions,Benny Lo,benny.lo@imperial.ac.uk,95%
https://arxiv.org/pdf/2302.03860.pdf,EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions,Peilun Shi,p.shi21@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.03860.pdf,EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions,Jianing Qiu,jianing.qiu17@imperial.ac.uk,95%
https://arxiv.org/pdf/2302.03860.pdf,EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions,Jiachuan Peng,j.peng21@imperial.ac.uk,82%
https://arxiv.org/pdf/2302.10894.pdf,Red Teaming Deep Neural Networks with Feature Synthesis Tools,Stephen Casper,scasper@mit.edu,82%
https://arxiv.org/pdf/2302.10894.pdf,Red Teaming Deep Neural Networks with Feature Synthesis Tools,Yuxiao Li,,0%
https://arxiv.org/pdf/2302.10894.pdf,Red Teaming Deep Neural Networks with Feature Synthesis Tools,Jiawei Li,,0%
https://arxiv.org/pdf/2302.10894.pdf,Red Teaming Deep Neural Networks with Feature Synthesis Tools,Tong Bu,,0%
https://arxiv.org/pdf/2302.10894.pdf,Red Teaming Deep Neural Networks with Feature Synthesis Tools,Kevin Zhang,,0%
https://arxiv.org/pdf/2302.10894.pdf,Red Teaming Deep Neural Networks with Feature Synthesis Tools,Kaivalya Hariharan,,0%
https://arxiv.org/pdf/2302.10894.pdf,Red Teaming Deep Neural Networks with Feature Synthesis Tools,Dylan Hadfield-menell,,0%
https://arxiv.org/pdf/2302.03840.pdf,MMPD: Multi-Domain Mobile Video Physiology Dataset,Daniel Mcduff,dmcduff@cs.washington.edu,82%
https://arxiv.org/pdf/2302.03840.pdf,MMPD: Multi-Domain Mobile Video Physiology Dataset,Yuntao Wang,yuntaowang@tsinghua.edu.cn,95%
https://arxiv.org/pdf/2302.03840.pdf,MMPD: Multi-Domain Mobile Video Physiology Dataset,Xin Liu,xliu0@cs.washington.edu,82%
https://arxiv.org/pdf/2302.03840.pdf,MMPD: Multi-Domain Mobile Video Physiology Dataset,Shwetak Patel,shwetak@cs.washington.edu,85%
https://arxiv.org/pdf/2302.03840.pdf,MMPD: Multi-Domain Mobile Video Physiology Dataset,Jiankai Tang,,0%
https://arxiv.org/pdf/2302.03840.pdf,MMPD: Multi-Domain Mobile Video Physiology Dataset,Kequan Chen,,0%
https://arxiv.org/pdf/2302.03840.pdf,MMPD: Multi-Domain Mobile Video Physiology Dataset,Yuanchun Shi,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Pei Wu Qin,pwqin@sz.tsinghua.edu.cn,82%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Ahmed Fateh Ameen,sandev@gmail.com,65%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Muhammad Hassan,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Hao Zhang,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Home Wu Zeng,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Shuye Ma,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Wen Liang,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Dingqi Shang,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Jiaming Ding,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Ziheng Zhan,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Tsz Kwan Lam,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Ming Xu,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Qiming Huang,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Dongmei Wu,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Can Yang Zhang,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Zhou You,,0%
https://arxiv.org/pdf/2302.03839.pdf,Futuristic Variations and Analysis in Fundus Images Corresponding to Biological Traits,Awiwu Ain,,0%
https://arxiv.org/pdf/2302.03830.pdf,TetCNN: Convolutional Neural Networks on Tetrahedral Meshes,Mohammad Farazi,,0%
https://arxiv.org/pdf/2302.03830.pdf,TetCNN: Convolutional Neural Networks on Tetrahedral Meshes,Zhangsihao Yang,,0%
https://arxiv.org/pdf/2302.03830.pdf,TetCNN: Convolutional Neural Networks on Tetrahedral Meshes,Wenhui Zhu,,0%
https://arxiv.org/pdf/2302.03830.pdf,TetCNN: Convolutional Neural Networks on Tetrahedral Meshes,Peijie Qiu,,0%
https://arxiv.org/pdf/2302.03830.pdf,TetCNN: Convolutional Neural Networks on Tetrahedral Meshes,Yalin Wang,,0%
https://arxiv.org/pdf/2302.03820.pdf,A Unified Multi-view Multi-person Tracking Framework,Fan Yang,fan.yang@fujitsu.com,95%
https://arxiv.org/pdf/2302.03820.pdf,A Unified Multi-view Multi-person Tracking Framework,Shigeyuki Odashima,,0%
https://arxiv.org/pdf/2302.03820.pdf,A Unified Multi-view Multi-person Tracking Framework,Sosuke Yamao,,0%
https://arxiv.org/pdf/2302.03820.pdf,A Unified Multi-view Multi-person Tracking Framework,Hiroaki Fujimoto,,0%
https://arxiv.org/pdf/2302.03820.pdf,A Unified Multi-view Multi-person Tracking Framework,Shoichi Masui,,0%
https://arxiv.org/pdf/2302.03820.pdf,A Unified Multi-view Multi-person Tracking Framework,Shan Jiang,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Wei-chung Lee,wei-chung_lee@hms.harvard.edu,95%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Aaron T. Kuan,Aaron_Kuan@hms.harvard.edu,95%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Tri Nguyen,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Mukul Narwani,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Mark Larson,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Yicong Li,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Shuhan Xie,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Hanspeter Pfister,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Donglai Wei,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Nir Shavit,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Lu Mi,,0%
https://arxiv.org/pdf/2302.03819.pdf,The XPRESS Challenge: Xray Projectomic Reconstruction -- Extracting Segmentation with Skeletons,Alexandra Pacureanu,,0%
https://arxiv.org/pdf/2302.03802.pdf,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,Ziqi Pang,ziqip2@illinois.edu,85%
https://arxiv.org/pdf/2302.03802.pdf,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,Yu-xiong Wang,yxw@illinois.edu,90%
https://arxiv.org/pdf/2302.03802.pdf,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,Jie Li,,0%
https://arxiv.org/pdf/2302.03802.pdf,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,Pavel Tokmakov,,0%
https://arxiv.org/pdf/2302.03802.pdf,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,Dian Chen,,0%
https://arxiv.org/pdf/2302.03802.pdf,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,Sergey Zagoruyko,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Charles Averill,firstname.lastname@utdallas.edu,70%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Yangxiao Lu,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Ninad Khargonkar,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Zesheng Xu,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Kamalesh Palanisamy,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Kaiyu Hang,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Yunhui Guo,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Nicholas Ruozzi,,0%
https://arxiv.org/pdf/2302.03793.pdf,Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction,Yu Xiang,,0%
https://arxiv.org/pdf/2302.03791.pdf,How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control,Jacopo Teneggi,jtenegg1@jhu.edu,65%
https://arxiv.org/pdf/2302.03791.pdf,How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control,Jeremias Sulam,jsulam@jhu.edu,82%
https://arxiv.org/pdf/2302.03791.pdf,How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control,Matthew Tivnan,,0%
https://arxiv.org/pdf/2302.03791.pdf,How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control,J. Webster Stayman,,0%
https://arxiv.org/pdf/2302.03751.pdf,Understanding Why ViT Trains Badly on Small Datasets: An Intuitive Perspective,Boyuan Chen,boyuan.chen@nyu.edu,95%
https://arxiv.org/pdf/2302.03751.pdf,Understanding Why ViT Trains Badly on Small Datasets: An Intuitive Perspective,Haoran Zhu,,0%
https://arxiv.org/pdf/2302.03751.pdf,Understanding Why ViT Trains Badly on Small Datasets: An Intuitive Perspective,Carter Yang,,0%
https://arxiv.org/pdf/2302.03750.pdf,Linking convolutional kernel size to generalization bias in face analysis CNNs,Josue Ortega Caro,josue.ortegacaro@yale.edu,95%
https://arxiv.org/pdf/2302.03750.pdf,Linking convolutional kernel size to generalization bias in face analysis CNNs,Ankit B. Patel,ankit.patel@rice.edu,95%
https://arxiv.org/pdf/2302.03750.pdf,Linking convolutional kernel size to generalization bias in face analysis CNNs,Vikram Maheshri,vmaheshri@uh.edu,82%
https://arxiv.org/pdf/2302.03750.pdf,Linking convolutional kernel size to generalization bias in face analysis CNNs,Guha Balakrishnan,guha@rice.edu,85%
https://arxiv.org/pdf/2302.03750.pdf,Linking convolutional kernel size to generalization bias in face analysis CNNs,Hao Liang,,0%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Dan Gutfreund,dgutfre@us.ibm.com,90%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Dileep George,dileepgeorge@google.com,95%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Nishad Gothoskar,nishad@mit.edu,85%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Lirui Wang,liruiw@mit.edu,85%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Guangyao Zhou,,0%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Joshua B. Tenenbaum,,0%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Miguel Lázaro-gredilla,,0%
https://arxiv.org/pdf/2302.03744.pdf,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,Vikash K. Mansinghka,,0%
https://arxiv.org/pdf/2302.07944.pdf,Effective Data Augmentation With Diffusion Models,Brandon Trabucco,brandon@btrabucco.com,85%
https://arxiv.org/pdf/2302.07944.pdf,Effective Data Augmentation With Diffusion Models,Ruslan Salakhutdinov,rsalakhu@cs.cmu.edu,90%
https://arxiv.org/pdf/2302.07944.pdf,Effective Data Augmentation With Diffusion Models,Kyle Doherty,,0%
https://arxiv.org/pdf/2302.07944.pdf,Effective Data Augmentation With Diffusion Models,Max Gurinas,,0%
https://arxiv.org/pdf/2302.03729.pdf,KENGIC: KEyword-driven and N-Gram Graph based Image Captioning,Adrian Muscat,adrian.muscat@um.edu.mt,95%
https://arxiv.org/pdf/2302.03729.pdf,KENGIC: KEyword-driven and N-Gram Graph based Image Captioning,Brandon Birmingham,brandon.birmingham.12@um.edu.mt,95%
https://arxiv.org/pdf/2302.03679.pdf,How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?,Fredrik K. Gustafsson,,0%
https://arxiv.org/pdf/2302.03679.pdf,How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?,Martin Danelljan,,0%
https://arxiv.org/pdf/2302.03679.pdf,How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?,Thomas B. Schön,,0%
https://arxiv.org/pdf/2302.03675.pdf,Auditing Gender Presentation Differences in Text-to-Image Models,Greg Turk,turk@cc.gatech.edu,78%
https://arxiv.org/pdf/2302.03675.pdf,Auditing Gender Presentation Differences in Text-to-Image Models,Diyi Yang,3diyiy@cs.stanford.edu,85%
https://arxiv.org/pdf/2302.03675.pdf,Auditing Gender Presentation Differences in Text-to-Image Models,Yanzhe Zhang,z_yanzhe@gatech.edu,85%
https://arxiv.org/pdf/2302.03675.pdf,Auditing Gender Presentation Differences in Text-to-Image Models,Lu Jiang,2lujiang@cmu.edu,95%
https://arxiv.org/pdf/2302.03665.pdf,HumanMAC: Masked Motion Completion for Human Motion Prediction,Xiaobo Xia,xiaoboxia.uni@gmail.com,95%
https://arxiv.org/pdf/2302.03665.pdf,HumanMAC: Masked Motion Completion for Human Motion Prediction,Tongliang Liu,tongliang.liu@sydney.edu.au,95%
https://arxiv.org/pdf/2302.03665.pdf,HumanMAC: Masked Motion Completion for Human Motion Prediction,Yewen Li,yewen001@e.ntu.edu.sg,85%
https://arxiv.org/pdf/2302.03665.pdf,HumanMAC: Masked Motion Completion for Human Motion Prediction,Yiren Pang,yrpang@outlook.com,82%
https://arxiv.org/pdf/2302.03665.pdf,HumanMAC: Masked Motion Completion for Human Motion Prediction,Ling-hao Chen,thu.lhchen@gmail.com,78%
https://arxiv.org/pdf/2302.03665.pdf,HumanMAC: Masked Motion Completion for Human Motion Prediction,Jiawei Zhang,,0%
https://arxiv.org/pdf/2302.10893.pdf,Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness,Felix Friedrich,friedrich@cs.tu-darmstadt.de,82%
https://arxiv.org/pdf/2302.10893.pdf,Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness,Manuel Brack,,0%
https://arxiv.org/pdf/2302.10893.pdf,Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness,Lukas Struppek,,0%
https://arxiv.org/pdf/2302.10893.pdf,Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness,Dominik Hintersdorf,,0%
https://arxiv.org/pdf/2302.10893.pdf,Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness,Patrick Schramowski,,0%
https://arxiv.org/pdf/2302.10893.pdf,Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness,Sasha Luccioni,,0%
https://arxiv.org/pdf/2302.10893.pdf,Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness,Kristian Kersting,,0%
https://arxiv.org/pdf/2302.03657.pdf,Toward Face Biometric De-identification using Adversarial Examples,Luis Felipe Gomez,luisf.gomez@uam.es,95%
https://arxiv.org/pdf/2302.03657.pdf,Toward Face Biometric De-identification using Adversarial Examples,Aythami Morales,aythami.morales@uam.es,95%
https://arxiv.org/pdf/2302.03657.pdf,Toward Face Biometric De-identification using Adversarial Examples,Julian Fierrez,julian.ﬁerrez@uam.es,95%
https://arxiv.org/pdf/2302.03657.pdf,Toward Face Biometric De-identification using Adversarial Examples,Zohra Rezgui,z.rezgui@utwente.nl,82%
https://arxiv.org/pdf/2302.03657.pdf,Toward Face Biometric De-identification using Adversarial Examples,Ruben Vera-rodriguez,ruben.vera@uam.es,85%
https://arxiv.org/pdf/2302.03657.pdf,Toward Face Biometric De-identification using Adversarial Examples,Raymond Veldhuis,r.n.j.veldhuis@utwente.nl,82%
https://arxiv.org/pdf/2302.03657.pdf,Toward Face Biometric De-identification using Adversarial Examples,Mahdi Ghafourian,mahdi.ghafourian@uam.es,95%
https://arxiv.org/pdf/2302.03648.pdf,Class-Incremental Learning: A Survey,Han-jia Ye,zhandc@lamda.nju.edu.cn,85%
https://arxiv.org/pdf/2302.03648.pdf,Class-Incremental Learning: A Survey,Ziwei Liu,ziwei.liu@ntu.edu.sg,95%
https://arxiv.org/pdf/2302.03648.pdf,Class-Incremental Learning: A Survey,Qi-wei Wang,wangqiwei@lamda.nju.edu.cn,95%
https://arxiv.org/pdf/2302.03648.pdf,Class-Incremental Learning: A Survey,Da-wei Zhou,zhoudw@lamda.nju.edu.cn,78%
https://arxiv.org/pdf/2302.03648.pdf,Class-Incremental Learning: A Survey,Zhi-hong Qi,,0%
https://arxiv.org/pdf/2302.03648.pdf,Class-Incremental Learning: A Survey,De-chuan Zhan,,0%
https://arxiv.org/pdf/2302.03640.pdf,SSR-2D: Semantic 3D Scene Reconstruction from 2D Images,Shuaifeng Zhi,zhishuaifeng@outlook.com,95%
https://arxiv.org/pdf/2302.03640.pdf,SSR-2D: Semantic 3D Scene Reconstruction from 2D Images,Junwen Huang,,0%
https://arxiv.org/pdf/2302.03640.pdf,SSR-2D: Semantic 3D Scene Reconstruction from 2D Images,Alexey Artemov,,0%
https://arxiv.org/pdf/2302.03640.pdf,SSR-2D: Semantic 3D Scene Reconstruction from 2D Images,Yujin Chen,,0%
https://arxiv.org/pdf/2302.03640.pdf,SSR-2D: Semantic 3D Scene Reconstruction from 2D Images,Kai Xu,,0%
https://arxiv.org/pdf/2302.03640.pdf,SSR-2D: Semantic 3D Scene Reconstruction from 2D Images,Matthias Nießner,,0%
https://arxiv.org/pdf/2302.03629.pdf,Ethical Considerations for Responsible Data Curation,Jerone T. A. Andrews,jerone.andrews@sony.com,95%
https://arxiv.org/pdf/2302.03629.pdf,Ethical Considerations for Responsible Data Curation,Dora Zhao,,0%
https://arxiv.org/pdf/2302.03629.pdf,Ethical Considerations for Responsible Data Curation,William Thong,,0%
https://arxiv.org/pdf/2302.03629.pdf,Ethical Considerations for Responsible Data Curation,Apostolos Modas,,0%
https://arxiv.org/pdf/2302.03629.pdf,Ethical Considerations for Responsible Data Curation,Orestis Papakyriakopoulos,,0%
https://arxiv.org/pdf/2302.03629.pdf,Ethical Considerations for Responsible Data Curation,Alice Xiang,,0%
https://arxiv.org/pdf/2302.03609.pdf,Pole Estimation and Optical Navigation using Circle of Latitude Projections,John A. Christian,,0%
https://arxiv.org/pdf/2302.03594.pdf,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,Zihan Zhu,,0%
https://arxiv.org/pdf/2302.03594.pdf,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,Songyou Peng,,0%
https://arxiv.org/pdf/2302.03594.pdf,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,Viktor Larsson,,0%
https://arxiv.org/pdf/2302.03594.pdf,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,Zhaopeng Cui,,0%
https://arxiv.org/pdf/2302.03594.pdf,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,Martin R. Oswald,,0%
https://arxiv.org/pdf/2302.03594.pdf,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,Andreas Geiger,,0%
https://arxiv.org/pdf/2302.03594.pdf,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,Marc Pollefeys,,0%
https://arxiv.org/pdf/2302.03573.pdf,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,Yilun Du,yilundu@mit.edu,95%
https://arxiv.org/pdf/2302.03573.pdf,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,Ethan Chun,,0%
https://arxiv.org/pdf/2302.03573.pdf,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,Anthony Simeonov,,0%
https://arxiv.org/pdf/2302.03573.pdf,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,Tomas Lozano-perez,,0%
https://arxiv.org/pdf/2302.03573.pdf,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,Leslie Kaelbling,,0%
https://arxiv.org/pdf/2302.03570.pdf,A Deep Learning-based in silico Framework for Optimization on Retinal Prosthetic Stimulation,Yuli Wu,yuli.wu@lfb.rwth-aachen.de,95%
https://arxiv.org/pdf/2302.03570.pdf,A Deep Learning-based in silico Framework for Optimization on Retinal Prosthetic Stimulation,Ivan Karetic,,0%
https://arxiv.org/pdf/2302.03570.pdf,A Deep Learning-based in silico Framework for Optimization on Retinal Prosthetic Stimulation,Johannes Stegmaier,,0%
https://arxiv.org/pdf/2302.03570.pdf,A Deep Learning-based in silico Framework for Optimization on Retinal Prosthetic Stimulation,Peter Walter,,0%
https://arxiv.org/pdf/2302.03570.pdf,A Deep Learning-based in silico Framework for Optimization on Retinal Prosthetic Stimulation,Dorit Merhof,,0%
https://arxiv.org/pdf/2302.03566.pdf,Look Around and Learn: Self-Training Object Detection by Exploration,Lorenzo Natale,name.surname@iit.it,70%
https://arxiv.org/pdf/2302.03566.pdf,Look Around and Learn: Self-Training Object Detection by Exploration,Gianluca Scarpellini,,0%
https://arxiv.org/pdf/2302.03566.pdf,Look Around and Learn: Self-Training Object Detection by Exploration,Stefano Rosa,,0%
https://arxiv.org/pdf/2302.03566.pdf,Look Around and Learn: Self-Training Object Detection by Exploration,Pietro Morerio,,0%
https://arxiv.org/pdf/2302.03566.pdf,Look Around and Learn: Self-Training Object Detection by Exploration,Alessio Del Bue,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Zitong Yu,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Yuming Shen,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Jingang Shi,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Hengshuang Zhao,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Yawen Cui,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Jiehua Zhang,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Philip Torr,,0%
https://arxiv.org/pdf/2302.03548.pdf,PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,Guoying Zhao,,0%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Lei Li,lei.li@eng.ox.ac.uk,95%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Shan Yang,yang.shan@zs-hospital.sh.cn,95%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Wangbin Ding,,0%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Junyi Qiu,,0%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Sihan Wang,,0%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Liqin Huang,,0%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Yinyin Chen,,0%
https://arxiv.org/pdf/2302.03537.pdf,Aligning Multi-Sequence CMR Towards Fully Automated Myocardial Pathology Segmentation,Xiahai Zhuang,,0%
https://arxiv.org/pdf/2302.03533.pdf,Revisiting Pre-training in Audio-Visual Learning,Wenke Xia,xiawenke2022@ruc.edu.cn,95%
https://arxiv.org/pdf/2302.03533.pdf,Revisiting Pre-training in Audio-Visual Learning,Ruoxuan Feng,fengruox@gmail.com,78%
https://arxiv.org/pdf/2302.03533.pdf,Revisiting Pre-training in Audio-Visual Learning,Di Hu,dihu@ruc.edu.cn,95%
https://arxiv.org/pdf/2302.03531.pdf,Structured Generative Models for Scene Understanding,Christopher K. I. Williams,c.k.i.williams@ed.ac.uk,82%
https://arxiv.org/pdf/2302.03477.pdf,Explainable Action Prediction through Self-Supervision on Scene Graphs,Lars Kunze,lars@robots.ox.ac.uk,85%
https://arxiv.org/pdf/2302.03477.pdf,Explainable Action Prediction through Self-Supervision on Scene Graphs,Daniel Omeiza,daniel@oxfordrobotics.institute,85%
https://arxiv.org/pdf/2302.03477.pdf,Explainable Action Prediction through Self-Supervision on Scene Graphs,Pawit Kochakarn,pkochakarn@oxfordrobotics.institute,82%
https://arxiv.org/pdf/2302.03477.pdf,Explainable Action Prediction through Self-Supervision on Scene Graphs,Daniele De Martini,daniele@robots.ox.ac.uk,85%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Bartlomiej W. Papiez,bartlomiej.papiez@bdi.ox.ac.uk,95%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Thibaud Coroller,thibaud.coroller@novartis.com,95%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Yao Chen,,0%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Yuanhan Mo,,0%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Aimee Readie,,0%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Gregory Ligozio,,0%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Indrajeet Mandal,,0%
https://arxiv.org/pdf/2302.03476.pdf,VertXNet: An Ensemble Method for Vertebrae Segmentation and Identification of Spinal X-Ray,Faiz Jabbar,,0%
https://arxiv.org/pdf/2302.03473.pdf,Med-NCA: Robust and Lightweight Segmentation with Neural Cellular Automata,John Kalkhof,,0%
https://arxiv.org/pdf/2302.03473.pdf,Med-NCA: Robust and Lightweight Segmentation with Neural Cellular Automata,Camila González,,0%
https://arxiv.org/pdf/2302.03473.pdf,Med-NCA: Robust and Lightweight Segmentation with Neural Cellular Automata,Anirban Mukhopadhyay,,0%
https://arxiv.org/pdf/2302.03453.pdf,OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer,Gen Li,genli@tencent.com,95%
https://arxiv.org/pdf/2302.03453.pdf,OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer,Chao Dong,chao.dong@siat.ac.cn,95%
https://arxiv.org/pdf/2302.03453.pdf,OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer,Xintao Wang,xintaowang@tencent.com,95%
https://arxiv.org/pdf/2302.03453.pdf,OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer,Fanghua Yu,fanghuayu96@gmail.com,95%
https://arxiv.org/pdf/2302.03453.pdf,OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer,Ying Shan,yingsshan@tencent.com,95%
https://arxiv.org/pdf/2302.03453.pdf,OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer,Mingdeng Cao,,0%
https://arxiv.org/pdf/2302.03442.pdf,Using t-distributed stochastic neighbor embedding for visualization and segmentation of 3D point clouds of plants,Helin Dutagaci,helindutagaci@gmail.com,95%
https://arxiv.org/pdf/2302.03432.pdf,SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation,Yusheng Xie,yushx@amazon.com,78%
https://arxiv.org/pdf/2302.03432.pdf,SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation,Srikar Appalaraju,srikara@amazon.com,85%
https://arxiv.org/pdf/2302.03432.pdf,SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation,Yash Patel,patelyas@fel.cvut.cz,78%
https://arxiv.org/pdf/2302.03432.pdf,SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation,R. Manmatha,manmatha@amazon.com,78%
https://arxiv.org/pdf/2302.03432.pdf,SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation,Yi Zhu,,0%
https://arxiv.org/pdf/2302.03406.pdf,High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets,Chuming Lin,chuminglin@tencent.com,95%
https://arxiv.org/pdf/2302.03406.pdf,High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets,Yuan Xie,yxie@cs.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.03406.pdf,High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets,Zhizhong Zhang,zzzhang@cs.ecnu.edu.cn,82%
https://arxiv.org/pdf/2302.03406.pdf,High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets,Ying Tai,yingtai@tencent.com,95%
https://arxiv.org/pdf/2302.03406.pdf,High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets,Donghao Luo,michaelluo@tencent.com,78%
https://arxiv.org/pdf/2302.03406.pdf,High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets,Yanbo Wang,,0%
https://arxiv.org/pdf/2302.03397.pdf,AniPixel: Towards Animatable Pixel-Aligned Human Avatar,Jing Zhang,jing.zhang1@sydney.edu.au,95%
https://arxiv.org/pdf/2302.03397.pdf,AniPixel: Towards Animatable Pixel-Aligned Human Avatar,Dacheng Tao,dacheng.tao@gmail.com,95%
https://arxiv.org/pdf/2302.03397.pdf,AniPixel: Towards Animatable Pixel-Aligned Human Avatar,Zhi Hou,zhou9878@uni.sydney.edu.au,82%
https://arxiv.org/pdf/2302.03397.pdf,AniPixel: Towards Animatable Pixel-Aligned Human Avatar,Jinlong Fan,jfan0939@uni.sydney.edu.au,82%
https://arxiv.org/pdf/2302.03318.pdf,PAMI: partition input and aggregate outputs for model interpretation,Wentao Zhang,zhangwt65@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.03318.pdf,PAMI: partition input and aggregate outputs for model interpretation,Wei Shi,shiw58@mail2.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.03318.pdf,PAMI: partition input and aggregate outputs for model interpretation,Ruixuan Wang,wangruix5@mail.sysu.edu.cn,78%
https://arxiv.org/pdf/2302.03318.pdf,PAMI: partition input and aggregate outputs for model interpretation,Weishi Zheng,,0%
https://arxiv.org/pdf/2302.03299.pdf,3D Vessel Segmentation with Limited Guidance of 2D Structure-agnostic Vessel Annotations,Lisheng Wang,lswang@sjtu.edu.cn,82%
https://arxiv.org/pdf/2302.03299.pdf,3D Vessel Segmentation with Limited Guidance of 2D Structure-agnostic Vessel Annotations,Huai Chen,,0%
https://arxiv.org/pdf/2302.03299.pdf,3D Vessel Segmentation with Limited Guidance of 2D Structure-agnostic Vessel Annotations,Xiuying Wang,,0%
https://arxiv.org/pdf/2302.03298.pdf,Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion,Wei Xiang,W.Xiang@latrobe.edu.au,82%
https://arxiv.org/pdf/2302.03298.pdf,Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion,Arnold Wiliem,arnoldw@sentientvision.com,85%
https://arxiv.org/pdf/2302.03298.pdf,Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion,Clinton Fookes,c.fookes@qut.edu.au,82%
https://arxiv.org/pdf/2302.03298.pdf,Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion,Jordan Shipard,,0%
https://arxiv.org/pdf/2302.03298.pdf,Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion,Kien Nguyen Thanh,,0%
https://arxiv.org/pdf/2302.03296.pdf,Multi-organ segmentation: a progressive exploration of learning paradigms under scarce annotation,Chenxi Zhang,Chenxizhang@fudan.edu.cn,95%
https://arxiv.org/pdf/2302.03296.pdf,Multi-organ segmentation: a progressive exploration of learning paradigms under scarce annotation,Zhijian Song,zjsong@fudan.edu.cn,82%
https://arxiv.org/pdf/2302.03296.pdf,Multi-organ segmentation: a progressive exploration of learning paradigms under scarce annotation,Shiman Li,,0%
https://arxiv.org/pdf/2302.03296.pdf,Multi-organ segmentation: a progressive exploration of learning paradigms under scarce annotation,Haoran Wang,,0%
https://arxiv.org/pdf/2302.03296.pdf,Multi-organ segmentation: a progressive exploration of learning paradigms under scarce annotation,Yucong Meng,,0%
https://arxiv.org/pdf/2302.03292.pdf,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,Ryosuke Furuta,furuta@iis.u-tokyo.ac.jp,78%
https://arxiv.org/pdf/2302.03292.pdf,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,Zecheng Yu,zch-yu@iis.u-tokyo.ac.jp,82%
https://arxiv.org/pdf/2302.03292.pdf,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,Yoichi Sato,ysato@iis.u-tokyo.ac.jp,82%
https://arxiv.org/pdf/2302.03292.pdf,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,Yusuke Goutsu,goutsu@iis.u-tokyo.ac.jp,78%
https://arxiv.org/pdf/2302.03292.pdf,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,Takuma Yagi,tyagi@iis.u-tokyo.ac.jp,82%
https://arxiv.org/pdf/2302.03292.pdf,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,Yifei Huang,,0%
https://arxiv.org/pdf/2302.03285.pdf,Improving CT Image Segmentation Accuracy Using StyleGAN Driven Data Augmentation,Soham Bhosale,,0%
https://arxiv.org/pdf/2302.03285.pdf,Improving CT Image Segmentation Accuracy Using StyleGAN Driven Data Augmentation,Arjun Krishna,,0%
https://arxiv.org/pdf/2302.03285.pdf,Improving CT Image Segmentation Accuracy Using StyleGAN Driven Data Augmentation,Ge Wang,,0%
https://arxiv.org/pdf/2302.03285.pdf,Improving CT Image Segmentation Accuracy Using StyleGAN Driven Data Augmentation,Klaus Mueller,,0%
https://arxiv.org/pdf/2302.03282.pdf,An End-to-End Two-Phase Deep Learning-Based workflow to Segment Man-made Objects Around Reservoirs,Nayereh Hamidishad,,0%
https://arxiv.org/pdf/2302.03282.pdf,An End-to-End Two-Phase Deep Learning-Based workflow to Segment Man-made Objects Around Reservoirs,Roberto Marcondes Cesar Junior,,0%
https://arxiv.org/pdf/2302.03264.pdf,Delving Deep into Simplicity Bias for Long-Tailed Image Recognition,Xiu-shen Wei,,0%
https://arxiv.org/pdf/2302.03264.pdf,Delving Deep into Simplicity Bias for Long-Tailed Image Recognition,Xuhao Sun,,0%
https://arxiv.org/pdf/2302.03264.pdf,Delving Deep into Simplicity Bias for Long-Tailed Image Recognition,Yang Shen,,0%
https://arxiv.org/pdf/2302.03264.pdf,Delving Deep into Simplicity Bias for Long-Tailed Image Recognition,Anqi Xu,,0%
https://arxiv.org/pdf/2302.03264.pdf,Delving Deep into Simplicity Bias for Long-Tailed Image Recognition,Peng Wang,,0%
https://arxiv.org/pdf/2302.03264.pdf,Delving Deep into Simplicity Bias for Long-Tailed Image Recognition,Faen Zhang,,0%
https://arxiv.org/pdf/2302.11338.pdf,Visual Watermark Removal Based on Deep Learning,Rongfeng Wei,rfwei2@my.cityu.edu.hk,82%
https://arxiv.org/pdf/2302.03242.pdf,"Combating Online Misinformation Videos: Characterization, Detection, and Future Directions",Jintao Li,jtli@ict.ac.cn,82%
https://arxiv.org/pdf/2302.03242.pdf,"Combating Online Misinformation Videos: Characterization, Detection, and Future Directions",Juan Cao,caojuan@ict.ac.cn,95%
https://arxiv.org/pdf/2302.03242.pdf,"Combating Online Misinformation Videos: Characterization, Detection, and Future Directions",Yuyan Bu,buyuyan22s@ict.ac.cn,95%
https://arxiv.org/pdf/2302.03242.pdf,"Combating Online Misinformation Videos: Characterization, Detection, and Future Directions",Danding Wang,wangdanding@ict.ac.cn,95%
https://arxiv.org/pdf/2302.03242.pdf,"Combating Online Misinformation Videos: Characterization, Detection, and Future Directions",Peng Qi,pengqi.qp@gmail.com,95%
https://arxiv.org/pdf/2302.03242.pdf,"Combating Online Misinformation Videos: Characterization, Detection, and Future Directions",Qiang Sheng,shengqiang18z@ict.ac.cn,95%
https://arxiv.org/pdf/2302.03198.pdf,Scaling Vision-based End-to-End Driving with Multi-View Attention Learning,Diego Porres,dporres@cvc.uab.cat,82%
https://arxiv.org/pdf/2302.03198.pdf,Scaling Vision-based End-to-End Driving with Multi-View Attention Learning,Yi Xiao,yxiao@cvc.uab.cat,82%
https://arxiv.org/pdf/2302.03198.pdf,Scaling Vision-based End-to-End Driving with Multi-View Attention Learning,Felipe Codevilla,felipe.alcm@gmail.com,85%
https://arxiv.org/pdf/2302.03198.pdf,Scaling Vision-based End-to-End Driving with Multi-View Attention Learning,Antonio M. Lopez,antonio@cvc.uab.cat,85%
https://arxiv.org/pdf/2302.03193.pdf,On the Ideal Number of Groups for Isometric Gradient Propagation,Bum Jun Kim,,0%
https://arxiv.org/pdf/2302.03193.pdf,On the Ideal Number of Groups for Isometric Gradient Propagation,Hyeyeon Choi,,0%
https://arxiv.org/pdf/2302.03193.pdf,On the Ideal Number of Groups for Isometric Gradient Propagation,Hyeonah Jang,,0%
https://arxiv.org/pdf/2302.03193.pdf,On the Ideal Number of Groups for Isometric Gradient Propagation,Sang Woo Kim,,0%
https://arxiv.org/pdf/2302.03156.pdf,Novel Building Detection and Location Intelligence Collection in Aerial Satellite Imagery,Sandeep Singh,ssingh600@gatech.edu,82%
https://arxiv.org/pdf/2302.03156.pdf,Novel Building Detection and Location Intelligence Collection in Aerial Satellite Imagery,Christian Wiles,cwiles7@gatech.edu,82%
https://arxiv.org/pdf/2302.03156.pdf,Novel Building Detection and Location Intelligence Collection in Aerial Satellite Imagery,Ahmed Bilal,abilal7@gatech.edu,82%
https://arxiv.org/pdf/2302.03130.pdf,Spatial Functa: Scaling Functa to ImageNet Classification and Generation,Hyunjik Kim,hyunjikk@google.com,85%
https://arxiv.org/pdf/2302.03130.pdf,Spatial Functa: Scaling Functa to ImageNet Classification and Generation,Matthias Bauer,msbauer@google.com,82%
https://arxiv.org/pdf/2302.03130.pdf,Spatial Functa: Scaling Functa to ImageNet Classification and Generation,Emilien Dupont,,0%
https://arxiv.org/pdf/2302.03130.pdf,Spatial Functa: Scaling Functa to ImageNet Classification and Generation,Andy Brock,,0%
https://arxiv.org/pdf/2302.03130.pdf,Spatial Functa: Scaling Functa to ImageNet Classification and Generation,Dan Rosenbaum,,0%
https://arxiv.org/pdf/2302.03130.pdf,Spatial Functa: Scaling Functa to ImageNet Classification and Generation,Jonathan Richard Schwarz,,0%
https://arxiv.org/pdf/2302.03128.pdf,Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation,Zhengwei Bai,zbai012@ucr.edu,82%
https://arxiv.org/pdf/2302.03128.pdf,Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation,Guoyuan Wu,,0%
https://arxiv.org/pdf/2302.03128.pdf,Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation,Matthew J. Barth,,0%
https://arxiv.org/pdf/2302.03128.pdf,Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation,Yongkang Liu,,0%
https://arxiv.org/pdf/2302.03128.pdf,Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation,Emrah Akin Sisbot,,0%
https://arxiv.org/pdf/2302.03128.pdf,Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation,Kentaro Oguchi,,0%
https://arxiv.org/pdf/2302.03120.pdf,Studying Therapy Effects and Disease Outcomes in Silico using Artificial Counterfactual Tissue Samples,Martin Paulikat,,0%
https://arxiv.org/pdf/2302.03120.pdf,Studying Therapy Effects and Disease Outcomes in Silico using Artificial Counterfactual Tissue Samples,Christian M. Schürch,,0%
https://arxiv.org/pdf/2302.03120.pdf,Studying Therapy Effects and Disease Outcomes in Silico using Artificial Counterfactual Tissue Samples,Christian F. Baumgartner,,0%
https://arxiv.org/pdf/2302.03114.pdf,From CAD models to soft point cloud labels: An automatic annotation pipeline for cheaply supervised 3D semantic segmentation,Simon Buus Jensen,sbje@create.aau.dk,65%
https://arxiv.org/pdf/2302.03114.pdf,From CAD models to soft point cloud labels: An automatic annotation pipeline for cheaply supervised 3D semantic segmentation,Galadrielle Humblot-renaux,,0%
https://arxiv.org/pdf/2302.03114.pdf,From CAD models to soft point cloud labels: An automatic annotation pipeline for cheaply supervised 3D semantic segmentation,Andreas Møgelmose,,0%
https://arxiv.org/pdf/2302.03084.pdf,Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval,Xiang Zhang,fancyzhx@google.com,65%
https://arxiv.org/pdf/2302.03084.pdf,Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval,Kihyuk Sohn,kihyuks@google.com,85%
https://arxiv.org/pdf/2302.03084.pdf,Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval,Kuniaki Saito,keisaito@bu.edu,82%
https://arxiv.org/pdf/2302.03084.pdf,Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval,Chun-liang Li,chunliang@google.com,95%
https://arxiv.org/pdf/2302.03084.pdf,Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval,Kate Saenko,saenko@bu.edu,78%
https://arxiv.org/pdf/2302.03084.pdf,Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval,Chen-yu Lee,chenyulee@google.com,95%
https://arxiv.org/pdf/2302.03084.pdf,Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval,Tomas Pfister,tpfister@google.com,82%
https://arxiv.org/pdf/2302.03064.pdf,Investigating Pulse-Echo Sound Speed Estimation in Breast Ultrasound with Deep Learning,Walter A. Simson,,0%
https://arxiv.org/pdf/2302.03064.pdf,Investigating Pulse-Echo Sound Speed Estimation in Breast Ultrasound with Deep Learning,Magdalini Paschali,,0%
https://arxiv.org/pdf/2302.03064.pdf,Investigating Pulse-Echo Sound Speed Estimation in Breast Ultrasound with Deep Learning,Vasiliki Sideri-lampretsa,,0%
https://arxiv.org/pdf/2302.03064.pdf,Investigating Pulse-Echo Sound Speed Estimation in Breast Ultrasound with Deep Learning,Nassir Navab,,0%
https://arxiv.org/pdf/2302.03064.pdf,Investigating Pulse-Echo Sound Speed Estimation in Breast Ultrasound with Deep Learning,Jeremy J. Dahl,,0%
https://arxiv.org/pdf/2302.03027.pdf,Zero-shot Image-to-Image Translation,Gaurav Parmar,,0%
https://arxiv.org/pdf/2302.03027.pdf,Zero-shot Image-to-Image Translation,Krishna Kumar Singh,,0%
https://arxiv.org/pdf/2302.03027.pdf,Zero-shot Image-to-Image Translation,Richard Zhang,,0%
https://arxiv.org/pdf/2302.03027.pdf,Zero-shot Image-to-Image Translation,Yijun Li,,0%
https://arxiv.org/pdf/2302.03027.pdf,Zero-shot Image-to-Image Translation,Jingwan Lu,,0%
https://arxiv.org/pdf/2302.03027.pdf,Zero-shot Image-to-Image Translation,Jun-yan Zhu,,0%
https://arxiv.org/pdf/2302.03024.pdf,AIM: Adapting Image Models for Efficient Video Action Recognition,Taojiannan Yang,,0%
https://arxiv.org/pdf/2302.03024.pdf,AIM: Adapting Image Models for Efficient Video Action Recognition,Yi Zhu,,0%
https://arxiv.org/pdf/2302.03024.pdf,AIM: Adapting Image Models for Efficient Video Action Recognition,Yusheng Xie,,0%
https://arxiv.org/pdf/2302.03024.pdf,AIM: Adapting Image Models for Efficient Video Action Recognition,Aston Zhang,,0%
https://arxiv.org/pdf/2302.03024.pdf,AIM: Adapting Image Models for Efficient Video Action Recognition,Chen Chen,,0%
https://arxiv.org/pdf/2302.03024.pdf,AIM: Adapting Image Models for Efficient Video Action Recognition,Mu Li,,0%
https://arxiv.org/pdf/2302.03023.pdf,V1T: large-scale mouse V1 response prediction using a Vision Transformer,Bryan M. Li,bryan.li@ed.ac.uk,95%
https://arxiv.org/pdf/2302.03023.pdf,V1T: large-scale mouse V1 response prediction using a Vision Transformer,Nathalie L. Rochefort,n.rochefort@ed.ac.uk,82%
https://arxiv.org/pdf/2302.03023.pdf,V1T: large-scale mouse V1 response prediction using a Vision Transformer,Isabel M. Cornacchia,isabel.cornacchia@ed.ac.uk,95%
https://arxiv.org/pdf/2302.03023.pdf,V1T: large-scale mouse V1 response prediction using a Vision Transformer,Arno Onken,aonken@ed.ac.uk,82%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Joao Cartucho,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Alistair Weld,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Samyakh Tukra,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Haozheng Xu,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Hiroki Matsuzaki,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Taiyo Ishikawa,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Minjun Kwon,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Yong Eun Jang,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Kwang-ju Kim,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Gwang Lee,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Bizhe Bai,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Lueder Kahrs,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Lars Boecking,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Simeon Allmendinger,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Leopold Muller,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Yitong Zhang,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Yueming Jin,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Sophia Bano,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Francisco Vasconcelos,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Wolfgang Reiter,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Jonas Hajek,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Bruno Silva,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Estevao Lima,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Joao L. Vilaca,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Sandro Queiros,,0%
https://arxiv.org/pdf/2302.03022.pdf,SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery,Stamatia Giannarou,,0%
https://arxiv.org/pdf/2302.03020.pdf,RLSbench: Domain Adaptation Under Relaxed Label Shift,Saurabh Garg,sgarg2@andrew.cmu.edu,82%
https://arxiv.org/pdf/2302.03020.pdf,RLSbench: Domain Adaptation Under Relaxed Label Shift,Nick Erickson,,0%
https://arxiv.org/pdf/2302.03020.pdf,RLSbench: Domain Adaptation Under Relaxed Label Shift,James Sharpnack,,0%
https://arxiv.org/pdf/2302.03020.pdf,RLSbench: Domain Adaptation Under Relaxed Label Shift,Alex Smola,,0%
https://arxiv.org/pdf/2302.03020.pdf,RLSbench: Domain Adaptation Under Relaxed Label Shift,Sivaraman Balakrishnan,,0%
https://arxiv.org/pdf/2302.03020.pdf,RLSbench: Domain Adaptation Under Relaxed Label Shift,Zachary C. Lipton,,0%
https://arxiv.org/pdf/2302.03018.pdf,DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models,Tiange Xiang,xtiange@stanford.edu,85%
https://arxiv.org/pdf/2302.03018.pdf,DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models,Akshay Chaudhari,akshaysc@stanford.edu,85%
https://arxiv.org/pdf/2302.03018.pdf,DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models,Ali B Syed,alibsyed@stanford.edu,95%
https://arxiv.org/pdf/2302.03018.pdf,DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models,Mahmut Yurt,myurt@stanford.edu,82%
https://arxiv.org/pdf/2302.03018.pdf,DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models,Kawin Setsompop,kawins@stanford.edu,85%
https://arxiv.org/pdf/2302.04634.pdf,Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study,Corina S. Pasareanu,,0%
https://arxiv.org/pdf/2302.04634.pdf,Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study,Ravi Mangal,,0%
https://arxiv.org/pdf/2302.04634.pdf,Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study,Divya Gopinath,,0%
https://arxiv.org/pdf/2302.04634.pdf,Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study,Sinem Getir Yaman,,0%
https://arxiv.org/pdf/2302.04634.pdf,Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study,Calum Imrie,,0%
https://arxiv.org/pdf/2302.04634.pdf,Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study,Radu Calinescu,,0%
https://arxiv.org/pdf/2302.04634.pdf,Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study,Huafeng Yu,,0%
https://arxiv.org/pdf/2302.03014.pdf,Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images,Neel Kanwal,neel.kanwal@uis.no,95%
https://arxiv.org/pdf/2302.03014.pdf,Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images,Roger Amundsen,,0%
https://arxiv.org/pdf/2302.03014.pdf,Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images,Helga Hardardottir,,0%
https://arxiv.org/pdf/2302.03014.pdf,Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images,Luca Tomasetti,,0%
https://arxiv.org/pdf/2302.03014.pdf,Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images,Erling Sandoy Undersrud,,0%
https://arxiv.org/pdf/2302.03014.pdf,Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images,Emiel A. M. Janssen,,0%
https://arxiv.org/pdf/2302.03014.pdf,Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images,Kjersti Engan,,0%
https://arxiv.org/pdf/2302.03011.pdf,Structure and Content-Guided Video Synthesis with Diffusion Models,Patrick Esser,,0%
https://arxiv.org/pdf/2302.03011.pdf,Structure and Content-Guided Video Synthesis with Diffusion Models,Johnathan Chiu,,0%
https://arxiv.org/pdf/2302.03011.pdf,Structure and Content-Guided Video Synthesis with Diffusion Models,Parmida Atighehchian,,0%
https://arxiv.org/pdf/2302.03011.pdf,Structure and Content-Guided Video Synthesis with Diffusion Models,Jonathan Granskog,,0%
https://arxiv.org/pdf/2302.03011.pdf,Structure and Content-Guided Video Synthesis with Diffusion Models,Anastasis Germanidis,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Wenhui Zhu,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Peijie Qiu,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Oana M. Dumitrascu,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Jacob M. Sobczak,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Mohammad Farazi,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Zhangsihao Yang,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Keshav Nandakumar,,0%
https://arxiv.org/pdf/2302.03003.pdf,OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing,Yalin Wang,,0%
https://arxiv.org/pdf/2302.03004.pdf,Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning,Yibo Yang,,0%
https://arxiv.org/pdf/2302.03004.pdf,Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning,Haobo Yuan,,0%
https://arxiv.org/pdf/2302.03004.pdf,Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning,Xiangtai Li,,0%
https://arxiv.org/pdf/2302.03004.pdf,Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning,Zhouchen Lin,,0%
https://arxiv.org/pdf/2302.03004.pdf,Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning,Philip Torr,,0%
https://arxiv.org/pdf/2302.03004.pdf,Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning,Dacheng Tao,,0%
https://arxiv.org/pdf/2302.02991.pdf,Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images,Wenhui Zhu,,0%
https://arxiv.org/pdf/2302.02991.pdf,Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images,Peijie Qiu,,0%
https://arxiv.org/pdf/2302.02991.pdf,Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images,Mohammad Farazi,,0%
https://arxiv.org/pdf/2302.02991.pdf,Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images,Keshav Nandakumar,,0%
https://arxiv.org/pdf/2302.02991.pdf,Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images,Oana M. Dumitrascu,,0%
https://arxiv.org/pdf/2302.02991.pdf,Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images,Yalin Wang,,0%
https://arxiv.org/pdf/2302.02979.pdf,Learning disentangled representations for explainable chest X-ray classification using Dirichlet VAEs,Rachael Harkness,,0%
https://arxiv.org/pdf/2302.02979.pdf,Learning disentangled representations for explainable chest X-ray classification using Dirichlet VAEs,Alejandro F Frangi,,0%
https://arxiv.org/pdf/2302.02979.pdf,Learning disentangled representations for explainable chest X-ray classification using Dirichlet VAEs,Kieran Zucker,,0%
https://arxiv.org/pdf/2302.02979.pdf,Learning disentangled representations for explainable chest X-ray classification using Dirichlet VAEs,Nishant Ravikumar,,0%
https://arxiv.org/pdf/2302.02976.pdf,ConvoWaste: An Automatic Waste Segregation Machine Using Deep Learning,Abdullah Al Juabir,abdullah@aiub.edu,85%
https://arxiv.org/pdf/2302.02976.pdf,ConvoWaste: An Automatic Waste Segregation Machine Using Deep Learning,Shuvra Smaran Das,shuvradas59@gmail.com,95%
https://arxiv.org/pdf/2302.02976.pdf,ConvoWaste: An Automatic Waste Segregation Machine Using Deep Learning,Md. Shahariar Nafiz,shahariarnafiz48@gmail.com,78%
https://arxiv.org/pdf/2302.02976.pdf,ConvoWaste: An Automatic Waste Segregation Machine Using Deep Learning,Dip Nandi,dip.nandi@aiub.edu,95%
https://arxiv.org/pdf/2302.02976.pdf,ConvoWaste: An Automatic Waste Segregation Machine Using Deep Learning,Md. Kishor Morol,kishor@aiub.edu,90%
https://arxiv.org/pdf/2302.02940.pdf,Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study,Catarina Moreira,catarina.pintomoreira@qut.edu.au,95%
https://arxiv.org/pdf/2302.02940.pdf,Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study,Anderson Maciel,anderson.maciel@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2302.02940.pdf,Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study,Sandra Costa Sousa,sandra.costa.sousa@lusiadas.pt,95%
https://arxiv.org/pdf/2302.02940.pdf,Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study,André Luís,andre.t.luis@tecnico.ulisboa.pt,95%
https://arxiv.org/pdf/2302.02940.pdf,Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study,Isabel Blanco Nobre,isabel.blanco.nobre@lusiadas.pt,95%
https://arxiv.org/pdf/2302.02940.pdf,Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study,Chihcheng Hsieh,chihcheng.hsieh@hdr.qut.edu.au,95%
https://arxiv.org/pdf/2302.02940.pdf,Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study,Joaquim Jorge,jorgej@acm.org,82%
https://arxiv.org/pdf/2302.02936.pdf,"Private GANs, Revisited",Guojun Zhang,guojun.zhang@huawei.com,95%
https://arxiv.org/pdf/2302.02936.pdf,"Private GANs, Revisited",Alex Bie,yabie@uwaterloo.ca,78%
https://arxiv.org/pdf/2302.02936.pdf,"Private GANs, Revisited",Gautam Kamath,,0%
https://arxiv.org/pdf/2302.02928.pdf,Generating Evidential BEV Maps in Continuous Driving Space,Yunshuang Yuan,,0%
https://arxiv.org/pdf/2302.02928.pdf,Generating Evidential BEV Maps in Continuous Driving Space,Hao Cheng,,0%
https://arxiv.org/pdf/2302.02928.pdf,Generating Evidential BEV Maps in Continuous Driving Space,Michael Ying Yang,,0%
https://arxiv.org/pdf/2302.02928.pdf,Generating Evidential BEV Maps in Continuous Driving Space,Monika Sester,,0%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Xiubo Geng,xigeng@microsoft.com,82%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Chongyang Tao,chongyang.tao@microsoft.com,95%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Jing Ma,majing@hkbu.edu.hk,95%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Can Xu,caxu@microsoft.com,82%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Qingwen Lin,qlin@microsoft.com,82%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Pu Zhao,pu.zhao@microsoft.com,95%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Ziyang Luo,cszyluo@comp.hkbu.edu.hk,78%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Tao Shen,shentao@microsoft.com,95%
https://arxiv.org/pdf/2302.02908.pdf,LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval,Daxin Jiang,,0%
https://arxiv.org/pdf/2302.02907.pdf,GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks,Jingfeng Zhang,jingfeng.zhang@riken.jp,95%
https://arxiv.org/pdf/2302.02907.pdf,GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks,Salah Ghamizi,,0%
https://arxiv.org/pdf/2302.02907.pdf,GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks,Maxime Cordy,,0%
https://arxiv.org/pdf/2302.02907.pdf,GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks,Mike Papadakis,,0%
https://arxiv.org/pdf/2302.02907.pdf,GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks,Masashi Sugiyama,,0%
https://arxiv.org/pdf/2302.02907.pdf,GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks,Yves Le Traon,,0%
https://arxiv.org/pdf/2302.02887.pdf,UVDoc: Neural Grid-based Document Unwarping,Olga Sorkine-hornung,sorkine@inf.ethz.ch,90%
https://arxiv.org/pdf/2302.02887.pdf,UVDoc: Neural Grid-based Document Unwarping,Floor Verhoeven,floor.verhoeven@inf.ethz.ch,95%
https://arxiv.org/pdf/2302.02887.pdf,UVDoc: Neural Grid-based Document Unwarping,Tanguy Magne,tanguy.magne@inf.ethz.ch,95%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Tommaso Giannantonio,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Anna Alperovich,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Piercosimo Semeraro,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Manfredo Atzori,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Xiaohan Zhang,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Christoph Hauger,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Alexander Freytag,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Siri Luthman,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Roeland Vandebriel,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Murali Jayapala,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Lien Solie,,0%
https://arxiv.org/pdf/2302.02884.pdf,Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging,Steven De Vleeschouwer,,0%
https://arxiv.org/pdf/2302.02871.pdf,Top-Down Beats Bottom-Up in 3D Instance Segmentation,Danila Rukhovich,d.rukhovich@samsung.com,82%
https://arxiv.org/pdf/2302.02871.pdf,Top-Down Beats Bottom-Up in 3D Instance Segmentation,Anna Vorontsova,a.vorontsova@samsung.com,82%
https://arxiv.org/pdf/2302.02871.pdf,Top-Down Beats Bottom-Up in 3D Instance Segmentation,Anton Konushin,a.konushin@samsung.com,82%
https://arxiv.org/pdf/2302.02871.pdf,Top-Down Beats Bottom-Up in 3D Instance Segmentation,Maksim Kolodiazhnyi,m.kolodiazhn@samsung.com,75%
https://arxiv.org/pdf/2302.02858.pdf,TR3D: Towards Real-Time Indoor 3D Object Detection,Danila Rukhovich,,0%
https://arxiv.org/pdf/2302.02858.pdf,TR3D: Towards Real-Time Indoor 3D Object Detection,Anna Vorontsova,,0%
https://arxiv.org/pdf/2302.02858.pdf,TR3D: Towards Real-Time Indoor 3D Object Detection,Anton Konushin,,0%
https://arxiv.org/pdf/2302.02849.pdf,An Unsupervised Framework for Joint MRI Super Resolution and Gibbs Artifact Removal,Xiao Chen,xiao.chen01@uii-ai.com,95%
https://arxiv.org/pdf/2302.02849.pdf,An Unsupervised Framework for Joint MRI Super Resolution and Gibbs Artifact Removal,Eric Z. Chen,zhang.chen@uii-ai.com,78%
https://arxiv.org/pdf/2302.02849.pdf,An Unsupervised Framework for Joint MRI Super Resolution and Gibbs Artifact Removal,Yikang Liu,yikang.liu@uii-ai.com,95%
https://arxiv.org/pdf/2302.02849.pdf,An Unsupervised Framework for Joint MRI Super Resolution and Gibbs Artifact Removal,Terrence Chen,terrence.chen@uii-ai.com,95%
https://arxiv.org/pdf/2302.02849.pdf,An Unsupervised Framework for Joint MRI Super Resolution and Gibbs Artifact Removal,Shanhui Sun,shanhui.sun@uii-ai.com,95%
https://arxiv.org/pdf/2302.02814.pdf,MixFormer: End-to-End Tracking with Iterative Mixed Attention,Gangshan Wu,gswu@nju.edu.cn,82%
https://arxiv.org/pdf/2302.02814.pdf,MixFormer: End-to-End Tracking with Iterative Mixed Attention,Yutao Cui,cuiyutao@smail.nju.edu.cn,95%
https://arxiv.org/pdf/2302.02814.pdf,MixFormer: End-to-End Tracking with Iterative Mixed Attention,Limin Wang,lmwang@nju.edu.cn,82%
https://arxiv.org/pdf/2302.02814.pdf,MixFormer: End-to-End Tracking with Iterative Mixed Attention,Cheng Jiang,,0%
https://arxiv.org/pdf/2302.02790.pdf,Perception Datasets for Anomaly Detection in Autonomous Driving: A Survey,Daniel Bogdoll,,0%
https://arxiv.org/pdf/2302.02790.pdf,Perception Datasets for Anomaly Detection in Autonomous Driving: A Survey,Svenja Uhlemeyer,,0%
https://arxiv.org/pdf/2302.02790.pdf,Perception Datasets for Anomaly Detection in Autonomous Driving: A Survey,Kamil Kowol,,0%
https://arxiv.org/pdf/2302.02790.pdf,Perception Datasets for Anomaly Detection in Autonomous Driving: A Survey,J. Marius Zöllner,,0%
https://arxiv.org/pdf/2302.02755.pdf,Fine-Grained Action Detection with RGB and Pose Information using Two Stream Convolutional Networks,Pierre-etienne Martin,pierre_etienne_martin@eva.mpg.de,95%
https://arxiv.org/pdf/2302.02755.pdf,Fine-Grained Action Detection with RGB and Pose Information using Two Stream Convolutional Networks,Leonard Hacker,,0%
https://arxiv.org/pdf/2302.02755.pdf,Fine-Grained Action Detection with RGB and Pose Information using Two Stream Convolutional Networks,Finn Bartels,,0%
https://arxiv.org/pdf/2302.02752.pdf,Baseline Method for the Sport Task of MediaEval 2022 with 3D CNNs using Attention Mechanisms,Pierre-etienne Martin,pierre_etienne_martin@eva.mpg.de,95%
https://arxiv.org/pdf/2302.02744.pdf,AMD-HookNet for Glacier Front Segmentation,Fei Wu,wufei171@mails.ucas.edu.cn,95%
https://arxiv.org/pdf/2302.02744.pdf,AMD-HookNet for Glacier Front Segmentation,Nora Gourmelon,,0%
https://arxiv.org/pdf/2302.02744.pdf,AMD-HookNet for Glacier Front Segmentation,Thorsten Seehaus,,0%
https://arxiv.org/pdf/2302.02744.pdf,AMD-HookNet for Glacier Front Segmentation,Jianlin Zhang,,0%
https://arxiv.org/pdf/2302.02744.pdf,AMD-HookNet for Glacier Front Segmentation,Matthias Braun,,0%
https://arxiv.org/pdf/2302.02744.pdf,AMD-HookNet for Glacier Front Segmentation,Andreas Maier,,0%
https://arxiv.org/pdf/2302.02744.pdf,AMD-HookNet for Glacier Front Segmentation,Vincent Christlein,,0%
https://arxiv.org/pdf/2302.02693.pdf,PatchDCT: Patch Refinement for High Quality Instance Segmentation,Xue Yang,yangxue-2019-sjtu@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.02693.pdf,PatchDCT: Patch Refinement for High Quality Instance Segmentation,Qinrou Wen,qinrou.wen@zju.edu.cn,95%
https://arxiv.org/pdf/2302.02693.pdf,PatchDCT: Patch Refinement for High Quality Instance Segmentation,Jirui Yang,jirui.yjr@alibaba-inc.com,85%
https://arxiv.org/pdf/2302.02693.pdf,PatchDCT: Patch Refinement for High Quality Instance Segmentation,Kewei Liang,,0%
https://arxiv.org/pdf/2302.02688.pdf,HyperSLICE: HyperBand optimized Spiral for Low-latency Interactive Cardiac Examination,Olivier Jaubert,o.jaubert@ucl.ac.uk,82%
https://arxiv.org/pdf/2302.02688.pdf,HyperSLICE: HyperBand optimized Spiral for Low-latency Interactive Cardiac Examination,Javier Montalt-tordera,,0%
https://arxiv.org/pdf/2302.02688.pdf,HyperSLICE: HyperBand optimized Spiral for Low-latency Interactive Cardiac Examination,Daniel Knight,,0%
https://arxiv.org/pdf/2302.02688.pdf,HyperSLICE: HyperBand optimized Spiral for Low-latency Interactive Cardiac Examination,Pr. Simon Arridge,,0%
https://arxiv.org/pdf/2302.02688.pdf,HyperSLICE: HyperBand optimized Spiral for Low-latency Interactive Cardiac Examination,Jennifer Steeden,,0%
https://arxiv.org/pdf/2302.02688.pdf,HyperSLICE: HyperBand optimized Spiral for Low-latency Interactive Cardiac Examination,Pr. Vivek Muthurangu,,0%
https://arxiv.org/pdf/2302.02651.pdf,1st Place Solution for PSG competition with ECCV'22 SenseHuman Workshop,Qixun Wang,wangqixun@xiaohongshu.com,95%
https://arxiv.org/pdf/2302.02651.pdf,1st Place Solution for PSG competition with ECCV'22 SenseHuman Workshop,Haofan Wang,wanghaofan@xiaohongshu.com,95%
https://arxiv.org/pdf/2302.02651.pdf,1st Place Solution for PSG competition with ECCV'22 SenseHuman Workshop,Xiaofeng Guo,xiaofengguo2010@gmail.com,95%
https://arxiv.org/pdf/2302.02641.pdf,Approximation of radiative transfer for surface spectral features,Frédéric Schmidt,,0%
https://arxiv.org/pdf/2302.02622.pdf,Uncertainty Calibration and its Application to Object Detection,Fabian Küppers,,0%
https://arxiv.org/pdf/2302.02619.pdf,COVID-19 Infection Analysis Framework using Novel Boosted CNNs and Radiological Images,Saddam Hussain Khan,saddamhkhan@ueas.edu.pk,95%
https://arxiv.org/pdf/2302.02615.pdf,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,Jiaya Jia,leojia@cse.cuhk.edu.hk,78%
https://arxiv.org/pdf/2302.02615.pdf,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,Jingyao Li,jingyao.li@link.cuhk.edu.hk,95%
https://arxiv.org/pdf/2302.02615.pdf,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,Pengguang Chen,,0%
https://arxiv.org/pdf/2302.02615.pdf,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,Shaozuo Yu,,0%
https://arxiv.org/pdf/2302.02615.pdf,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,Zexin He,,0%
https://arxiv.org/pdf/2302.02615.pdf,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,Shu Liu,,0%
https://arxiv.org/pdf/2302.02598.pdf,Cluster-aware Contrastive Learning for Unsupervised Out-of-distribution Detection,Xingtai Gui,tabgui@std.uestc.edu.cn,78%
https://arxiv.org/pdf/2302.02598.pdf,Cluster-aware Contrastive Learning for Unsupervised Out-of-distribution Detection,Menglong Chen,menglongchen@std.uestc.edu.cn,95%
https://arxiv.org/pdf/2302.02598.pdf,Cluster-aware Contrastive Learning for Unsupervised Out-of-distribution Detection,Shicai Fan,shicaifan@uestc.edu.cn,95%
https://arxiv.org/pdf/2302.02553.pdf,A Correction-Based Dynamic Enhancement Framework towards Underwater Detection,Weiling Chen,weiling.chen@fzu.edu.cn,95%
https://arxiv.org/pdf/2302.02553.pdf,A Correction-Based Dynamic Enhancement Framework towards Underwater Detection,Hongan Wei,weihongan@fzu.edu.cn,95%
https://arxiv.org/pdf/2302.02553.pdf,A Correction-Based Dynamic Enhancement Framework towards Underwater Detection,Yanling Qiu,,0%
https://arxiv.org/pdf/2302.02553.pdf,A Correction-Based Dynamic Enhancement Framework towards Underwater Detection,Qianxue Feng,,0%
https://arxiv.org/pdf/2302.02553.pdf,A Correction-Based Dynamic Enhancement Framework towards Underwater Detection,Boqin Cai,,0%
https://arxiv.org/pdf/2302.02551.pdf,CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets,Zachary Novack,znovack@ucsd.edu,82%
https://arxiv.org/pdf/2302.02551.pdf,CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets,Saurabh Garg,sgarg2@andrew.cmu.edu,82%
https://arxiv.org/pdf/2302.02551.pdf,CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets,Julian Mcauley,,0%
https://arxiv.org/pdf/2302.02551.pdf,CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets,Zachary C. Lipton,,0%
https://arxiv.org/pdf/2302.02550.pdf,Domain Re-Modulation for Few-Shot Generative Domain Adaptation,Yi Wu,,0%
https://arxiv.org/pdf/2302.02550.pdf,Domain Re-Modulation for Few-Shot Generative Domain Adaptation,Ziqiang Li,,0%
https://arxiv.org/pdf/2302.02550.pdf,Domain Re-Modulation for Few-Shot Generative Domain Adaptation,Chaoyue Wang,,0%
https://arxiv.org/pdf/2302.02550.pdf,Domain Re-Modulation for Few-Shot Generative Domain Adaptation,Heliang Zheng,,0%
https://arxiv.org/pdf/2302.02550.pdf,Domain Re-Modulation for Few-Shot Generative Domain Adaptation,Shanshan Zhao,,0%
https://arxiv.org/pdf/2302.02550.pdf,Domain Re-Modulation for Few-Shot Generative Domain Adaptation,Bin Li,,0%
https://arxiv.org/pdf/2302.02550.pdf,Domain Re-Modulation for Few-Shot Generative Domain Adaptation,Dacheng Tao,,0%
https://arxiv.org/pdf/2302.02535.pdf,PaRot: Patch-Wise Rotation-Invariant Network via Feature Disentanglement and Pose Restoration,Jianhui Yu,jianhui.yu@sydney.edu.au,95%
https://arxiv.org/pdf/2302.02535.pdf,PaRot: Patch-Wise Rotation-Invariant Network via Feature Disentanglement and Pose Restoration,Weidong Cai,tom.cai@sydney.edu.au,78%
https://arxiv.org/pdf/2302.02535.pdf,PaRot: Patch-Wise Rotation-Invariant Network via Feature Disentanglement and Pose Restoration,Chaoyi Zhang,chaoyi.zhang@sydney.edu.au,95%
https://arxiv.org/pdf/2302.02535.pdf,PaRot: Patch-Wise Rotation-Invariant Network via Feature Disentanglement and Pose Restoration,Dingxin Zhang,,0%
https://arxiv.org/pdf/2302.02524.pdf,Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity,Sajid Rahim,,0%
https://arxiv.org/pdf/2302.02524.pdf,Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity,Kourosh Sabri,,0%
https://arxiv.org/pdf/2302.02524.pdf,Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity,Anna Ells,,0%
https://arxiv.org/pdf/2302.02524.pdf,Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity,Alan Wassyng,,0%
https://arxiv.org/pdf/2302.02524.pdf,Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity,Mark Lawford,,0%
https://arxiv.org/pdf/2302.02524.pdf,Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity,Linyang Chu,,0%
https://arxiv.org/pdf/2302.02524.pdf,Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity,Wenbo He,,0%
https://arxiv.org/pdf/2302.02521.pdf,Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation,Tian Lan,tlan@gwu.edu,82%
https://arxiv.org/pdf/2302.02521.pdf,Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation,Guru Venkataramani,guruv@gwu.edu,85%
https://arxiv.org/pdf/2302.02521.pdf,Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation,Yongsheng Mei,ysmei@gwu.edu,82%
https://arxiv.org/pdf/2302.02519.pdf,RDFNet: Regional Dynamic FISTA-Net for Spectral Snapshot Compressive Imaging,Jianan Li,lijianan@bit.edu.cn,95%
https://arxiv.org/pdf/2302.02519.pdf,RDFNet: Regional Dynamic FISTA-Net for Spectral Snapshot Compressive Imaging,Shaocong Dong,shaocong@bit.edu.cn,85%
https://arxiv.org/pdf/2302.02519.pdf,RDFNet: Regional Dynamic FISTA-Net for Spectral Snapshot Compressive Imaging,Shiyun Zhou,zhoushiyun@bit.edu.cn,95%
https://arxiv.org/pdf/2302.02519.pdf,RDFNet: Regional Dynamic FISTA-Net for Spectral Snapshot Compressive Imaging,Tingfa Xu,,0%
https://arxiv.org/pdf/2302.02515.pdf,Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey,Germain Forestier,germain.forestier@uha.fr,95%
https://arxiv.org/pdf/2302.02515.pdf,Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey,Chang Wei Tan,chang.tan@monash.edu,95%
https://arxiv.org/pdf/2302.02515.pdf,Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey,Navid Mohammadi Foumani,navid.foumani@monash.edu.com,95%
https://arxiv.org/pdf/2302.02515.pdf,Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey,Geoffrey I. Webb,geoff.webb@monash.edu,82%
https://arxiv.org/pdf/2302.02515.pdf,Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey,Mahsa Salehi,mahsa.salehi@monash.edu,95%
https://arxiv.org/pdf/2302.02515.pdf,Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey,Lynn Miller,lynn.miller1@monash.edu,95%
https://arxiv.org/pdf/2302.02504.pdf,Motion-compensated MR CINE reconstruction with reconstruction-driven motion estimation,Kerstin Hammernik,thomas.kuestner@med.uni,85%
https://arxiv.org/pdf/2302.02504.pdf,Motion-compensated MR CINE reconstruction with reconstruction-driven motion estimation,Jiazhen Pan,jiazhen.pan@tum.de,95%
https://arxiv.org/pdf/2302.02504.pdf,Motion-compensated MR CINE reconstruction with reconstruction-driven motion estimation,Daniel Rueckert,daniel.rueckert@tum.de,95%
https://arxiv.org/pdf/2302.02504.pdf,Motion-compensated MR CINE reconstruction with reconstruction-driven motion estimation,Wenqi Huang,wenqi.huang@tum.de,95%
https://arxiv.org/pdf/2302.02504.pdf,Motion-compensated MR CINE reconstruction with reconstruction-driven motion estimation,Thomas Küstner,,0%
https://arxiv.org/pdf/2302.02503.pdf,Leaving Reality to Imagination: Robust Classification via Generated Datasets,Aditya Grover,adityag@cs.ucla.edu,85%
https://arxiv.org/pdf/2302.02503.pdf,Leaving Reality to Imagination: Robust Classification via Generated Datasets,Hritik Bansal,hbansal@ucla.edu,82%
https://arxiv.org/pdf/2302.02502.pdf,Rethinking Robust Contrastive Learning from the Adversarial Perspective,Mehdi Yaghouti,yaghouti@mailbox.sc.edu,78%
https://arxiv.org/pdf/2302.02502.pdf,Rethinking Robust Contrastive Learning from the Adversarial Perspective,Pooyan Jamshidi,pjamshid@cse.sc.edu,90%
https://arxiv.org/pdf/2302.02502.pdf,Rethinking Robust Contrastive Learning from the Adversarial Perspective,Fatemeh Ghofrani,ghofrani@email.sc.edu,78%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Marcos Faundez-zanuy,faundez@tecnocampus.cat,90%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Alessandro Vinciarelli,alessandro.vinciarelli@glasgow.ac.uk,95%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Fiammetta Marulli,fiammetta.marulli@unicampania.it,95%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Anna Esposito,anna.esposito@unicampania.it,95%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Gennaro Raimo,,0%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Michele Buonanno,,0%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Massimiliano Conson,,0%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Gennaro Cordasco,,0%
https://arxiv.org/pdf/2302.02499.pdf,Handwriting and Drawing for Depression Detection: A Preliminary Study,Stefano Marrone,,0%
https://arxiv.org/pdf/2302.02483.pdf,Multi-Task Self-Supervised Learning for Image Segmentation Task,Chinmaya Khamesra,ckhamesra@wpi.edu,82%
https://arxiv.org/pdf/2302.02483.pdf,Multi-Task Self-Supervised Learning for Image Segmentation Task,Lichun Gao,lgao2@wpi.edu,82%
https://arxiv.org/pdf/2302.02483.pdf,Multi-Task Self-Supervised Learning for Image Segmentation Task,Ashay Aglawe,alaglawe@wpi.edu,82%
https://arxiv.org/pdf/2302.02483.pdf,Multi-Task Self-Supervised Learning for Image Segmentation Task,Uday Kumbhar,ukumbhar@wpi.edu,82%
https://arxiv.org/pdf/2302.02456.pdf,Deep Learning Approach for Early Stage Lung Cancer Detection,Saleh Abunajm,,0%
https://arxiv.org/pdf/2302.02456.pdf,Deep Learning Approach for Early Stage Lung Cancer Detection,Nelly Elsayed,,0%
https://arxiv.org/pdf/2302.02456.pdf,Deep Learning Approach for Early Stage Lung Cancer Detection,Zag Elsayed,,0%
https://arxiv.org/pdf/2302.02456.pdf,Deep Learning Approach for Early Stage Lung Cancer Detection,Murat Ozer,,0%
https://arxiv.org/pdf/2302.02451.pdf,KDEformer: Accelerating Transformers via Kernel Density Estimation,Amir Zandieh,,0%
https://arxiv.org/pdf/2302.02451.pdf,KDEformer: Accelerating Transformers via Kernel Density Estimation,Insu Han,,0%
https://arxiv.org/pdf/2302.02451.pdf,KDEformer: Accelerating Transformers via Kernel Density Estimation,Majid Daliri,,0%
https://arxiv.org/pdf/2302.02451.pdf,KDEformer: Accelerating Transformers via Kernel Density Estimation,Amin Karbasi,,0%
https://arxiv.org/pdf/2302.02444.pdf,Spatio-Temporal Point Process for Multiple Object Tracking,Qian Xu,xu.qian5@zte.com.cn,95%
https://arxiv.org/pdf/2302.02444.pdf,Spatio-Temporal Point Process for Multiple Object Tracking,Xia Jia,jia.xia@zte.com.cn,95%
https://arxiv.org/pdf/2302.02444.pdf,Spatio-Temporal Point Process for Multiple Object Tracking,Weiyao Lin,wylin@sjtu.edu.cn,82%
https://arxiv.org/pdf/2302.02444.pdf,Spatio-Temporal Point Process for Multiple Object Tracking,Kean Chen,ckadashuaige@sjtu.edu.cn,75%
https://arxiv.org/pdf/2302.02444.pdf,Spatio-Temporal Point Process for Multiple Object Tracking,Zenghui Zhang,zenghui.zhang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.02444.pdf,Spatio-Temporal Point Process for Multiple Object Tracking,Tao Wang,wang tao1111@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.02444.pdf,Spatio-Temporal Point Process for Multiple Object Tracking,John See,johnsee@mmu.edu.my,95%
https://arxiv.org/pdf/2302.02412.pdf,Mixture of Diffusers for scene composition and high resolution image generation,Álvaro Barbero Jiménez,alvaro.barbero@iic.uam.es,85%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Chao Wen,wenchao.w@bytedance.com,95%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Jianxin Liao,liaojx@bupt.edu.cn,78%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Zhou Xue,xuezhou08@gmail.com,95%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Haifeng Sun,hfsun@bupt.edu.cn,82%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Qi Qi,qiqi8266@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Xiaozheng Zheng,zhengxiaozheng@bytedance.com,95%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Jingyu Wang,wangjingyu@bupt.edu.cn,95%
https://arxiv.org/pdf/2302.02410.pdf,Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image,Pengfei Ren,,0%
https://arxiv.org/pdf/2302.02408.pdf,Multi-View Masked World Models for Visual Robotic Manipulation,Younggyo Seo,younggyo.seo@kaist.ac.kr,95%
https://arxiv.org/pdf/2302.02408.pdf,Multi-View Masked World Models for Visual Robotic Manipulation,Junsu Kim,,0%
https://arxiv.org/pdf/2302.02408.pdf,Multi-View Masked World Models for Visual Robotic Manipulation,Stephen James,,0%
https://arxiv.org/pdf/2302.02408.pdf,Multi-View Masked World Models for Visual Robotic Manipulation,Kimin Lee,,0%
https://arxiv.org/pdf/2302.02408.pdf,Multi-View Masked World Models for Visual Robotic Manipulation,Jinwoo Shin,,0%
https://arxiv.org/pdf/2302.02408.pdf,Multi-View Masked World Models for Visual Robotic Manipulation,Pieter Abbeel,,0%
https://arxiv.org/pdf/2302.02398.pdf,Diffusion Model for Generative Image Denoising,Yutong Xie,,0%
https://arxiv.org/pdf/2302.02398.pdf,Diffusion Model for Generative Image Denoising,Minne Yuan,,0%
https://arxiv.org/pdf/2302.02398.pdf,Diffusion Model for Generative Image Denoising,Bin Dong,,0%
https://arxiv.org/pdf/2302.02398.pdf,Diffusion Model for Generative Image Denoising,Quanzheng Li,,0%
https://arxiv.org/pdf/2302.02394.pdf,Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion,Chaoyue Wang,chaoyue.wang@outlook.com,95%
https://arxiv.org/pdf/2302.02394.pdf,Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion,Zuopeng Yang,yzpeng@sjtu.edu.cn,65%
https://arxiv.org/pdf/2302.02394.pdf,Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion,Xin Lin,linxj68@gmail.com,78%
https://arxiv.org/pdf/2302.02394.pdf,Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion,Jie Yang,jieyang@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.02394.pdf,Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion,Tianshu Chu,chutianshu@sjtu.edu.cn,95%
https://arxiv.org/pdf/2302.02394.pdf,Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion,Erdun Gao,erdun.gao@student.unimelb.edu.au,95%
https://arxiv.org/pdf/2302.02394.pdf,Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion,Daqing Liu,,0%
https://arxiv.org/pdf/2302.02373.pdf,ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories,Qi Tian,tian.qi1@huawei.com,95%
https://arxiv.org/pdf/2302.02373.pdf,ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories,Jun Yu,yujun@hdu.edu.cn,95%
https://arxiv.org/pdf/2302.02373.pdf,ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories,Zhou Zhao,zhaozhou@zju.edu.cn,95%
https://arxiv.org/pdf/2302.02373.pdf,ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories,Zijian Zhang,,0%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Zhi Tian,tianzhi02@meituan.com,95%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Xiangxiang Chu,chuxiangxiang@meituan.com,95%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Lin Ma,malin11@meituan.com,95%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Patrick Yin Chiang,pchiang@photonic-tech.com,82%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Bo Zhang,zhangbo97@meituan.com,95%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Xinyu Zhang,zhangxinyu35@meituan.com,95%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Sifan Zhou,sifanjay@gmail.com,85%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Xiaobo Lu,xblu2013@126.com,82%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Zequn Jie,jiezequn@meituan.com,95%
https://arxiv.org/pdf/2302.02367.pdf,FastPillars: A Deployment-friendly Pillar-based 3D Detector,Chengjian Feng,fengchenhgjian@meituan.com,78%
https://arxiv.org/pdf/2302.02353.pdf,Towards Precision in Appearance-based Gaze Estimation in the Wild,Abhishek Mukhopadhyay,abhishekmukh@iisc.ac.in,85%
https://arxiv.org/pdf/2302.02353.pdf,Towards Precision in Appearance-based Gaze Estimation in the Wild,Shambhavi Aggarwal,agg.shambhavi@gmail.com,85%
https://arxiv.org/pdf/2302.02353.pdf,Towards Precision in Appearance-based Gaze Estimation in the Wild,Murthy L. R. D.,lrdmurthy@iisc.ac.in,85%
https://arxiv.org/pdf/2302.02353.pdf,Towards Precision in Appearance-based Gaze Estimation in the Wild,Ketan Anand,ketan@outlook.com,85%
https://arxiv.org/pdf/2302.02353.pdf,Towards Precision in Appearance-based Gaze Estimation in the Wild,Pradipta Biswas,pradipta@iisc.ac.in,85%
https://arxiv.org/pdf/2302.02350.pdf,Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization,Daoan Zhang,,0%
https://arxiv.org/pdf/2302.02350.pdf,Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization,Mingkai Chen,,0%
https://arxiv.org/pdf/2302.02350.pdf,Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization,Chenming Li,,0%
https://arxiv.org/pdf/2302.02350.pdf,Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization,Lingyun Huang,,0%
https://arxiv.org/pdf/2302.02350.pdf,Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization,Jianguo Zhang,,0%
https://arxiv.org/pdf/2302.02347.pdf,Explainable Machine Learning: The Importance of a System-Centric Perspective,Manish Narwaria,narwaria@iitj.ac.in,78%
https://arxiv.org/pdf/2302.02336.pdf,Using Intermediate Forward Iterates for Intermediate Generator Optimization,Harsh Mishra,,0%
https://arxiv.org/pdf/2302.02336.pdf,Using Intermediate Forward Iterates for Intermediate Generator Optimization,Jurijs Nazarovs,,0%
https://arxiv.org/pdf/2302.02336.pdf,Using Intermediate Forward Iterates for Intermediate Generator Optimization,Manmohan Dogra,,0%
https://arxiv.org/pdf/2302.02336.pdf,Using Intermediate Forward Iterates for Intermediate Generator Optimization,Sathya N. Ravi,,0%
https://arxiv.org/pdf/2302.02335.pdf,Semi-Supervised Domain Adaptation with Source Label Adaptation,Hsuan-tien Lin,htlin@csie.ntu.edu.tw,82%
https://arxiv.org/pdf/2302.02335.pdf,Semi-Supervised Domain Adaptation with Source Label Adaptation,Yu-chu Yu,,0%
https://arxiv.org/pdf/2303.11169.pdf,Self-supervised Geometric Features Discovery via Interpretable Attentio for Vehicle Re-Identification and Beyond (Complete Version),Xinming Huang,xhuang@wpi.edu,82%
https://arxiv.org/pdf/2303.11169.pdf,Self-supervised Geometric Features Discovery via Interpretable Attentio for Vehicle Re-Identification and Beyond (Complete Version),Ming Li,ming.li@u.nus.edu,95%
https://arxiv.org/pdf/2303.11169.pdf,Self-supervised Geometric Features Discovery via Interpretable Attentio for Vehicle Re-Identification and Beyond (Complete Version),Ziming Zhang,zzhang15@wpi.edu,82%
https://arxiv.org/pdf/2302.02330.pdf,CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning,Jochen Triesch,triesch@fias.uni-frankfurt.de,78%
https://arxiv.org/pdf/2302.02330.pdf,CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning,Xia Xu,xiaxu@fias.uni-frankfurt.de,95%
https://arxiv.org/pdf/2302.02327.pdf,Pyramid Self-attention Polymerization Learning for Semi-supervised Skeleton-based Action Recognition,Binqian Xu,xubinq11@gmail.com,78%
https://arxiv.org/pdf/2302.02327.pdf,Pyramid Self-attention Polymerization Learning for Semi-supervised Skeleton-based Action Recognition,Xiangbo Shu,shuxb@njust.edu.cn,78%
https://arxiv.org/pdf/2302.02318.pdf,Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining,Kaisheng Ma,kaisheng@mail.tsinghua.edu.cn,85%
https://arxiv.org/pdf/2302.02318.pdf,Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining,Li Yi,ericyi@mail.tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.02318.pdf,Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining,Runpei Dong,runpei.dong@gmail.com,95%
https://arxiv.org/pdf/2302.02318.pdf,Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining,Zekun Qi,qizekun@gmail.com,95%
https://arxiv.org/pdf/2302.02318.pdf,Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining,Guofan Fan,,0%
https://arxiv.org/pdf/2302.02318.pdf,Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining,Zheng Ge,,0%
https://arxiv.org/pdf/2302.02318.pdf,Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining,Xiangyu Zhang,,0%
https://arxiv.org/pdf/2302.02316.pdf,Spatiotemporal Decouple-and-Squeeze Contrastive Learning for Semi-Supervised Skeleton-based Action Recognition,Binqian Xu,xubinq11@gmail.com,78%
https://arxiv.org/pdf/2302.02316.pdf,Spatiotemporal Decouple-and-Squeeze Contrastive Learning for Semi-Supervised Skeleton-based Action Recognition,Xiangbo Shu,shuxb@njust.edu.cn,78%
https://arxiv.org/pdf/2302.02314.pdf,CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification,Lei Shen,mpeshel@nus.edu.sg,65%
https://arxiv.org/pdf/2302.02314.pdf,CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification,Zhaoshan Liu,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Daniel D Kim,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Rajat S Chandra,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Jian Peng,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Jing Wu,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Xue Feng,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Michael Atalay,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Chetan Bettegowda,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Craig Jones,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Haris Sair,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Wei-hua Liao,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Chengzhang Zhu,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Beiji Zou,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Li Yang,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Anahita Fathi Kazerooni,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Ali Nabavizadeh,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Harrison X Bai,,0%
https://arxiv.org/pdf/2302.10185.pdf,"Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization",Zhicheng Jiao,,0%
https://arxiv.org/pdf/2302.02294.pdf,A Disparity Refinement Framework for Learning-based Stereo Matching Methods in Cross-domain Setting for Laparoscopic Images,Zixin Yang,,0%
https://arxiv.org/pdf/2302.02294.pdf,A Disparity Refinement Framework for Learning-based Stereo Matching Methods in Cross-domain Setting for Laparoscopic Images,Richard Simon,,0%
https://arxiv.org/pdf/2302.02294.pdf,A Disparity Refinement Framework for Learning-based Stereo Matching Methods in Cross-domain Setting for Laparoscopic Images,Cristian A. Linte,,0%
https://arxiv.org/pdf/2302.02289.pdf,Selecting the Best Optimizers for Deep Learning based Medical Image Segmentation,Ulas Bagci,ulasbagci@gmail.com,95%
https://arxiv.org/pdf/2302.02289.pdf,Selecting the Best Optimizers for Deep Learning based Medical Image Segmentation,Aliasghar Mortazi,,0%
https://arxiv.org/pdf/2302.02289.pdf,Selecting the Best Optimizers for Deep Learning based Medical Image Segmentation,Vedat Cicek,,0%
https://arxiv.org/pdf/2302.02289.pdf,Selecting the Best Optimizers for Deep Learning based Medical Image Segmentation,Elif Keles,,0%
https://arxiv.org/pdf/2302.02285.pdf,ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval,Kexun Zhang,kexun@ucsb.edu,85%
https://arxiv.org/pdf/2302.02285.pdf,ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval,Xianjun Yang,,0%
https://arxiv.org/pdf/2302.02285.pdf,ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval,William Yang Wang,,0%
https://arxiv.org/pdf/2302.02285.pdf,ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval,Lei Li,,0%
https://arxiv.org/pdf/2302.02284.pdf,Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation,Qian He,heqian@bytedance.com,95%
https://arxiv.org/pdf/2302.02284.pdf,Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation,Wei Liu,liuwei.jikun@bytedance.com,95%
https://arxiv.org/pdf/2302.02284.pdf,Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation,Shiqi Sun,sunshiqi.2333@bytedance.com,95%
https://arxiv.org/pdf/2302.02284.pdf,Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation,Shancheng Fang,fangshancheng.lh@bytedance.com,95%
https://arxiv.org/pdf/2302.02283.pdf,Recurrence With Correlation Network for Medical Image Registration,Vignesh Sivan,,0%
https://arxiv.org/pdf/2302.02283.pdf,Recurrence With Correlation Network for Medical Image Registration,Teodora Vujovic,,0%
https://arxiv.org/pdf/2302.02283.pdf,Recurrence With Correlation Network for Medical Image Registration,Raj Ranabhat,,0%
https://arxiv.org/pdf/2302.02283.pdf,Recurrence With Correlation Network for Medical Image Registration,Alexander Wong,,0%
https://arxiv.org/pdf/2302.02283.pdf,Recurrence With Correlation Network for Medical Image Registration,Stewart Mclachlin,,0%
https://arxiv.org/pdf/2302.02283.pdf,Recurrence With Correlation Network for Medical Image Registration,Michael Hardisty,,0%
https://arxiv.org/pdf/2302.02276.pdf,JPEG Steganalysis Based on Steganographic Feature Enhancement and Graph Attention Learning,Hanzhou Wu,h.wu.phd@ieee.org,82%
https://arxiv.org/pdf/2302.02276.pdf,JPEG Steganalysis Based on Steganographic Feature Enhancement and Graph Attention Learning,Qiyun Liu,,0%
https://arxiv.org/pdf/2302.02276.pdf,JPEG Steganalysis Based on Steganographic Feature Enhancement and Graph Attention Learning,Zhiguang Yang,,0%
https://arxiv.org/pdf/2302.02272.pdf,Divide and Compose with Score Based Generative Models,Jennifer Dy,jdy@ece.neu.edu,82%
https://arxiv.org/pdf/2302.02272.pdf,Divide and Compose with Score Based Generative Models,Davin Hill,dhill@ece.neu.edu,82%
https://arxiv.org/pdf/2302.02272.pdf,Divide and Compose with Score Based Generative Models,Octavia Camps,camps@coe.neu.edu,78%
https://arxiv.org/pdf/2302.02272.pdf,Divide and Compose with Score Based Generative Models,Sandesh Ghimire,drsandeshghimire@gmail.com,95%
https://arxiv.org/pdf/2302.02272.pdf,Divide and Compose with Score Based Generative Models,Armand Comas,comasmassague.a@northeastern.edu,78%
https://arxiv.org/pdf/2302.02272.pdf,Divide and Compose with Score Based Generative Models,Aria Masoomi,masoomi.a@northeastern.edu,78%
https://arxiv.org/pdf/2302.02259.pdf,CLiNet: Joint Detection of Road Network Centerlines in 2D and 3D,David Paz,,0%
https://arxiv.org/pdf/2302.02259.pdf,CLiNet: Joint Detection of Road Network Centerlines in 2D and 3D,Srinidhi Kalgundi Srinivas,,0%
https://arxiv.org/pdf/2302.02259.pdf,CLiNet: Joint Detection of Road Network Centerlines in 2D and 3D,Yunchao Yao,,0%
https://arxiv.org/pdf/2302.02259.pdf,CLiNet: Joint Detection of Road Network Centerlines in 2D and 3D,Henrik I. Christensen,,0%
https://arxiv.org/pdf/2302.02255.pdf,Human-Imperceptible Identification with Learnable Lensless Imaging,Thuong Nguyen Canh,,0%
https://arxiv.org/pdf/2302.02255.pdf,Human-Imperceptible Identification with Learnable Lensless Imaging,Trung Thanh Ngo,,0%
https://arxiv.org/pdf/2302.02255.pdf,Human-Imperceptible Identification with Learnable Lensless Imaging,Hajime Nagahara,,0%
https://arxiv.org/pdf/2302.10271.pdf,Thermal Analysis of Malignant Brain Tumors by Employing a Morphological Differentiation-Based Method in Conjunction with Artificial Neural Network,Afsaneh Mojra,mojra@kntu.ac.ir,78%
https://arxiv.org/pdf/2302.10271.pdf,Thermal Analysis of Malignant Brain Tumors by Employing a Morphological Differentiation-Based Method in Conjunction with Artificial Neural Network,Hamed Hani,hani.4@osu.edu,82%
https://arxiv.org/pdf/2302.02249.pdf,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,Vishwa Vinay,vinay@adobe.com,82%
https://arxiv.org/pdf/2302.02249.pdf,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,Paridhi Maheshwari,paridhi@stanford.edu,85%
https://arxiv.org/pdf/2302.02249.pdf,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,Praneetha Vaddamanu,pvaddama@cs.cmu.edu,90%
https://arxiv.org/pdf/2302.02249.pdf,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,Nihal Jain,nihalj@cs.cmu.edu,85%
https://arxiv.org/pdf/2302.02249.pdf,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,Kuldeep Kulkarni,,0%
https://arxiv.org/pdf/2302.02234.pdf,Revisiting Image Deblurring with an Efficient ConvNet,Lingyan Ruan,,0%
https://arxiv.org/pdf/2302.02234.pdf,Revisiting Image Deblurring with an Efficient ConvNet,Mojtaba Bemana,,0%
https://arxiv.org/pdf/2302.02234.pdf,Revisiting Image Deblurring with an Efficient ConvNet,Hans-peter Seidel,,0%
https://arxiv.org/pdf/2302.02234.pdf,Revisiting Image Deblurring with an Efficient ConvNet,Karol Myszkowski,,0%
https://arxiv.org/pdf/2302.02234.pdf,Revisiting Image Deblurring with an Efficient ConvNet,Bin Chen,,0%
https://arxiv.org/pdf/2302.02216.pdf,A Minimax Approach Against Multi-Armed Adversarial Attacks Detection,Federica Granese,federica.granese@inria.fr,95%
https://arxiv.org/pdf/2302.02216.pdf,A Minimax Approach Against Multi-Armed Adversarial Attacks Detection,Pablo Piantanida,pablo.piantanida@centralesupelec.fr,95%
https://arxiv.org/pdf/2302.02216.pdf,A Minimax Approach Against Multi-Armed Adversarial Attacks Detection,Marco Romanelli,,0%
https://arxiv.org/pdf/2302.02216.pdf,A Minimax Approach Against Multi-Armed Adversarial Attacks Detection,Siddharth Garg,,0%
https://arxiv.org/pdf/2302.02214.pdf,Variational multichannel multiclass segmentation using unsupervised lifting with CNNs,Nadja Gruber,,0%
https://arxiv.org/pdf/2302.02214.pdf,Variational multichannel multiclass segmentation using unsupervised lifting with CNNs,Johannes Schwab,,0%
https://arxiv.org/pdf/2302.02214.pdf,Variational multichannel multiclass segmentation using unsupervised lifting with CNNs,Sebastien Court,,0%
https://arxiv.org/pdf/2302.02214.pdf,Variational multichannel multiclass segmentation using unsupervised lifting with CNNs,Elke Gizewski,,0%
https://arxiv.org/pdf/2302.02214.pdf,Variational multichannel multiclass segmentation using unsupervised lifting with CNNs,Markus Haltmeier,,0%
https://arxiv.org/pdf/2302.02213.pdf,CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks,Shashank Agnihotri,shashank.agnihotri@uni-mannheim.de,95%
https://arxiv.org/pdf/2302.02213.pdf,CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks,Steffen Jung,,0%
https://arxiv.org/pdf/2302.02213.pdf,CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks,Margret Keuper,,0%
https://arxiv.org/pdf/2302.02210.pdf,Oscillation-free Quantization for Low-bit Vision Transformers,Shih-yang Liu,sliuau@connect.ust.hk,82%
https://arxiv.org/pdf/2302.02210.pdf,Oscillation-free Quantization for Low-bit Vision Transformers,Zechun Liu,,0%
https://arxiv.org/pdf/2302.02210.pdf,Oscillation-free Quantization for Low-bit Vision Transformers,Kwang-ting Cheng,,0%
https://arxiv.org/pdf/2302.02194.pdf,Laplacian ICP for Progressive Registration of 3D Human Head Meshes,Nick Pears,,0%
https://arxiv.org/pdf/2302.02194.pdf,Laplacian ICP for Progressive Registration of 3D Human Head Meshes,Hang Dai,,0%
https://arxiv.org/pdf/2302.02194.pdf,Laplacian ICP for Progressive Registration of 3D Human Head Meshes,Will Smith,,0%
https://arxiv.org/pdf/2302.02194.pdf,Laplacian ICP for Progressive Registration of 3D Human Head Meshes,Hao Sun,,0%
https://arxiv.org/pdf/2302.08913.pdf,Referential communication in heterogeneous communities of pre-trained visual deep networks,Matéo Mahaut,mateo.mahaut@upf.edu,95%
https://arxiv.org/pdf/2302.08913.pdf,Referential communication in heterogeneous communities of pre-trained visual deep networks,Roberto Dessì,roberto.dessi@upf.edu,95%
https://arxiv.org/pdf/2302.08913.pdf,Referential communication in heterogeneous communities of pre-trained visual deep networks,Marco Baroni,marco.baroni@upf.edu,95%
https://arxiv.org/pdf/2302.08913.pdf,Referential communication in heterogeneous communities of pre-trained visual deep networks,Francesca Franzon,francesca.franzon@upf.edu,95%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Rongrong Ji,rrji@xmu.edu.cn,82%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Yuxin Zhang,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Mingbao Lin,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Xunchao Li,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Han Liu,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Guozhi Wang,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Fei Chao,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Shuai Ren,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Yafei Wen,,0%
https://arxiv.org/pdf/2302.02184.pdf,Real-Time Image Demoireing on Mobile Devices,Xiaoxin Chen,,0%
https://arxiv.org/pdf/2302.02181.pdf,Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time,Rudolf Herdt,rherdt@uni-bremen.de,82%
https://arxiv.org/pdf/2302.02181.pdf,Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time,Peter Maass,pmaass@uni-bremen.de,82%
https://arxiv.org/pdf/2302.02181.pdf,Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time,Maximilian Schmidt,schmidt4@uni-bremen.de,78%
https://arxiv.org/pdf/2302.02181.pdf,Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time,Daniel Otero Baguer,otero@uni-bremen.de,90%
https://arxiv.org/pdf/2302.02181.pdf,Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time,Jean Le'clerc Arrastia,,0%
https://arxiv.org/pdf/2302.02155.pdf,Guaranteed Tensor Recovery Fused Low-rankness and Smoothness,Hailin Wang,wanghailin97@163.com,95%
https://arxiv.org/pdf/2302.02155.pdf,Guaranteed Tensor Recovery Fused Low-rankness and Smoothness,Deyu Meng,dymeng@mail.xjtu.edu.cn,82%
https://arxiv.org/pdf/2302.02155.pdf,Guaranteed Tensor Recovery Fused Low-rankness and Smoothness,Jiangjun Peng,andrew.pengjj@gmail.com,78%
https://arxiv.org/pdf/2302.02155.pdf,Guaranteed Tensor Recovery Fused Low-rankness and Smoothness,Wenjin Qin,qinwenjin2021@163.com,95%
https://arxiv.org/pdf/2302.02155.pdf,Guaranteed Tensor Recovery Fused Low-rankness and Smoothness,Jianjun Wang,,0%
https://arxiv.org/pdf/2302.02150.pdf,This Intestine Does Not Exist: Multiscale Residual Variational Autoencoder for Realistic Wireless Capsule Endoscopy Image Generation,Dimitris K. Iakovidis,diakovidis@uth.gr,82%
https://arxiv.org/pdf/2302.02150.pdf,This Intestine Does Not Exist: Multiscale Residual Variational Autoencoder for Realistic Wireless Capsule Endoscopy Image Generation,Panagiota Gatoula,pgatoula@uth.gr,82%
https://arxiv.org/pdf/2302.02150.pdf,This Intestine Does Not Exist: Multiscale Residual Variational Autoencoder for Realistic Wireless Capsule Endoscopy Image Generation,Dimitrios E. Diamantis,didiamantis@uth.gr,82%
https://arxiv.org/pdf/2302.02150.pdf,This Intestine Does Not Exist: Multiscale Residual Variational Autoencoder for Realistic Wireless Capsule Endoscopy Image Generation,Anastasios Koulaouzidis,anastasios.koulaouzidis@rsyd.dk,95%
https://arxiv.org/pdf/2302.02141.pdf,LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers,Feng Xue,,0%
https://arxiv.org/pdf/2302.02141.pdf,LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers,Yu Li,,0%
https://arxiv.org/pdf/2302.02141.pdf,LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers,Deyin Liu,,0%
https://arxiv.org/pdf/2302.02141.pdf,LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers,Yincen Xie,,0%
https://arxiv.org/pdf/2302.02141.pdf,LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers,Lin Wu,,0%
https://arxiv.org/pdf/2302.02141.pdf,LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers,Richang Hong,,0%
https://arxiv.org/pdf/2302.02136.pdf,Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer,Yu Shi,shiyu@cigit.ac.cn,95%
https://arxiv.org/pdf/2302.02136.pdf,Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer,Min Peng,pengmin@cigit.ac.cn,95%
https://arxiv.org/pdf/2302.02136.pdf,Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer,Xiang-dong Zhou,zhouxiangdong@cigit.ac.cn,95%
https://arxiv.org/pdf/2302.02136.pdf,Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer,Chongyang Wang,,0%
https://arxiv.org/pdf/2302.02125.pdf,Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity,Jing Liao,jingliao@cityu.edu.hk,95%
https://arxiv.org/pdf/2302.02125.pdf,Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity,Hao Du,haodu8-c@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.02125.pdf,Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity,Qihua Dong,qihuadong2-c@my.cityu.edu.hk,95%
https://arxiv.org/pdf/2302.02125.pdf,Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity,Yan Xu,xuyan04@gmail.com,95%
https://arxiv.org/pdf/2302.02124.pdf,"Transform, Contrast and Tell: Coherent Entity-Aware Multi-Image Captioning",Jingqiang Chen,,0%
https://arxiv.org/pdf/2302.02117.pdf,Learning to Agree on Vision Attention for Visual Commonsense Reasoning,Yangyang Guo,guoyang.eric@gmail.com,78%
https://arxiv.org/pdf/2302.02117.pdf,Learning to Agree on Vision Attention for Visual Commonsense Reasoning,Fan Liu,liufancs@gmail.com,95%
https://arxiv.org/pdf/2302.02117.pdf,Learning to Agree on Vision Attention for Visual Commonsense Reasoning,Zhenyang Li,zhenyanglidz@gmail.com,95%
https://arxiv.org/pdf/2302.02117.pdf,Learning to Agree on Vision Attention for Visual Commonsense Reasoning,Kejie Wang,kjwang.henry@gmail.com,82%
https://arxiv.org/pdf/2302.02117.pdf,Learning to Agree on Vision Attention for Visual Commonsense Reasoning,Liqiang Nie,nieliqiang@gmail.com,95%
https://arxiv.org/pdf/2302.02117.pdf,Learning to Agree on Vision Attention for Visual Commonsense Reasoning,Mohan Kankanhalli,mohan@comp.nus.edu.sg,85%
https://arxiv.org/pdf/2302.02108.pdf,Knowledge Distillation in Vision Transformers: A Critical Review,Brejesh Lall,Brejesh.lall@iitd.ac.in,95%
https://arxiv.org/pdf/2302.02108.pdf,Knowledge Distillation in Vision Transformers: A Critical Review,Tausifa Jan Saleem,tausifa.cstaff@iitd.ac.in,85%
https://arxiv.org/pdf/2302.02108.pdf,Knowledge Distillation in Vision Transformers: A Critical Review,Gousia Habib,gousiya.cstaff@iitd.ac.in,60%
https://arxiv.org/pdf/2302.03689.pdf,PartitionVAE -- a human-interpretable VAE,Sameer Pai,sampai@mit.edu,82%
https://arxiv.org/pdf/2302.03689.pdf,PartitionVAE -- a human-interpretable VAE,Fareed Sheriff,fareeds@mit.edu,85%
https://arxiv.org/pdf/2302.02091.pdf,Reducing ANN-SNN Conversion Error through Residual Membrane Potential,Zecheng Hao,zechenghao@pku.edu.cn,95%
https://arxiv.org/pdf/2302.02091.pdf,Reducing ANN-SNN Conversion Error through Residual Membrane Potential,Tiejun Huang,tjhuang@pku.edu.cn,82%
https://arxiv.org/pdf/2302.02091.pdf,Reducing ANN-SNN Conversion Error through Residual Membrane Potential,Tong Bu,putong30@pku.edu.cn,85%
https://arxiv.org/pdf/2302.02091.pdf,Reducing ANN-SNN Conversion Error through Residual Membrane Potential,Zhaofei Yu,yuzf12@pku.edu.cn,78%
https://arxiv.org/pdf/2302.02091.pdf,Reducing ANN-SNN Conversion Error through Residual Membrane Potential,Jianhao Ding,,0%
https://arxiv.org/pdf/2302.02089.pdf,MOMA:Distill from Self-Supervised Teachers,Yuchong Yao,,0%
https://arxiv.org/pdf/2302.02089.pdf,MOMA:Distill from Self-Supervised Teachers,Nandakishor Desai,,0%
https://arxiv.org/pdf/2302.02089.pdf,MOMA:Distill from Self-Supervised Teachers,Marimuthu Palaniswami,,0%
https://arxiv.org/pdf/2302.02088.pdf,AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis,Susan Liang,,0%
https://arxiv.org/pdf/2302.02088.pdf,AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis,Chao Huang,,0%
https://arxiv.org/pdf/2302.02088.pdf,AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis,Yapeng Tian,,0%
https://arxiv.org/pdf/2302.02088.pdf,AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis,Anurag Kumar,,0%
https://arxiv.org/pdf/2302.02088.pdf,AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis,Chenliang Xu,,0%
https://arxiv.org/pdf/2302.02075.pdf,X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification,Tao He,kevin.92.he@gmail.com,78%
https://arxiv.org/pdf/2302.02075.pdf,X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification,Guiguang Ding,dinggg@tsinghua.edu.cn,78%
https://arxiv.org/pdf/2302.02075.pdf,X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification,Leqi Shen,lunarshen@gmail.com,82%
https://arxiv.org/pdf/2302.02075.pdf,X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification,Yuchen Guo,yuchen.w.guo@gmail.com,95%
